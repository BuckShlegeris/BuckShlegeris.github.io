---
layout: post
title:  "Rationalists and explicit models"
date:   "2018-10-23"
---

I think the most prominent feature of rationalist epistemics is how much we focus on building explicit concepts and models of the world. We love things like Slate Star Codex articles of this form—for example, Toxoplasma, I Can Tolerate Anything Except The Outgroup. Rationalists use a lot of jargon; this often arises from coming up with new concepts that we like so much that we all want to describe the world with them. CFAR often explicitly encourages people to build explicit models of things that they normally wouldn’t build models of—for example, they claim that you should be interested in the precise mechanisms by which you’re failing to keep your habits, and that it’s better to have bad models of how your mind works than to not try to build models at all.

You could contrast this with a more empirical perspective, which places less emphasis on the mechanisms by which people engage in particular behavior, and more emphasis on looking at collecting and analyzing particular facts about the world. (I am probably making a terrible mistake by reusing the words rationalist and empiricist for this kind of disagreement, given that these words have been used to describe opposing views before, but I like them so I’m sticking with it.)

Other ways of describing this: Rationalists are more like [hedgehogs than foxes](https://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox). Nate Silver is more empiricist; political commentators who talk a lot about how particular messages are turning off particular demographic groups are more like rationalists along this dimension.

I have a lot of respect for the rationalist perspective on this. I feel in my heart that I’ve been made better off by trying to build explicit internal models of things and people. The rationalist perspective leads to things like preferring internal family systems therapy to cognitive behavioral therapy, which I would bet is empirically more effective for rationalists. And I think that trying to build large explicit models of things is helpful for being convinced of things like the importance of cause prioritization.

But I think that we often take our interest in explicit models too far, in ways that harm our ability to accurately predict the world.

Here are three major types of mistakes that I think rationalists make along these lines:

## Overenthusiasm for people who are good at articulating models

(I need to flesh out this section a bunch)

I think we are too open to being convinced of things by good stories, and too uninterested in checking how these claims cash out in real world evidence.

This is kind of an embarrassing mistake in my opinion.

Examples:

- Brent
- Vassar
- Leverage
- People who are good at circling, focusing

Sometimes this isn’t an issue. For example, I think Slate Star Codex normally does good thinking and deserves its good reputation.





## Treating ability to articulate a mental process as evidence that it's a good process

Earlier this year, I was trying to figure out whether Alice was a good fit for a particular role, and so I asked my friend Bob, who knew them, to tell me about what he thought of them. He methodically told me his guess about Alice’s Big Five personality, their Myers-Briggs type, their MtG color, their IQ, and so on.

I was super impressed. I found it very helpful to hear their opinion broken down like that, and I wished that I was skillful enough at judging people to think about them so methodically.

A few months later, I learned that Bob had catastrophically failed to judge some other person’s character. I was shocked—I had Bob stored on my list of super competent judges of character, and I was really confused by what had happened there.

I think the mistake I made was treating Bob’s ability to articulate his models as strong evidence of his judgement. In hindsight, this is obvious.

I now try to remember that it makes me happy when I hear good articulations of things, and that this isn’t correlated with the articulations being true, and so I shouldn’t believe someone much more when they say things clearly.

## Being too slow to update on evidence when it conflicts with models.

A while ago, I had the following conversation with Claire:

Claire: I think Carol has terrible judgement about people. For example, lots of people thought Dave was creepy, but Carol thought he was fine, and then he got arrested for some kind of sex crimes a few months later. And she thought Eve would be a good salesman, even though she’s obviously socially incompetent. And she thought Fred was really smart, even though in conversation he clearly just spews bullshit sprinkled with keywords that make rationalists think he’s smart.

Buck: Hmm. I agree with the Dave case and the Eve case. I agree that Carol is bad at judging character. But the Fred case is confusing. Carol is obviously really smart and I don’t understand how she could fail to notice people being as dumb as Fred; that doesn’t fit well with my model of her.

Claire: I am scared that you’re going to make a mistake here. I think that for the moment, your belief about Carol should be “I have the following model of Carol, and also I know that Carol sucks at judging intelligence sometimes”. I’m afraid that you’re too focused on having coherent models, and this makes you unwilling to remember or use evidence that doesn’t nicely fit into them.

I think Claire was right. I was too unwilling to incorporate evidence into my beliefs where it didn’t fit nicely with my models. I think that doing this makes it too hard for us to change our minds. I think that it’s often better to have bad models than no models, but the costs of bad models get really high when we aren’t able to notice that they’re false.

## Recommendations

So what should we do?

TODO: figure this out.

---


Other points to maybe make

- Sometimes people don’t get things that I think are important notes on this topic. For example, I think Ozy missed Scott’s point in their response on Moloch.
- I notice that I feel very uninterested in the question of whether I’m actually more productive due to thinking about internal alignment. I know people who swear by circling, but it’s not obvious whether it’s actually helped them. An unsympathetic observer would ask if we’re just getting duped by artificially produced sensations of insight.
- There’s a distinction between articulation of models/experiences we already have, and building things as consequences of things we already believe.
