---
layout: post
title:  "Would a paperclip-maximizing AI have a positive subjective experience?"
date: "not a real date"
---

[content warning: futurism. epistemic status: pretty confident in my conclusions.]

I think it's pretty plausible that humanity will accidentally wipe itself out by creating a superintelligent AI that optimizes the universe for some goal that is totally morally neutral to me--the canonical example being a paperclip maximizer whose goal is maximizing the number of paperclips in the universe. If you haven't heard of any of this before, [go read the Wait But Why post explaining it](waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html).

One obvious question is whether the paperclip-maximizing AI would overall suffer or feel pleasure, and how much pleasure or suffering it would end up feeling.

## Previous opinions

Brian Tomasik [writes](https://foundational-research.org/risks-of-astronomical-future-suffering/#What_about_paperclippers):

> **What about paperclippers?**
>
> Above I was largely assuming a human-oriented civilization with values that we recognize. But what if, as seems mildly likely, Earth is taken over by a paperclip maximizer, i.e., an unconstrained automation or optimization process? Wouldn't that reduce suffering because it would eliminate wild ETs as the paperclipper spread throughout the galaxy, without causing any additional suffering?
>
> Maybe, but if the paperclip maximizer is actually generally intelligent, then it won't stop at tiling the solar system with paperclips. It will want to do science, perform lab experiments on sentient creatures, possibly run suffering subroutines, and so forth. It will require lots of intelligent and potentially sentient robots to coordinate and maintain its paperclip factories, energy harvesters, and mining operations, as well as scientists and engineers to design them. And the paperclipping scenario would entail similar black swans as a human-inspired AI. Paperclippers would presumably be less intrinsically humane than a "friendly AI," so some might cause significantly more suffering than a friendly AI, though others might cause less, especially the "minimizing" paperclippers, e.g., cancer minimizers or death minimizers.
>
> If the paperclipper is not generally intelligent, I have a hard time seeing how it could cause human extinction. In this case it would be like many other catastrophic risks -- deadly and destabilizing, but not capable of wiping out the human race.

Michael Dickens [writes](http://mdickens.me/2016/04/17/preventing_human_extinction,_now_with_numbers!/#paperclip-condition):

> Paperclip maximizers might experience consciousness or include conscious components. The critical question here is, do we have reason to expect these conscious components to have net negative lives on balance?
>
> There are a couple of reasons to believe that suffering might dominate in paperclip maximizers:
>
> 1. Maximal suffering is more severe than maximal happiness in all the beings we know about.
> 2. Failure conditions tend to be more severe than success conditions, and if happiness/suffering link to success/failure conditions as they do in sentient animals, then we would expect suffering to be more severe than happiness.
> 3. Even considering these, we may find it plausible that happiness could dominate suffering. For example, most humans have net positive lives, even though they’re capable of greater suffering than happiness, simply because they spend more time (mildly) happy than they spend suffering. This might hold true for paperclip maximizers, as well.
>
> But paperclip maximizers probably would look extremely different from animals, so we can’t necessarily expect that they would experience consciousness in the same way (if they’re even conscious at all).
>
> Given how little we know, we can reasonably say that paperclip maximizers in expectation look similar to wild animals in terms of their distribution of happiness and suffering. Perhaps a paperclip maximizer brain experiences between -0.01 and -1 QALYs per watt-year. Then to calculate the expected utility of the paperclip maximizer scenario, we’ll use the same numbers as the previous section but change the number of brains per watt to 10<sup>-1</sup>. Then parameters (median, σ<sup>2</sup>) for the number of suffering life years in the paperclip condition are (6x10<sup>44</sup>, 5.0).

## My opinion

I think that paperclip maximizers would probably not create a universe with much conscious experience relative to their energy consumption, and I think that their conscious experience has an expected value near zero.

Brian writes:

> If the paperclip maximizer is actually generally intelligent, then it won't stop at tiling the solar system with paperclips. It will want to do science, perform lab experiments on sentient creatures, possibly run suffering subroutines, and so forth. It will require lots of intelligent and potentially sentient robots to coordinate and maintain its paperclip factories, energy harvesters, and mining operations, as well as scientists and engineers to design them.

Here are all my disagreements with that.

Brian says that the paperclipper will cause suffering by doing science involving the suffering of experimental subjects or subroutines. But I think that only a small proportion of the energy consumption of a paperclipper will go into doing science on conscious experimental subjects. There's only so much experimentation on conscious subjects that you need to do, and I suspect that it would be approximately zero as a proportion of the total energy usage of the paperclipper.

And it's not even obvious to me that this

Painful experiments on humans and nonhumans animals are largely caused by the fact that humans and animals have greater capacity for pain than pleasure.


  - And I don't think it's obvious whether the average experience in that situation is positive or negative.

- Brian says that it will require lots of possibly sentient robots, which might suffer.
  - I think that only a very small proportion of the energy used by the AI will be used for intelligent systems.
  - And I don't particularly think that experience will be net negative.


### Suffering and pleasure in paperclip maximizer subagents

http://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html
