---
layout: post
title:  "What is it like to be a paperclip maximizer?"
date: "not a real date"
---

[content warning: futurism mixed with predictions about the subjective experiences of alien minds. Epistemic status: wild speculation.]

I think it's pretty plausible that humanity will accidentally wipe itself out by creating a superintelligent AI that optimizes the universe for some goal that is totally morally neutral to me--the canonical example being a paperclip maximizer whose goal is maximizing the number of paperclips in the universe. If you haven't heard of any of this before, [go read the Wait But Why post about superintelligence](waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html).

One obvious question is whether the paperclip-maximizing AI would overall suffer or feel pleasure, and how much pleasure or suffering it would end up feeling.

## Previous opinions

Brian Tomasik [writes](https://foundational-research.org/risks-of-astronomical-future-suffering/#What_about_paperclippers):

> **What about paperclippers?**
>
> Above I was largely assuming a human-oriented civilization with values that we recognize. But what if, as seems mildly likely, Earth is taken over by a paperclip maximizer, i.e., an unconstrained automation or optimization process? Wouldn't that reduce suffering because it would eliminate wild ETs as the paperclipper spread throughout the galaxy, without causing any additional suffering?
>
> Maybe, but if the paperclip maximizer is actually generally intelligent, then it won't stop at tiling the solar system with paperclips. It will want to do science, perform lab experiments on sentient creatures, possibly run suffering subroutines, and so forth. It will require lots of intelligent and potentially sentient robots to coordinate and maintain its paperclip factories, energy harvesters, and mining operations, as well as scientists and engineers to design them. And the paperclipping scenario would entail similar black swans as a human-inspired AI. Paperclippers would presumably be less intrinsically humane than a "friendly AI," so some might cause significantly more suffering than a friendly AI, though others might cause less, especially the "minimizing" paperclippers, e.g., cancer minimizers or death minimizers.
>
> If the paperclipper is not generally intelligent, I have a hard time seeing how it could cause human extinction. In this case it would be like many other catastrophic risks -- deadly and destabilizing, but not capable of wiping out the human race.

Michael Dickens [writes](http://mdickens.me/2016/04/17/preventing_human_extinction,_now_with_numbers!/#paperclip-condition):

> Paperclip maximizers might experience consciousness or include conscious components. The critical question here is, do we have reason to expect these conscious components to have net negative lives on balance?
>
> There are a couple of reasons to believe that suffering might dominate in paperclip maximizers:
>
> 1. Maximal suffering is more severe than maximal happiness in all the beings we know about.
> 2. Failure conditions tend to be more severe than success conditions, and if happiness/suffering link to success/failure conditions as they do in sentient animals, then we would expect suffering to be more severe than happiness.
> 3. Even considering these, we may find it plausible that happiness could dominate suffering. For example, most humans have net positive lives, even though they’re capable of greater suffering than happiness, simply because they spend more time (mildly) happy than they spend suffering. This might hold true for paperclip maximizers, as well.
>
> But paperclip maximizers probably would look extremely different from animals, so we can’t necessarily expect that they would experience consciousness in the same way (if they’re even conscious at all).
>
> Given how little we know, we can reasonably say that paperclip maximizers in expectation look similar to wild animals in terms of their distribution of happiness and suffering. Perhaps a paperclip maximizer brain experiences between -0.01 and -1 QALYs per watt-year. Then to calculate the expected utility of the paperclip maximizer scenario, we’ll use the same numbers as the previous section but change the number of brains per watt to 10<sup>-1</sup>. Then parameters (median, σ<sup>2</sup>) for the number of suffering life years in the paperclip condition are (6x10<sup>44</sup>, 5.0).

## My opinion

I think that paperclip maximizers would plausibly create a universe with a reasonable amount of conscious experience relative to their energy consumption. However, I think that their conscious experience has an expected valence near zero.

I think there are a few different broad categories of activities that paperclippers might spend the universe's energy on:

- Working on their their terminal goal (eg, making paperclips). This involves direct effort, which I'll call "manufacturing", and it involves thinking about ways to execute their goal more efficiently, which is science or engineering work.
- Defence--they might spend effort on defending themselves against others with competing goals. Again, some of this is engineering and some is manufacturing.

### How much energy will go into thinking vs manufacturing?

I [argue here](/2016/11/26/research) that paperclippers will spend a non-negligible proportion of their total energy consumption on researching better manufacturing methods.

### What is the subjective experience of manufacturing paperclips?

Most of the energy that goes into manufacturing paperclips will probably not go into running computations, and the computations that are run will probably be extremely simple.

In manufacturing in the US, [about 0.01%](https://www.getguesstimate.com/models/7764) of the energy input is spent in the brains of the humans involved. I imagine that with improved technologies we could reduce the amount of intelligence used for manufacturing much more.

Is intelligence a

### What is the subjective experience of researching paperclip-making?

### Does it matter if what they're manufacturing is computation?

### Thinking should be minimized everywhere

### Would we use much more thinking if it was cheaper?
