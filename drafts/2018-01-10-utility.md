---
layout: post
title:  "How to learn utility functions in the presence of irrationality"
date:   "2018-01-10"
---

Stuart Armstrong has [a post where he argues](https://www.lesserwrong.com/posts/rtphbZbMHTLCepd6d/humans-can-be-assigned-any-values-whatsoever) that humans can be assigned any values whatsoever, if you allow the possibility that humans aren't exactly rational agents, which seems like a possibility we should consider.

Here's the problem we're trying to solve. We see how an agent acts in a particular environment (defined by its action space $$A$$, state space $$S$$, and transition function which is a probability distribution over (old state, action, new state) triples. We want to figure out the agent's utility function, because we want to make some decisions on its behalf that will make it happier.

(Of course, it's impossible to figure out the utility function, because behavior is the same under affine transformations of utility functions. But if we just want to make the agent happier later, it's fine if can only estimate the utility function up to affine transformation.)

However, we think that the agent might be systematically irrational. So we want to decompose the agent's policy $$\pi :: (\text{History}, S) \Rightarrow A$$ into two parts: a utility function $$u :: \text{History} -> \mathbb{R}$$, and a planner function of type $$p :: (u, \text{History}, S) \Rightarrow A$$, such that $$\pi(\text{history}, s) = p(u, \text{history}, s)$$. That is, we want the planner's behavior given the utility function to match the observed policy.

As Stuart points out, this is pretty underspecified. If you're an agent with planner and utility functions $$p$$ and $$u$$, then we can describe you just as well these ways:

- **Inverse utility hypothesis**: We could describe you as an agent with a flipped utility function and a planner that tries to minimize its utility with all the irrationalities that the real planner has.
- **Optimal imitation hypothesis**: We could describe you as an agent whose goal is to imitate the policy $$\pi$$. The utility function is calculated such that you get 1 if you are correctly imitating $$\pi$$ and 0 otherwise, and the planner is maximizing expected utility.

These are both problematic, but for different reasons. If we settle on the optimal imitation hypothesis, then we'll never be able to do better for the agent than it does for itself, because we can't fix its irrationalities. And if we have the inverse utility hypothesis, then we'll do exactly the opposite of what the agent wanted to do, which is a horrifying prospect.

Stuart points out that neither of these problems can be fixed by a simplicity prior over $$u$$ and $$p$$--both of those hypotheses have only an additive constant more length than the hypothesis we want.

I think the problem goes away if you weight by

----

You should estimate the utility function by considering all pairs $$(u, p)$$, weighted as the product of the following factors:

- Solomonoff prior of $$u$$.
- Solomonoff prior of $$p$$.
- Universal intelligence of $$p$$. The [universal intelligence](https://arxiv.org/abs/0712.3329) of a planner is defined as the average reward that it achieves on a randomly selected environment selected from a Solomonoff prior. (This is the metric that AIXI optimizes.)

Let's see how weighting by universal intelligence fixes all the problems Stuart described.

- The hypothesis where your planner is trying to do badly and your utility function is inverted is deeply penalized by the universal intelligence score of the planner.
- The hypothesis where your planner is an expected utility




things to note:

- maybe we're talking about functions from some continuous-ish state space.
- maybe it would be nice for this to have some kind of



-----------------

issues with this:

- I don't think it totally solves the problem where your utility function might be an imitation one
- Is there a way to do the inverse of the operation "your utility is how well you optimize this other policy"?
