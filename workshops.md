---
layout: post
date:   ""
title: "Workshops on rationality and AI risk for programmers"
---

The following is some details about the workshops that MIRI is running, on AI safety and human rationality. I've put these notes here so that I can share them with people; this page isn't linked to publicly. Please don't link it to people.

Here are some details about what the workshops are like:

MIRI's funding these workshops, so they're free for attendees. We also pay for travel to/from the Bay Area. And accommodation/food is sorted out for you.

The workshops are four days long. During the first two days, we spend a lot of time on human rationality. Human minds have all kinds of interesting quirks that I think it's useful to try to understand if you want to have accurate beliefs about things or if you want to make good plans. For example, many people think flossing is a good idea, but find themselves not ever flossing. This is kind of theoretically interesting--you wouldn't naively expect an agent to systematically give different answers to the question "is it worth it to floss" at different times. I think it's useful to speculate about what kinds of mental architectures might lead to properties like this, and then compare our speculations to our experiences of thinking about stuff. So in this section, we present some models of how our minds work, and show some techniques that I think are helpful for understanding and working with our minds.

The content presented in these two days is kind of similar to the content presented in the workshops run by the Center for Applied Rationality (CFAR). You can read a sample schedule of one of those [here](http://www.rationality.org/workshops/sample-schedule).

For the second two days, the workshop mostly focuses on existential risks from smarter-than-human artificial intelligence. If you're unfamiliar with this subject, I recommend starting out by looking at [this page](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/). It's totally fine to come to this workshop having not thought much about AI existential risk before. These second two days are a lot less structured and scheduled than the first two days of the workshop: I and other staff have a couple of presentations that we know how to make, but the workshop as a whole votes on what we should do for each block of time. There's a lot of discussion of different parts of this issue, and lots of people talking in pairs or small groups about how their intuitions differ.

Why do we pair these subjects together? My main answer is that I think that AI safety is a really hard problem and a really weird problem, and thinking about it well demands more from your metacognition than thinking about many other subjects. Also it's kind of surreal to think seriously about existential risk, and I think a lot of the rationality skills make it easier and safer to think about existential risk carefully.

Why are we running this workshop? Basically because we want more smart technical people to be thinking about AI safety, and we especially want them to be thinking about it well and carefully. My hope is that some proportion of the people who come to this workshop will end up deciding that they'd like to work on this problem.

The attendees to the workshop are an interesting bunch of technical people. A lot of them are Haskellers who heard about this from Edward Kmett; many of the others are people from the rationalist community.

Theyâ€™re held at a big house in Bodega Bay (about 1.5 hours north of Berkeley); we normally take people up via a bus from the MIRI office in Berkeley, leaving about 5pm on the Wednesday the workshop starts. Everyone stays at the house for the full four days and then leaves sometime between 7pm on the Sunday and the Monday morning. Food and such is provided.

