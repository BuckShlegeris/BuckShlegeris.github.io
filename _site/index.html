<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Buck Shlegeris</title>
  <meta name="description" content="Website of Buck Shlegeris.
">


  <link rel="stylesheet" href="/bootstrap.css">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://bshlgrs.github.io/">
  <link rel="alternate" type="application/rss+xml" title="Buck Shlegeris" href="http://bshlgrs.github.io/feed.xml">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$latex', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
  </script>
  <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
  <script src="/jquery.js"></script>
  <script type="text/babel" src="/main.js"></script>
  
</head>


  <body>

    

    <div class="container">
  <div class="row">
    <div class="col-xs-7 col-xs-offset-1 markdownify">
      
      <p class="post-meta"><time datetime="1969-12-31T16:00:00-08:00" itemprop="datePublished"></time></p>

      <hr/>

      <h1>Buck Shlegeris</h1>


  <h2><a href="/2016/06/12/quickselect-lemma.html"> Quickselect on an unordered array and an order statistic tree</a></h2>

  <p><strong>I did this work myself, so there are probably mistakes. I think the conclusion is right though.</strong></p>

<p>Suppose I have an <a href="https://en.wikipedia.org/wiki/Order_statistic_tree">order statistic tree</a> with $latex n$ elements and an unordered list with $latex m$ elements. Let’s say for the sake of simplicity that both are representing a set of items with no duplicates, and their intersection is empty.</p>

<p>I want to find the $latex k$th smallest item in the union of these sets.</p>

<p>You can do this trivially in $latex O(m + n)$ time, by flattening the order statistic tree (which I’ll call an OST from here onwards) onto the end of the array. Or you can add everything in the array to the OST and then query the OST, in $latex O(m \cdot \log(n + m))$ time.</p>

<p>I have found solutions that run in $latex O(m \cdot \log(n))$, $latex O(m + \log(m) \cdot \log(n))$, and $latex O(m + \log(n))$. The last of these is really complicated and annoying; the first two are pretty simple.</p>

<h2 id="order-statistic-trees">Order statistic trees</h2>

<p>I’m going to write code in Ruby, in which the <code class="highlighter-rouge">filter</code> method is stupidly named <code class="highlighter-rouge">select</code>. OSTs are usually presented with a method called <code class="highlighter-rouge">select(k)</code> which finds the $latex k$th smallest element. In this post I’m going to use the method name <code class="highlighter-rouge">find_kth_smallest</code> instead.</p>

<p>I’m going to assume that my OSTs have the following methods:</p>

<ul>
  <li><code class="highlighter-rouge">smallers</code>: returns the left subtree</li>
  <li><code class="highlighter-rouge">largers</code>: returns the right subtree</li>
  <li><code class="highlighter-rouge">pivot</code>: returns the value of the root node</li>
  <li><code class="highlighter-rouge">count</code>: returns the number of items in the node</li>
  <li><code class="highlighter-rouge">find_kth_smallest(k)</code>: as discussed above</li>
  <li><code class="highlighter-rouge">rank(x)</code>: finds the number of elements in the OST smaller than <code class="highlighter-rouge">x</code>. This takes $latex O(\log(n))$ in an OST.</li>
  <li><code class="highlighter-rouge">split_on_left_by_value(x)</code>: returns a new tree with only the items in the OST which are less than <code class="highlighter-rouge">x</code>. If our OSTs are immutable, this only takes <code class="highlighter-rouge">\log(n)</code> time.</li>
  <li><code class="highlighter-rouge">split_on_right_by_value(x)</code>: like <code class="highlighter-rouge">split_on_left_by_value</code>, but the other side.</li>
</ul>

<h2 id="standard-quickselect">Standard quickselect</h2>

<p>Just for reference, here’s an unoptimized implementation of quickselect. This has average case performance $latex O(n)$.</p>

<div class="language-ruby highlighter-rouge"><pre class="highlight"><code><span class="c1"># assumes that the list has all unique elements</span>
<span class="c1"># returns the same thing as array.sort[n]</span>
<span class="k">def</span> <span class="nf">quickselect</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
  <span class="c1"># choose a random pivot</span>
  <span class="n">pivot_element</span> <span class="o">=</span> <span class="n">array</span><span class="p">.</span><span class="nf">sample</span>

  <span class="n">smallers</span> <span class="o">=</span> <span class="n">array</span><span class="p">.</span><span class="nf">select</span> <span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">pivot_element</span> <span class="p">}</span>
  <span class="n">largers</span> <span class="o">=</span> <span class="n">array</span><span class="p">.</span><span class="nf">select</span> <span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="n">pivot_element</span> <span class="p">}</span>

  <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">smallers</span><span class="p">.</span><span class="nf">length</span>
    <span class="n">quickselect</span><span class="p">(</span><span class="n">smallers</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
  <span class="k">elsif</span> <span class="n">n</span> <span class="o">==</span> <span class="n">smallers</span><span class="p">.</span><span class="nf">length</span>
    <span class="n">pivot_element</span>
  <span class="k">else</span>
    <span class="n">quickselect</span><span class="p">(</span><span class="n">largers</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">smallers</span><span class="p">.</span><span class="nf">length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre>
</div>

<p>This has average case $latex O(n)$ performance because the recurrence relation is:</p>

<script type="math/tex; mode=display">f(n) = f\left(\frac{n}2\right) + n</script>

<h2 id="modified-quickselect-attempt-1-latex-om-cdot-logn">Modified quickselect, attempt 1: $latex O(m \cdot \log(n))$</h2>

<p>Let’s modify this to also use an OST.</p>

<div class="language-ruby highlighter-rouge"><pre class="highlight"><code><span class="c1"># OST has size n</span>
<span class="c1"># array has size m</span>
<span class="c1"># this returns the same thing as (array + ost.to_a).sort[k]</span>
<span class="k">def</span> <span class="nf">double_quickselect</span><span class="p">(</span><span class="n">ost</span><span class="p">,</span> <span class="n">array</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
  <span class="n">smallers</span> <span class="o">=</span> <span class="n">arr</span><span class="p">.</span><span class="nf">select</span> <span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">ost</span><span class="p">.</span><span class="nf">pivot</span> <span class="p">}</span>
  <span class="n">largers</span> <span class="o">=</span> <span class="n">arr</span><span class="p">.</span><span class="nf">select</span> <span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="n">ost</span><span class="p">.</span><span class="nf">pivot</span> <span class="p">}</span>

  <span class="n">number_of_smaller_things</span> <span class="o">=</span> <span class="n">smallers</span><span class="p">.</span><span class="nf">length</span> <span class="o">+</span> <span class="n">ost</span><span class="p">.</span><span class="nf">smallers</span><span class="p">.</span><span class="nf">count</span>

  <span class="k">if</span> <span class="n">number_of_smaller_things</span> <span class="o">&gt;</span> <span class="n">k</span>
    <span class="n">double_quickselect</span><span class="p">(</span><span class="n">smallers</span><span class="p">,</span> <span class="n">ost</span><span class="p">.</span><span class="nf">smallers</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
  <span class="k">elsif</span> <span class="n">number_of_smaller_things</span> <span class="o">==</span> <span class="n">k</span>
    <span class="n">ost</span><span class="p">.</span><span class="nf">pivot</span>
  <span class="k">else</span>
    <span class="n">double_quickselect</span><span class="p">(</span><span class="n">largers</span><span class="p">,</span> <span class="n">ost</span><span class="p">.</span><span class="nf">largers</span><span class="p">,</span> <span class="n">k</span> <span class="o">-</span> <span class="n">number_of_smaller_things</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre>
</div>

<p>Every time we make a recursive call, our order statistic tree halves in size. Our array might not get smaller though: eg if everything in your array is smaller than everything in your OST. Our OST has depth $latex \log(n)$, and in the worst case you need to iterate over everything in your array every time. So this is $latex O(m \cdot \log(n))$.</p>

<p>Can we do better? I think we can. Intuitively, it seems like this algorithm works to shrink the OST as fast as possible, and not really worry about the array. But the array is where most of the cost comes from. So we should try to organize this algorithm so that instead of halving the size of the OST every time, it halves the size of the array every time.</p>

<h2 id="modified-quickselect-attempt-2-latex-om--logm-cdot-logn">Modified quickselect, attempt 2: $latex O(m + \log(m) \cdot \log(n))$</h2>

<p>Instead of using the OST’s pivot, let’s pivot on a randomly selected member of the array, like in normal quickselect. This means that the array is probably going to shrink with every recursive call.</p>

<div class="language-ruby highlighter-rouge"><pre class="highlight"><code><span class="c1"># OST has size n</span>
<span class="c1"># array has size m</span>
<span class="c1"># this returns the same thing as (array + ost.to_a).sort[k]</span>
<span class="k">def</span> <span class="nf">double_quickselect</span><span class="p">(</span><span class="n">ost</span><span class="p">,</span> <span class="n">array</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
  <span class="n">pivot_element</span> <span class="o">=</span> <span class="n">arr</span><span class="p">.</span><span class="nf">sample</span>
  <span class="n">smallers</span> <span class="o">=</span> <span class="n">arr</span><span class="p">.</span><span class="nf">select</span> <span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">pivot_element</span> <span class="p">}</span>
  <span class="n">largers</span> <span class="o">=</span> <span class="n">arr</span><span class="p">.</span><span class="nf">select</span> <span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="n">pivot_element</span> <span class="p">}</span>

  <span class="n">number_of_smaller_things</span> <span class="o">=</span> <span class="n">smallers</span><span class="p">.</span><span class="nf">length</span> <span class="o">+</span> <span class="n">ost</span><span class="p">.</span><span class="nf">smallers</span><span class="p">.</span><span class="nf">count</span>

  <span class="k">if</span> <span class="n">number_of_smaller_things</span> <span class="o">&gt;</span> <span class="n">k</span>
    <span class="n">double_quickselect</span><span class="p">(</span><span class="n">smallers</span><span class="p">,</span>
                        <span class="n">ost</span><span class="p">.</span><span class="nf">split_on_right_by_value</span><span class="p">(</span><span class="n">pivot_element</span><span class="p">),</span>
                        <span class="n">k</span><span class="p">)</span>
  <span class="k">elsif</span> <span class="n">number_of_smaller_things</span> <span class="o">==</span> <span class="n">k</span>
    <span class="n">ost</span><span class="p">.</span><span class="nf">pivot</span>
  <span class="k">else</span>
    <span class="n">double_quickselect</span><span class="p">(</span><span class="n">largers</span><span class="p">,</span>
                        <span class="n">ost</span><span class="p">.</span><span class="nf">split_on_left_by_value</span><span class="p">(</span><span class="n">pivot_element</span><span class="p">),</span>
                        <span class="n">k</span> <span class="o">-</span> <span class="n">number_of_smaller_things</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre>
</div>

<p>So now the array is going to shrink on average by a factor of 2 every recursive call, but in the worst case the tree will stay the same size every time. So we now expect to $latex \log(m)$ recursive calls, for a total cost of $latex O(m + \log(m) \cdot \log(n))$.</p>

<p>Okay, this is better. But can we improve it even more?</p>

<h2 id="sketch-of-a-latex-om--logn-solution">Sketch of a $latex O(m + \log(n))$ solution</h2>

<p>I’m pretty sure I have an optimal solution, but it’s really complicated and annoying.</p>

<p>The problem was that in the previous algorithms, sometimes I computed expensive things for the data structures without any guarantee that they’d end up smaller. This time, I want to either compute tertiles for a data structure and shrink it, or ignore it.</p>

<p>For this to work, we’re going to need our OST to be a <a href="https://en.wikipedia.org/wiki/Weight-balanced_tree">weight-balanced tree</a>. Let’s set $latex \alpha = \frac 14$. (Actually, I think this algorithm works with non-weight-balanced trees, but I don’t know how to prove the time bound without the weight balance.)</p>

<p>Here’s how: At the start of every recursive call, we look at the relative sizes of the two data structures. If one is much larger than the other, we’re going to shrink the large one and ignore the small one. If they’re within a factor of 2 of each other, we’re going to call the expensive methods on both then shrink one of them.</p>

<p>If you want the gory details, read on.</p>

<h3 id="case-1-one-is-much-larger-than-the-other">Case 1: one is much larger than the other</h3>

<p>Suppose one data structure has more than three times as many things in it as the other one does. Let’s call the two data structures <code class="highlighter-rouge">Big</code> and <code class="highlighter-rouge">Small</code>. <code class="highlighter-rouge">Big</code> has size $latex b$, <code class="highlighter-rouge">Small</code> has size $latex s$.</p>

<p>Suppose we have 100 things in <code class="highlighter-rouge">Big</code> and 10 things in <code class="highlighter-rouge">Small</code>. We want to find the 15th largest thing in <code class="highlighter-rouge">Big ++ Small</code>. Let’s call the result $latex x$.</p>

<p>$latex x$ can’t be more than <code class="highlighter-rouge">Big.find_kth_smallest(15)</code>, because by definition there are 15 things in <code class="highlighter-rouge">Big</code> less than that.</p>

<p>And $latex x$ can’t be less than <code class="highlighter-rouge">Big.find_kth_smallest(5)</code>, because there are only 5 things in <code class="highlighter-rouge">Big</code> less than that, and there are only 10 things in Small.</p>

<p>So if we’re looking for the <code class="highlighter-rouge">k</code>th item in <code class="highlighter-rouge">Big ++ Small</code>, we can discard everything bigger than <code class="highlighter-rouge">Big.find_kth_smallest(k)</code>, and everything smaller than <code class="highlighter-rouge">Big.find_kth_smallest(k + s)</code>.</p>

<h4 id="case-1a-the-array-is-larger">Case 1a: the array is larger</h4>

<p>If the array is the larger data structure, then we do this discard by calling quickselect twice and copying everything between the two results to a new array. This takes $latex O(m)$ time, and the resulting array is at most size $latex s$. So we made our array half its original size.</p>

<h4 id="case-1b-the-ost-is-larger">Case 1b: the OST is larger</h4>

<p>If the tree is the larger data structure, then things are somewhat more annoying, because we only want to take constant time.</p>

<p>Selecting exactly the first $latex k$ things in an OST takes $latex \log(n)$ time. I’m going to suggest that instead of selecting exactly the first $latex k$ things, we should go a few layers deep in our OST and then delete only the nodes which we know we can safely delete.</p>

<p>Because we’re using weight-balanced trees with a weight balancing factor of $latex \frac 14$, we might need to go down maybe 3 layers in order to get a node with enough weight that deleting it deletes half the weight that you’d be able to delete if you called <code class="highlighter-rouge">split_on_left_by_value</code>. (I’m not sure about the number 3 being correct, but I think this is true for some constant.)</p>

<p>We could safely remove $latex \frac 12$ of the tree if we used the normal split methods. We’re going to remove more than half of that, so at worst our tree will end up $\frac 34$ of its original size.</p>

<p>So in both cases, we end up with the bigger data structure being a constant factor smaller.</p>

<h3 id="case-2-the-data-structures-are-a-similar-size">Case 2: the data structures are a similar size</h3>

<p>Oh man, this gets messy. I’m going to call this bit an “algorithm sketch”, because then you can’t criticise me for being handwavy.</p>

<p>Get approximate tertiles from the OST and exact tertiles from the array. This takes $latex O(m)$ time for the array and $latex O(1)$ time for the OST.</p>

<p>Then you compare these two inter-tertile ranges. There are three cases: they can intersect, they can be disjoint, or one can be inside the other. In each of these cases, based on your value of $latex k$ you can rule out at least a third of at least one of the two data structures. This is just a massive mess of cases. Here’s a diagram of one way it can play out when the ranges intersect:</p>

<p><img src="/img/ost_diagram.jpg" alt="diagram" /></p>

<p>This is a diagram of what happens when one data structure has 30 items and the other has 60 items. It contains all of the different cases for $latex k$. For example, when $latex k$ is 35, then we can discard the upper and lower thirds of the 30 item data structure, and we can discard the upper third of the 60 item data structure.</p>

<p>You can draw similar diagrams for the other cases.</p>

<p>This part is the sketchiest part of the whole algorithm. I’m pretty sure that you can always decrease at least one of the data structures by one third. I’m not sure if you can always shrink both of them. I’m not sure how much you can shrink the OST in constant time; I’m pretty sure you can do at least $\frac 16$, and I think that for any fraction less than 1/3, you can choose a node depth such that you can always cut that fraction out in constant time.</p>

<h3 id="runtime-of-this-solution">Runtime of this solution</h3>

<p>If one of the data structures starts out much larger than the other one, it will shrink until they’re similar sizes. This takes $latex O(m)$ time for the array and $latex O(\log(n))$ time for the OST.</p>

<p>Once they’re the same size, I think that at least one of them will both shrink by one third every time. So we have the recurrence relation:</p>

<script type="math/tex; mode=display">f(n, m) = f \left( \frac {2n}3, \frac {2m}3\right) + m + 1</script>

<p>which evaluates to $latex \log(n) + m$.</p>

<h2 id="conclusion">Conclusion</h2>

<p>So we can do quick select on an OST and array at the same time in $latex O(m + \log(n))$.</p>

<p>My work here leaves a lot to be desired. Most obviously, my fastest algorithm is extremely complicated and inelegant; I bet that can be simplified.</p>

<p>An obvious related question is to figure out what happens if you have two OSTs, or more. There’s literature on this last topic but I haven’t read it yet.</p>


  <hr />

  <h2><a href="/2016/06/02/say.html"> Not thinking of things I can't say</a></h2>

  <p>I noticed recently that I usually don’t think of thoughts which I’m not going to be able to say for political correctness reasons. Occasionally when I talk to people who don’t care about offending progressives, they say really insightful things which I would never have thought of on my own, because my brain just blocks those thoughts out before I can think them.</p>

<p>This is annoying, because I’d rather think of things like that and not say them publicly than not think of them at all.</p>

<p>I think the main cause of this is that I do a lot of my thinking while talking to people about things, and if certain kinds of discussion are stifled, I don’t get practice thinking about them.</p>

<p>This has a few implications. Firstly, it means that efforts to stifle offensive thoughts concern me somewhat more than they did previously, because I am more concerned that restricting speech restricts thoughts. Secondly, it makes me think more that private spaces for conversations are important. Third, it makes me really glad for circle-jerking EAs who are privately enjoy biting bullets and saying purposefully controversial things—I’d previously thought of that as harmless fun, but now I think that it might have the important function of countering the natural push towards orthodoxy.</p>

<hr />

<p><a href="https://www.facebook.com/bshlgrs/posts/10207784018613166">view comments on Facebook</a></p>


  <hr />

  <h2><a href="/2016/05/29/explicit.html"> Optimize dating for non-interference with platonic relationships</a></h2>

  <p>[content warning—I make a lot of claims that certain ways of hitting on people are much better than other ways. If you have a tendency to freak out about whether you’re a terrible person for being sexually attracted to people, maybe don’t read this.]</p>

<p>It’s often hard for men and women to hang out with each other non-romantically. This is terrible! Terrible! So terrible that reducing this problem should be a major priority for progressive subcultures. I think we can make progress by pushing for better norms about hitting on people. Let’s look at some common dating advice:</p>

<p>“When you’re asking a girl out, don’t explicitly ask her on a date. Instead, ask her if she wants to get coffee with you. It’s obvious that you’re asking her on a date, but this way if she wants to reject you she can do it more subtly and it’s less awkward for everyone involved.”</p>

<p>I think this is absolutely terrible advice! Not for the man who receives that advice, but for the society where that’s the social norm. Because sometimes I meet women and I think they’re interesting, and I would love to meet them again to hang out with them sometime. What do I do? I sure as hell can’t ask them to meet up for coffee or a meal.</p>

<p>So this faux ambiguity ends up creating a situation where it is very hard for men and women to spend time with each other.</p>

<p>This is particularly terrible in a professional context, especially one like programming where hanging out with people is really valuable, and there are relatively few women around. It’s not so hard for me if I can only meet up with male friends and talk about data structures—I’ve only lost 20% or whatever of my total conversational options. It’s much harder for the women who lose 80%.</p>

<p>(one argument you could make: “well if p is the proportion of women who are in a field, and you can’t hang out with people of the other gender, the total hangout opportunities = p**2 + (1-p)**2, which is maximized at p=0 and p=1, therefore we want to segregate everything as much as possible.”)</p>

<p>I’ve heard women talk about how one factor making career development harder for them is that it’s much harder for them to get dinner with their managers or peers without it seeming weird.</p>

<p>Even without that particular gender imbalance element, this is bad and worth fighting against. But I think that as a society and as a subculture, we can make choices which make this less of a problem. Even better, I don’t think we need to totally overhaul society to improve this: as long as individual men are known for having clear and unproblematic behaviors, women don’t have this problem around them.</p>

<hr />

<p>So here goes. Here are some rules which I think might reduce these problems.</p>

<ul>
  <li>When you ask people out, always ask them out extremely explicitly. “Would you like to go on a date with me next week” is a good sentence to use. (If you’re on a dating website or other environment where everyone is forgoing the ability to make plausibly-non-romantic connections, you don’t have to follow this rule.)</li>
  <li>If you specifically invite someone to hang out with you and it wasn’t you asking them out, never ask them out or proposition them while you’re hanging out. Relatedly (and this should be obvious but apparently isn’t), don’t hit on people in situations where they can’t get away from you without awkwardness.</li>
  <li>It’s fine to ask people out over Facebook messages or email. (In fact, it’s probably better to ask people out asynchronously than putting them on the spot by asking in person, especially if it’s someone you already know.)</li>
</ul>

<p>The goal of the first is to make it easier to hang out with people platonically. The goal of the second is to establish common knowledge that if I invite you to hang out with me sometime, I’m not going to hit on you and cause you to awkwardly have to deflect my advances and consider leaving.</p>

<p>I feel pretty good about following these rules in my current context. I’ve pretty much always followed that first rule.</p>

<p>One objection to the first rule is that it makes it somewhat more awkward for the rejectee. I think that this is better than the problems caused by ambiguity. And you can always add the ambiguity back in by asking, for example, for a date at a specific time; they can reject you by saying they’re busy then if they want. (Incidentally, I don’t think that what makes this less awkward is the faux ambiguity. I think that it’s just less awkward because you’ve set it up so that you’re basically supplied them with a rejection sentence if they want to say no; this means they’re less likely to be caught flat-footed and awkwardly not know what to say.) Alternatively or additionally, you can ask people out online, which reduces the pressure further.</p>

<p>I broke that second rule several times at college, and I really enjoyed that. Hanging out with people late at night and knowing that it might or might not lead to making out was really fun. But college was a much better place for that than adult life. At college you can usually go home, which kind of defuses some of the discomfort. I’m still somewhat confused by my intuitions here—I definitely feel that ambiguity was less of a problem at college than it is in adult life. (I also feel like this is slightly less of a problem within rationalism than in general society—maybe it’s partially because college and rationalism are both promiscuous subcultures? Maybe it’s because rationalists are better at average at talking through some kinds of problems? Maybe it’s because college girls are less cynical and for whatever reason seem to be put off somewhat less by these problems than other women, in my experience?)</p>

<hr />

<p>If the current norm is bad, why is it a stable equilibrium? Partially this problem is worse now than it used to be, because women started joining the workforce recently enough that society might not have shifted properly yet. And partially it’s because the costs are mostly felt by women (and somewhat, men scrupulous enough to worry about imposing them). It’s slightly convenient for individual men to have some ambiguity when they’re asking out women, or asking them to hang out; it’s inconvenient for women but they aren’t the ones making the decision so it happens anyway.</p>

<p>I think there’s legitimately an opportunity for a community to talk about this a lot and come up with significantly better norms about it. Communities which I’m supposedly a part of—reckon we should do this?</p>

<p>[I wrote this after talking to Claire Zabel about this a lot over the last few weeks; many of the ideas are hers.]</p>

<hr />

<p><a href="https://www.facebook.com/bshlgrs/posts/10207777395247586">view comments on Facebook</a></p>


  <hr />

  <h2><a href="/2016/05/29/flinching.html"> Flinching away from hard things</a></h2>

  <p>I have a strong reflex to not do complicated things when I’m writing code—I flinch away and try really hard to avoid them. I think this reflex often serves me well, because complicated code is a pain to maintain and in real life you usually don’t need it. When I watch other people code, I often see them start down a complicated path to solve a problem, and then persevere through it. When faced with the same problems as them, I’m much more inclined to start down a complicated path, then realize it’s going to be complicated and reflexively give up. Most problems that I run across actually have relatively simple solutions, so this serves me well: I end up flailing for longer at the start, but then I end up with simpler code than I would have had otherwise.</p>

<p>But sometimes programming tasks are actually complicated and difficult. And my reflex against doing hard things really serves me wrong there. I find myself procrastinating and circling around the problem. I start attempts and then give up on them. Eventually I notice that I’m not getting anywhere and I need to just suck it up, get a coffee and sit in a quiet room and actually plough through it. But I’m sure I’m much worse at this than many other programmers who are as good as I am at easy things.</p>

<p>For this reason, I’ve never implemented in-place quick sort, or a correct alpha-beta pruned minimax, or any kind of self balancing binary search tree. I’ve started all of them, but I always give up. I’m pretty sure that the most complicated small-scale code I’ve written in my life was written in my technical interviews at Apple, because they asked hard questions and they were watching me so I was motivated to not give up. (But even that pressure wasn’t enough—towards the end of one of the interviews, my interviewer asked if I could modify the code I’d written to handle a more general problem. I basically said “I could, but I really don’t want to” and half-assedly tried to evade the question.)</p>

<hr />

<p><a href="https://www.facebook.com/bshlgrs/posts/10207724688649954">view comments on Facebook</a></p>

<p>Satvik Beri made a <a href="https://www.facebook.com/bshlgrs/posts/10207748156956647?comment_id=10207748263919321&amp;comment_tracking=%7B%22tn%22%3A%22R0%22%7D">particularly great comment</a>:</p>

<blockquote>
  <p>I try to maintain separate mental modes, searching for simple solutions vs. implementing tricky ones, with a trigger to switch–if I’m stuck on a problem and it genuinely looks like there’s no easy solution, I mutter “I ain’t afraid of nothin’” (stolen from Claude Shannon: https://www.cs.utexas.edu/~dahlin/bookshelf/hamming.html) and dive in.</p>
</blockquote>


  <hr />

  <h2><a href="/2016/05/24/mistakes.html"> My main effective altruism mistakes so far</a></h2>

  <p>Here are the biggest things I got wrong in my attempts at effective altruism over the last ~3 years.</p>

<p><strong>1.</strong> I thought leafleting about factory farming was more effective than GiveWell top charities. I am now unsure about this. But I was way too confident based on evidence which I now think isn’t very good.</p>

<p>I probably made this mistake because of emotional bias. I was frustrated by people who advocated for global poverty charities for dumb reasons. A lot of them hadn’t thought much about animal suffering, which I thought was embarrasingly negligent. And a lot of them claimed to prefer human-targeting charities because of the stronger evidence base; I thought that if they really had that belief, they should either save their money just in case we found a great intervention for animals in the future, or donate it to the people who were trying to find effective animal right interventions.</p>

<p>I think that this latter argument was correct, but I didn’t make it exclusively. This mistake didn’t cost me much except $500 to Vegan Outreach which I kind of regret, and probably I looked dumb to the people who disagreed with me and noticed what mistake I was making.</p>

<p>(An alternative way of looking at this mistake is as a failure caused by tribalism–I believed correctly that animal welfare is a more promising cause than global poverty, and I identified pretty strongly with the animal-focused EAs over the global poverty EAs. And then I made the non-sequitur leap to “this particular animal-focused intervention is better than this particular global-poverty-focused intervention.”)</p>

<p><strong>2.</strong> In 2014 and early 2015, I didn’t pay as much attention to OpenPhil as I should have. At the time I thought that even though Good Ventures has so much money that anything it wants to fund isn’t funding constrained, OpenPhil plausibly wouldn’t fund the causes I’m most tempted to fund–most obviously existential risk and farm animal welfare. This belief led me to focus much more intently on earning-to-give that was probably reasonable in hindsight.</p>

<p>Being wrong about OpenPhil’s values is forgivable, but what was really dumb is that I didn’t realize how incredibly important it was to my life plan that I understand OpenPhil’s values. I could have tried a lot harder to figure these things out earlier, but I didn’t think of that.</p>

<p><strong>3.</strong> I wish I’d thought seriously about donating to MIRI sooner. In 2014 to mid 2015 I was agnostic about basic questions like “is MIRI doing anything useful”. These questions were hard to answer, and I didn’t really try until much later. (Honestly, I mostly didn’t think about this myself, I just made friends with people whose cause prioritization I trust, and let them convince me.)</p>

<p>Like my error #2, this is an example of failing to realize that when there’s an unknown which is extremely important to my plans but I’m very unsure about it and haven’t really seriously thought about it, I should probably try to learn more about it.</p>

<hr />

<p><a href="https://www.facebook.com/bshlgrs/posts/10207724688649954">view many comments on Facebook</a></p>


  <hr />

  <h2><a href="/2016/04/22/dumbest-algorithm-problem.html"> The dumbest algorithm problem in the entire world</a></h2>

  <p>Want to know the dumbest algorithm problem in the entire world? Here it is:</p>

<blockquote>
  <p>Find the $latex k$th last element of a linked list, in a single pass, with constant memory.</p>
</blockquote>

<p>Let the length of the linked list be $latex n$. If we didn’t have to follow the single-pass constraint, we’d solve this by finding the length of the linked list (which takes $latex n$ steps), then looking for the $latex(n - k)$th element. In total, we traversed $latex(2n - k)$ links.</p>

<p>However, this interviewer apparently measures code efficiency by “number of while loops in the function body”, so we have to do better.</p>

<p>The canonical answer to this question is to make a while loop with two pointers, one of which trails $latex k$ links behind the other. As soon as you get to the end with your leading pointer, you return whatever’s at your trailing pointer.</p>

<p>Wow, so efficient, only one while loop! Let’s see how much faster we made it!</p>

<p>Well, the leading pointer traversed $latex n$ links, and the trailing pointer traversed $latex (n - k)$ links. That’s…uh…exactly the same as the previous answer! Except you had to write a custom function instead of just calling <code class="highlighter-rouge">.length</code> and <code class="highlighter-rouge">.get</code>, which I assume your linked list already had! And now you’re more exposed to a bug when the linked list is too short to have a $latex k$th last element!</p>

<p>(Full disclosure: for small $latex k$, this solution might be faster because of memory locality. But I’ve never heard anyone mention that as the rationale for this problem, so I’m not going to give any credit for that.)</p>

<p>In real life, if this was method call was going to happen a lot you’d just store the length of your linked list in a wrapper object. Knowing that you should maintain the length as a field is a million times more useful than being able to solve the problem as written.</p>

<p>It makes me really sad that this question appears so often on lists of most common interview questions. Cracking the Coding Interview mentions this question and gives this answer but doesn’t claim it’s better. Elements of Programming Interviews actively claims that this answer is the better one (page 105).</p>

<p>This is algorithm interviewing at its worst. It’s a brainteaser which actively penalizes you for a good sense of algorithmics or software engineering.</p>

<p>EDIT: In the comments, David gives a great solution that is way smarter than the traditional one. This makes me dislike the traditional answer even more.</p>

<blockquote>
  <p>Leading pointer is L steps ahead of trailing one. Remember where it is (say, OLD_LEAD). Move forward L more steps, while remembering how far ahead the leading one is of the trailing one (STEPS_AHEAD).</p>
  <p>If you get to the end, do the right thing (using STEPS_AHEAD).</p>
  <p>If STEPS_AHEAD gets to L, move trailing pointer to OLD_LEAD and update STEPS_AHEAD.</p>
</blockquote>

<hr />

<p><a href="https://www.facebook.com/endofunctor/posts/10207496299980380?pnref=story">view comments on Facebook</a></p>


  <hr />

  <h2><a href="/2016/04/20/overteaching.html"> Overteaching and overlearning</a></h2>

  <p>(I’m talking about computer science here because that’s the only topic I know really well, but this might generalize anyway.)</p>

<p>Most university classes have a few concepts in them that you should remember forever, and a bunch of other concepts that you forget immediately after the final.</p>

<p>Is it a total waste of time to teach things that people are going to forget? I don’t think so, for a few reasons. To start with, sometimes you teach people about a particular specific topic with the goal of them getting a feel for the kind of things that you do in that field more generally. A classic example is sorting algorithms—you rarely actually need to implement a sorting algorithm, but studying them is a pretty good way of getting a feel for different ways of designing algorithms[1]. Even though people will forget the fiddly details of implementing quicksort in place, hopefully they’ll remember the insight that you can split things into smaller subproblems, recursively solve those, and merge the results. Relatedly, it often works nicely to teach people about an important problem by making them study the solutions to it—for example, I don’t remember the algorithms which are actually used in practice for scheduling in operating systems, but I remember the concepts better because I studied them in my OS class.</p>

<p>More generally, students empirically learn concepts better if they then practice using those concepts in more advanced ways. For example, you remember your algebra course better if you then do a calculus course that relies on it. A lot of practical computer science courses could be justified by pointing out that even if they teach you totally useless things, you hopefully at least practiced programming.</p>

<p>However, for this to work, the more advanced material needs to actually rely on the core material. I think many computer science courses do not do a good job of this.</p>

<p>For example. Suppose you want to ensure that your students finish their algorithms course and remember binary search trees. You spend the obligatory week on binary search trees, and teach them about red-black trees to the extent that when I interview them a year later they can say “uh, some nodes are red and some nodes are black and black children only have red parents or something like that.” And now you’re not sure what to teach them next.</p>

<p>Algorithms courses often say “Well, I guess we’re done with BSTs. Now that the core stuff is out of the way, let’s teach them about max flow and spanning tree algorithms!”</p>

<p>I think this is a terrible idea! Students learn those algorithms, but they’re kind of confusing and don’t come up much in the rest of your life, so they forget them. And the learning process didn’t even work as over learning, because it wasn’t giving them practice reasoning about BSTs.</p>

<p>Instead, you should teach them more advanced things about BSTs! Teach them about BTrees (and the <a href="http://web.stanford.edu/class/cs166/lectures/05/Small05.pdf">isometry between those and red-black trees</a>)! Make them implement splay trees and treaps. Tell them about <a href="http://web.stanford.edu/class/cs166/lectures/06/Small06.pdf">augmented binary search</a> trees.</p>

<p>These things are not particularly easier or harder than just teaching them about other algorithms stuff. And the students will probably forget all of it in a year. But they will do a much better job of giving the students excuses to think really carefully about how BSTs work, so they’ll be more fluent with those when they come up.</p>

<p>(This is the approach taken by the amazing course CS166 at Stanford, which I was linking above. As far as I’m concerned, all other algorithms courses should imitate CS166 as much as possible.)</p>

<p>Classic examples of things which CS classes teach which do not work well as over learning: Minimum spanning tree algorithms. The Floyd Marshall algorithm. Dijkstra’s algorithm, when it isn’t taught as uniform cost search. (This one particularly annoys me because most students try to learn it, but most of them don’t realize the connection between it and BFS. AFAICT this is just for historical reasons: when Dijkstra originally presented the algorithm it wasn’t written as BFS with a priority queue, and CS departments have a bizarre fixation on presenting ideas as they’ve been historically presented.</p>

<p>[1] Sadly, most courses which do this teach you about things like divide and conquer and greedy algorithms, but neglect the single most important part of designing a good algorithm, which is choosing the right data structures. Skiena’s textbook does a great job of avoiding this mistake: it presents heapsort as selection sort optimized by using a faster data structure.</p>

<hr />

<p><a href="https://www.facebook.com/endofunctor/posts/10207490190427645?pnref=story">view comments on Facebook</a></p>


  <hr />

  <h2><a href="/2016/04/14/hyperactive-record.html"> Hyperactive Record</a></h2>

  <p>Here is a fun open source project which someone should build! I’ve written a readme and put it <a href="https://github.com/bshlgrs/HyperactiveRecord/blob/master/README.md">on Github</a> but I don’t think I have the time to actually build this. I present it here in the hope that one of you might decide that you like it and want to build it!</p>

<p>It’s called HyperactiveRecord. It’s an ActiveRecord plugin which tries harder to provide an intuitive, consistent, simple interface for SQL.</p>

<p>For example. You’re making a blog site. You have posts and comments. Posts are either public or private. I want to load all of the publicly available comments of a user–that is, all their comments which are on public posts. (I use this example because it’s analogous to something that I did at work today.)</p>

<p>Here’s what I intuitively want to do: <code class="highlighter-rouge">user.comments.filter { |c| c.post.public }</code>. This is much simpler and more readable than the ActiveRecord solution. I want to make a library whose goal is to be as simple, composable, and unsurprising as that. We’re already used to the APIs of map, flatMap, filter, reduce, and so on. That’s how our queries should work.</p>

<p>This would be written basically the same way ActiveRecord is: each query method is just constructing a lazy query object which is only actually executed when accessed. Making DSLs which accept blocks is already done in Ruby all over the place; why not do it here too?</p>

<p>There are some super fun places you could take this project. For example, a SQL enthusiast could devote themselves to the project of building a query generator for recursive method calls. That is, compiling</p>

<div class="language-ruby highlighter-rouge"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Comment</span>
  <span class="k">def</span> <span class="nf">descendants</span>
     <span class="no">Hyperactive</span><span class="p">.</span><span class="nf">union</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nb">self</span><span class="p">.</span><span class="nf">children</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="o">&amp;</span><span class="ss">:descendants</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre>
</div>

<p>into something which you can do in SQL with stored procedures or whatever. You’d have to do this by building the query graph, detecting the cycle, and doing magic SQL things which I don’t understand.</p>

<p>I think this would be a super fun project! The one downside is that it involves learning to deal with ActiveRecord, which seems potentially difficult. If you want to just build a proof of concept, you could build this as a standalone library with Arel or just building SQL queries manually.</p>

<p>I’d love to work with someone on this if they wanted to do the bulk of the implementation work!</p>


  <hr />


<p><a href="/posts">All posts</a></p>

<p><a href="/feed.xml">RSS feed</a></p>


      <hr/>

<div class="PageNavigation">
  
  
</div>

    </div>
    <div class="col-xs-3 col-xs-offset-1">
      <img src="https://scontent-sjc2-1.xx.fbcdn.net/hphotos-xtp1/t31.0-8/11154688_10205041372168719_3725604149367069581_o.jpg" class="img-responsive" alt="Picture of Buck">
      
<hr/>
<a class="arrow" href="/"><strong>Buck</strong></a>
<ul>
  <li><a class="arrow" href="/about">About</a>
    <ul>
      <li><a href="/teaching">Tutoring services</a></li>
    </ul>
  </li>
  <li>Links
    <ul>
      <li>
          <a class="arrow" href="http://github.com/bshlgrs">GitHub</a>
      </li>
      <li>
          <a class="arrow" href="mailto:bshlegeris@gmail.com">Email</a>
      </li>
      <li>
          <a class="arrow" href="http://www.facebook.com/bshlgrs">Facebook</a>
      </li>
      <li>
          <a class="arrow" href="http://lnkd.in/bnBJ6EF">LinkedIn</a>
      </li>
      <li>
          <a class="arrow" href="/resume.pdf">Resume</a>
      </li>
    </ul>
  </li>
  <li><a class="arrow external" href="https://docs.google.com/forms/d/1SOombLPHlKIMut-wJzIRg7DGdJjh9PJV4yAkrmTXKn4/viewform?usp=send_form">Anonymous feedback</a></li>

  <li><a href="/posts" class="arrow">Blog</a>
    <ul>
      
        <li>
          <a class="arrow" href="/2016/06/12/quickselect-lemma.html">Quickselect on an unordered array and an order statistic tree</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/06/02/say.html">Not thinking of things I can't say</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/05/29/explicit.html">Optimize dating for non-interference with platonic relationships</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/05/29/flinching.html">Flinching away from hard things</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/05/24/mistakes.html">My main effective altruism mistakes so far</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/04/22/dumbest-algorithm-problem.html">The dumbest algorithm problem in the entire world</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/04/20/overteaching.html">Overteaching and overlearning</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/04/14/hyperactive-record.html">Hyperactive Record</a>
        </li>
      
    </ul>
  </li>
  <li>
      <a class="arrow" href="/cute">Pictures of me</a>
  </li>
</ul>

    </div>
  </div>
</div>




    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Buck Shlegeris</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Buck Shlegeris</li>
          <li><a href="mailto:bshlegeris@gmail.com">bshlegeris@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/bshlgrs"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">bshlgrs</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/bshlgrs"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">bshlgrs</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Website of Buck Shlegeris.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52980069-1', 'auto');
  ga('send', 'pageview');

</script>

</html>

