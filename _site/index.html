<!DOCTYPE html>
<html>

  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Buck Shlegeris</title>
  <meta name="description" content="Website of Buck Shlegeris.
">


  <link rel="stylesheet" href="/bootstrap.css">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://bshlgrs.github.io/">
  <link rel="alternate" type="application/rss+xml" title="Buck Shlegeris" href="http://bshlgrs.github.io/feed.xml">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$latex', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script src="/jquery.js"></script>
  <script type="text/babel" src="/main.js"></script>
  
</head>


  <body>

    

    <div class="container">
  <div class="row">
    <div class="col-sm-7 col-sm-offset-1 markdownify">
      
      <p class="post-meta"><time datetime="1969-12-31T16:00:00-08:00" itemprop="datePublished"></time></p>

      <hr>

      <h1>Buck Shlegeris</h1>

<div class="lead">
<p>I am a software engineer from Australia. I have lived in San Francisco for most of the last three years. I'm interested in data structures and effective altruism.</p>
<p>Here are some things that I've written recently:</p>
</div>


  <h2><a href="/2016/07/02/graph.html"> Terse notes on graph algorithms</a></h2>

  <p class="post-meta"><time datetime="2016-07-02T00:00:00-07:00" itemprop="datePublished">Jul 2, 2016</time></p>

  <div class="shrink-headings">
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>

<p>As part of my work at Triplebyte, I spend some time explaining algorithms to people. On some topics, my explanations are quite different to the explanations that students normally see. Here is my attempt to tersely sketch my explanation of the differences between graph search algorithms.</p>

<p>When you do a graph search, you basically do it by maintaining some set of nodes that you need to explore at some point. Graph search looks something like this:</p>

<div class="highlighter-rouge">
<pre class="highlight"><code>def search(graph, start_node):
  explored = set()
  frontier = set()

  while frontier:
    node = frontier.pop()

    # skip this node if we've already explored it
    if node in explored:
      next

    explored.add(node)

    for neighbor in graph.neighbors(node):
      if neighbor not in explored:
        frontier.add(node)

  return explored
</code></pre>
</div>

<p>That algorithm returns a set of nodes which it could reach from <code class="highlighter-rouge">start_node</code>.</p>

<p>In the <code class="highlighter-rouge">node = frontier.pop()</code> line, which node should we pop from the frontier?</p>

<p>We could have <code class="highlighter-rouge">frontier</code> be a stack, which uses a last-in-first-out rule. If we do that, our graph search is a depth-first search.</p>

<p>Alternatively, we could have <code class="highlighter-rouge">frontier</code> be a queue. Queues give you elements in a first-in-first-out order. Assuming our graph is unweighted, this means that you’re always going to explore nodes closest to the start node before nodes farther away. So if you are keeping track of the path, you’re guaranteed to find the shortest path from <code class="highlighter-rouge">start_node</code> to every other node.</p>

<p>So <strong>BFS will get you the shortest path from one node to another on an unweighted graph, while DFS makes no such guarantee. BFS gets you shortest paths because it explores the nodes in ascending order of distance from the start node.</strong></p>

<p><strong>Resources on this:</strong></p>

<ul>
  <li><a href="https://www.cs.usfca.edu/~galles/visualization/BFS.html">visualization</a></li>
  <li><a href="https://www.quora.com/Graph-Theory-What-is-the-difference-between-depth-first-search-and-breadth-first-search/answer/Eliot-Ball">similar explanation on Quora</a></li>
</ul>

<h3 id="generalizing-to-weighted-graphs">Generalizing to weighted graphs</h3>

<p>Why does BFS always get shortest paths? Because it explores nodes in order of their distance from the start node, and once it’s explored a node it never explores it again. So each node will only be explored via its shortest path.</p>

<p>So the key is the FIFO structure of the queue–because the graph is unweighted, if you explore nodes in the order in which you first encounter them, you’re finding the shortest paths.</p>

<p>But that doesn’t work for weighted graphs, because FIFO queues don’t take into account the edge costs. We still want to explore nodes in ascending order of distance from the start node. How can we do this?</p>

<p>Easy! Instead of a FIFO queue, we can use a priority queue, where the priority of each node in the queue is its distance from the start node.</p>

<p><strong>This is known as Dijkstra’s algorithm. It’s just BFS generalized to weighted graphs by using a priority queue instead of a FIFO queue.</strong></p>

<h3 id="limitations-of-dijkstras-algorithm">Limitations of Dijkstra’s algorithm</h3>

<p>Dijkstra’s algorithm finds shortest paths because it explores all the nodes in ascending order of their distance from the start node.</p>

<p>If your graph has negative edge weights in it, Dijkstra’s algorithm will not correctly explore the graph in ascending order of distance. This means that it might not return shortest paths.</p>

<p>So <strong>Dijkstra’s algorithm only finds shortest paths on graphs with positive edge weights.</strong></p>

<h3 id="other-notes">Other notes</h3>

<p>You can <a href="http://stackoverflow.com/questions/69192/how-to-implement-a-queue-using-two-stacks">implement a queue with two stacks</a>. Your explored set should be implemented as a hash set, to minimize lookup times. You can implement a priority queue with a binary heap.</p>

<p>My graph search implementation above involves adding the same node to <code class="highlighter-rouge">frontier</code> multiple times. In the case of BFS and DFS, this is unhelpful and you should consider adding an auxilliary hash set which stores the contents of <code class="highlighter-rouge">frontier</code> and allows you to quickly check whether nodes are in it and then avoid adding them to the frontier set if they’re already in there. However, in the case of Dijkstra’s algorithm, we might find a shorter path to a node after the node has already been added to the priority queue. In my code above, this case is handled by the logic to skip exploring a node if it’s already been explored. So it’s only going to actually be explored with its lowest priority. If you want to be super fancy, you can instead do this by using a priority queue which quickly allows you to decrease the priority of one of its elements. You can implement a queue which efficiently supports this <code class="highlighter-rouge">decreaseKey</code> operation with a Fibonacci heap, which lets you do <code class="highlighter-rouge">decreaseKey</code> in amortized $latex O(1)$ time. However Fibonacci heaps are massively complicated and have a big constant factor (due to the fact that you can’t represent them in an array like binary heaps), so you probably shouldn’t actually do this.</p>

<p>DFS usually takes less memory than BFS. <a href="https://www.quora.com/Why-is-DFS-usually-more-space-efficient-than-BFS">Explanation here</a>. So if you need to completely explore a graph, DFS is better.</p>

<p>If you need to explore a graph roughly in order but don’t want to pay the memory cost of BFS, you can use <a href="https://en.wikipedia.org/wiki/Iterative_deepening_depth-first_search">iterative deepening depth-first search</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/A*_search_algorithm">A*</a> is a modification of Dijsktra’s algorithm which uses a heuristic added to the distance from the start node as the priority in the priority queue. If this heuristic always underestimates the distance from the node to the end node, <strong>A* will give you correct results</strong>. <strong>A* needs to have an optimistic heuristic for the same reason that Dijkstra’s algorithm fails on graphs with negative edge weights.</strong></p>

<p>If you want to find shortest paths on a graph with negative edge weights, you can use the <a href="https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm">Floyd-Warshall algorithm</a>, which is much slower than the graph search algorithms I’ve been describing, because it can’t only look at nodes once and then be confident that it has found the shortest path to them.</p>

<h2 id="questions-to-test-your-understanding">Questions to test your understanding</h2>

<p><strong>Does Dijkstra’s algorithm find shortest paths on a graph if some of its edges have weight 0?</strong> <a role="button" data-toggle="collapse" href="#q1">
  show answer
</a></p>

<div class="collapse" id="q1">
  <div class="well">
    Yes, because the nodes are still going to be explored in ascending order of distance from the start. If I'm exploring a node which is 6.7 away from the start node, and I see it have an edge with weight 0, then I'll push that neighbor node into the priority queue with priority 6.7. It will be explored before any nodes farther away from the start are explored.
  </div>
</div>

<p><strong>Does BFS find shortest paths on a graph which is weighted, but all its edge weights are the same positive number?</strong> <a role="button" data-toggle="collapse" href="#q2">
  show answer
</a></p>

<div class="collapse" id="q2">
  <div class="well">
    Yes.
  </div>
</div>

<p><strong>If you want to find the shortest path from point A to point B on an unweighted graph, should you use BFS or DFS?</strong> <a role="button" data-toggle="collapse" href="#q3">
  show answer
</a></p>

<div class="collapse" id="q3">
  <div class="well">
    You should use BFS, because unlike DFS it finds shortest paths.
  </div>
</div>

<p><strong>Suppose I’m implementing a garbage collection algorithm, which is designed to find all the objects in memory which are still reachable, and then delete all the other elements. I want to do this by a graph search starting from the objects pointed to by local variables. Should I use BFS or DFS?</strong> <a role="button" data-toggle="collapse" href="#q4">
  show answer
</a></p>

<div class="collapse" id="q4">
  <div class="well">
    You should use DFS, because you need to explore the entire graph and DFS usually requires less memory than BFS to explore an entire graph.
  </div>
</div>


  </div>

  <hr>

  <h2><a href="/2016/07/02/sms.html"> SMS vs email responsiveness</a></h2>

  <p class="post-meta"><time datetime="2016-07-02T00:00:00-07:00" itemprop="datePublished">Jul 2, 2016</time></p>

  <div class="shrink-headings">
    <p>Here’s something I’ve learned recently: People are waaaaay more responsive to SMS than email. If you need to get a quick short response from someone, you should definitely text them.</p>

<p>Why is this? Two hypotheses:</p>

<ol>
  <li>I’m burning some collective commons where SMS is reserved for higher priority communications and by exploiting this, I’m degrading its signal to noise ratio.</li>
  <li>You can’t write elegantly on SMS because of the phone keyboard limitation, so you feel less awkward about replying with super short messages like “ok then”. This means that you’re less tempted to put off replying. (I have a web interface to SMS, so this limitation doesn’t actually change my behavior. But this limitation changes the social norms in a useful way even if it’s not present for everyone.)</li>
</ol>

<hr>

<p><a href="https://www.facebook.com/bshlgrs/posts/10208079991852312">view responses on Facebook</a></p>

  </div>

  <hr>

  <h2><a href="/2016/07/02/poisson.html"> Testing whether two integers are different</a></h2>

  <p class="post-meta"><time datetime="2016-07-02T00:00:00-07:00" itemprop="datePublished">Jul 2, 2016</time></p>

  <div class="shrink-headings">
    <p>Here is a super simple statistics question whose answer I didn’t know until just now: How do you test whether one integer is different from another?</p>

<p>Like, I do two things and count 3 the first time and 5 the second time. Did I do the same thing both times? I dunno, maybe I did.</p>

<p>But if I counted 20 the first time and 50 the second time, I’m starting to feel like the things are different. And if I counted 100000 one time and 10 the other, the things seem very different indeed.</p>

<p>The right way to formalize this is apparently to use the Poisson distribution family. You treat each count as a measurement of the rate at which one process does something. And then you can do statistical tests to see whether the two different counts seem to come from processes with different rates.</p>

<p>You can see a calculator for this <a href="http://www.evanmiller.org/ab-testing/poisson-means.html">here</a>.</p>

<p>I’m unsettled by how often I accidentally think of really basic-seeming statistics questions which I can’t answer <img class="emoji" title=":pensive:" alt=":pensive:" src="https://assets.github.com/images/icons/emoji/unicode/1f614.png" height="20" width="20" align="absmiddle"></p>

  </div>

  <hr>

  <h2><a href="/2016/07/02/hash-ordered-treaps.html"> Hash-ordered treaps</a></h2>

  <p class="post-meta"><time datetime="2016-07-02T00:00:00-07:00" itemprop="datePublished">Jul 2, 2016</time></p>

  <div class="shrink-headings">
    <p>Hash-ordered treaps are a cool kind of binary search tree. They have the neat property that the mapping from ordered sets to hash-ordered treaps is one-to-one and onto. That is, for every ordered set, there’s exactly one valid hash-ordered treap which represents it.</p>

<p>So what’s a hash-ordered treap? Well, choose your favorite hash function and call it h. A hash-ordered treap is a binary search tree which follows one additional restriction: for every parent and child, h(value of parent) &gt; h(value of child).</p>

<p>(It’s called a hash-ordered treap because as well as being a tree, it’s also a heap ordered on the hash of the node value.)</p>

<p>There’s exactly one hash-ordered treap for a given ordered set. This wasn’t totally obvious to me at first, but the proof is pretty simple. Suppose you have a ordered set. One of its values must have the highest hash value. That value is going to be the root of the tree. Everything in the ordered set less than the root is going to be in a hash-ordered treap on the left, and everything greater than the root is going to be in a hash-ordered treap on the right. So by structural induction, QED.</p>

<p>Insertion is pretty simple: you just search down for the right place to insert as usual, then you do tree rotations up to restore the heap ordering property. I think this takes an average of O(1) time if you’re inserting random keys into random treaps, for the same reason that heap insertion is sort of average O(1) time. (But I made that proof up myself, so it might be wrong. Worst case it’s O(log(n)), so no big deal if so.)</p>

<p>This nice isomorphism between ordered sets and hash-ordered treaps is useful when you want to be able to build BSTs separately places and later merge them together, taking advantage of the fact that the trees will be very similar if
they have similar elements. I first saw this used in <a href="http://arxiv.org/abs/1301.3388">this paper</a>.</p>

  </div>

  <hr>

  <h2><a href="/2016/06/30/triplebyte.html"> You should work at Triplebyte</a></h2>

  <p class="post-meta"><time datetime="2016-06-30T00:00:00-07:00" itemprop="datePublished">Jun 30, 2016</time></p>

  <div class="shrink-headings">
    <p>Triplebyte, the startup where I work, is hiring. We’re looking for engineers, a writer, a UX designer, and some generalist roles.</p>

<p>I think Triplebyte is a great place to work. In particular, I think that <strong>if you’re an EA, working at Triplebyte is a great opportunity for you if you would otherwise be earning to give and developing career capital.</strong> Here’s why.</p>

<p>I’ve learned an enormous amount since I started working here. I have a lot of freedom in what I do and how I do it. For example, my first major project had a significant UI component, and I wanted to do it in React; my CTO didn’t know React but was fine with me bringing it into the site. My second project was a complicated backend web project which involved, among other things, learning how to take care of a bunch of complicated servers on AWS. And my current project is essentially a interviewing research project: I’m developing a coding problem very different to everything we currently ask, and currently I’m collecting data to determine whether it correlates with our other measurements well enough that we should use it.</p>

<p>Empiricism is core to what we do. I’ve never before worked in a setting which puts as much work into statistical analysis of its various projects. Almost every task I have involves coming up with a hypothesis that if we work on some particular thing, we’ll improve some particular metric. I then build whatever software is required. After a while, we do a statistical analysis to determine whether my project has a positive effect. If it doesn’t, we modify it or abandon it.</p>

<p>The core skill I’m learning is the extremely useful general skill of figuring out how to make progress on hard and confusing problems, given incomplete information and significant costs to gathering data. I am super glad to be practicing this. This will be extremely useful to me regardless of what I end up doing long term—most obviously, it’s a useful skill for many kinds of direct work, many kinds of research, and entrepreneurship.</p>

<p>I think Triplebyte is an unusually good opportunity to develop these skills for a few reasons. To start with, as companies get bigger, they have more of an idea of what they’re doing, so you don’t get to work on problems with such large scope. Especially if you’re fundamentally just an empirically minded engineer—at most companies you’ll end up just working on engineering specific solutions, rather than broadly investigating the space of solutions. Among small companies, I don’t think many have founding teams as experienced, or engineers as good (I feel qualified to say this because I’ve interviewed a lot of engineers, and my coworkers are generally stronger programmers.)</p>

<p>(Triplebyte also has some downsides. Most obviously, engineers spend like 20 hours a week interviewing people. This is sort of a pain. On the other hand, I’ve certainly gotten something useful out of it: I have a way better understanding of a diverse range of topics in computer science now, and I’m somewhat better at communicating about some things, and I’ve learned that lots of people sound really smart when they talk about things they’ve done, then absolutely go to pieces when you ask them for technical details. I have gotten a much better feel for the ways and the extent that programmers vary.)</p>

<p>One of the projects I’m working on at the moment is something which I came up with on a whim one evening and coded a prototype of—my coworkers liked it, so I’m now building it into the main site. You don’t get to do this kind of thing at most companies.</p>

<p>The company culture is currently very well suited to EAs and rationalists: one reason I want to recruit my friends is that I’d like to keep it that way.</p>

<p>We’re hiring for the following roles (more info <a href="https://triplebyte.com/careers">here</a>):</p>

<ul>
  <li>Software engineer. You need to be good at thinking and programming. You need really broad knowledge of computer science, because you need to be able to talk intelligently about anything that your interviewee might want to talk about. You need to be friendly, because you spend a bunch of time interviewing.</li>
  <li>Writer. We would love to hire someone to spend all their time taking our data and turning it into blog posts. If you like writing about data, this would be a great job for you.</li>
  <li>Designer/UX. We need someone to design all our user interfaces and do all our other visual design work.</li>
  <li>Talent manager. This is a person who handles all our interaction with our candidates. You need to be really organized, good at interacting with people (especially software engineers), and you need to be excited about doing experiments and using data to improve how we manage candidates.</li>
  <li>Other generalist roles: Product Operations and an Operation Manager</li>
</ul>

<p>Get in touch with me if you’re interested in any of these roles. We are specifically interested in talking to you if you’re junior, really smart, and want to be given lots of responsibility to figure out the right way to do your job.</p>

<p>(And as always, even if you aren’t interested in any of this, if you’re a software engineer and you need a new job, you should use us! Talk to me about this too.)</p>

  </div>

  <hr>

  <h2><a href="/2016/06/22/ctci.html"> Studying for startup interviews with 'Cracking the Coding Interview'</a></h2>

  <p class="post-meta"><time datetime="2016-06-22T00:00:00-07:00" itemprop="datePublished">Jun 22, 2016</time></p>

  <div class="shrink-headings">
    <p><a href="https://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/098478280X">Cracking the Coding Interview</a> is by far the most popular interview prep book for software engineers. It’s a great book. But it was written for a few years ago, and software engineering interviews seem to have changed in the meantime, at least in the Silicon Valley engineering culture which I’m most involved in.</p>

<p>So I’ve written this guide to the book. It contains notes on things which I think have changed since it was written, and a list of the questions from the book which I think are most relevant to people preparing for software engineering interviews at good companies in the Bay Area.</p>

<p>I was inspired to write this by <a href="https://www.interviewcake.com/">InterviewCake</a>, which is a competitor to Cracking the Coding Interview. But InterviewCake is much more expensive than CtCI. I think this list of CtCI questions is about as good a list of interview questions as InterviewCake. There, I just saved you $180.</p>

<p>How have things changed? Most obviously, people use more scripting languages here and less C++ and Java. This has some direct effects. For example, there aren’t as many questions about merging arrays given buffer space anymore: that question still makes sense in scripting languages, but it doesn’t come up as much as it does in C. And I think Silicon Valley dislikes what it sees as verbose enterprise trivia bullshit, and purposefully doesn’t talk about such things. I think this is why I don’t hear about people being asked about design patterns.</p>

<p>The 5th edition is somewhat better than the 4th edition, because it has more content at the start of every chapter. I have not seen the 6th edition.</p>

<h2 id="the-most-relevant-questions">The most relevant questions</h2>

<p>Here is a subset of the questions which I think reflects SF software interviews more accurately:</p>

<p>Questions in the 4th edition:</p>

<p>1.1, 1.7, 1.8, 2.1, 3.1, 3.2, 3.3, 3.5, 4.1, 4.2, 4.3, 4.5, 4.8, 7.1, 7.2, 7.4, 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 9.2, 9.3, 9.6, 12.1, 12.2, 12.6, 12.7, 15.3, 17.5, 18.1, 18.2, 19.2, 19.7, 19.11, 20.10, 20.11.</p>

<p>Questions in the 5th edition:</p>

<p>1.1, 1.7, 1.8, 2.1, 3.1, 3.2, 3.3, 3.5, 3.7, 4.1, 4.2, 4.3, 4.5, 4.6, 4.8, 5.1, 8.1, 8.2, 8.5, 8.7, 8.9, 8.10, 9.1, 9.2, 9.3, 9.4, 9.5, 10.1, 10.2, 10.5, 10.6, 11.3, 11.6, 11.8, 15.5, 16.1, 17.2, 17.8, 17.12, 18.10, 18.11.</p>

<p>(Making those two lists felt like maintaining a codebase to work in both Python 2 and 3)</p>

<p>I would recommend giving these questions higher priority in your study.</p>

<p>Many of the questions that I didn’t include there are still good practice. If you want more practise on a particular topic, you should definitely do even the questions I didn’t include.</p>

<h2 id="general-notes">General notes</h2>

<p>Every chapter in the interview questions section of CtCI starts with a short summary of the subject matter for the questions. I think these notes are quite good. It’s well worth reading the notes for chapters even if you don’t do questions from them.</p>

<p>The first 40-ish pages of the book are general advice about software engineering interviews. I agree with all of it except the following points, organized by section:</p>

<p><strong>Before the interview</strong>: Companies don’t ask behavioral questions much.</p>

<p><strong>What You Need To Know (page 26)</strong>: Tries are not absolutely must-have knowledge. I’ve never heard of people being asked to implement quicksort in place. Singleton and Factory design patterns are not crucial knowledge.</p>

<p><strong>At the Interview</strong>: You don’t have to use C, C++, or Java. People will possibly be biased against you if you do. Ruby, Python, and Javascript are fine choices. I think it’s fine to do the problem in whatever language you want. You can ask your recruiter about this before the interview.</p>

<h2 id="comments-on-individual-question-chapters-in-the-4th-edition">Comments on individual question chapters (in the 4th edition):</h2>

<h3 id="chapter-1-arrays-and-strings">Chapter 1: Arrays and Strings</h3>

<p>Companies care less about seeing you edit data in place than they used to. I’d only expect questions on this if you’re applying for a C job. For this reason, questions [1.2, 1.3, 1.5, 1.6, 1.7] are lower priority.</p>

<h3 id="chapter-2-linked-lists">Chapter 2: Linked Lists</h3>

<p>All of these questions are less common these days because linked lists come up less, because we don’t use C as much and have more support for generics, leading to us coding our own data structures less often.</p>

<p><a href="http://bshlgrs.github.io/2016/04/22/dumbest-algorithm-problem.html">Fuck question 2.2.</a></p>

<p>2.5 is a common bullshit brainteaser question.</p>

<h3 id="chapter-3-queues-and-stacks">Chapter 3: Queues and stacks</h3>

<p>I agree that you’ll look good if you can flawlessly implement a queue and stack.</p>

<h3 id="chapter-4-graphs-and-trees">Chapter 4: Graphs and trees</h3>

<p>Study BFS: it’s a really common question. These questions all seem reasonable.</p>

<h3 id="chapter-5-bit-manipulation">Chapter 5: Bit manipulation</h3>

<p>I’ve rarely seen these problems come up, even when I interviewed for a C++ systems job.</p>

<p>I include one question of this type in my list above.</p>

<h3 id="chapter-6-brainteasers">Chapter 6: Brainteasers</h3>

<p>These questions are widely reviled, and seem to be getting less common. Maybe memorize the answers in case you run across a bad interviewer.</p>

<h3 id="chapter-7-oop">Chapter 7: OOP</h3>

<p>These questions are good. The provided answers are idiomatic Java; if you’re answering in Python you should model your answers after idiomatic Python rather than the answers given.</p>

<p>I don’t hear about people being asked abstract questions like 7.3.</p>

<h3 id="chapter-8-recursion">Chapter 8: Recursion</h3>

<p>Many of these questions are classics which I’ve been asked many times before.</p>

<h3 id="chapter-9-searching-and-sorting">Chapter 9: Searching and sorting</h3>

<p>I think that it’s somewhat less common to be asked to write sorting algorithms these days.</p>

<h3 id="chapter-10-math">Chapter 10: Math</h3>

<p>I don’t hear about math questions being asked, except at extremely math heavy companies to applicants with a hardcore math background.</p>

<h3 id="chapter-11-testing">Chapter 11: Testing</h3>

<p>I don’t hear about these questions very much. I’ve never heard questions like “how would you test a pen”.</p>

<h3 id="chapter-12-system-design-and-memory-limits">Chapter 12: System design and memory limits</h3>

<p>I don’t hear about memory limit questions very often.</p>

<h3 id="chapter-13-and-14-c-and-java">Chapter 13 and 14: C++ and Java</h3>

<p>Only relevant if you’re interviewing in C++ or Java, and even then I’d be slightly surprised to get these questions–they seem somewhat like trivia to me. That said, I’ve been asked one of these questions before.</p>

<h3 id="chapter-15-databases">Chapter 15: Databases</h3>

<p>The advice given is somewhat out of date. I don’t think that I’ve ever heard about someone being asked to denormalize their SQL database schema for speed in an interview.</p>

<p>I think the “design a database to hold certain data” type questions are more common now. The rise of ORMs has led to SQL being somewhat less important for most developers.</p>

<h3 id="chapter-16-low-level">Chapter 16: Low level</h3>

<p>These questions are great, but I wouldn’t expect them at most generalist job interviews.</p>

<h3 id="chapter-17-networking">Chapter 17: Networking</h3>

<p>This is somewhat more low level than I think most people get asked. It’s worth knowing the answer to the famous “what happens after you type a URL into a browser” question though.</p>

<h3 id="chapter-18-threads-and-locks">Chapter 18: Threads and Locks</h3>

<p>Companies don’t seem to ask about concurrency primitives without warning you beforehand. Some of these questions are Java-specific.</p>

<h3 id="chapter-19-and-20-moderate-and-hard">Chapter 19 and 20: Moderate and Hard</h3>

<p>All these questions are popular questions. I don’t particularly like them.</p>

  </div>

  <hr>

  <h2><a href="/2016/06/16/kth-richest.html"> A data structure for range kth-smallest queries</a></h2>

  <p class="post-meta"><time datetime="2016-06-16T00:00:00-07:00" itemprop="datePublished">Jun 16, 2016</time></p>

  <div class="shrink-headings">
    <p>Suppose I want to maintain a set of people where each has an age and a wealth. I want to be able to quickly insert people, delete people, and answer queries of the form “find the $latex k$th richest person whose age is between $latex x$ and $latex y$”.</p>

<p>Here is a summary of solutions to different variants on this question:</p>

<table class="table" id="table1">
  <tr>
    <th>Variation</th>
    <th>Solution details</th>
    <th>Space</th>
    <th>Update time</th>
    <th>Query time</th>
  </tr>
  <tr>
    <td>No insertion or deletion, one-sided interval</td>
    <td><a href="http://stackoverflow.com/a/31162190/1360429">Persistent binary search trees</a></td>
    <td>$latex n \cdot \log(n)$</td>
    <td>N/A</td>
    <td>$latex \log(n)$</td>
  </tr>
  <tr>
    <td>No insertion or deletion, two-sided interval</td>
    <td><a href="http://stackoverflow.com/questions/26296624/order-statistic-on-intervals/26299986#26299986">Persistent binary search trees</a></td>
    <td>$latex n \cdot \log(n)$</td>
    <td>N/A</td>
    <td>$latex \log(n)$</td>
  </tr>
  <tr>
    <td>Insertion, deletion, two-sided interval</td>
    <td>My answer, presented here</td>
    <td>$latex n \cdot \log(n)$</td>
    <td>$latex \log^2(n)$</td>
    <td>$latex \log^3(n)$</td>
  </tr>
</table>

<p>I have been somewhat interested in this question for about a year, and I’ve asked about related questions <a href="https://www.facebook.com/bshlgrs/posts/10205556609689335">a few</a> <a href="http://stackoverflow.com/questions/31153033/data-structure-to-support-a-particular-query-on-a-set-of-2d-points">times</a>. But no-one’s ever managed to give me a complete answer.</p>

<p>The other day, I ran across <a href="http://stackoverflow.com/a/26299986/1360429">this StackOverflow answer</a>, which presents an answer which I modified to make a full solution, which I’ll present here.</p>

<p>My solution allows insertion and deletion in $latex O(\log^2(n))$, and allows the query in $latex O(\log^3(n))$.</p>

<h2 id="solution">Solution</h2>

<p>Store an <a href="https://en.wikipedia.org/wiki/Order_statistic_tree">order statistic tree</a> ordered on age. At every node, store a pointer to an auxiliary order statistic tree, of all of that node’s descendants ordered on income.</p>

<p>At every node, this requires an extra amount of memory which is linear in the number of descendants of that node. So this means that the tree will take $latex O(n\cdot \log(n))$ memory overall.</p>

<h3 id="query">Query</h3>

<p>You might want to go read <a href="http://stackoverflow.com/a/26299986/1360429">that second StackOverflow answer</a> again, because this algorithm is similar to that one, and it’s easier to understand the algorithm on arrays.</p>

<p>The query is similar to how queries across ranges of an augmented BST usually work: we start out by finding the set of nodes whose descendants contain the whole subsection of the tree that you care about. There will be $latex \log(n)$ of these nodes, and finding all of them takes $latex \log(n)$ time.</p>

<p>We end up with $latex \log(n)$ OSTs of maximum size $latex O(n)$, and we want to find the $latex k$th smallest item in their disjoint union.</p>

<p>As discussed <a href="/2016/06/16/generalized-multi-quickselect.html">here</a>, we can solve that query in $latex O\left(\log(n)^3\right)$.</p>

<p>Alternatively, if your $latex k$ is small, you can directly traverse the trees to find the correct answer, which takes $latex k \log(n) + n$ time.</p>

<h3 id="updates">Updates</h3>

<p>When I insert a new value into my set, I need to update the auxiliary OSTs of all of the ancestor nodes of my new node.</p>

<p>Usually, it’s easy to argue that maintaining auxiliary data in your OST is fast, because usually your auxiliary data is something like “the sum of your descendants” or something which is $latex O(1)$. In this data structure, the efficiency argument is somewhat more complicated.</p>

<p>Inserting a single item into an OST takes $latex O(\log(n))$ time. But making the OST from scratch takes $latex O(n)$. This is concerning because it means that tree rotations are potentially extremely expensive. If I had to do tree rotations all the way up from my new node to the root of the tree every single insertion, then insertion would take linear time.</p>

<p>Luckily, we can decide that our OST is balanced using the red-black tree rules. Insertion in a red-black tree only involves amortized $latex O(1)$ rotations. (See <a href="web.stanford.edu/class/cs166/lectures/05/Small05.pdf">here</a> for an explanation of this.)</p>

<p>The node at the lowest level will have to totally regenerate its auxiliary OST every insertion, of course. Its parent will have to do a tree rotation which requires it to totally regenerate its auxiliary OST every second insertion. Its grandparent will need to do that $latex \frac14$ of the time. And then $latex \frac18$ and so on.</p>

<p>At height $latex h$ in the tree, defining the leaves to be $latex h=0$, the amortized cost of insertion is going to be $latex O(\log(2^h)) = O(h)$ for insertion plus $latex O\left(\frac{2^h}{2^h}\right) = O(1)$ for totally recreating the OST after a rebalance, for a total cost of $latex O(h)$.</p>

<p>The total time required for updating all the auxiliary OSTs after an insert is therefore:</p>

<script type="math/tex; mode=display">O\left(\sum_{h=0}^{log(n)} h \right)= O\left(\log(n)^2\right)</script>

<p>Updating or deleting a node also takes $latex O\left(\log(n)^2\right)$, for the same reason.</p>

<h2 id="variations">Variations</h2>

<h3 id="limited-latex-k">limited $latex k$</h3>

<p>If $latex k$ is always going to fixed below a particular limit $latex l$–say, you know ahead of time that you’re never going to need to know farther back than the 50th richest person between two ages–each node in your main OST can store a smaller auxiliary tree with only $latex l$ elements in it.</p>

<p>This reduces memory requirements to $latex O(n \cdot l)$.</p>

<p><strong>Queries</strong>: Using the algorithm for OSTs in <a href="/2016/06/16/generalized-multi-quickselect.html#table1">this table</a>, the cost is now $latex \log(\log(n) \cdot l) \cdot \log(n) \cdot \log(l)$, which looks like $latex \log(\log(n)) \cdot \log(n) \cdot \log(l)$ as $latex n$ grows.</p>

<p><strong>Updates</strong>: Every ancestor needs to do $latex \log(l)$ work now, instead of $latex \log(n)$, but you still have $latex \log(n)$ ancestors. So update takes overall $latex \log(n)\log(l)$ time.</p>

  </div>

  <hr>

  <h2><a href="/2016/06/16/generalized-multi-quickselect.html"> Generalized multi-quickselect</a></h2>

  <p class="post-meta"><time datetime="2016-06-16T00:00:00-07:00" itemprop="datePublished">Jun 16, 2016</time></p>

  <div class="shrink-headings">
    <p>I’ve been thinking a lot recently about how you find the $latex k$th smallest element in the disjoint union of several data structures.</p>

<h2 id="multi-quickselect-on-data-structures-with-fast-rank">Multi-quickselect on data structures with fast <code class="highlighter-rouge">rank</code>
</h2>

<p>Yesterday I <a href="/2016/06/15/multi-sorted-array-quickselect">came up with an algorithm similar to quickselect</a> for the version of this problem where you have multiple sorted arrays.</p>

<p>But this algorithm also works on any other data structure which supports the <code class="highlighter-rouge">rank</code> method in $latex O(\log(n))$ time.</p>

<p>On a collection of data structures with a total of $latex O(t)$ elements, the modified quickselect algorithm involves $latex O(log(t))$ iterations. In each iteration, the rank of a random element is computed in every data structure.</p>

<p>So the total time taken is (time to calculate rank in every data structure) * (log of total number of elements).</p>

<table class="table" id="table1">
  <tr>
    <th>Collection</th>
    <th>Time complexity</th>
  </tr>
  <tr>
    <td>$latex m$ sorted arrays of average length $latex n$</td>
    <td>$latex \log(m \cdot n) \cdot m \cdot \log(n)$</td>
  </tr>
    <tr>
    <td>$latex m$ sorted arrays of total length $latex t$</td>
    <td>$latex \log(t) \cdot m \cdot \log(\frac tm)$</td>
  </tr>
  <tr>
    <td>Two sorted arrays, of lengths $latex a$ and $latex b$</td>
    <td>$latex \log(a + b)^2$</td>
  </tr>
  <tr>
    <td>$latex m$ order statistic trees of average size $latex n$</td>
    <td>$latex \log(m \cdot n) \cdot m \cdot \log(n)$</td>
  </tr>
  <tr>
    <td>$latex \log(n)$ sorted arrays of sizes $latex [1, 2, 4, ...n]$</td>
    <td>$latex \log(n)^3$ </td>
  </tr>
</table>

<p>(See <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> for more details on the exponentially shrinking arrays.)</p>

<p>For data structures with $latex O(log(n))$ <code class="highlighter-rouge">rank</code>, I think this is optimal.</p>

<h2 id="allowing-data-structures-without-fast-rank">Allowing data structures without fast <code class="highlighter-rouge">rank</code>
</h2>

<p>How about if some of your data structures are unsorted?</p>

<p>This algorithm needs to be modified, because there’s no particular guarantee that any data structure gets smaller on a particular iteration. This is okay in cases where <code class="highlighter-rouge">rank</code> is cheap enough that the asymptotic complexity isn’t affected by having a data structure which stays at its original size for most of the runtime of the algorithm. But on an unsorted array, <code class="highlighter-rouge">rank</code> takes $latex O(n)$ and a key part of the argument for the good runtime of quickselect is that the unordered array usually gets smaller every time you call <code class="highlighter-rouge">rank</code>.</p>

<p>This issue is why my attempt at an <a href="/2016/06/12/quickselect-lemma.html">optimal algorithm for selection on an OST and an unsorted array</a> was so complex.</p>

<p>Obviously there’s not going to be a sublinear time solution to this problem. So we might as well take linear time to add all our unsorted structures together into an unsorted array, in linear time. So we only need to consider the problem where we have a single unsorted array.</p>

<h3 id="non-optimal-solution">Non-optimal solution</h3>

<p>When I have a collection of data structures such that <code class="highlighter-rouge">rank</code> takes time $latex O(r)$ and <code class="highlighter-rouge">select</code> takes time $latex O(s)$, I can run <a href="/2016/06/12/quickselect-lemma.html">my <code class="highlighter-rouge">double_quickselect_v2</code> algorithm</a> on an unordered array of length $latex n$ and that collection, with query time $latex O(\log(n) \cdot r + s)$. For example, this algorithm can deal with a sorted array of size $latex m$ and an unsorted array of size $latex n$ in overall $latex O(\log(n) \cdot \log(m) + \log(m)) = O(\log(m)\cdot\log(n))$.</p>

<h3 id="what-an-optimal-solution-might-look-like">What an optimal solution might look like</h3>

<p>I bet we can generalize my alleged <a href="/2016/06/12/quickselect-lemma.html">optimal algorithm for selection on an OST and an unsorted array</a>.</p>

<h2 id="summary">Summary</h2>

<p>I have a bunch of data structures and want to find the $latex k$th smallest item in their union. How long will it take me?</p>

<table class="table" id="table1">
  <tr>
    <th>Collection</th>
    <th>Algorithm</th>
    <th>Time complexity</th>
  </tr>
  <tr>
    <td>Unsorted array of size $latex n$</td>
    <td>Median of medians</td>
    <td>Worst case $latex O(n)$</td>
  </tr>
  <tr>
    <td>Sorted array of size $latex n$</td>
    <td>Binary search</td>
    <td>Worst case $latex O(\log(n))$</td>
  </tr>
  <tr>
    <td>Order statistic tree</td>
    <td>its native `find` implementation</td>
    <td>Worst case $latex O(\log(n))$</td>
  </tr>
  <tr>
    <td>A bunch of unsorted data structures, of total size $latex O(n)$</td>
    <td>Stick it all in an array then call median of medians</td>
    <td>Worst case $latex O(\log(n))$</td>
  </tr>
  <tr>
    <td>$latex m$ data structures which support <code>rank</code> in $latex O(\log(n))$, with maximum size $latex O(n)$</td>
    <td><a href="/2016/06/15/multi-sorted-array-quickselect.html">Multi sorted array quickselect</a></td>
    <td>Average case $latex O(m \cdot \log(n) \cdot \log(m \cdot n))$</td>
  </tr>
  <tr>
    <td>$latex m$ data structures which support <code>rank</code> in $latex O(\log(n))$, with maximum size $latex O(n)$, and also a bunch of unsorted data with total size $latex u$</td>
    <td>"Non-optimal solution" as described above</td>
    <td>Average case $latex O(\log(u) \cdot m \cdot \log(n) +\\ m \cdot \log(n) \cdot \log(m \cdot n))$</td>
  </tr>
</table>

<p>I suspect that I can improve upon most of the algorithms listed there that I invented myself; I’ll keep this table updated.</p>

<p>If you know a faster algorithm for one of these problems, please let me know!</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">

      <p>There are a total of $latex 2\cdot n - 1 = O(n)$ elements in those $latex n$ arrays. The inner loop will happen $latex \log(n)$ times.</p>

      <p>Each iteration will need to do a binary search within its array. On the first iteration the time taken will be $latex \sum_{i=0}^{\log(n)} \log(2^i) = O(\log(n)^2)$. Further iterations obviously won’t be slower than that. So we can bound above this runtime by $latex \log(n)$.</p>

      <p>We can also give a proof sketch for bounding it below. Suppose that all our arrays have roughly the same distribution, so that on the $latex w$th iteration, every array has a size of only $latex 2^{-w}$ its original size.</p>

      <script type="math/tex; mode=display">% <![CDATA[
\begin{align} &\sum_{w=0}^{\log(n)} \sum_{i=0}^{\log(n)} \log\left(max\left(2^i \cdot 2^{-w}, 0\right)\right)  \\
        = &\sum_{w=0}^{\log(n)} \sum_{i=0}^{w} i  \\
        = &\sum_{w=0}^{\log(n)} O\left( w^2 \right) \\
        = &O\left(\log(n)^3\right) \end{align} %]]></script>
      <p><a href="#fnref:1" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

  </div>

  <hr>


<p><a href="/posts">All posts</a></p>

<p><a href="/feed.xml">RSS feed</a></p>

<script>
$(function() {
  $(".shrink-headings").map(function(idx, div) {
    $(div).html(
      $(div)
        .html()
        .replace(/<h4/g, "<h5")
        .replace(/<h3/g, "<h4")
        .replace(/<h2/g, "<h3")
    )
  })
})
</script>


      <hr>

<div class="PageNavigation">
  
  
</div>

    </div>
    <div class="col-sm-3 col-sm-offset-1">
      <img src="https://scontent-sjc2-1.xx.fbcdn.net/hphotos-xtp1/t31.0-8/11154688_10205041372168719_3725604149367069581_o.jpg" class="img-responsive" alt="Picture of Buck">
      
<hr>
<a class="arrow" href="/"><strong>Buck</strong></a>
<ul>
  <li>
<a class="arrow" href="/about">About</a>
    <ul>
      <li><a href="http://triplebyte.com?ref=bshlgrs.github.io">Triplebyte</a></li>
    </ul>
  </li>
  <li>Links
    <ul>
      <li>
          <a class="arrow" href="http://github.com/bshlgrs">GitHub</a>
      </li>
      <li>
          <a class="arrow" href="mailto:bshlegeris@gmail.com">Email</a>
      </li>
      <li>
          <a class="arrow" href="http://www.facebook.com/bshlgrs">Facebook</a>
      </li>
      <li>
          <a class="arrow" href="http://lnkd.in/bnBJ6EF">LinkedIn</a>
      </li>
    </ul>
  </li>
  <li><a class="arrow external" href="https://docs.google.com/forms/d/1SOombLPHlKIMut-wJzIRg7DGdJjh9PJV4yAkrmTXKn4/viewform?usp=send_form">Anonymous feedback</a></li>

  <li>
<a href="/posts" class="arrow">Blog</a>
    <ul>
      
        <li>
          <a class="arrow" href="/2016/07/02/graph.html">Terse notes on graph algorithms</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/07/02/sms.html">SMS vs email responsiveness</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/07/02/poisson.html">Testing whether two integers are different</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/07/02/hash-ordered-treaps.html">Hash-ordered treaps</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/06/30/triplebyte.html">You should work at Triplebyte</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/06/22/ctci.html">Studying for startup interviews with 'Cracking the Coding Interview'</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/06/16/kth-richest.html">A data structure for range kth-smallest queries</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/06/16/generalized-multi-quickselect.html">Generalized multi-quickselect</a>
        </li>
      
    </ul>
  </li>
  <li>
      <a class="arrow" href="/cute">Pictures of me</a>
  </li>
</ul>

    </div>
  </div>
</div>




    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Buck Shlegeris</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Buck Shlegeris</li>
          <li><a href="mailto:bshlegeris@gmail.com">bshlegeris@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/bshlgrs"><span class="icon icon--github"><svg viewbox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path></svg>
</span><span class="username">bshlgrs</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/bshlgrs"><span class="icon icon--twitter"><svg viewbox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path></svg>
</span><span class="username">bshlgrs</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Website of Buck Shlegeris.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52980069-1', 'auto');
  ga('send', 'pageview');

</script>

</html>
