<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">

  <title>Buck Shlegeris</title>
  <meta name="description" content="Website of Buck Shlegeris.
">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/">
  <link rel="alternate" type="application/rss+xml" title="Buck Shlegeris" href="http://localhost:4000/feed.xml">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$$', '$$'] ],
        displayMath: [ ['$^$', '$^$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
  </script>
  <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
  <script src="/jquery.js"></script>
  <script type="text/babel" src="/main.js"></script>
  
</head>


  <body>

    

    <div class="container">
  <div class="row">
    <div class="sidebar-on-large-screens">
      <img src="/img/dual_n_buck.jpeg" class="img-responsive" alt="Picture of Buck" height="213" width="213">
<br/>
<a class="arrow" href="/"><strong>Buck Shlegeris</strong></a>
<ul>
  <li><a class="arrow" href="/about">About</a>
    <ul>
      <li><a href="http://triplebyte.com?ref=shlegeris.com">Triplebyte</a></li>
    </ul>
  </li>
  <li>Links
    <ul>
      <li>
          <a class="arrow" href="http://github.com/bshlgrs">GitHub</a>
      </li>
      <li>
          <a class="arrow" href="mailto:bshlegeris@gmail.com">Email</a>
      </li>
      <li>
          <a class="arrow" href="http://www.facebook.com/bshlgrs">Facebook</a>
      </li>
      <li>
          <a class="arrow" href="http://lnkd.in/bnBJ6EF">LinkedIn</a>
      </li>
    </ul>
  </li>
  <li><a class="arrow external" href="/anonymous_feedback">Anonymous feedback</a></li>
  <li>Software to play with
    <ul>
      <li>
          <a class="arrow" href="http://ds.shlegeris.com/">Data structure search engine</a>
      </li>
      <li>
          <a class="arrow" href="https://bshlgrs.github.io/music-game/">Relative pitch game</a>
      </li>
      <li>
          <a class="arrow" href="http://shlegeris.com/gini">Gini coefficient tool</a>
      </li>
      <li>
          <a class="arrow" href="http://shlegeris.com/dice">Extremely serious election probability tool</a>
      </li>
    </ul>
  </li>
  <li><a href="/posts" class="arrow">Blog</a> (<a href="/best" class="arrow">Best posts</a>) (<a href="/feed.xml">RSS</a>)
    <ul>
      
        <li>
          <a class="arrow" href="/2017/01/06/hash-maps.html">Hash map implementations in practice</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/12/30/pain.html">Wild speculations on the balance of pain and pleasure</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/12/29/gini.html">A dynamic programming algorithm for the Gini coefficient</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/11/26/research.html">Optimal resource allocation between manufacturing and research</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/11/13/phd.html">Advice on whether you should get a PhD in engineering</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/11/13/ds.html">Building a search engine for data structures</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/10/23/poor-college-graduates.html">Poor college graduates do much better than rich high-school dropouts</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/10/20/libertarians.html">Left-libertarianism</a>
        </li>
      
    </ul>
  </li>
  <li>
      <a class="arrow" href="/talks">Talks</a>
  </li>
  <li>
      <a class="arrow" href="/to-prove-list">My "to-prove" list</a>
  </li>
  <li>
      <a class="arrow" href="/mistakes">Mistakes</a>
  </li>
  <li>
      <a class="arrow" href="/notes">Notes</a>
  </li>
  <li>
      <a class="arrow" href="/cute">Pictures of me</a>
  </li>
</ul>

    </div>

    <div class="main-column">
      <div class="title-on-small-screens">
        <a href="/">website of Buck Shlegeris</a>
      </div>

      <!-- <script async src="//genius.codes"></script> -->


<p class="post-meta"><time datetime="1969-12-31T16:00:00-08:00" itemprop="datePublished"></time></p>
<p>
  
</p>
<hr/>

<div class="post-content">
  <h1>Buck Shlegeris</h1>

<div class="lead">
<p>I am a software engineer from Australia. I have lived in San Francisco for most of the last three years. I'm interested in data structures, economics, and effective altruism.</p>
<p><a href="/best">Here</a> are the most interesting things I've written. My most notable software project is <a href="http://ds.shlegeris.com">a search engine for data structures</a>.</p>
<p>If, <a href="http://www.overcomingbias.com/2008/03/against-news.html">like many</a>, you prefer to consume content ordered by recency rather than quality, here are some things I wrote recently:</p>
</div>


  <div class="buck-post">
    <h2><a href="/2017/01/06/hash-maps.html"> Hash map implementations in practice</a></h2>

    <p>
      
        <a href="/tag/algorithms">algorithms</a>
      
        <a href="/tag/programming">programming</a>
      
    </p>

    <p class="post-meta"><time datetime="2017-01-06T00:00:00-08:00" itemprop="datePublished">Jan 6, 2017</time></p>

    <div class="shrink-headings">
      <p><strong>Thanks to Kent Ross for finding several of the details which I list here.</strong></p>

<p>I think hash maps are a really interesting data structure. They’re probably the second most widely used data structure in modern programming languages, behind dynamic arrays. But they involve many much more complicated design decisions than dynamic arrays do. So they’re probably the data structure where we can learn the most about how they’re used in practice.</p>

<p>(I don’t mean to imply dynamic arrays don’t have any interesting subtleties. To start with, there’s a way to modify dynamic arrays so that their <script type="math/tex">O(1)</script> time to append is worst case instead of amortized. There’s also a fun way to make them so that they only waste <script type="math/tex">O(\sqrt{n})</script> space instead of <script type="math/tex">O(n)</script>. <a href="https://cs.uwaterloo.ca/~imunro/cs840/ResizableArrays.pdf">This paper</a> is about the latter; it explains the former in passing.)</p>

<p>In this post, I’ll briefly describe a few of the key ways that hash map implementations vary, and then I’ll do a review of the hash map implementations in Ruby, Python, Go, Rust, Java, C++, PHP, and C#, as well as some other influential implementations such as those in Google’s SparseHash package.</p>

<h2 id="key-concepts-in-hash-map-design">Key concepts in hash map design</h2>

<p>Hash maps differ in their collision resolution strategies. The simplest strategy to implement is probably chaining. There’s a decent explanation of chaining <a href="http://www.algolist.net/Data_structures/Hash_table/Chaining">here</a>. When you’re implementing a hash map with chaining, you need to choose a maximum load factor–that is, the number of elements per bucket at which you resize the hash map. Most languages which use chaining seem to use a load factor of about 5x. Another fun detail here is that you can use data structures that aren’t linked lists for your buckets. For example, Java switches buckets to be balanced trees instead of linked lists if they have more than 8 elements, in an effort to make the worst case runtime logarithmic instead of linear.</p>

<p>A wide variety of different open addressing strategies are also used in practice–linear probing, quadratic probing, double hashing, and others.</p>

<p>One exciting and relatively recent development in hash map implementations is Robin Hood hashing. Here’s a quote from a <a href="http://www.sebastiansylvan.com/post/robin-hood-hashing-should-be-your-default-hash-table-implementation/">great article about it</a>:</p>

<blockquote>
  <p>The clever trick is just this: when you probe for a position to insert a new element, if the probe length for the existing element is less than the current probe length for the element being inserted, swap the two elements and keep going.</p>

  <p>That way elements that were inserted early and thus “lucked out” on their probe lengths, will gradually be moved away from their preferred slot as new elements come in that could make better use of that place in the table (hence the name - the insertion “takes from the rich”, i.e. the elements with low probe counts). It leads to an “evening out” of the probe lengths.</p>

  <p>Why is low variance better? Well, with modern cache architectures a probe count of 1 isn’t really much faster than a probe count of 3, because the main cost is fetching the cache line, so the ideal scenario is for all elements to have the same probe count, and having that probe count fitting within one cache line.</p>

  <p>It turns out that Robin Hood hashing has the same expected probe count as normal open addressing (about 2.55) - it just has substantially less variance, and therefore ends up with far fewer cache misses. Yes, there are fewer elements that can early out after 1 probe, but there also far fewer elements that end up needing to fetch multiple cache lines because of long probe lengths.</p>
</blockquote>

<p>Currently, Rust is the only mainstream language which uses Robin Hood hashing in its default hash map implementation.</p>

<h2 id="hash-maps-in-practice">Hash maps in practice</h2>

<h3 id="python">Python</h3>

<p>Python’s dictionary implementation uses an open addressing scheme. It resizes when the hash map is 2/3 full. You can read the source code <a href="https://github.com/python/cpython/blob/master/Objects/dictobject.c">here</a>.</p>

<p>Unlike most programming languages, Python doesn’t try to use a hash function which appears random. To quote the source:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Major subtleties ahead:  Most hash schemes depend on having a "good" hash
function, in the sense of simulating randomness.  Python doesn't:  its most
important hash functions (for ints) are very regular in common
cases:

  &gt;&gt;&gt;[hash(i) for i in range(4)]
  [0, 1, 2, 3]

This isn't necessarily bad!  To the contrary, in a table of size 2**i, taking
the low-order i bits as the initial table index is extremely fast, and there
are no collisions at all for dicts indexed by a contiguous range of ints. So
this gives better-than-random behavior in common cases, and that's very
desirable.

OTOH, when collisions occur, the tendency to fill contiguous slices of the
hash table makes a good collision resolution strategy crucial.  Taking only
the last i bits of the hash code is also vulnerable:  for example, consider
the list [i &lt;&lt; 16 for i in range(20000)] as a set of keys.  Since ints are
their own hash codes, and this fits in a dict of size 2**15, the last 15 bits
of every hash code are all 0:  they *all* map to the same table index.

But catering to unusual cases should not slow the usual ones, so we just take
the last i bits anyway.  It's up to collision resolution to do the rest.  If
we *usually* find the key we're looking for on the first try (and, it turns
out, we usually do -- the table load factor is kept under 2/3, so the odds
are solidly in our favor), then it makes best sense to keep the initial index
computation dirt cheap.
</code></pre>
</div>

<p>In Python 3.6, an additional layer of indirection <a href="https://mail.python.org/pipermail/python-dev/2012-December/123028.html">was added</a>, with the following reasoning:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>The current memory layout for dictionaries is
unnecessarily inefficient.  It has a sparse table of
24-byte entries containing the hash value, key pointer,
and value pointer.

Instead, the 24-byte entries should be stored in a
dense table referenced by a sparse table of indices.

For example, the dictionary:

    d = {'timmy': 'red', 'barry': 'green', 'guido': 'blue'}

is currently stored as:

    entries = [['--', '--', '--'],
               [-8522787127447073495, 'barry', 'green'],
               ['--', '--', '--'],
               ['--', '--', '--'],
               ['--', '--', '--'],
               [-9092791511155847987, 'timmy', 'red'],
               ['--', '--', '--'],
               [-6480567542315338377, 'guido', 'blue']]

Instead, the data should be organized as follows:

    indices =  [None, 1, None, None, None, 0, None, 2]
    entries =  [[-9092791511155847987, 'timmy', 'red'],
                [-8522787127447073495, 'barry', 'green'],
                [-6480567542315338377, 'guido', 'blue']]
</code></pre>
</div>

<p>This significantly reduces memory usage. It also means that Python dictionaries are now ordered, which makes <a href="https://news.ycombinator.com/item?id=12460936">Hacker News</a> (and me) unhappy.</p>

<p>Since version 3.3, Python dicts double in size when they resize. Before version 3.3, it <a href="https://github.com/python/cpython/blob/master/Objects/dictobject.c#L401-L411">quadrupled its capacity on resize</a>.</p>

<h3 id="v8">V8</h3>

<p><a href="https://github.com/v8/v8/blob/master/src/base/hashmap.h">Source here</a>.</p>

<p>This uses open addressing and has a maximum load capacity of 80%.</p>

<p><a href="https://github.com/v8/v8/blob/master/src/base/hashmap.h#L193-L202">One interesting detail</a>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>// To remove an entry we need to ensure that it does not create an empty
// entry that will cause the search for another entry to stop too soon. If all
// the entries between the entry to remove and the next empty slot have their
// initial position inside this interval, clearing the entry to remove will
// not break the search. If, while searching for the next empty entry, an
// entry is encountered which does not have its initial position between the
// entry to remove and the position looked at, then this entry can be moved to
// the place of the entry to remove without breaking the search for it. The
// entry made vacant by this move is now the entry to remove and the process
// starts over.
</code></pre>
</div>

<h3 id="java">Java</h3>

<p>Java’s HashMap class <a href="http://netjs.blogspot.in/2015/05/how-hashmap-internally-works-in-java.html">uses chaining</a>, but with a neat twist!</p>

<blockquote>
  <p>in Java 8 hash elements use balanced trees instead of linked lists after a certain threshold is reached. Which means HashMap starts with storing Entry objects in linked list but after the number of items in a hash becomes larger than a certain threshold, the hash will change from using a linked list to a balanced tree, this will improve the worst case performance from O(n) to O(log n).</p>
</blockquote>

<p>According to <a href="http://www.nurkiewicz.com/2014/04/hashmap-performance-improvements-in.html">this page</a> the threshold is 8 elements:</p>

<blockquote>
  <p>Well, this optimization is described in <a href="http://openjdk.java.net/jeps/180">JEP-180</a>. Basically when a bucket becomes too big (currently: <code class="highlighter-rouge">TREEIFY_THRESHOLD = 8</code>), <code class="highlighter-rouge">HashMap</code> dynamically replaces it with an ad-hoc implementation of tree map. This way rather than having pessimistic O(n) we get much better O(logn). How does it work? Well, previously entries with conflicting keys were simply appended to linked list, which later had to be traversed. Now <code class="highlighter-rouge">HashMap</code> promotes list into binary tree, using hash code as a branching variable. If two hashes are different but ended up in the same bucket, one is considered bigger and goes to the right. If hashes are equal (as in our case), <code class="highlighter-rouge">HashMap</code> hopes that the keys are <code class="highlighter-rouge">Comparable</code>, so that it can establish some order. This is not a requirement of <code class="highlighter-rouge">HashMap</code> keys, but apparently a good practice. If keys are not comparable, don’t expect any performance improvements in case of heavy hash collisions.</p>
</blockquote>

<p>So if you implement <code class="highlighter-rouge">Comparable</code> properly, hash map retrieval is worst case <script type="math/tex">O(log(n))</script>! What an exciting world we live in!</p>

<h3 id="c-stl">C++ STL</h3>

<p><a href="http://stackoverflow.com/a/31113618/1360429">chaining</a>. Amusingly enough, this seem to be a direct result of a requirement in the C++ standard:</p>

<blockquote>
  <p>The Standard effectively mandates std::unordered_set and std::unordered_map implementations that use open hashing, which means an array of buckets, each of which holds the head of a logical (and typically actual) list. That requirement is subtle: it’s a consequence of the default max load factor being 1.0 and the guarantee that the table will not be rehashed unless grown beyond that load factor: that would be impractical without chaining, as the collisions with closed hashing become overwhelming as the load factor approaches 1</p>
</blockquote>

<h3 id="linux-hashtable">Linux hashtable</h3>

<p>Linux has a <a href="http://lxr.free-electrons.com/source/include/linux/hashtable.h">fixed sized hashtable which uses chaining</a>, which it uses internally <a href="https://www.quora.com/How-are-hash-tables-implemented-in-Linux-Kernel-How-do-they-work-for-different-data-types-and-structures/answer/Davidlohr-Bueso">a bunch of places</a>.</p>

<h3 id="google-sparsehashmap-and-densehashmap">Google SparseHashMap and DenseHashMap</h3>

<p>Google SparseHashMap and DenseHashMap: <a href="https://github.com/sparsehash/sparsehash">quadratic probing</a></p>

<blockquote>
  <p>This directory contains several hash-map implementations, similar in API to SGI’s hash_map class, but with different performance characteristics.  sparse_hash_map uses very little space overhead, 1-2 bits per entry.  dense_hash_map is very fast, particulary on lookup. (sparse_hash_set and dense_hash_set are the set versions of these routines.)  On the other hand, these classes have requirements that may not make them appropriate for all applications.</p>

  <p>All these implementation use a hashtable with internal quadratic probing.  This method is space-efficient – there is no pointer overhead – and time-efficient for good hash functions.</p>

  <p>…</p>

  <p>The usage of these classes differ from SGI’s hash_map, and other
 hashtable implementations, in the following major ways:</p>

  <p>1) dense_hash_map requires you to set aside one key value as the ‘empty bucket’ value, set via the set_empty_key() method.  This <em>MUST</em> be called before you can use the dense_hash_map.  It is illegal to insert any elements into a dense_hash_map whose key is equal to the empty-key.</p>

  <p>2) For both dense_hash_map and sparse_hash_map, if you wish to delete elements from the hashtable, you must set aside a key value as the ‘deleted bucket’ value, set via the set_deleted_key() method.  If your hash-map is insert-only, there is no need to call this method.  If you call set_deleted_key(), it is illegal to insert any elements into a dense_hash_map or sparse_hash_map whose key is equal to the deleted-key.</p>
</blockquote>

<h3 id="ruby">Ruby</h3>

<p>According to <a href="https://github.com/ruby/ruby/blob/trunk/st.c">the source</a>, Ruby used chaining (with a threshold load factor of 5x) until 2.4, when it decided to follow Python’s lead and switch to a similar scheme with open addressing and two separate tables. As my colleague Kent points out, the Ruby hash table is basically the same as the Python one, except Ruby misspells “perturb” as “perterb” for some reason, and in Ruby the hash code is shifted down 11 bits instead of 5 in each perturbation.</p>

<h3 id="rust">Rust</h3>

<p>In an exciting change of pace, Rust uses <a href="https://doc.rust-lang.org/std/collections/struct.HashMap.html">“linear probing with Robin Hood bucket stealing.”</a></p>

<p>This had a <a href="http://accidentallyquadratic.tumblr.com/post/153545455987/rust-hash-iteration-reinsertion">super neat bug</a> which led to a sequence of hash map insertions taking quadratic time.</p>

<p>To quote that Tumblr piece:</p>

<blockquote>
  <p>Surprisingly to me, the specific dynamics of Robin Hood hashing end up being relatively unimportant here; I believe that vanilla linear probing would exhibit similar behaviors. The key effect of Robin Hood hashing is just that it gives you confidence and/or hubris to push a table to 90% capacity, which greatly exacerbates the problem.</p>
</blockquote>

<p>I’m glad that Rust is doing the trailblazing here. I think that long term,  more languages should probably switch to some kind of Robin Hood hashing, and it’s nice that we’re working out these kinks now.</p>

<h3 id="c">C</h3>

<p>C# uses <a href="https://msdn.microsoft.com/en-us/library/ms379571(v=vs.80).aspx#datastructures20_2_topic5">open addressing with double hashing</a>. That article also contains this hilarious tidbit:</p>

<blockquote>
  <p>In an overloaded form of the Hashtable’s constructor, you can specify a loadFactor value between 0.1 and 1.0. Realize, however, that whatever value you provide, it is scaled down 72%, so even if you pass in a value of 1.0 the Hashtable class’s actual loadFactor will be 0.72. The 0.72 was found by Microsoft to be the optimal load factor, so consider using the default 1.0 load factor value (which gets scaled automatically to 0.72). Therefore, you would be encouraged to use the default of 1.0 (which is really 0.72).</p>
</blockquote>

<p>Also, that documentation consistently writes ‘rehasing’ when I am almost sure they mean ‘rehashing’. Apparently Ruby isn’t the only programming language which needs a copy editor.</p>

<h3 id="golang">Golang</h3>

<p>Golang <a href="https://golang.org/src/runtime/hashmap.go">uses chaining</a>:</p>

<blockquote>
  <p>A map is just a hash table. The data is arranged
into an array of buckets. Each bucket contains up to
8 key/value pairs. The low-order bits of the hash are
used to select a bucket. Each bucket contains a few
high-order bits of each hash to distinguish the entries
within a single bucket.</p>

  <p>If more than 8 keys hash to a bucket, we chain on
extra buckets.</p>

  <p>When the hashtable grows, we allocate a new array
of buckets twice as big. Buckets are incrementally
copied from the old bucket array to the new bucket array.</p>

</blockquote>

<p>According to the same file, the default average load factor at which a resizing is triggered is 6.5.</p>

<h3 id="php">PHP</h3>

<p>PHP <a href="http://nikic.github.io/2014/12/22/PHPs-new-hashtable-implementation.html">uses chaining</a>, with the additional constraint that PHP semantics require that PHP hash tables be ordered by default, which <a href="https://news.ycombinator.com/item?id=8787638">sparks controversy on HN</a>.</p>

<p>PHP’s hash map implementation uses the identity function as the hash function for integers. This has great performance in the case where your hash keys are a contiguous sequence of integers. However, it also means that if you have a hash map with capacity 64, and you insert a bunch of numbers which all have the same remainder mod 64, they’ll be in the same hash bucket, so insert will take linear time. Under this condition, inserts can take linear time. <a href="https://nikic.github.io/2011/12/28/Supercolliding-a-PHP-array.html">This blog post</a> shows how inserting 65536 evil elements can take 30 seconds.</p>

<!-- ### Others

- JRuby [uses chaining](https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/RubyHash.java).
 -->

<h2 id="other-topics-in-hash-maps">Other topics in hash maps</h2>

<p>If you want to learn more about hash maps, here are some topics to look up:</p>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Cuckoo_hashing">Cuckoo hashing</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Hopscotch_hashing">Hopscotch hashing</a></li>
  <li>Concurrent hash maps</li>
  <li>Distributed hash maps</li>
</ul>

    </div>
  </div>

  <div class="buck-post">
    <h2><a href="/2016/12/30/pain.html"> Wild speculations on the balance of pain and pleasure</a></h2>

    <p>
      
    </p>

    <p class="post-meta"><time datetime="2016-12-30T00:00:00-08:00" itemprop="datePublished">Dec 30, 2016</time></p>

    <div class="shrink-headings">
      <p><em>Epistemic status: mad speculation, mixed with math that’s so much fun that I find it hard to believe it’s useful. I’m serious about thinking these questions are important, I’m much less confident in any of my answers.</em></p>

<p>It’s kind of interesting that living creatures seem to feel both pain and pleasure, which seem like distinctly negative and positive experiences. When you construct a theory of reinforcement learning, you don’t see anything like this distinction between positive and negative sensations: reward is just a real-valued function, and the behavior of agents isn’t affected by any transformation of their reward function which is either a shift (eg you feel +3 happy about every outcome) or a positive scaling (eg your reward function is multiplied by two). Obviously humans aren’t literally reinforcement learners, but it’s surprising that our sense of reward is <em>so</em> different.</p>

<p>So why do humans experience reward as a function centered around zero? Why does it feel meaningful to think about what it would be like for all your sensations to be twice as powerful? And given that we seem to have experiences that work this way, what factors determine where the zero point for an organism is, and how powerful its sensations are?</p>

<hr />

<p>Suppose I’m trying to train a little robot to pick up trash for me. Every day, it goes out and picks up some trash and brings it home for inspection. I need to give it a reward signal so that it knows how well it’s done. The reward signal that works for it is slightly costly though: I need to either give it small chocolates as a reward, or small pieces of coal as punishment. The cost of a piece of chocolate or coal is $1, and the robot views one piece of coal as exactly as bad as a piece of chocolate is good.</p>

<p>There’s some distribution of robot success which I’m expecting. Let’s say the robot always brings home 10, 11, or 13 pieces of trash. I need to choose how to reward the robot. That is, I need to choose a function from the success of the robot to the reward experienced by the robot.</p>

<p>One really simple option is to just give the robot one chocolate per piece of trash that it brings home. But this is pointlessly inefficient: you’re always going to be giving the robot a bunch of chocolate regardless of its actual success level.</p>

<p>Or you could give the robot a number of chocolates equal to the amount of trash minus 10. This is more efficient—you give out 0, 1, or 3 chocolates.</p>

<p>More efficient still, you could give the robot a punishment of 1 when it brings home 10 pieces of trash, give it nothing for 11 pieces, and give it a reward of 2 for bringing home 13 pieces. This is the best option.</p>

<p>So given a distribution, we need to choose a zero point for the reward function of the robot. What’s the optimal choice? Why, it’s the point which minimizes the expected cost, so it’s the point with minimum expected absolute distance to a randomly chosen point in the distribution. The median of a distribution is the number with this property.</p>

<p>(Why? Suppose that you want to choose a point to minimize the sum of absolute distance to the set [10, 11, 13, 16, 20, 23, 28]. Which point is better out of 20 and 23? Well, if you go from 23 to 20 you’re only making the distance longer for the two points 23 and 28, and you’re making it shorter for all 5 smaller points. And all these distance changes are the same magnitude, so all that matters is how many points you’re getting closer to. So 20 is better than 23. So the sum of absolute distances is minimized if you choose the point in the middle of the list.)</p>

<p>Now, I don’t care about my little robot. But suppose you do care about my little robot. You might care about how it likes its reward scheme. You care about its mean reward, which is (-1 + 0 + 2)/3 = 0.33. This is positive, which is great—the robot has a life worth living!</p>

<p>So in this situation, the robot-master will set the zero point to the median amount of trash, and the utilitarian onlooker cares about the mean value of this reward. The mean value of the reward is going to be positive iff the mean of the trash distribution is greater than the median of the trash distribution.</p>

<p>The median of a distribution is greater than the mean if the distribution is skewed to the left:</p>

<p><img src="http://shlegeris.com/img/skewedness.svg" alt="" /></p>

<p>So left-skewed trash distributions will lead to overall sad robots.</p>

<hr />

<p>I’ve been thinking a lot recently about cases like this where one optimization function is choosing the reward function for another system to maximize some function of its own. For example, evolution tries to pick reward functions for animals to maximize the animal’s reproductive fitness. But evolution has a bunch of constraints on this choice of reward function, and it’s also a really stupid optimization process, so the reward function is only a pretty rough approximation of the fitness benefit of various actions.</p>

<p>For example, humans have a tendency to consume an unhealthy amount of sugar: this is because evolution gave me a reward function that tries to motivate me to get sugar, which made sense in the ancestral environment but doesn’t make as much sense now. Other examples are when people are scared of the dark, or when they watch porn and masturbate, or do drugs.</p>

<p>The way that optimization functions choose reward functions is crucially important for utilitarians. Understanding the factors which affect the shape of reward functions is essential to predicting the welfare of beings like: ems in a competitive em world, subroutines in a paperclip maximizer, subroutines used by a galactic human civilization, factory farms if they exist a thousand years in the future, and wild animals.</p>

<p>I have lots of speculations about all this. I’ve been having trouble writing it all up, because there are a lot of different angles to approach these issues from. But with this post, at least I’ve made a start. Let me know if you’re interested in hearing my less well-formed crazy ideas on this topic.</p>

<hr />

<p>The only academic discussion of this general question I’ve seen is in the 1995 paper “Towards Welfare Biology”, which was one of the first academic papers about the ethical importance of wild animal suffering, where Yew-Kwang Ng writes:</p>

<blockquote>
  <p>Our common sense recognition of the suffering of a typical non-surviving individual in most species may be supported by a simple argument based on evolution. We start by asking, why do we enjoy eating but suffer in starvation? The answer is that this genetic program provides us with the right incentives to do things favourable to survival. But why suffering? Why not just less enjoyment when starving and more enjoyment when eating? If the difference in the degrees of enjoyment between the two is big enough, we will still do the “right things”. However, the existence of suffering may be explained below.</p>

  <p>First, both enjoyment and suffering are costly in terms of energy requirement, tissue maintenance, etc. This is why we feel neutral most of the time when we are not starving, eating, having sex, etc. (It would be nice if we could be programmed to feel ecstatic most of the time.) Secondly, it is likely that the extra (or marginal) costs involved in having an extra unit of enjoyment (or suffering) increases with the amount of enjoyment (suffering). Viewed differently, we have diminishing marginal returns in both enjoyment and suffering per unit of cost. Thirdly, it is likely that the costs (generalized resource costs, not subjective welfare costs) of suffering are unlikely to be significantly less, and maybe actually more, than those of enjoyment.</p>
</blockquote>

<p>I used the first and third these claims in my robot story, and ignored the second.</p>

<p>If we also assume that the metabolic cost of rewards increases superlinearly with their absolute value, then some other things might happen. For example, if the cost of a reward is the square of its magnitude, then the reward distribution will have mean zero. More generally, if the cost of a reward is its magnitude to the power of p, then:</p>

<ul>
  <li>if p == 1, then the zero point is the median, and the creature has a life worth living iff the distribution is skewed to the right.</li>
  <li>if 1 &lt; p &lt; 2, again the creature has a life worth living iff the distribution is skewed to the right.</li>
  <li>if p == 2, the reward distribution has mean zero.</li>
  <li>if p &gt; 2, the creature has a life worth living iff the distribution is skewed to the left.</li>
</ul>

<hr />

<p>view Facebook comments <a href="https://www.facebook.com/bshlgrs/posts/10209575825887228">here</a></p>

    </div>
  </div>

  <div class="buck-post">
    <h2><a href="/2016/12/29/gini.html"> A dynamic programming algorithm for the Gini coefficient</a></h2>

    <p>
      
        <a href="/tag/algorithms">algorithms</a>
      
        <a href="/tag/programming">programming</a>
      
    </p>

    <p class="post-meta"><time datetime="2016-12-29T00:00:00-08:00" itemprop="datePublished">Dec 29, 2016</time></p>

    <div class="shrink-headings">
      <p>There’s a very simple <script type="math/tex">O(n)</script> DP algorithm for calculating the <a href="https://en.wikipedia.org/wiki/Gini_coefficient">Gini coefficient</a> of a sorted list. The code for it honestly looks almost simpler than the naive formula. Code is at the bottom.</p>

<p>If <script type="math/tex">x</script> is a sorted list of values of length <script type="math/tex">n</script>, then the Gini coefficient <script type="math/tex">G</script> is given by:</p>

<p>$^$ G = \frac{\sum_{i=0}^n \sum_{j=0}^n \text{abs}(x_i - x_j) }{2 \cdot n^2 \cdot \text{mean}(x)} $^$</p>

<p>Where does this come from?</p>

<p>Here’s one way of looking at it. The <a href="https://en.wikipedia.org/wiki/Mean_absolute_difference">mean absolute difference</a> of a distribution is the expected value of drawing two items from the distribution and returning the absolute value of their difference. Here’s a formula for the mean absolute difference <script type="math/tex">MD</script> of the sorted list <script type="math/tex">x</script> from before:</p>

<p>$^$MD = \frac{\sum_{i=0}^n \sum_{j=0}^n \text{abs}(x_i - x_j)}{n^2}$^$</p>

<p>Using this, we can write the Gini coefficient as</p>

<p>$^$G = \frac{MD}{2 \cdot \text{mean}(x)}$^$</p>

<p>This makes more sense. We divide by the mean because we want the Gini coefficient of a distribution to be unaffected by multiplying by a constant. (<script type="math/tex">\frac{MD}{\text{mean}(x)}</script> is known as <a href="https://en.wikipedia.org/wiki/Mean_absolute_difference#Relative_mean_absolute_difference">relative mean absolute distance</a>.) And then we divide by 2 so that our Gini coefficient varies between 0 meaning perfect equality and 1 meaning perfect inequality.</p>

<h2 id="calculating-it">Calculating it</h2>

<p>We can calculate this directly. But this takes <script type="math/tex">O(n^2)</script>, which is super slow.</p>

<p>Alternatively, we can use a dynamic programming approach to cut the runtime down to <script type="math/tex">O(n)</script> plus the cost of getting the list in sorted order (which is <script type="math/tex">O(n \log(n))</script> if we start out with an unsorted list of incomes, but we plausibly get our data in some other form than that).</p>

<p>Let’s look at the sum on the numerator of the Mean Absolute Difference formula:</p>

<p>$^$ \sum_{i=0}^n \sum_{j=0}^n \text{abs}(x_i - x_j) $^$</p>

<p>Here’s a table of absolute differences for items in the list <script type="math/tex">[1, 3, 4, 5]</script>.</p>

<style>
.table {
  margin: auto;
  text-align: center;
}

.table td,th {
  padding-left: 5px;
  padding-right: 5px;
}
</style>

<table class="table">
  <tbody>
    <tr><th> </th><th>1</th><th>3</th><th>4</th><th>5</th></tr>
    <tr><th>1</th><td>0</td><td>2</td><td>3</td><td>4</td></tr>
    <tr><th>3</th><td>2</td><td>0</td><td>1</td><td>2</td> </tr>
    <tr><th>4</th><td>3</td><td>1</td><td>0</td><td>1</td></tr>
    <tr><th>5</th><td>4</td><td>2</td><td>1</td><td>0</td></tr>
  </tbody>
</table>

<p>First thing to notice here is that the table is symmetrical around its diagonal. If we can calculate the sum of one of those sides, we’re done. Let’s choose the lower triangle.</p>

<table class="table">
  <tbody>
    <tr><th> </th><th>1</th><th>3</th><th>4</th><th>5</th></tr>
    <tr><th>1</th><td>0</td><td> </td><td> </td><td> </td></tr>
    <tr><th>3</th><td>2</td><td>0</td><td> </td><td> </td> </tr>
    <tr><th>4</th><td>3</td><td>1</td><td>0</td><td> </td></tr>
    <tr><th>5</th><td>4</td><td>2</td><td>1</td><td>0</td></tr>
  </tbody>
</table>

<p>Let’s see how to compute the sum of that triangle efficiently.</p>

<p>First, let’s compute a list <script type="math/tex">y</script> of prefix sums of <script type="math/tex">x</script>–that is, <script type="math/tex">y_i</script> is the sum of the first <script type="math/tex">i</script> elements in <script type="math/tex">x</script>. So <script type="math/tex">y_0 = 0</script>, and <script type="math/tex">y_i = y_{i-1} + x_{i-1}</script>.</p>

<p>Let’s see what this list looks like. Let’s also look at the sum of the elements in the <script type="math/tex">i</script>th row of the table.</p>

<table class="table">
  <tbody>
    <tr><th>$$i$$</th><th>0</th><th>1</th><th>2</th><th>3</th></tr>
    <tr><th>$$x_i$$</th><td>1</td><td>3</td><td>4</td><td>5</td></tr>
    <tr><th>$$y_i$$</th><td>0</td><td>1</td><td>4</td><td>8</td></tr>
    <tr><th>$$\sum_{j=0}^i x_i - x_j$$</th><td>0</td><td>2</td><td>4</td><td>7</td></tr>
  </tbody>
</table>

<p>Can we rewrite this</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  &\sum_{j=0}^{i} \left| x_i - x_j \right|\\
  = &\sum_{j=0}^{i} \left(x_i - x_j \right) \\
  = &i\cdot x_i - \left( \sum_{j=0}^i x_j \right)
  \end{align} %]]></script>

<p>Or, taking advantage of <script type="math/tex">y</script>:</p>

<p>$^$i\cdot x_i - \left( \sum_{j=0}^i x_j \right) =i \cdot x_i - y_i$^$</p>

<p>Feel free to verify that this is correct:</p>

<table class="table">
  <tbody>
    <tr><th>$$i$$</th><th>0</th><th>1</th><th>2</th><th>3</th></tr>
    <tr><th>$$x_i$$</th><td>1</td><td>3</td><td>4</td><td>5</td></tr>
    <tr><th>$$y_i$$</th><td>0</td><td>1</td><td>4</td><td>8</td></tr>
    <tr><th>$$\sum_{j=0}^i x_i - x_j$$</th><td>0</td><td>2</td><td>4</td><td>7</td></tr>
    <tr><th>$$i \cdot x_i - y_i$$</th><td>0</td><td>2</td><td>4</td><td>7</td></tr>
  </tbody>
</table>

<p>Now we can write a single <code class="highlighter-rouge">for</code> loop to calculate our sum of absolute differences. And so we can write a very fast function for the Gini coefficient of a list of incomes:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gini</span><span class="p">(</span><span class="n">incomes</span><span class="p">):</span>
    <span class="c"># skip this if it's unneeded</span>
    <span class="n">incomes</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="n">sum_of_absolute_differences</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">subsum</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">incomes</span><span class="p">):</span>
        <span class="n">sum_of_absolute_differences</span> <span class="o">+=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">subsum</span>
        <span class="n">subsum</span> <span class="o">+=</span> <span class="n">x</span>

    <span class="k">return</span> <span class="n">sum_of_absolute_differences</span> <span class="o">/</span> <span class="n">subsum</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">incomes</span><span class="p">)</span>
</code></pre>
</div>

<pre><code class="language-{.python}">&gt;&gt;&gt; gini([1, 1, 1])
0.0
&gt;&gt;&gt; gini([0, 0, 0, 0, 10])
0.8
&gt;&gt;&gt; gini([1, 2, 3, 4, 5])
0.26666666666666666

</code></pre>

<p>All of these match <a href="/gini">my what Gini coefficient visualizer says</a>.</p>

    </div>
  </div>


<p><a href="/posts">All posts</a></p>

<p><a href="/feed.xml">RSS feed</a></p>

<script>
$(() => {
  $(".shrink-headings").map(function(idx, div) {
    $(div).html(
      $(div)
        .html()
        .replace(/<h4/g, "<h5")
        .replace(/<h3/g, "<h4")
        .replace(/<h2/g, "<h3")
    )
  });
})
</script>
<script async src="//genius.codes"></script>

</div>


      <div class="PageNavigation">
        
        
      </div>
    </div>

    <div class="sidebar-on-small-screens">
      <img src="/img/dual_n_buck.jpeg" class="img-responsive" alt="Picture of Buck" height="213" width="213">
<br/>
<a class="arrow" href="/"><strong>Buck Shlegeris</strong></a>
<ul>
  <li><a class="arrow" href="/about">About</a>
    <ul>
      <li><a href="http://triplebyte.com?ref=shlegeris.com">Triplebyte</a></li>
    </ul>
  </li>
  <li>Links
    <ul>
      <li>
          <a class="arrow" href="http://github.com/bshlgrs">GitHub</a>
      </li>
      <li>
          <a class="arrow" href="mailto:bshlegeris@gmail.com">Email</a>
      </li>
      <li>
          <a class="arrow" href="http://www.facebook.com/bshlgrs">Facebook</a>
      </li>
      <li>
          <a class="arrow" href="http://lnkd.in/bnBJ6EF">LinkedIn</a>
      </li>
    </ul>
  </li>
  <li><a class="arrow external" href="/anonymous_feedback">Anonymous feedback</a></li>
  <li>Software to play with
    <ul>
      <li>
          <a class="arrow" href="http://ds.shlegeris.com/">Data structure search engine</a>
      </li>
      <li>
          <a class="arrow" href="https://bshlgrs.github.io/music-game/">Relative pitch game</a>
      </li>
      <li>
          <a class="arrow" href="http://shlegeris.com/gini">Gini coefficient tool</a>
      </li>
      <li>
          <a class="arrow" href="http://shlegeris.com/dice">Extremely serious election probability tool</a>
      </li>
    </ul>
  </li>
  <li><a href="/posts" class="arrow">Blog</a> (<a href="/best" class="arrow">Best posts</a>) (<a href="/feed.xml">RSS</a>)
    <ul>
      
        <li>
          <a class="arrow" href="/2017/01/06/hash-maps.html">Hash map implementations in practice</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/12/30/pain.html">Wild speculations on the balance of pain and pleasure</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/12/29/gini.html">A dynamic programming algorithm for the Gini coefficient</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/11/26/research.html">Optimal resource allocation between manufacturing and research</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/11/13/phd.html">Advice on whether you should get a PhD in engineering</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/11/13/ds.html">Building a search engine for data structures</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/10/23/poor-college-graduates.html">Poor college graduates do much better than rich high-school dropouts</a>
        </li>
      
        <li>
          <a class="arrow" href="/2016/10/20/libertarians.html">Left-libertarianism</a>
        </li>
      
    </ul>
  </li>
  <li>
      <a class="arrow" href="/talks">Talks</a>
  </li>
  <li>
      <a class="arrow" href="/to-prove-list">My "to-prove" list</a>
  </li>
  <li>
      <a class="arrow" href="/mistakes">Mistakes</a>
  </li>
  <li>
      <a class="arrow" href="/notes">Notes</a>
  </li>
  <li>
      <a class="arrow" href="/cute">Pictures of me</a>
  </li>
</ul>

    </div>

    <div class="clear" />
  </div>
</div>




    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Buck Shlegeris</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Buck Shlegeris</li>
          <li><a href="mailto:bshlegeris@gmail.com">bshlegeris@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/bshlgrs"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">bshlgrs</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/bshlgrs"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">bshlgrs</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Website of Buck Shlegeris.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>
<script>
if (window.location.hostname == "bshlgrs.github.io") {
  window.location.hostname = "shlegeris.com";
}
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52980069-1', 'auto');
  ga('send', 'pageview');

</script>

</html>

