---
layout: post
title:  "AI safety reading list"
date:   ""
---

Here’s a big list of most of the resources about AI safety that I like.

If you want to read six articles about this, here's the stuff I recommend:

- [Kelsey Piper's Vox explainer on AI risk](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment)
- [Summary of why people think we should maybe work on AI safety](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/)
- [Transcript of a talk from MIRI at Google about how we think about the technical problem](https://intelligence.org/2017/04/12/ensuring/)
- [A different perspective, from Paul Christiano](https://ai-alignment.com/prosaic-ai-control-b959644d79c2)
- [There’s No Fire Alarm for Artificial General Intelligence](https://intelligence.org/2017/10/13/fire-alarm/). In this, Eliezer argues that we won’t unambiguously know ahead of time that AGI is coming very soon.

----

A few notes about all this stuff:

- I think it's reasonable to split AI safety into the categories of: technical AI safety, forecasting, and AI strategy.
- There's widespread disagreement between AI safety researchers about how to think about safety problems, and what kinds of solutions might work, and what kind of work we should be doing at the moment.


-----

Here's a more complete list of resources that I've found useful. If you want to do an initial survey, I recommend just reading the first article in each section.

- Superintelligence
  - If you find this book kind of boring, I think it's okay to not read it initially, or just read the bits you find interesting.
- [Eliezer's stuff on Arbital](https://arbital.com/explore/ai_alignment/)
  - This is where most of Eliezer’s recent AI safety thoughts are.
  - I recommend Paul’s dissenting comments on many of these articles.
- Paul Christiano’s blog: [AI alignment](https://ai-alignment.com/)
  - The key ideas are listed in [this sequence](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd).
  - Responses:
    - [Eliezer’s challenges to capability amplification](https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal)
  - Paul's other blogs, which aren't as relevant but have a lot of excellent ideas:
    - [Sideways View](https://sideways-view.com/best/)
      -  One relevant post: [Takeoff speeds](https://sideways-view.com/2018/02/24/takeoff-speeds/)
    - [Ordinary Ideas](https://ordinaryideas.wordpress.com/best-of/)
    - [Rational Altruist](https://rationalaltruist.com/)
    - [LessWrong posts](https://www.lesswrong.com/users/paulfchristiano)
      - [one particular highlight](https://www.lesswrong.com/posts/QmWNbCRMgRBcMK6RK/the-absolute-self-selection-assumption). But this falls more into “weird philosophy” than “AI safety”.
    - Another: [is theoretical math useful?](https://www.lesswrong.com/posts/s5sy3qiknFs7ehsLA/the-value-of-theoretical-research)
- MIRI
  - [Nate’s google talk](https://intelligence.org/2017/04/12/ensuring/)—this is a nice intro to how MIRI thinks about the problem.
  - [There’s No Fire Alarm for Artificial General Intelligence](https://intelligence.org/2017/10/13/fire-alarm/). In this, Eliezer argues that we won’t unambiguously know ahead of time that AGI is coming very soon.
  - [2018 update](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/) is the best summary of MIRI's current strategy.
  - [Eliezer’s interview with Sam Harris](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/) is okay
  - [Embedded Agency](https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version). This is an introduction to the work done by MIRI's Agent Foundations team.
  - Logical induction is probably MIRI’s coolest published paper. The abridged paper is [here](https://intelligence.org/files/LogicalInductionAbridged.pdf).
- EA forum
  - [Daniel Dewey from Open Phil being skeptical of MIRI's HRAD agenda](http://effective-altruism.com/ea/1ca/my_current_thoughts_on_miris_highly_reliable/)
  - Ben Henry’s [AI Risk Literature Review](http://effective-altruism.com/ea/14w/2017_ai_risk_literature_review_and_charity/)
- [80000 Hours Podcast](https://80000hours.org/podcast/episodes/)
  - highlights:
    - Technical stuff: Dario, Jan
    - Policy stuff: Miles Brundage, Alan Dafoe
    - Other maybe relevant stuff: Holden, Julia Galef, Nick Beckstead
    - Weird futurism shit: Anders Sandberg, William MacAskill
- 80000 Hours articles on AI safety as a potentially high impact career option:
  - [Problem profile](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/)
- OpenPhil
  - [Potential risks from advanced AI: the philanthropic opportunity](https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity). OpenPhil was originally skeptical of AI safety (Holden wrote [this post](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si) in 2012 about his skepticism); this post was them explaining their interest in the topic when they were just starting their involvement. OpenPhil is worth understanding because they’re by far the biggest funder, and they have a lot of really good thinkers.
  - [Landscape of current work on potential risks from advanced AI](https://docs.google.com/document/d/16Te6HnZN2OEviYFA-42Tf9Pal_Idovtgr5Y1RGEPW_g/edit)
  - [Some background on our views regarding advanced artificial intelligence](https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence)
  - [Luke Muehlhauser on timelines](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines)
  - [Initial grant to MIRI, in which they are quite skeptical](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support)
  - [Second grant to MIRI, in which they become much more optimistic](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017)
  - [List of topics in AI alignment](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-philanthropy-project-ai-fellows-program#examples)
  - Holden’s ["Three Key Issues I’ve Changed My Mind About"](https://www.openphilanthropy.org/blog/three-key-issues-ive-changed-my-mind-about). The three are: importance of AI risk, value of EA, and how to think about feedback loops.
- OpenAI safety team
  - [Safety via debate](https://arxiv.org/abs/1805.00899)
  - How quickly has the amount of compute used in cutting edge ML research changed over time? Dario Amodei and Danny Hernandez [have data](https://blog.openai.com/ai-and-compute/).
- Papers
  - Concrete Problems in AI Safety: [https://arxiv.org/abs/1606.06565](https://arxiv.org/abs/1606.06565)
    - This was the first legitimate-looking mainstream paper on AI safety, written by some cool people.
  - [Cooperative inverse reinforcement learning](https://arxiv.org/abs/1606.03137)
    - I'm pretty skeptical of this idea, but I think it's worth reading at some point.
- AI Impacts, led by Katja Grace, tries to answer questions like “how often does technology rapidly get better”?
  - [https://aiimpacts.org/](https://aiimpacts.org/)
  - [Brain performance in FLOPS](https://aiimpacts.org/brain-performance-in-flops/)
  - [Costs of human-level hardware](https://aiimpacts.org/costs-of-human-level-hardware/)
  - Takeoff speeds: [https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/)
- Mailing lists:
  - Rohin Shah’s alignment mailing list is good for staying up-to-date: [http://rohinshah.com/alignment-newsletter/](http://rohinshah.com/alignment-newsletter/)
- job boards:
  - [https://80000hours.org/job-board/](https://80000hours.org/job-board/)
- Other reading lists:
  - Center for Human-Compatible AI [reading list](http://humancompatible.ai/bibliography)
  - MIRI [research guide](https://intelligence.org/research-guide/)
  - There are a few links [here](https://concepts.effectivealtruism.org/concepts/existential-risks-from-artificial-intelligence/)
