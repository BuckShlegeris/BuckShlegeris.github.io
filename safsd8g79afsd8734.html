<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <title>2017 Report on Consciousness and Moral Patienthood | Open Philanthropy Project

</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="shortcut icon" href="http://www.openphilanthropy.org/sites/all/themes/op_basic/favicon.ico" type="image/vnd.microsoft.icon" />
<link rel="shortlink" href="/node/858" />
<link rel="canonical" href="/some-initial-thoughts-moral-patients" />
<meta name="Generator" content="Drupal 7 (http://drupal.org)" />
  <style type="text/css" media="all">
@import url("http://www.openphilanthropy.org/modules/system/system.base.css?om4jnn");
@import url("http://www.openphilanthropy.org/modules/system/system.menus.css?om4jnn");
@import url("http://www.openphilanthropy.org/modules/system/system.messages.css?om4jnn");
@import url("http://www.openphilanthropy.org/modules/system/system.theme.css?om4jnn");
</style>
<style type="text/css" media="all">
@import url("http://www.openphilanthropy.org/modules/contextual/contextual.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/jquery_update/replace/ui/themes/base/minified/jquery.ui.core.min.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/jquery_update/replace/ui/themes/base/minified/jquery.ui.theme.min.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/jquery_update/replace/ui/themes/base/minified/jquery.ui.accordion.min.css?om4jnn");
</style>
<style type="text/css" media="all">
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/comment_notify/comment_notify.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/footnotes/footnotes.css?om4jnn");
@import url("http://www.openphilanthropy.org/modules/comment/comment.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/date/date_api/date.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/date/date_popup/themes/datepicker.1.7.css?om4jnn");
@import url("http://www.openphilanthropy.org/modules/field/theme/field.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/google_cse/google_cse.css?om4jnn");
@import url("http://www.openphilanthropy.org/modules/node/node.css?om4jnn");
@import url("http://www.openphilanthropy.org/modules/search/search.css?om4jnn");
@import url("http://www.openphilanthropy.org/modules/user/user.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/views/css/views.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/caption_filter/caption-filter.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/ckeditor/css/ckeditor.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/admin_menu/admin_menu.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/admin_menu/admin_menu_toolbar/admin_menu_toolbar.css?om4jnn");
</style>
<style type="text/css" media="all">
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/ctools/css/ctools.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/typogrify/typogrify.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/contrib/content_type_extras/css/content_type_extras.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/modules/custom/tableofcontents/tableofcontents.css?om4jnn");
</style>
<style type="text/css" media="screen">
@import url("http://www.openphilanthropy.org/sites/all/themes/op_basic/styles/css/global/normalize.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/themes/op_basic/styles/css/global/op-fonts.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/themes/op_basic/styles/css/global/html.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/themes/op_basic/styles/css/global/meanmenu.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/themes/op_basic/styles/css/global/global.css?om4jnn");
@import url("http://www.openphilanthropy.org/sites/all/themes/op_basic/styles/css/pages/pages.css?om4jnn");
</style>
<style type="text/css" media="all">
@import url("http://www.openphilanthropy.org/sites/all/themes/op_basic/styles/css/global/admin.css?om4jnn");
</style>
<link type="text/css" rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:300,300italic,400,400italic,700,700italic,900,900italic&amp;om4jnn" media="all" />
  <script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
window.jQuery || document.write("<script src='/sites/all/modules/contrib/jquery_update/replace/jquery/2.1/jquery.min.js'>\x3C/script>")
//--><!]]>
</script>
<script type="text/javascript" src="http://www.openphilanthropy.org/misc/jquery.once.js?v=1.2"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/misc/drupal.js?om4jnn"></script>
<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jqueryui/1.10.2/jquery-ui.min.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
window.jQuery.ui || document.write("<script src='/sites/all/modules/contrib/jquery_update/replace/ui/ui/minified/jquery-ui.min.js'>\x3C/script>")
//--><!]]>
</script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/contrib/jquery_update/replace/ui/external/jquery.cookie.js?v=67fb34f6a866c40d0570"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/modules/contextual/contextual.js?v=1.0"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/contrib/comment_notify/comment_notify.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/contrib/google_cse/google_cse.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/contrib/tablesorter/tablesortervar.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/contrib/caption_filter/js/caption-filter.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/contrib/admin_menu/admin_menu.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/contrib/admin_menu/admin_menu_toolbar/admin_menu_toolbar.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/contrib/resp_img/resp_img.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/libraries/tablesorter/jquery.tablesorter.min.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/libraries/tablesorter/jquery.metadata.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/libraries/tablesorter/addons/pager/jquery.tablesorter.pager.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/custom/tableofcontents/js/jquery.scrollTo-min.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/custom/tableofcontents/js/jquery.localscroll-min.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/modules/custom/tableofcontents/js/tableofcontents.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/themes/op_basic/scripts/lib/jquery-migrate-1.0.0.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/themes/op_basic/scripts/lib/mean-menu/jquery.meanmenu.min.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/themes/op_basic/scripts/lib/jquery-validation/dist/jquery.validate.min.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/themes/op_basic/scripts/lib/multiselect/jquery.multiselect.min.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/themes/op_basic/scripts/lib/dropkick.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/themes/op_basic/scripts/lib/jquery.pep.js?om4jnn"></script>
<script type="text/javascript" src="http://www.openphilanthropy.org/sites/all/themes/op_basic/scripts/main.js?om4jnn"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"op_basic","theme_token":"bj6FL47MQ0Yx7egj9Aokmodg5wh3TLdDIT0Vcg4Lj7Q","js":{"\/\/ajax.googleapis.com\/ajax\/libs\/jquery\/2.1.4\/jquery.min.js":1,"0":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"\/\/ajax.googleapis.com\/ajax\/libs\/jqueryui\/1.10.2\/jquery-ui.min.js":1,"1":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/external\/jquery.cookie.js":1,"modules\/contextual\/contextual.js":1,"sites\/all\/modules\/contrib\/comment_notify\/comment_notify.js":1,"sites\/all\/modules\/contrib\/google_cse\/google_cse.js":1,"sites\/all\/modules\/contrib\/tablesorter\/tablesortervar.js":1,"sites\/all\/modules\/contrib\/caption_filter\/js\/caption-filter.js":1,"sites\/all\/modules\/contrib\/admin_menu\/admin_menu.js":1,"sites\/all\/modules\/contrib\/admin_menu\/admin_menu_toolbar\/admin_menu_toolbar.js":1,"sites\/all\/modules\/contrib\/resp_img\/resp_img.js":1,"sites\/all\/libraries\/tablesorter\/jquery.tablesorter.min.js":1,"sites\/all\/libraries\/tablesorter\/jquery.metadata.js":1,"sites\/all\/libraries\/tablesorter\/addons\/pager\/jquery.tablesorter.pager.js":1,"sites\/all\/modules\/custom\/tableofcontents\/js\/jquery.scrollTo-min.js":1,"sites\/all\/modules\/custom\/tableofcontents\/js\/jquery.localscroll-min.js":1,"sites\/all\/modules\/custom\/tableofcontents\/js\/tableofcontents.js":1,"sites\/all\/themes\/op_basic\/scripts\/lib\/jquery-migrate-1.0.0.js":1,"sites\/all\/themes\/op_basic\/scripts\/lib\/mean-menu\/jquery.meanmenu.min.js":1,"sites\/all\/themes\/op_basic\/scripts\/lib\/jquery-validation\/dist\/jquery.validate.min.js":1,"sites\/all\/themes\/op_basic\/scripts\/lib\/multiselect\/jquery.multiselect.min.js":1,"sites\/all\/themes\/op_basic\/scripts\/lib\/dropkick.js":1,"sites\/all\/themes\/op_basic\/scripts\/lib\/jquery.pep.js":1,"sites\/all\/themes\/op_basic\/scripts\/main.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/system\/system.theme.css":1,"modules\/contextual\/contextual.css":1,"misc\/ui\/jquery.ui.core.css":1,"misc\/ui\/jquery.ui.theme.css":1,"misc\/ui\/jquery.ui.accordion.css":1,"sites\/all\/modules\/contrib\/comment_notify\/comment_notify.css":1,"sites\/all\/modules\/contrib\/footnotes\/footnotes.css":1,"modules\/comment\/comment.css":1,"sites\/all\/modules\/contrib\/date\/date_api\/date.css":1,"sites\/all\/modules\/contrib\/date\/date_popup\/themes\/datepicker.1.7.css":1,"modules\/field\/theme\/field.css":1,"sites\/all\/modules\/contrib\/google_cse\/google_cse.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/contrib\/views\/css\/views.css":1,"sites\/all\/modules\/contrib\/caption_filter\/caption-filter.css":1,"sites\/all\/modules\/contrib\/ckeditor\/css\/ckeditor.css":1,"sites\/all\/modules\/contrib\/admin_menu\/admin_menu.css":1,"sites\/all\/modules\/contrib\/admin_menu\/admin_menu_toolbar\/admin_menu_toolbar.css":1,"sites\/all\/modules\/contrib\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/contrib\/typogrify\/typogrify.css":1,"sites\/all\/modules\/contrib\/content_type_extras\/css\/content_type_extras.css":1,"sites\/all\/modules\/custom\/tableofcontents\/tableofcontents.css":1,"sites\/all\/themes\/op_basic\/styles\/css\/global\/normalize.css":1,"sites\/all\/themes\/op_basic\/styles\/css\/global\/op-fonts.css":1,"sites\/all\/themes\/op_basic\/styles\/css\/global\/html.css":1,"sites\/all\/themes\/op_basic\/styles\/css\/global\/meanmenu.css":1,"sites\/all\/themes\/op_basic\/styles\/css\/global\/global.css":1,"sites\/all\/themes\/op_basic\/styles\/css\/pages\/pages.css":1,"sites\/all\/themes\/op_basic\/styles\/css\/pages\/generic.css":1,"sites\/all\/themes\/op_basic\/styles\/css\/global\/admin.css":1,"\/\/fonts.googleapis.com\/css?family=Merriweather:300,300italic,400,400italic,700,700italic,900,900italic":1}},"googleCSE":{"cx":"016483604775238136155:sx0cq_pczrc","language":"","resultsWidth":100,"domain":"www.google.com"},"respImg":{"default_suffix":"_mobile","current_suffix":"_mobile","forceRedirect":"0","forceResize":"0","reloadOnResize":"0","useDevicePixelRatio":1,"suffixes":{"_mobile":1}},"tablesorter":{"zebra":1,"odd":"odd","even":"even"},"tableofcontents":{"collapse":false,"scroll":true},"better_exposed_filters":{"views":{"related_content_manual":{"displays":{"block":{"filters":[]}}}}},"urlIsAjaxTrusted":{"\/some-initial-thoughts-moral-patients":true},"admin_menu":{"destination":"destination=node\/858","replacements":{".admin-menu-users a":"1 \/ 0"},"margin_top":1,"toolbar":[]}});
//--><!]]>
</script>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=2.0, user-scalable=1" />
</head>
<body class="html not-front logged-in no-sidebars page-node page-node- page-node-858 node-type-page layout-generic layout-generic" >
  <div id="skip">
    <a href="#content">Jump to Navigation</a>
  </div>
    <div id="page" class="page">

  <header id="header">
    <div class="container">

              <a href="/" title="Home" rel="home" id="logo">
          <img src="http://www.openphilanthropy.org/sites/all/themes/op_basic/logo.png" alt="Home"/>
        </a>
      
      
              <div id="header-region">
            <div class="region region-header">
    <div id="block-system-main-menu" class="block block-system contextual-links-region block-menu">

    <div class="contextual-links-wrapper"><ul class="contextual-links"><li class="menu-list first"><a href="/admin/structure/menu/manage/main-menu/list?destination=node/858">List links</a></li>
<li class="menu-edit last"><a href="/admin/structure/menu/manage/main-menu/edit?destination=node/858">Edit menu</a></li>
</ul></div>
  <div class="content">
    <ul class="menu"><li class="first expanded"><a href="/research">Research &amp; Ideas</a><ul class="menu"><li class="first leaf"><a href="/research/our-process">Our Process</a></li>
<li class="leaf"><a href="/notable-lessons">Notable Lessons</a></li>
<li class="collapsed"><a href="/research/cause-reports">Cause Reports</a></li>
<li class="leaf"><a href="/research/conversations" title="Conversations">Conversations</a></li>
<li class="last leaf"><a href="/research/history-of-philanthropy">History of Philanthropy</a></li>
</ul></li>
<li class="expanded"><a href="/focus">Focus Areas</a><ul class="menu"><li class="first collapsed"><a href="/focus/us-policy" style="font-weight: bold;">U.S. Policy</a></li>
<li class="collapsed"><a href="/focus/us-policy/criminal-justice-reform" style="margin-left: 10px;">Criminal Justice Reform</a></li>
<li class="leaf"><a href="/focus/us-policy/farm-animal-welfare" style="margin-left: 10px;">Farm Animal Welfare</a></li>
<li class="leaf"><a href="/focus/us-policy/macroeconomic-policy" style="margin-left: 10px;">Macroeconomic Stabilization Policy</a></li>
<li class="leaf"><a href="/focus/us-policy/immigration-policy" style="margin-left: 10px;">Immigration Policy</a></li>
<li class="leaf"><a href="/focus/us-policy/land-use-reform" style="margin-left: 10px;">Land Use Reform</a></li>
<li class="leaf"><a href="/focus/global-catastrophic-risks" style="font-weight: bold;">Global Catastrophic Risks</a></li>
<li class="leaf"><a href="/focus/global-catastrophic-risks/biosecurity" style="margin-left: 10px;">Biosecurity and Pandemic Preparedness</a></li>
<li class="leaf"><a href="/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence" style="margin-left: 10px;">Potential Risks from Advanced Artificial Intelligence</a></li>
<li class="collapsed"><a href="/focus/scientific-research" style="font-weight: bold;">Scientific Research</a></li>
<li class="collapsed"><a href="/focus/global-health-and-development" style="font-weight: bold;">Global Health &amp; Development</a></li>
<li class="last leaf"><a href="/focus/other-areas" style="font-weight: bold;">Other areas</a></li>
</ul></li>
<li class="expanded"><a href="/giving/grants">Giving</a><ul class="menu"><li class="first leaf"><a href="/giving/grants">Grants Database</a></li>
<li class="leaf"><a href="/giving/current-priorities">Current Priorities</a></li>
<li class="last leaf"><a href="/giving/guide-for-grant-seekers">Guide for Grant Seekers</a></li>
</ul></li>
<li class="expanded"><a href="/about">About Us</a><ul class="menu"><li class="first leaf"><a href="/about/good-ventures-and-givewell">Good Ventures &amp; GiveWell</a></li>
<li class="leaf"><a href="/about/vision-and-values">Vision &amp; Values</a></li>
<li class="leaf"><a href="/what-open-means-us">Openness</a></li>
<li class="leaf"><a href="/about/progress-to-date">Progress to Date</a></li>
<li class="leaf"><a href="/about/team">Team</a></li>
<li class="last leaf"><a href="/about/press-kit">Press Kit</a></li>
</ul></li>
<li class="collapsed"><a href="/blog">Blog</a></li>
<li class="last expanded"><a href="/get-involved">Get Involved</a><ul class="menu"><li class="first leaf"><a href="/get-involved/contact-us">Contact Us</a></li>
<li class="collapsed"><a href="/get-involved/jobs">Jobs</a></li>
<li class="last leaf"><a href="/get-involved">Stay Updated</a></li>
</ul></li>
</ul>  </div>
</div>
<div id="block-search-form" class="block block-search contextual-links-region">

    
  <div class="content">
    <form class="google-cse" action="/some-initial-thoughts-moral-patients" method="post" id="search-block-form" accept-charset="UTF-8"><div><span class="btn-open"></span>
<span class="search-arr"></span>
<div class="container-inline">
      <h2 class="element-invisible">Search form</h2>
    <div class="form-item form-type-textfield form-item-search-block-form">
  <label class="element-invisible" for="edit-search-block-form--2">Search </label>
 <input title="Enter the terms you wish to search for." placeholder="Start typing your search" type="text" id="edit-search-block-form--2" name="search_block_form" value="" size="15" maxlength="128" class="form-text" />
</div>
<div class="form-actions form-wrapper" id="edit-actions"><input type="submit" id="edit-submit" name="op" value="Go" class="form-submit" /></div><input type="hidden" name="form_build_id" value="form-QiSsWiSf7licqrbig_iPHk_V5gDQnyZWRJPo_d-EH3I" />
<input type="hidden" name="form_token" value="GGBd-abYdTfT0O8OxkpkhRe8EiaTAr094uCa2aJXnn8" />
<input type="hidden" name="form_id" value="search_block_form" />
  <span class="btn-close"></span>
</div>

</div></form>  </div>
</div>
  </div>
        </div>
      
    </div>
  </header>

  
  <div id="main">
    <section id="content">
  
            <div id="content-header">
        <div class="container">

                      <div class="tabs"><h2 class="element-invisible">Primary tabs</h2><ul class="tabs primary"><li class="active"><a href="/some-initial-thoughts-moral-patients" class="active">View<span class="element-invisible">(active tab)</span></a></li>
<li><a href="/node/858/edit">Edit</a></li>
<li><a href="/node/858/revisions">Revisions</a></li>
</ul></div>
                    <div class="easy-breadcrumb"><a href="/" class="easy-breadcrumb_segment easy-breadcrumb_segment-front">Home</a></div>          
                              
                                <h1 class="title">2017 Report on Consciousness and Moral Patienthood

</h1>
                                          <aside id="header-sidebar">  <div class="region region-content-header-sidebar">
    <div id="block-block-3" class="block block-openphil-social-block contextual-links-region page-toolbar">

    
  <div class="content">
    <div class="social"><ul><li class="facebook"><a href="http://www.facebook.com/sharer/sharer.php?u=http://www.openphilanthropy.org/some-initial-thoughts-moral-patients" target="_blank" title="Share on Facebook">Facebook</a></li><li class="twitter"><a href="http://www.twitter.com/share?url=http://www.openphilanthropy.org/some-initial-thoughts-moral-patients" target="_blank" title="Share on Twitter">Twitter</a></li><li class="print"><a href="?q=/print/some-initial-thoughts-moral-patients" title="Printer-friendly version of this page." target="_blank">Print</a></li><li class="save"><a href="?q=/printpdf/some-initial-thoughts-moral-patients" title="Save this page as a PDF.">Save</a></li><li class="rss"><a href="?q=RSS" title="Subscribe to this RSS feed.">RSS</a></li><li class="email"><a href="/forward?path=some-initial-thoughts-moral-patients" title="Send this page by email.">Email</a></li></ul></div>  </div>
</div>
  </div>
</aside>
          
        </div>
      </div>
      
      <div id="content-area">
      <div class="container">
                  <div class="region region-content">
    <div id="block-system-main" class="block block-system">

    
  <div class="content">
    <article id="node-858" class="node node-page node-unpublished expanded-footnotes">

  
  <div class="content">
    <span class="print-link"></span><div class="field field-name-body field-type-text-with-summary field-label-hidden">
    <div class="field-items">
              <div class="field-item even">
           <p>This investigation began with the question: “Given that we make <a href="http://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms">chicken welfare grants</a>, should we also make fish welfare grants?” Eventually, the project expanded in scope to ask: “In general, which types of beings merit moral concern?” Or, to phrase the question as some philosophers do, “Which beings are <em>moral patients</em>?”<a class="see-footnote" id="footnoteref1_5a070m1" title="Related terms include &quot;moral status,&quot; &quot;moral standing,&quot; &quot;moral considerability,&quot; &quot;personhood,&quot; &quot;moral subject&quot; (this is fairly rare, but see e.g. Wetlesen 1999), and &quot;member of the moral community.&quot; Sometimes these terms are used more-or-less interchangeably, and sometimes they are not.  My preference for the terms &quot;moral patient&quot; and &quot;moral patienthood&quot; (see e.g. Gray &amp; Wegner 2009; Bernstein 1998) is a pragmatic one. &quot;Moral patient&quot; is more succinct than &quot;being with moral status,&quot; &quot;being with moral standing,&quot; &quot;being worthy of moral consideration,&quot; and &quot;member of the moral community.&quot; It is less succinct and common than &quot;person,&quot; but &quot;person&quot; comes with fairly strong connotations of properties such as (1) having a temporally extended narrative about one's life, (2) having a fairly sophisticated kind of agency in the world, (3) being human or human-like, (4) being able to participate in a community of other moral agents, and so on. Moreover, it is often used to mean some or all of those things denotatively, though in some other cases it is operationally defined as &quot;being with moral status.&quot;  One strike against &quot;moral patient&quot; is that it comes with a problematic connotation of helplessness, but not a terribly strong one, I don't think. I presume &quot;moral patient&quot; gets its connotations by association with the concept of a medical patient, and while medical patients are primarily in a role of receiving help (or harm) from medical professionals (the intended connotation), they are not in most cases helpless (an unintended connotation).  For reviews of these related terms and concepts, see Newson (2007), Hancock (2002), Jaworska &amp; Tannenbaum (2013), Morris (2011), ch. 3 of Beauchamp &amp; Childress (2012), Kagan (2016), and the entry on &quot;Moral Status&quot; by James W. Walters on pp. 1855-1864 of Post (2004).  Some authors define &quot;moral patient&quot; such that a moral patient cannot also be a moral agent (e.g. Rodd 1990, p. 241), but that is not how I use the term.  Some authors use the term &quot;moral patiency&quot; instead of &quot;moral patienthood.&quot;  One more clarification: in this report I refer to moral patients as distinctly identifiable individuals, but this is largely for convenience of communication. In the limit of scientific understanding, I suspect group minds (see e.g. Schwitzgebel 2015; Roelofs 2016; Langland-Hassan 2015, sec. 4; Theiner 2014) and other structures for morally relevant cognitive processing (e.g. perhaps the &quot;utilitronium&quot; of Pearce 2013) will challenge our notions of morally relevant individual identity. (See also Chappell 2010 on valuing individuals vs. parts of individuals.)  For some historical background on the term &quot;moral patient,&quot; see Hajdin (1994), p. 180.  " href="#footnote1_5a070m1">1</a> Should we aim to improve the welfare of crabs and shrimp? Might some future artificial intelligence systems be worthy of moral consideration? See also our blog post on <a href="http://openphilanthropy.org/blog/radical-empathy">radical empathy</a>.</p>
<p>For this preliminary investigation, I (Luke Muehlhauser) focused on just <em>one</em> commonly-endorsed criterion for moral patienthood: phenomenal consciousness. I have not come to any strong conclusions about which (non-human) beings are conscious, but I think some beings are more likely to be conscious than others, and I make several suggestions for how we might make progress on the question. I hope to examine other plausible criteria for moral patienthood in the future.</p>
<p>This report is unusually personal in nature, as it necessarily draws heavily from the empirical and moral intuitions of the investigator. Thus, the rest of this report does not necessarily reflect the intuitions and judgments of the Open Philanthropy Project in general. I explain my views in this report merely so they can serve as one input among many as the Open Philanthropy Project considers how to clarify its values and make its grantmaking choices. </p>
<div id="toc" class='toc'>
<div class='toc-title'>Table of Contents</div>
<div class='toc-list'>
<ol>
	<li class="toc-level-1"><a href="#HowToRead">How to read this report</a></li>
	<li class="toc-level-1"><a href="#Explaining">Explaining my approach to the question</a>
<ol>
	<li class="toc-level-2"><a href="#WhyWeCare">Why we care about the question of moral patienthood</a></li>
	<li class="toc-level-2"><a href="#Patienthood">Moral patienthood and consciousness</a>
<ol>
	<li class="toc-level-3"><a href="#Metaethical">My metaethical approach</a></li>
	<li class="toc-level-3"><a href="#ProposedCriteria">Proposed criteria for moral patienthood</a></li>
	<li class="toc-level-3"><a href="#WhyI">Why I investigated phenomenal consciousness first</a></li>
</ol>
</li>
	<li class="toc-level-2"><a href="#MyApproach">My approach to thinking about consciousness</a>
<ol>
	<li class="toc-level-3"><a href="#Defined">Consciousness, innocently defined</a></li>
	<li class="toc-level-3"><a href="#Nature">My assumptions about the nature of consciousness</a></li>
</ol>
</li>
</ol>
</li>
	<li class="toc-level-1"><a href="#DistributionQuestion">Specific efforts to sharpen my views about the distribution question</a>
<ol>
	<li class="toc-level-2"><a href="#Theories">Theories of consciousness</a></li>
	<li class="toc-level-2"><a href="#PCIFs">Potentially consciousness-indicating features (PCIFs)</a>
<ol>
	<li class="toc-level-3"><a href="#PCIFsWork">How PCIF arguments work</a></li>
	<li class="toc-level-3"><a href="#PCIFsTable">A large (and incomplete) table of PCIFs and taxa</a></li>
	<li class="toc-level-3"><a href="#PCIFsOverall">My overall thoughts on PCIF arguments</a></li>
</ol>
</li>
	<li class="toc-level-2"><a href="#Necessary">Hunting for necessary or sufficient conditions</a>
<ol>
	<li class="toc-level-3"><a href="#Cortex">Is a cortex required for consciousness?</a>
<ol>
	<li class="toc-level-4"><a href="#ProCRV">Arguments for cortex-required views</a></li>
	<li class="toc-level-4"><a href="#UnconsciousVision">Unconscious vision</a></li>
	<li class="toc-level-4"><a href="#ProCRVsSummary">Suggested other lines of evidence for CRVs</a></li>
	<li class="toc-level-4"><a href="#ProCRVSummary">Overall thoughts on arguments for CRVs</a></li>
	<li class="toc-level-4"><a href="#AntiCRV">Arguments against cortex-required views</a></li>
</ol>
</li>
</ol>
</li>
	<li class="toc-level-2"><a href="#BigPicture">Big-picture considerations that pull toward or away from “consciousness is rare”</a>
<ol>
	<li class="toc-level-3"><a href="#Inessentialism">Consciousness inessentialism</a></li>
	<li class="toc-level-3"><a href="#ComplexityConsciousness">The complexity of consciousness</a>
<ol>
	<li class="toc-level-4"><a href="#Panpsychism">The complexity of panpsychism</a></li>
	<li class="toc-level-4"><a href="#FirstOrder">The complexity of first-order representationalism</a></li>
	<li class="toc-level-4"><a href="#HigherOrder">The complexity of higher-order approaches</a></li>
	<li class="toc-level-4"><a href="#IllusionistTheories">The complexity of illusionist theories</a></li>
	<li class="toc-level-4"><a href="#ComplexityOverview">So, how complex will consciousness turn out to be?</a></li>
	<li class="toc-level-4"><a href="#EarlyProgress">Early scientific progress tends to lead to more complicated models of phenomena</a></li>
</ol>
</li>
	<li class="toc-level-3"><a href="#SophisticatedBehaviors">We continue to find that many sophisticated behaviors are more extensive than we once thought</a></li>
	<li class="toc-level-3"><a href="#Anthropomorphism">Rampant anthropomorphism</a></li>
</ol>
</li>
</ol>
</li>
	<li class="toc-level-1"><a href="#SummaryCurrentThinking">Summary of my current thinking about the distribution question</a>
<ol>
	<li class="toc-level-2"><a href="#HighLevel">High-level summary</a></li>
	<li class="toc-level-2"><a href="#Probabilities">My current probabilities</a></li>
	<li class="toc-level-2"><a href="#Why_these_probabilities">Why these probabilities?</a></li>
	<li class="toc-level-2"><a href="#Acting">Acting on my probabilities</a></li>
	<li class="toc-level-2"><a href="#MindChanged">How my mind changed during this investigation</a></li>
	<li class="toc-level-2"><a href="#SomeOutputs">Some outputs from this investigation</a></li>
</ol>
</li>
	<li class="toc-level-1"><a href="#FutureInvestigations">Potential future investigations</a>
<ol>
	<li class="toc-level-2"><a href="#ThingsNotDone">Things I considered doing, but didn&#8217;t, due to time constraints</a></li>
	<li class="toc-level-2"><a href="#ProjectsForOthers">Projects that others could conduct</a>
<ol>
	<li class="toc-level-3"><a href="#TheoryProjects">Projects related to theories of consciousness</a></li>
	<li class="toc-level-3"><a href="#GuessingProjects">Projects related to theory-agnostic guesses about the distribution of consciousness</a></li>
	<li class="toc-level-3"><a href="#PatienthoodProjects">Projects related to moral judgments about moral patienthood</a></li>
	<li class="toc-level-3"><a href="#AdditionalProjects">Additional thoughts on useful projects</a></li>
</ol>
</li>
</ol>
</li>
	<li class="toc-level-1"><a href="#appendices">Appendices</a>
<ol>
	<li class="toc-level-2"><a href="#AppendixA">Appendix A. Elaborating my moral intuitions</a>
<ol>
	<li class="toc-level-3"><a href="#WhichKinds">Which kinds of consciousness-related processes do I morally care about?
</a></li>
	<li class="toc-level-3"><a href="#ExtremeEffort">The “extreme effort” version of my process for making moral judgments</a></li>
	<li class="toc-level-3"><a href="#MoralJudgments">My moral judgments about some particular cases</a>
<ol>
	<li class="toc-level-4"><a href="#FirstPerson">My moral judgments about some first-person cases
</a></li>
	<li class="toc-level-4"><a href="#Phenumb">The case of Phenumb</a></li>
	<li class="toc-level-4"><a href="#ThirdPerson">My moral judgments about some third-person cases
</a></li>
	<li class="toc-level-4"><a href="#Hero">My moral judgments, illustrated with the help of a simple game
</a></li>
</ol>
</li>
</ol>
</li>
	<li class="toc-level-2"><a href="#AppendixB">Appendix B. Toward a more satisfying theory of consciousness</a>
<ol>
	<li class="toc-level-3"><a href="#TemporalBinding">Temporal binding theory</a></li>
	<li class="toc-level-3"><a href="#IntegratedInformation">Integrated information theory</a></li>
	<li class="toc-level-3"><a href="#GlobalWorkspace">Global workspace theory</a></li>
	<li class="toc-level-3"><a href="#MoreSatisfying">What a more satisfying theory of consciousness could look like</a></li>
</ol>
</li>
	<li class="toc-level-2"><a href="#AppendixC">Appendix C. Evidence concerning unconscious vision</a>
<ol>
	<li class="toc-level-3"><a href="#MultipleVisionSystems">Multiple vision systems in simpler animals</a></li>
	<li class="toc-level-3"><a href="#PrimateVisionSystems">Two vision systems in primates</a></li>
	<li class="toc-level-3"><a href="#Dee">Visual form agnosia in Dee Fletcher</a></li>
	<li class="toc-level-3"><a href="#OpticAtaxia">Optic ataxia</a></li>
	<li class="toc-level-3"><a href="#MonkeyLesions">Lesions in monkeys</a></li>
	<li class="toc-level-3"><a href="#HealthySubjects">Dissociation studies in healthy subjects</a></li>
	<li class="toc-level-3"><a href="#SingleNeuron">Single-neuron recordings</a></li>
	<li class="toc-level-3"><a href="#VisionChallenges">Challenges</a></li>
</ol>
</li>
	<li class="toc-level-2"><a href="#AppendixD">Appendix D. Some clarifications on nociception and pain</a></li>
	<li class="toc-level-2"><a href="#AppendixE">Appendix E. Some clarifications on “neuroanatomical similarity”</a></li>
	<li class="toc-level-2"><a href="#AppendixF">Appendix F. Illusionism and its implications</a>
<ol>
	<li class="toc-level-3"><a href="#IllusionismDefined">What I mean by “illusionism”</a></li>
	<li class="toc-level-3"><a href="#OtherIllusions">Other cognitive illusions
</a></li>
	<li class="toc-level-3"><a href="#IllusionistRealistDisagree">Where do the illusionist and the realist disagree?</a></li>
	<li class="toc-level-3"><a href="#IllusionismPatienthood">Illusionism and moral patienthood</a></li>
</ol>
</li>
	<li class="toc-level-2"><a href="#AppendixG">Appendix G. Consciousness and fuzziness</a>
<ol>
	<li class="toc-level-3"><a href="#FuzzinessMoral">Fuzziness and moral patienthood</a></li>
	<li class="toc-level-3"><a href="#FuzzinessDarwin">Fuzziness and Darwin</a></li>
	<li class="toc-level-3"><a href="#FuzzinessAAD">Fuzziness and auto-activation deficit</a></li>
</ol>
</li>
	<li class="toc-level-2"><a href="#AppendixH">Appendix H. First-order views, higher-order views, and hidden qualia</a>
<ol>
	<li class="toc-level-3"><a href="#overflowargument">Block&#8217;s overflow argument</a></li>
	<li class="toc-level-3"><a href="#splitbrain">Split-brain patients</a></li>
	<li class="toc-level-3"><a href="#hemispheredisconnection">Other cases of hemisphere disconnection</a></li>
	<li class="toc-level-3"><a href="#Shiller">Shiller&#8217;s arguments</a></li>
</ol>
</li>
	<li class="toc-level-2"><a href="#AppendixZ">Appendix Z. Miscellaneous elaborations and clarifications</a>
<ol>
	<li class="toc-level-3"><a href="#AppendixZ1">Appendix Z.1. Some theories of consciousness</a></li>
	<li class="toc-level-3"><a href="#AppendixZ2">Appendix Z.2. Some varieties of conscious experience</a></li>
	<li class="toc-level-3"><a href="#AppendixZ3">Appendix Z.3. Challenging dualist intuitions</a></li>
	<li class="toc-level-3"><a href="#AppendixZ4">Appendix Z.4. Brief comments on unconscious emotions</a></li>
	<li class="toc-level-3"><a href="#AppendixZ5">Appendix Z.5. The lack of consensus in consciousness studies</a></li>
	<li class="toc-level-3"><a href="#AppendixZ6">Appendix Z.6. Against hasty eliminativism</a></li>
	<li class="toc-level-3"><a href="#AppendixZ7">Appendix Z.7. Some candidate dimensions of moral concern</a></li>
	<li class="toc-level-3"><a href="#AppendixZ8">Appendix Z.8. Some reasons for my default skepticism of published studies</a></li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div>
<p><span style="float: right;">[<a href="/node/858/edit/0">Edit this section</a>]</span><br /></p><h2 id="HowToRead">How to read this report</h2>
<p>The length of this report, compared to the length of <a href="http://lukemuehlhauser.com/writings/#openphil">my other reports</a> for the Open Philanthropy Project, might suggest to the reader that I am a specialist on consciousness and moral patienthood. Let me be clear, then, that I am <em>not</em> a specialist on these topics. This report is long not because it engages its subject with the <em>depth</em> of an expert, but because it engages an unusual <em>breadth</em> of material — with the shallowness of a non-expert.</p>
<p>The report&#8217;s unusual breadth is a consequence of the fact that, when it comes to examining the likely distribution of consciousness (what I call “the distribution question”), we barely even know which kinds of evidence are <em>relevant</em> (besides human self-report), and thus I must survey an unusually broad variety of types of evidence that <em>might</em> be relevant. Compare to my report on <a href="http://www.openphilanthropy.org/behavioral-treatments-insomnia">behavioral treatments for insomnia</a>: in that case, it was quite clear which studies will be most informative, so I summarized only a tiny portion of the available literature.<a class="see-footnote" id="footnoteref2_p3jbfjm" title="The relevant studies, naturally enough, are those testing the effectiveness of behavioral treatments on sleep quality! And, assuming the perspective of evidence-based medicine, it's fairly clear which of those studies are most informative: it's the randomized controlled trials, especially those with large numbers of subjects, well-validated outcome measures, long-term follow-up, and other properties which improve the internal and external validity of a study.  " href="#footnote2_p3jbfjm">2</a> But when it comes to the distribution-of-consciousness question, there is <a href="#AppendixZ5">extreme expert disagreement</a> about which types of evidence are most informative. Hence, this report draws from a very large set of studies across a wide variety of domains — comparative ethology, comparative neuroanatomy, cognitive neuroscience, neurology, moral philosophy, philosophy of mind, etc. — and I am not an expert in any of those fields.<a class="see-footnote" id="footnoteref3_bw1ma1z" title="Thus I have no doubt gotten some things wrong, and said some things that are silly, and I hope readers will call my attention to whatever errors I have made." href="#footnote3_bw1ma1z">3</a></p>
<p>Given all this, my goal for this report cannot be to <em>argue</em> for my conclusions, in the style of a scholarly monograph on consciousness, written by a domain expert.<a class="see-footnote" id="footnoteref4_4xetcyt" title="If you want to read a series of arguments about the likely distribution of conscious experience, see e.g. Tye (2016). Also see notes from my conversation with Michael Tye.  " href="#footnote4_4xetcyt">4</a> Nor is it my goal to survey the evidence which plausibly bears on the distribution question, as such a survey would likely run thousands of pages, and require the input of dozens of domain experts. Instead, my more modest <strong>goals for this report</strong> are to:</p>
<ol><li>survey the <em>types</em> of evidence and argument that have been brought to bear on the distribution question,</li>
<li>briefly describe <em>example pieces of evidence</em> of each type,<a class="see-footnote" id="footnoteref5_c1l81l5" title="Hence, each sub-investigation reported below was cut short long before I &quot;completed&quot; it, to save time (following something like the 80/20 rule). Thus, I can only share initial tentative conclusions based on a variety of partially-completed inquiries.  " href="#footnote5_c1l81l5">5</a> without attempting to summarize the vast majority of the evidence (of each type) that is currently available,</li>
<li>report what my own intuitions and conclusions are as a result of my shallow survey of those data and arguments,</li>
<li>try to give <em>some</em> indication of why I have those intuitions, without spending many months to deeply and rigorously <em>argue</em> for each of my many reported intuitions, and</li>
<li>list some research projects that seem (to me) like they could make progress on the key questions of this report, given the current state of evidence and argument.</li>
</ol><p>Given these limited goals, I don&#8217;t expect to convince career consciousness researchers of any non-obvious substantive claims about the distribution of consciousness. Instead, I focused on finding out whether I could convince <em>myself</em> of any non-obvious substantive claims about the distribution of consciousness. As you&#8217;ll see, even this goal proved challenging enough.</p>
<p>Despite this report&#8217;s length, I have attempted to keep the “main text” (sections 2-4) modular and short ([TODO] words). I provide many clarifications, elaborations, and links to related readings in the <a href="#Appendices">appendices</a> and footnotes.</p>
<p>In my review of the relevant literature, I noticed that it&#8217;s often hard to interpret claims about consciousness because they are often grounded in unarticulated assumptions, and (perhaps unavoidably) stated vaguely. To mitigate this problem somewhat for this report, <a href="#Explaining">section 2</a> provides some background on “where I&#8217;m coming from,” and can be summarized in a single jargon-filled paragraph:</p>
<blockquote><p>This report examines which beings and processes might be moral patients given their phenomenal consciousness, but does not examine other possible criteria for moral patienthood, and does not examine the question of moral weight. I define phenomenal consciousness extensionally, with as much metaphysical innocence and theoretical neutrality as I can. My broad philosophical approach is naturalistic (<em>a la</em> Dennett or Wimsatt) rather than rationalistic (<em>a la</em> Chalmers or Chisholm),<a class="see-footnote" id="footnoteref6_1ytgq4h" title="For more details on what I mean by &quot;naturalistic&quot; vs. &quot;rationalistic,&quot; see the introductory chapter in Fischer &amp; Collins (2015).  Examples might be more helpful, though. Example works in the &quot;rationalistic&quot; tradition are Chalmers (1996) and Jackson (1998). Example works in the &quot;naturalistic&quot; tradition are Wimsatt (2007) and Smith (2016).  Another way to indicate the tradition of my thinking is to identify myself with (what is often called) &quot;Quinean naturalism,&quot; after Willard van Orman Quine (Hylton 2014; Harman &amp; Lepore 2014). Even better would be to coin the term &quot;Dennettian naturalism&quot; and identify myself with it, due especially (but not exclusively) to the way that Daniel Dennett updated and transformed Quinean naturalism with his more thorough appreciation for the impacts of both Darwin and the computer on philosophy, two lines of thinking have greatly impacted my own philosophical thinking. (This doesn't mean I agree with Dennett on everything about consciousness, of course.)  " href="#footnote6_1ytgq4h">6</a> and I assume physicalism, functionalism, and illusionism about consciousness. I also assume the boundary around “consciousness” is fuzzy (<em>a la</em> “life”) rather than sharp (<em>a la</em> “water” = H<sub>2</sub>O). My meta-ethical approach employs an anti-realist kind of ideal advisor theory.</p></blockquote>
<p>If that paragraph made sense to you, then you might want to jump ahead to <a href="#DistributionQuestion">section 3</a>, where I survey the types of evidence and arguments that have been brought to bear on the distribution question. Otherwise, you might want to read the full report.</p>
<p>In section 3, I <a href="#Theories">conclude</a> that no existing theory of consciousness (that I&#8217;ve seen) is satisfying, and thus I investigate the distribution question via relatively theory-agnostic means, examining <a href="#PCIFs">analogy-driven arguments</a>, potential <a href="#Necessary">necessary or sufficient conditions for consciousness</a>, and some <a href="BigPicture">big-picture considerations</a> that pull toward or away from a “consciousness is rare” conclusion.</p>
<p>To read only my overall conclusions, see <a href="#HighLevel">section 4</a>. In short, I think most fishes, birds, and mammals are more likely than not to be conscious, while (e.g.) insects are unlikely to be conscious. However, my probabilities are very “made-up” and difficult to justify, and it&#8217;s not clear to me what actions should be taken on the basis of such made-up probabilities.</p>
<p>I also prepared a list of <a href="#FutureInvestigations">potential future investigations</a> that I think could further clarify some of these issues for us — at least, given <em>my</em> approach to the problem.</p>
<p>This report includes several appendices:</p>
<ul><li><a href="#AppendixA">Appendix A</a> explains how I use my moral intuitions, reports some of my moral intuitions about particular cases, and illustrates how existing theories of consciousness and moral patienthood could be clarified by frequent reference to code snippets or existing computer programs.</li>
<li><a href="#AppendixB">Appendix B</a> explains what I find unsatisfying about current theories of consciousness, and says a bit about what a more satisfying theory of consciousness could look like.</li>
<li><a href="#AppendixC">Appendix C</a> summarizes the evidence concerning unconscious vision and the “two streams of visual processing” theory that is discussed briefly in my section on <a href="#Cortex">whether a cortex is required for consciousness.</a></li>
<li><a href="#AppendixD">Appendix D</a> makes several clarifications concerning the distinction between nociception (which can be unconscious) and pain (which cannot).</li>
<li><a href="#AppendixE">Appendix E</a> makes some clarifications about how I&#8217;m currently estimating “neuroanatomical similarity,” which plays a role in my “theory-agnostic estimation process” for guessing whether a being is conscious (described <a href="#HighLevel">here</a>).</li>
<li><a href="#AppendixF">Appendix F</a> explains illusionism in a bit more detail, explains why the illusionist approach tends to assume consciousness is more complicated than other approaches do, and makes some brief comments about how illusionism interacts with my intuitions about moral patienthood.</li>
<li><a href="#AppendixG">Appendix G</a> elaborates my views on the “fuzziness” of consciousness.</li>
<li><a href="#AppendixH">Appendix H</a> examines how the possibility of hidden qualia might undermine the central argument for higher-order theories.</li>
<li><a href="#AppendixZ">Appendix Z</a> collects a variety of less-important sub-appendices, for example a <a href="#AppendixZ1">list of theories of consciousness</a>, a <a href="#AppendixZ2">list of varieties of conscious experience</a>, a <a href="#AppendixZ5">list of questions about which consciousness scholars exhibit wild disagreement</a>, a <a href="#AppendixZ7">list of candidate dimensions of moral concern</a> (for estimating moral weight), <a href="#AppendixZ4">some brief comments on unconscious emotions</a>, and <a href="AppendixZ8">some reasons for my default skepticism about published studies</a>.</li>
</ul><p><small><strong>Acknowledgements</strong>: Many thanks to those who gave me substantial feedback on earlier drafts of this report: Scott Aaronson, David Chalmers, Daniel Dewey, Julia Galef, Jared Kaplan, Holden Karnofsky, Buck Shlegeris, Carl Shulman, Brian Tomasik, [TODO], and those who participated in a series of GiveWell roundtables on this topic. I am also grateful to several people for helping me find some of the data related to potentially consciousness-indicating features presented <a href="#PCIFsTable">below</a>: GiveWell 2016 summer research analysts Julie Chen, Robin Dey, and Laura Ong, and GiveWell operations associate Laura Muñoz. My thanks also to <a href="http://global.oup.com/">Oxford University Press</a> and <a href="https://mitpress.mit.edu/">MIT Press</a> for granting permission to reproduce some images to which they own the copyright (see corresponding foontotes for details).</small></p>
<p><span style="float: right;">[<a href="/node/858/edit/1">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h2 id="Explaining">Explaining my approach to the question</h2>
<p><span style="float: right;">[<a href="/node/858/edit/2">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="WhyWeCare">Why we care about the question of moral patienthood</h3>
<p>How does the question of moral patienthood fit into <a href="http://www.openphilanthropy.org/research/our-process">our framework</a> for thinking about effective giving?</p>
<p>The Open Philanthropy Project focuses on causes that score well on <a href="http://www.openphilanthropy.org/focus">our three criteria</a> — importance, neglectedness, and tractability. Our “importance” criterion is: “How many individuals does this issue affect, and how deeply?” Elaborating on this, we might say our importance criterion is: “How many <em>moral patients</em> does this issue affect, and how much could we benefit them, with respect to <em>appropriate dimensions of moral concern</em> (e.g. pain, pleasure, desire fulfillment, life satisfaction)?”</p>
<p>As with many framing choices in this report, this is far from the only way to approach the question,<a class="see-footnote" id="footnoteref7_wdtyzkb" title="Below is an incomplete list of alternative approaches for how to think about what should count as &quot;good.&quot; These alternative approaches have their own merits, and in some cases might lead us to make different grantmaking choices if we adopted them in favor of our current framing (about moral patients, and dimensions of moral concern). I have not considered these alternatives in detail yet, but I have tried to at least expose myself to a wide range of viewpoints.  Below are some alternate approaches to the question of what should count as &quot;good,&quot; which I have considered at least briefly.  Rachels (2004) writes:  There is no characteristic, or reasonably small set of characteristics, that sets some creatures apart from others as meriting respectful treatment. That is the wrong way to think about the relation between an individual's characteristics and how he or she may be treated. Instead we have an array of characteristics and an array of treatments, with each characteristic relevant to justifying some types of treatment but not others. If an individual possesses a particular characteristic (such as the ability to feel pain), then we may have a duty to treat it in a certain way (not to torture it), even if that same individual does not possess other characteristics (such as autonomy) that would mandate other sorts of treatment (refraining from coercion).  We could spin these observations into a theory of moral standing that would compete with the other theories. Our theory would start like this: There is no such thing as moral standing simpliciter. Rather, moral standing is always moral standing with respect to some particular mode of treatment. A sentient being has moral standing with respect to not being tortured. A self-conscious being has moral standing with respect to not being humiliated. An autonomous being has moral standing with respect to not being coerced. And so on…  It would do no harm, however, and it might be helpful for clarity's sake, to drop the notion of [moral] &quot;standing&quot; altogether and replace it with a simpler conception. We could just say that the fact that doing so-and-so would cause pain to someone (to any individual) is a reason not to do it. The fact that doing so-and-so would humiliate someone (any individual) is a reason not to do it. And so on. Sentience and self-consciousness fit into the picture like this: Someone's sentience and someone's self-consciousness are facts about them that explain why they are susceptible to the evils of pain and humiliation.  We would then see our subject as part of the theory of reasons for action. We would distinguish three elements: what is done to the individual; the reason for doing it or not doing it, which connects the action to some benefit or harm to the individual; and the pertinent facts about the individual that help to explain why he or she is susceptible to that particular benefit or harm…  So, part of our theory of reasons for action would go like this: We always have reason not to do harm. If treating an individual in a certain way harms him or her, that is a reason not to do it. The fact that he or she is autonomous, or self-conscious, or sentient simply helps to explain why he or she is susceptible to particular kinds of harms.  A related but not-identical point is made by Bostrom &amp; Yudkowsky (2014):  Alternatively, one might deny that moral status comes in degrees. Instead, one might hold that certain beings have more significant interests than other beings. Thus, for instance, one could claim that it is better to save a human than to save a bird, not because the human has higher moral status, but because the human has a more significant interest in having her life saved than does the bird in having its life saved.  For additional related arguments against &quot;moral status talk,&quot; see Sachs (2011).  Another approach is to think about moral status in the context of the concept of personhood, which might or might not be sufficient for a being to have moral status. For an overview of such approaches, see Newson (2007).  Substantially different approaches to thinking about what is &quot;good&quot; include deontological approaches, virtue ethics approaches, capabilities approaches, contractualist approaches, and more. For example essays on how these approaches can be applied to concerns about animals, see e.g. Sachs (2015) and the chapters in Part II of Beauchamp &amp; Frey (2011).  See also Crisp &amp; Pummer (2016) on &quot;effective justice&quot; and &quot;effective altruism,&quot; which is relevant due to our substantial ties to the effective altruism community (see e.g. this blog post).  " href="#footnote7_wdtyzkb">7</a> but we find it to be a framing that is pragmatically useful to us as we try to execute our mission to “accomplish as much good as possible with our giving” without waiting to first resolve all major debates in moral philosophy.<a class="see-footnote" id="footnoteref8_1a1sx5d" title="That is, we must act under &quot;moral uncertainty.&quot; See e.g. MacAskill (2014); Bogosian (2016); Lockhart (2000); Möller (2016); Greaves &amp; Ord (2016).  See also our blog post on worldview diversification.  " href="#footnote8_1a1sx5d">8</a> (See also our blog post on <a href="http://openphilanthropy.org/blog/radical-empathy">radical empathy</a>.)</p>
<p>In the long run, we&#8217;d like to have better-developed views not just about which beings are moral patients, but also about how to weigh the interests of different kinds of moral patients against each other. For example: suppose we conclude that fishes, pigs, and humans are all moral patients, and we estimate that, for a fixed amount of money, we can (in expectation) dramatically improve the welfare of (a) 10,000 rainbow trout, (b) 1,000 pigs, or (c) 100 adult humans. In that situation, which option should we choose? This depends on how much “moral weight” we give to the well-being of different kinds of moral patients. Or, more granularly, it depends on how much moral weight we give to various appropriate <em>dimensions</em> of moral concern, which then <em>collectively determine</em> the moral weight of each particular moral patient.<a class="see-footnote" id="footnoteref9_zry3ia1" title="For example, if you think fishes are moral patients but have very low &quot;intensity of valenced subjective experience&quot; compared to chickens, and you consider &quot;intensity of valenced subjective experience&quot; to be a very important (i.e., heavily-weighted) dimension of moral concern, then you might still prioritize chicken welfare interventions over fish welfare investigations, even if you think that fish and chickens have roughly the same probability of being moral patients at all.  Of course, even after we've come to tentative conclusions about a taxon's &quot;moral weight,&quot; there remain various second-order effects of welfare interventions to evaluate, among other considerations. Example sources on second-order effects and other considerations, with respect to animal welfare in particular, include Matheny &amp; Chan (2005), Norwood &amp; Lusk (2011), Christensen et al. (2012), Višak (2013), and Shulman (2015).  There are also those who argue that the numbers of moral patients helped or harmed shouldn't matter, e.g. Taurek (1977), but I won't discuss that issue here.  " href="#footnote9_zry3ia1">9</a></p>
<p>This report, however, focuses on articulating my early thinking about which beings are <em>moral patients at all</em>.<a class="see-footnote" id="footnoteref10_0ltzae6" title="Though, there may not be a sharp dividing line between beings which are moral patients and beings which are not; see my later comments on the likely-fuzzy line between conscious and non-conscious beings.  " href="#footnote10_0ltzae6">10</a> We hope to investigate plausibly appropriate <em>dimensions</em> of moral concern — i.e., the question of “moral weight” — in the future. For now, I merely list some candidate dimensions in <a href="#AppendixZ7">Appendix Z.7</a>.</p>
<p><span style="float: right;">[<a href="/node/858/edit/3">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="Patienthood">Moral patienthood and consciousness</h3>
<p><span style="float: right;">[<a href="/node/858/edit/4">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="Metaethical">My metaethical approach</h4>
<p>Philosophers, along with everyone else, have very different views about “metaethics,” i.e. about the foundations of morality and the meaning of moral terms.<a class="see-footnote" id="footnoteref11_q8ddby2" title="Introductions to metaethics include Sayre-McCord (2012), Miller (2013), and Prinz (2015).   Personally, I think it's pretty clear that different people use moral language in different ways, and often a single person uses moral language in different ways at different times. This view is sometimes called &quot;meta-ethical pluralism&quot; (Wright et al. 2012).  " href="#footnote11_q8ddby2">11</a> In this section, I explain my own metaethical approach — not because my moral judgments <em>depend</em> on this metaethical approach (they don&#8217;t), but merely to give my readers a sense of “where I&#8217;m coming from.”</p>
<p>Terms like “moral patient” and “moral judgment” can mean different things depending on one&#8217;s metaethical views, for example whether one is a “moral realist.” I have tried to phrase this report in a relatively “metaethically neutral” way, so that e.g. if you are a moral realist you can interpret “moral judgment” to mean “my best judgment as to what the moral facts are,” whereas if you are a certain kind of moral anti-realist you can interpret “moral judgment” to mean “my best guess as to what I would personally value if I knew more and had more time to think about my values,” and if you have different metaethical views, you might mean something <em>else</em> by “moral judgment.” But of course, my own metaethical views unavoidably lead my report down some paths and not others.</p>
<p>Personally, I use moral terms in such a way that my “moral judgments” are not about objective moral facts, but instead about my own values, idealized in various ways, such as by being better-informed (more on this in <a href="#AppendixA">Appendix A</a>).<a class="see-footnote" id="footnoteref12_294q9sa" title="I don't claim this is necessarily how other people use moral language, though.  " href="#footnote12_294q9sa">12</a> Under such a view, the question of (e.g.) whether some particular fish is a moral patient, given my values, is a question about whether that fish has certain properties (e.g. conscious experience, or desires that can be satisfied or frustrated) about which I have idealized preferences. For example: if the fish isn&#8217;t conscious, I&#8217;m not sure I care whether its preferences are satisfied or not, any more than I care whether a (presumably non-conscious) chess-playing computer wins its chess matches or not. But if the fish <em>is</em> conscious (in a certain way), then I probably <em>do</em> care about how much pleasure and how little pain it experiences, for the same reason I care about the pleasure and pain of my fellow humans.</p>
<p>Thus, my aim is not to conduct a conceptual analysis<a class="see-footnote" id="footnoteref13_wndl62d" title="For more on conceptual analysis, see Beaney (2014); King (2016).  " href="#footnote13_wndl62d">13</a> of “moral patient,” nor is my aim to discover what the objective moral facts are about which beings are moral patients. Instead, my aim is merely to examine which beings I should consider to be moral patients, given what I predict my values would be if they were better-informed, and idealized in other ways.</p>
<p>I suspect my metaethical approach and my moral judgments overlap substantially with those of at least some other Open Philanthropy Project staff members, and also with those of many likely readers, but I also assume there will be a great deal of non-overlap with my colleagues at the Open Philanthropy Project and especially with other readers. My only means for dealing with that fact is to explain as clearly as I can which judgments I am making and why, so that others can consider what the findings of this report might imply given their own metaethical approach and their own moral judgments.</p>
<p><span style="float: right;">[<a href="/node/858/edit/5">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="ProposedCriteria">Proposed criteria for moral patienthood</h4>
<p>Presumably a cognitively unimpaired adult human is a moral patient, and a rock is not.<a class="see-footnote" id="footnoteref14_d25ddn3" title="Though, some panpsychists would say a rock is conscious, and thus a moral patient if one also assumes consciousness is sufficient for moral patienthood.  " href="#footnote14_d25ddn3">14</a> But what about someone in a <a href="https://en.wikipedia.org/wiki/Persistent_vegetative_state">persistent vegetative state</a>? What about an <a href="https://en.wikipedia.org/wiki/Anencephaly">anencephalic infant</a>? What about a third-trimester human fetus?<a class="see-footnote" id="footnoteref15_drytp2y" title="See e.g. Lagercrantz (2016).  " href="#footnote15_drytp2y">15</a> What about future humans? What about chimpanzees, dogs, cows, chickens, fishes, squid, lobsters, beetles, bees, Venus flytraps, and bacteria? What about sophisticated artificial intelligence systems, such as Facebook&#8217;s face recognition system or a self-driving car?<a class="see-footnote" id="footnoteref16_qpphniz" title="See e.g. Basl (2014); Schwitzgebel &amp; Garza (2015); Bostrom &amp; Yudkowsky (2014); Sandberg (2014); Reggia (2013); Gamez (2008).  " href="#footnote16_qpphniz">16</a> What about a (so-called) self-aware, self-expressive, and self-adaptive camera network?<a class="see-footnote" id="footnoteref17_m3z9f4j" title="See e.g. Rinner et al. (2015).   " href="#footnote17_m3z9f4j">17</a> What about a non-player character in a first-person shooter video game, which makes plans and carries them out, ducks for cover when the player shoots virtual bullets at it, and cries out when hit?<a class="see-footnote" id="footnoteref18_aunemhh" title="See e.g. Tomasik (2015).  " href="#footnote18_aunemhh">18</a> What about the <a href="https://en.wikipedia.org/wiki/Enteric_nervous_system">enteric nervous system</a> in your gut, which employs about 5 times as many neurons as the brain of a rat, and would continue to autonomously coordinate your digestion even if its main connection with your brain was severed?<a class="see-footnote" id="footnoteref19_puasdyd" title="See Young (2012).  For a detailed discussion of whether the autonomic nervous system (of which the enteric nervous system is one part) satisfies various theories and criteria of consciousness, see Ryder (1996). For example, Ryder examines Daniel Dennett's theory about which processes are sufficient for various kinds of consciousness, and argues that those processes occur in the autonomic nervous system (ANS). Ryder reports:  In conversation, after I pointed out some of the complexities of ANS operation, [Dennett] suggested to me that the ANS would have approximately the same degree of consciousness as someone blind and deaf since birth.  If Dennett is right about that, then my moral intuitions suggest that I should consider the ANS a moral patient, for the same reasons I morally care about the subjective experiences of a human born blind and deaf. Others' moral intuitions may vary.  " href="#footnote19_puasdyd">19</a> Is each brain hemisphere in a split-brain patient a separate moral patient?<a class="see-footnote" id="footnoteref20_sp207hh" title="See e.g. Gazzaniga &amp; LeDoux (1978), ch. 7; Gazzaniga (1992), pp. 121- 137; Schechter (2012); Blackmon (2016); Marinsek &amp; Gazzaniga (2016); Pinto et al. (2017).  " href="#footnote20_sp207hh">20</a> Can ecosystems or companies or nations be moral patients?<a class="see-footnote" id="footnoteref21_kp7xhww" title="On ecosystems as moral patients, see e.g. Brennan &amp; Lo (2015) and Johnson (1993). On companies as moral patients, see e.g. Graham (2001). On nations as potentially conscious, and thus moral patients under some views, see e.g. Schwitzgebel (2015).  " href="#footnote21_kp7xhww">21</a></p>
<p>Such questions are usually addressed by asking whether a potential moral patient satisfies some <em>criteria for moral patienthood</em>. Criteria I have seen proposed in the academic literature include:</p>
<ul><li><em>Personhood</em> or <em>interests</em>. (I won&#8217;t discuss these criteria separately, as they are usually composed of one or more of the criteria listed below.<a class="see-footnote" id="footnoteref22_mp7najs" title="On personhood, see e.g. Newson (2007). On interests, see e.g. Jaworska &amp; Tannenbaum (2013).  " href="#footnote22_mp7najs">22</a>)</li>
<li><em>Phenomenal consciousness</em>, <em>a.k.a.</em> “subjective experience.” See the detailed discussion <a href="#MyApproach">below</a>.<a class="see-footnote" id="footnoteref23_mq24c2m" title="For discussions of the relevance of phenomenal consciousness to moral patienthood, see e.g. Shepherd &amp; Levy (forthcoming).  " href="#footnote23_mq24c2m">23</a></li>
<li><em>Capacity to suffer</em>: Typically, this presumes not just phenomenal consciousness but also some sense in which phenomenal consciousness can be “valenced.”</li>
<li><em>Various sophisticated cognitive capacities</em> such as rational agency, self-awareness, desires about the future, ability to abide by moral responsibilities, etc.<a class="see-footnote" id="footnoteref24_fppy7hy" title="See section 4.1 of Jaworska &amp; Tannenbaum (2013). Also see Wasserman et al. (2012).  " href="#footnote24_fppy7hy">24</a></li>
<li><em>Capacity to develop these sophisticated cognitive capacities</em>, e.g. as is true of human fetuses.<a class="see-footnote" id="footnoteref25_4o7d642" title="See section 4.2 of Jaworska &amp; Tannenbaum (2013).  " href="#footnote25_4o7d642">25</a></li>
<li><em>Less sophisticated cognitive capacities, or the capacity to develop them</em>, e.g. learning, nociception, memory, selective attention, etc.<a class="see-footnote" id="footnoteref26_comp5i5" title="See section 4.3 of Jaworska &amp; Tannenbaum (2013).  " href="#footnote26_comp5i5">26</a></li>
<li><em>Group membership</em>: e.g. all members of the human species, or all living things.<a class="see-footnote" id="footnoteref27_6smhaeb" title="See references in sections 4.4 and 4.5 of Jaworska &amp; Tannenbaum (2013) and the discussion of deep ecology in Brennan &amp; Lo (2015).  " href="#footnote27_6smhaeb">27</a></li>
</ul><p>Note that moral patienthood can be seen as binary or scalar,<a class="see-footnote" id="footnoteref28_c7nil48" title="Jaworska &amp; Tannenbaum (2013):  Accounts differ on what it is about the individual that grounds or confers moral status and to what degree, with implications for which beings do or do not have moral status and for their comparative status… For each account discussed, one could hold either a threshold or scalar conception of moral status, though the former is more commonly found in the literature… According to the threshold conception, as it is usually discussed, if capacity C grounds FMS [full moral status], then any being that has C, regardless of how well it can exercise this capacity, has as much moral status as any other being that has C and this status is full. If C is not only sufficient but necessary for FMS, then all beings lacking C would not have FMS, though the threshold conception would nevertheless leave it open whether having some other feature (e.g., parts of C or something lesser but akin to C) might ground lesser degrees of moral status. In contrast, a scalar conception of moral status would hold that if capacity C grounds moral status, then any being who has C has some status; the better it can exercise this capacity, the higher its degree of moral status… [Or] instead of focusing on how well capacity C is exercised, the views could instead focus on the number of relevant capacities a being has. A threshold view might specify some number n of the relevant capacities as both necessary and sufficient for FMS. A scalar conception would hold, on the other hand, that a being with n+1 capacities would have a higher moral status than one with merely n capacities.  " href="#footnote28_c7nil48">28</a> and the boundary between beings that are and are not moral patients might be “fuzzy” (see <a href="#fuzzy">below</a>).</p>
<p>It is also important to remember that, whatever criteria for moral patienthood we endorse upon reflection, our intuitive attributions of moral patienthood are probably unconsciously affected<a class="see-footnote" id="footnoteref29_ibubqrf" title="Some authors distinguish &quot;unconscious&quot; from &quot;non-conscious,&quot; but I use these terms interchangeably.  " href="#footnote29_ibubqrf">29</a> by factors that we would <em>not</em> endorse if we understood how they were affecting us. For example, we might be more likely to attribute moral patienthood to something if it has a roughly human-like face, even though few if any of us would endorse “possession of a human-like face” as a legitimate criterion of moral patienthood. A similar warning can be made about factors which might affect our attributions of phenomenal consciousness and other proposed criteria for moral patienthood.<a class="see-footnote" id="footnoteref30_mk7pzi6" title="The findings in this literature are relatively new and under substantial debate. For reviews, see Sytsma (2014); Goodwin (2015); Jack &amp; Robbins (2012).  " href="#footnote30_mk7pzi6">30</a></p>
<p>An interesting test case is <a href="https://www.youtube.com/watch?v=iwJgIaVzjvA&amp;t=1m01s">this video</a> of a crab tearing off its own claw. To me, the crab looks “nonchalant” while doing this, which gives me the initial intuition that the crab must not be consciousness, or else it would be “writhing in agony.” But crabs are different from humans in many ways. Perhaps this is just what a crab in conscious agony looks like. Or perhaps not.<a class="see-footnote" id="footnoteref31_tr1oadx" title="For other examples of self-amputation behaviors, see Wikipedia's article on autootomy; Maginnis (2006); Fleming et al. (2007).  " href="#footnote31_tr1oadx">31</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/6">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="WhyI">Why I investigated phenomenal consciousness first</h4>
<p>The only proposed criterion of moral patienthood I have investigated in any depth thus far is phenomenal consciousness. I chose to examine phenomenal consciousness first because:</p>
<ol><li>My impression is that phenomenal consciousness is perhaps the most commonly-endorsed criterion of moral patienthood, and that it is often considered to be the most <em>important</em> such criterion (by those who use multiple criteria). Self-consciousness and capacity to suffer are other contenders for being the most commonly-endorsed criterion of moral patienthood, but in most cases it is assumed that the kinds of self-consciousness or capacity to suffer that confer moral patienthood necessarily involve phenomenal consciousness as well.</li>
<li>Phenomenal consciousness, or a capacity to suffer that presumes phenomenal consciousness, are especially commonly-endorsed criteria of moral patienthood among <em>consequentialists</em>, whose normative theories most easily map onto our mission to “accomplish as much good as possible with our giving.”</li>
<li>Personally, I&#8217;m not sure whether consciousness is the <em>only</em> thing I care about, but it is the criterion of moral patienthood I feel <em>most</em> confident about (for my own values, anyway).</li>
</ol><p>However, it&#8217;s worth bearing in mind that most of us probably intuitively morally care about other things besides consciousness. I focus on consciousness in this report not because I&#8217;m confident it&#8217;s the <em>only</em> thing that matters, but because my report on <em>consciousness alone</em> is long enough already! I hope to investigate other potential criteria for moral patienthood in the future.</p>
<p>I&#8217;m especially eager to think more about <em>capacity to suffer</em>, i.e. “valenced” experiences such as pain and pleasure. As I explain below, my own intuitions are that if a being had conscious experience, but <em>literally</em> none of it was “valenced” in any way, then I <em>might</em> not have any moral concern for such a creature. But in this report, I focus on the issue of phenomenal consciousness itself, and say very little about the issue of <em>valenced</em> experience.</p>
<p><span style="float: right;">[<a href="/node/858/edit/7">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="MyApproach">My approach to thinking about consciousness</h3>
<p>In consciousness studies there is so little consensus on <em>anything</em> — what&#8217;s meant by “consciousness,” what it&#8217;s made of, what kinds of methods are useful for studying it, how widely it is distributed, which theories of consciousness are most promising, etc. (see <a href="#AppendixZ5">Appendix Z.5</a>) — that there are no safe guesses about “where someone is coming from” when they write about consciousness. This often made it difficult for me to understand what writers on consciousness were trying to say, as I read through the literature.</p>
<p>To mitigate this problem somewhat for this explanation of my own tentative views about consciousness, I&#8217;ll try to explain “where I&#8217;m coming from” on consciousness, even if I can&#8217;t afford the time to explain in much detail <em>why</em> I make the assumptions I do.</p>
<p><span style="float: right;">[<a href="/node/858/edit/8">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="Defined">Consciousness, innocently defined</h4>
<p><a href="http://plato.stanford.edu/entries/consciousness/">Van Gulick (2014)</a> describes six different senses in which “an animal, person, or other cognitive system” can be regarded as “conscious,” and the four I can explain most quickly are:</p>
<ul><li><em>Sentience</em>: capable of sensing and responding to its environment</li>
<li><em>Wakefulness</em>: awake (e.g. not asleep or in a coma)</li>
<li><em>Self-consciousness</em>: aware of itself as being aware</li>
<li><em>What it is like</em>: subjectively experiencing<a class="see-footnote" id="footnoteref32_ahyddbm" title="In this report I use &quot;subjective experience&quot; as a synonym for &quot;phenomenal consciousness.&quot; Some authors use the term &quot;experience&quot; more broadly, though. For example, Carruthers (1992), pp. 170-171 writes:  Suppose that Abbie is driving her car over a route she knows well, her conscious attention wholly abstracted from her surroundings. Perhaps she is thinking deeply about some aspect of her work, or fantasising about her next summer holiday, to the extent of being unaware of what she is doing on the road. Suddenly she 'comes to,' returning her attention to the task in hand with a startled realisation that she has not the faintest idea what she has been doing or seeing for some minutes past. Yet there is a clear sense in which she must have been seeing, or she would have crashed the car. Her passenger sitting next to her may correctly report that she had seen a vehicle double-parked by the side of the road, for example, since she deftly steered the car around it. But she was not aware of seeing that obstacle, either at the time or later in memory.  Another example: when washing up dishes I generally put on music to help pass the time. If it is a piece that I love particularly well I may become totally absorbed, ceasing to be conscious of what I am doing at the sink. Yet someone observing me position a glass neatly on the rack to dry between two coffee mugs would correctly say that I must have seen that those mugs were already there, or I should not have placed the glass where I did. Yet I was not aware of seeing those mugs, or of placing the glass between them. At the time I was swept up in the Finale of Schubert's Arpeggione Sonata, and if asked even a moment later I should not have been able to recall what I had been looking at.  Let us call such experiences non-conscious ones. What does it feel like to be the subject of a non-conscious experience? It feels like nothing. It does not feel like anything to have a non-conscious visual experience as of a vehicle parked at the side of the road, or as of two coffee mugs placed on a draining rack — precisely because to have such an experience is not to be conscious of it. Only conscious experiences have a distinctive phenomenology, a distinctive feel. Non-conscious experiences are ones that may help to control behavior without being felt by the conscious subject.  [These points] are already sufficient to show that it is wrong to identify the question [of] whether a creature has experiences with the question [of] whether there is something it feels like to be that thing. For there is a class — perhaps a large class — of non-conscious experiences that have no phenomenology.  In contrast to Carruthers' usage of &quot;experience,&quot; I shall in this report only use the phrase &quot;subjective experience&quot; to refer to what Carruthers calls &quot;conscious experiences.&quot;  " href="#footnote32_ahyddbm">32</a> a certain “something it is like to be” (<a href="http://www.jstor.org/stable/2183914?origin=crossref">Nagel 1974</a>), <em>a.k.a</em> “phenomenal consciousness” (<a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=6740052&amp;fulltextType=RA&amp;fileId=S0140525X00038188">Block 1995</a>), <em>a.k.a</em> “raw feels” (<a href="https://books.google.com/books?id=RBwonQEACAAJ">Tolman 1932</a>)</li>
</ul><p>When I say “consciousness,” I have in mind the fourth concept.<a class="see-footnote" id="footnoteref33_p6jy861" title="I do not, however, assume (like Block) that &quot;phenomenal consciousness&quot; must be &quot;distinct from any cognitive, intentional, or functional property.&quot; In Weisberg (2011)'s terms, I intend a &quot;moderate&quot; rather than &quot;zealous&quot; reading of the phrase &quot;phenomenal consciousness.&quot;  " href="#footnote33_p6jy861">33</a></p>
<p>In particular, I have in mind a relatively “metaphysically and epistemically innocent” definition, <em>a la</em> <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00018">Schwitzgebel (2016)</a>:<a class="see-footnote" id="footnoteref34_om62x2g" title="Schwitzgebel is hardly the first to propose such a definition for &quot;consciousness.&quot; As Schwitzgebel notes:  Definition by example is a common approach among recent phenomenal realists. I interpret Searle (1992, p. 83), Block (1995/2007, p. 166–8), and Chalmers (1996, p. 4) as aiming to define phenomenal consciousness by a mix of synonymy and appeal to example…  See also e.g. Shiller (2016):  I intend for the content of the concept [of &quot;qualia&quot;] to be fixed by its prototypical examples. Qualia are whatever kind of mental qualities we associate with experiences of redness, pain, satisfaction, and déjà vu. This approach leaves the veracity of our intuitive assumptions open to investigation.  Note that Schwitzgebel's additional &quot;wonderfulness&quot; criterion also seems useful in a definition of consciousness. I skip discussing it here merely for brevity.  " href="#footnote34_om62x2g">34</a></p>
<blockquote><p>Phenomenal consciousness can be conceptualized innocently enough that its existence should be accepted even by philosophers who wish to avoid dubious epistemic and metaphysical commitments such as dualism, infallibilism, privacy, inexplicability, or intrinsic simplicity. Definition by example allows us this innocence. Positive examples include sensory experiences, imagery experiences, vivid emotions, and dreams. Negative examples include growth hormone release, dispositional knowledge, standing intentions, and sensory reactivity to masked visual displays. Phenomenal consciousness is the most folk psychologically obvious thing or feature that the positive examples possess and that the negative examples lack. Phenomenal consciousness is the most folk-psychologically obvious thing or feature that the positive examples possess and that the negative examples lack…</p></blockquote>
<p>There are many other examples we can point to.<a class="see-footnote" id="footnoteref35_mab7s3l" title="For example, see Chalmers' &quot;catalog of conscious experiences&quot; on pp. 4-9 of Chalmers (1996), and also Perry (2009).  " href="#footnote35_mab7s3l">35</a> For example, when I played sports as a teenager, I would occasionally twist my ankle or acquire some other minor injury while chasing after (e.g.) the basketball, and I didn&#8217;t realize I had hurt myself until after the play ended and I exited my <a href="https://en.wikipedia.org/wiki/Flow_(psychology)">flow state</a>. In these cases, a “rush of pain” suddenly “flooded” my conscious experience — not because I had just then twisted my ankle, but because I had twisted it 5 seconds earlier, and was only just then becoming aware of it. The pain I felt 5 seconds <em>after</em> I twisted my ankle is a positive example of conscious experience, and whatever injury-related processing occurred in my nervous system <em>during</em> those initial 5 seconds is, as far as I know, a negative example.</p>
<p>However, I would qualify Schwitzgebel&#8217;s extensional definition of consciousness by noting that the negative examples, in particular, are at least <em>somewhat</em> contested. A rock is an obvious negative example for most people, but panpsychists disagree, and it is easy to identify other contested examples.<a class="see-footnote" id="footnoteref36_t60ucxy" title="Another contested negative example is this: one might be tempted to say that in a binocular rivalry task (see e.g. Sterzer 2013), the image the subject self-reports as having experienced (at a given time) provides a positive example of consciousness, and the other image that was not consciously experienced despite being processed to some extent by the brain provides a negative example.  Yet another contested negative example is illustrated by an online exchange between Scott Aaronson and Guilio Tononi, discussed in Cerullo (2015). Tononi has developed the Integrated Information Theory (IIT) of consciousness, according to which consciousness is equal to a measure of integrated information denoted Φ (&quot;phi&quot;). Aaronson (2014a), in reply to Tononi's theory, argued that IIT &quot;unavoidably predicts vast amounts of consciousness in physical systems that no sane person would regard as particularly 'conscious' at all.&quot; To illustrate this, Aaronson defined a particular kind of expander graph that, according to IIT, has enormous amounts of consciousness, despite not doing anything remotely intelligent, nor exhibiting any features we typically think of as &quot;conscious.&quot; Tononi agreed that certain kinds of simple systems could generate arbitrarily large values of Φ, but disagreed with Aaronson that we should credit our intuitions that such mathematical objects cannot be enormously more conscious than humans (Tononi 2014).  Another contested negative example is dreamless sleep, which is often given as a paradigm case of a state during which one has no phenomenal experience. However, this example has recently been contested (Windt et al. 2016).  The &quot;distracted driver&quot; case seems to be another contested example. For me, it is most natural to say the stimuli to which the distracted driver is responding (e.g. to keep the car in the correct lane), but which she has no memory of consciously experiencing and which she cannot report (because her conscious attention was focused on her cell phone call), were not consciously experienced (as far as we know). But Tye (2016) seems instead to count the distracted driver's processing of stimuli (that she can't remember or report experiencing) as an example of conscious experience (pp. 14-15):  Take, for example, the visual experiences of the distracted driver as she drives her car down the road. She is concentrating hard on other matters (the phone call she is answering about the overdue rent; the coffee in her right hand, etc.), so her visual experiences are unconscious. But her experiences exist alright. How else does she keep the car on the road?…  …Sometimes when we say that a mental state is conscious, we mean that it is, in itself, an inherently conscious state. At other times, when we say that a mental state is conscious, we have in mind the subject's attitude toward the state. We mean that the subject of the mental state is conscious of it or conscious that it is occurring. The latter consciousness is a species of what is sometimes called &quot;creature consciousness.&quot; The [view I've articulated] has it that, in the first sense, a mental state is conscious (conscious1) if and only if it is an experience. In the latter sense, a mental state is conscious (conscious2) if and only if another conscious1 state, for example, a conscious thought, is directed upon it. [My view] holds that this higher-order conscious state (in being a conscious1 state) is itself an experience, for example, the experience of thinking of the first-order state or thinking that the first-order state is occurring.  The visual experiences of the distracted driver are unconscious in that she is not conscious of them. Nor is she conscious that they are occurring. So she lacks creature consciousness with respect to certain mental states that are themselves conscious1. Being distracted, she is not conscious of what those experiences are like. There is no inconsistency here. The [distracted driver] objection conflates high-order consciousness with first-order consciousness. Experiences are first-order conscious states on which second-order conscious states may or may not be directed.  A similar example is provided by Block (1995):  …suppose you are engaged in intense conversation when suddenly at noon you realize that right outside your window there is — and has been for some time — a deafening pneumatic drill digging up the street. You were aware of the noise all along, but only at noon are you consciously aware of it. That is, you were [phenomenally conscious] of the noise all along…  But here, I am instead inclined to say that, in this hypothetical scenario, I was not phenomenally conscious of the pneumatic drill. Or, perhaps somewhere in my brain there was a phenomenally conscious experience of the pneumatic drill, but I don't yet have any (introspective) evidence of that.  " href="#footnote36_t60ucxy">36</a></p>
<p>More plausible than rock consciousness, I think, is the possibility that somewhere in my brain, there was a conscious experience of my injured ankle before “I” became aware of it. Indeed, there may be <em>many</em> conscious cognitive processes that “I” never have cognitive access to. If this is the case, it can in principle be <em>weakly</em> suggested by certain kinds of studies (see <a href="#AppendixH">Appendix H</a>), and could in principle be <em>strongly</em> suggested once we have a compelling theory of consciousness. (Compare to the situation in physics, where strongly-confirmed theories give us strong reasons to believe in the presence of structures we cannot directly observe in the usual way.) But for now, I&#8217;ll count the injury-related cognitive processing that happened “before I noticed it” as a <em>likely negative</em> example of conscious experience, while allowing that it could be discovered to be a positive example due to future scientific progress.</p>
<p>So perhaps we should say that “Phenomenal consciousness is the most folk psychologically obvious thing (or set of things) that the uncontested positive examples possess and that the least-contested negative examples <em>plausibly</em> lack,”<a class="see-footnote" id="footnoteref37_fzawukj" title="Or, if we want to make things more complicated, we could construct a spectrum from clear positive examples to clear negative examples, similar to Baars (1988), Figure 1.1 (p. 12).  " href="#footnote37_fzawukj">37</a> or something along those lines. Similarly, when I use related terms like “qualia” and “phenomenal properties,” I intend them to be defined by example as above, with as much metaphysical innocence as possible. Ideally, one would “flesh out” these definitions with many more examples and clarifications, but I shall leave that exercise to others.<a class="see-footnote" id="footnoteref38_rfa7r7y" title="In particular, as I have already hinted, my Schwitzgebel-inspired definition of consciousness may have some trouble distinguishing &quot;phenomenal consciousness&quot; from &quot;access consciousness&quot; (in roughly the sense of Block 1995), if indeed the two can be distinguished. In any case, I expect our consciousness-related definitions to evolve and become more useful as we learn more.  " href="#footnote38_rfa7r7y">38</a></p>
<p>Importantly, this definition is as “innocent” and theory-neutral as I know how to make it. On this definition, consciousness could still be physical or non-physical, scientifically tractable or intractable, ubiquitous or rare, ineffable or not-ineffable, “real” or “illusory” (see next section), and so on. And in my revised version of Schwitzgebel&#8217;s definition, we are not committed to absolute certainty that purported negative examples will turn out to <em>actually</em> be negative examples as we learn more.</p>
<p>Furthermore, I do not define consciousness as “cognitive processes I morally care about,” as that blends together scientific explanation and moral judgment (see <a href="#AppendixG">Appendix G</a>) in a way can be confusing to disentangle and interpret.</p>
<p>No doubt our concept of “consciousness” and related concepts will evolve over time in response to new discoveries, and our evolving concepts will influence which empirical inquiries we prioritize, and those inquiries will suggest further revisions to our concepts, as is typically the case.<a class="see-footnote" id="footnoteref39_fy5ry7o" title="For examples, see the sources relating to the evolution of scientific concepts listed in Appendix Z.6 and this footnote. See also e.g. Wimsatt (2007), especially ch. 6.  " href="#footnote39_fy5ry7o">39</a> But in our current state of ignorance, I prefer to use a notion of “consciousness” that is defined as innocently as I can manage.</p>
<p>I must also stress that my aim here is <em>not</em> to figure out what we “mean” by “consciousness,” any more than <a href="https://en.wikipedia.org/wiki/Antonie_van_Leeuwenhoek">Antonie van Leeuwenhoek</a> (1632-1723) was, in studying microbiology, trying to figure out what people meant by “life.”<a class="see-footnote" id="footnoteref40_hy12szm" title="I borrow this example from McDermott (2001), pp. 25-26:  Suppose one had demanded of Van Loewenhook and his contemporaries that they provide a similar sort of definition for the concept of life and its subconcepts, such as respiration and reproduction. It would have been a complete waste of time, because what Van Loewenhook wanted to know, and what we are now figuring out, is how life works. We know there are borderline cases, such as viruses, but we don't care exactly where the border lies, because our understanding encompasses both sides. The only progress we have made in defining &quot;life&quot; is to realize that it doesn't need to be defined. Similarly, what we want to know about minds is how they work.  " href="#footnote40_hy12szm">40</a> Rather, my aim is to understand how the cluster of stuff we now naively call “consciousness” <em>works</em>. Once we understand how those things work, we&#8217;ll be in a better position to make moral judgments about which beings are and aren&#8217;t moral patients (insofar as consciousness-related properties affect those judgments, anyway). Whether we continue to use the concept of “consciousness” at that point is of little consequence. But for now, since we don&#8217;t yet know the details of how consciousness works, I will use terms like “consciousness” and “subjective experience” to point at the ill-defined cluster of stuff I&#8217;m talking about, as defined by example above.</p>
<p><span style="float: right;">[<a href="/node/858/edit/9">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="Nature">My assumptions about the nature of consciousness</h4>
<p>Despite prefering a metaphysically innocent <em>definition</em> of consciousness, I will, for this report, make four key assumptions about the <em>nature</em> of consciousness. It is beyond the scope of this report to survey and engage with the arguments for or against these assumptions; instead, I merely report what my assumptions are, and provide links to the relevant scholarly debates. My purpose here isn&#8217;t to contribute to these debates, but merely to expain “where I&#8217;m coming from.”</p>
<p>First, I assume <strong>physicalism</strong>. I assume consciousness will turn out to be fully explained by physical processes.<a class="see-footnote" id="footnoteref41_wma3bbn" title="In the context of consciousness, some philosophers use &quot;physicalism&quot; to refer to &quot;identity theory&quot; (Smart 2007), but that isn't how I use the term.  In brief, I assume physicalism about consciousness for two major reasons.  First, it seems to me that physicalism is supported by much stronger evidence than can be assembled by the human intuitions used to argue against it. To illustrate: which of these do you think has greater evidential justification?   &quot;The universe is a mathematically simple low-level unified causal process with no non-natural elements or attachments.&quot; (This phrasing is from Eliezer Yudkowsky's &quot;Executable philosophy.&quot;) My intuitive judgments about anti-physicalist thought experiments involving zombies, &quot;Mary the super-scientist,&quot; etc.   I think the evidence for (1) is overwhelming at this point, and the trust we should put in (2) is pretty weak. For that reason, I'm comfortable betting that the mystery of consciousness, like every mystery ever solved before it, will eventually be resolved (if it is ever resolved) by understanding some set of physical processes better than we do today — and that the poorly-understood stuff we're trying to point at with words like &quot;consciousness&quot; will turn out to be constituted by some set of physical processes. (Here, I mean &quot;physical processes&quot; in a broad sense that includes, e.g. water conceived of as H2O.)  Second, the assumption of physicalism has been enormously productive in the past, and has generally seemed ever more reasonable as evidence has accumulated about any given phenomenon. On this, see the Appendix of Papineau (2002), Papineau (2009), and just about any history of any science. (For arguments against this fairly standard view, see e.g. section 1.1 of Goff 2017.)  " href="#footnote41_wma3bbn">41</a> Specifically, I lean toward a variety of physicalism called “type A materialism,” or perhaps toward the varieties of “type Q” or “type C” materialism that threaten to collapse into “type A” materialism anyway (see footnote<a class="see-footnote" id="footnoteref42_xmyeryb" title="See Chalmers (2003)'s explanation of different types of physicalism/materialism. Note that most physicalists in philosophy appear to be &quot;type B&quot; materialists (see Yetter-Chappell 2015, footnote 2).  Technically, I can see a case for classifying my view as either &quot;type Q materialism&quot; or &quot;type C materialism&quot; (i.e. &quot;physicalism, fingers crossed&quot; as Tye 2000, p. 22 puts it), but if so, then I'm the sort of type Q or type C materialist (about consciousness) for whom the usual pros and cons of type A materialism arise in more-or-less the same form (rather than, say, the usual pros and cons of type B materialism).  Either way, I should clarify that unlike some type A materialists, I don't think that merely explaining verbal reports and beliefs is all that needs to be explained. Even if external-to-me scientists could explain my beliefs and verbal reports, I would still want to additionally explain why it feels like something to be me. (See also Chalmers 2010, pp. 52-58.) I just think that in the end, explaining certain functions will explain everything there is to explain, and hence I am probably best described as a type A materialist.  " href="#footnote42_xmyeryb">42</a>).</p>
<p>Second, I assume <strong>functionalism</strong>. I assume that anything which “does the right thing” — e.g., anything which implements a certain kind of information processing — is an example of consciousness, regardless of what that thing is made of.<a class="see-footnote" id="footnoteref43_atzfhty" title="Here is a fuller explanation of functionalism, from Mandik (2013), p. 110:  Two key ideas that functionalists have appealed to in developing their position are the ideas of a functional kind and of a multiply realizable kind. A kind is a grouping of things or entities, usually grouped in terms of one or more features common to members of the group. Examples of kinds include cats, diamonds, planets, and mousetraps. To illustrate the idea of a multiply realizable kind, let us draw a contrast between diamonds, which are not multiply realizable, and mousetraps, which are. What makes something a diamond? First off, a diamond has to be made out of carbon. Anything superficially resembling a diamond that is not made out of carbon is not a genuine diamond. Crystals of zirconium dioxide superficially resemble diamonds, but are composed of the chemical elements zirconium and oxygen. Further, the carbon atoms that compose diamonds need to be arranged in a certain way (tetrahedral lattices). Carbon atoms not so arranged make up coal and graphite, not diamonds.  Diamonds may be physically realized in only one way — with tetrahedral lattices of carbon atoms. Thus they are not multiply realizable. Contrast this with mousetraps, which are multiply realizable. There are many ways to make a mousetrap. Some involve metal spring-loaded killing bars mounted on wooden platforms. Others involve a strong sticky glue applied to a flat surface on which the mouse gets stuck. There is no particular chemical element that is necessary for making a mousetrap.  Mousetraps help to illustrate not just the idea of multiply realizable kinds, but also the idea of functional kinds. Functional kinds are defined by what they do, and are so named because they are defined by the function they perform. Mousetraps perform the function of restraining or killing mice… As long as a system is able to achieve its defining function, it is largely irrelevant which physical stuff it happens to be realized by.  Mandik continues (pp. 110-111) with another important point about functionalism and consciousness:  Much of the contemporary enthusiasm for functionalism stems from enthusiasm about analogies drawn between minds and computers… Computers are clearly both functional kinds and multiply realizable kinds. What makes something a computer is what it does — it computes…  All sorts of materials can be deployed to construct computers. Computers have been built from transistors and other electronic components. Others have been built from mechanical components such as cams and gears. A computer that plays tic-tac-toe has even been constructed out of Tinkertoys!  …a [computer] program is not identical to the activity of a particular computer. If brains made out of brainy stuff can just as well give rise to a mind as an electronic computer made out of non-brainy stuff, then perhaps [many functionalists suggest] the solution to the mind–body problem is to think of the mind as the software that is running on the hardware of the brain.  I should clarify, however, that even those functionalists who speak of mind (including consciousness) as a type of computation don't necessarily think a human mind is a &quot;traditional&quot; computer program (e.g. a GOFAI program) running on a brain-implemented Von Neumann architecture, nor do they necessarily think that all relevant information processing occurs at the scale of neurons or larger — see e.g. Edelman (2008), chs. 1-4.  I use computational language regularly in this report because I think it helps to clarify what I mean, but I have not studied the philosophy of computation (Turner 2013) much, and I don't mean to assume any particular narrow conception of what does and doesn't count as &quot;computation&quot; or a &quot;computer program.&quot; My assumption for this report is just functionalism, broadly defined.  There are different types of functionalism (about consciousness), of course (Block 1978; Van Gulick (2009); ch. 9 of Prinz 2012; Maley &amp; Piccinini 2013). Intuitively, the sort of functionalism (about consciousness) to which I subscribe is perhaps most similar to what Aaron Sloman calls &quot;virtual machine functionalism&quot; (Sloman &amp; Chrisley 2003; Sloman 2016).  " href="#footnote43_atzfhty">43</a> Compare to various kinds of memory, attention, learning, and so on: these processes are found not just in humans and animals, but also in, for example, some artificial intelligence systems.<a class="see-footnote" id="footnoteref44_7pced46" title="For a recent review of the many parallels between contemporary neuroscience and machine learning research, see Marblestone et al. (2016).  " href="#footnote44_7pced46">44</a> These kinds of memory, attention, and learning are implemented by a wide variety of substrates (but, they are all <em>physical</em> substrates).</p>
<p>Third, I assume <a name="illusionism" id="illusionism"></a><strong>illusionism</strong>, at least about <em>human</em> consciousness. What this means is that I assume that some seemingly-core features of human conscious experience are illusions, and thus need to be “explained away” rather than “explained.” Consider your <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)">blind spot</a>: your vision appears to you as continuous, without any spatial “gaps,” but physiological inspection shows us that there aren&#8217;t any rods and cones where your optic nerve exits the eyeball, so you can&#8217;t <em>possibly</em> be transducing light from a certain part of your (apparent) visual field. Knowing this, the job of cognitive scientists studying vision is not to explain how it is that your vision is <em>really</em> continuous despite the existence of your physiological blind spot, but instead to explain why your visual field <em>seems</em> to be continuous even though it&#8217;s not.<a class="see-footnote" id="footnoteref45_wshh6i0" title="And indeed, careful experiments have taught us much about how this illusion is produced. See e.g. Pessoa &amp; Weerd (2003).  " href="#footnote45_wshh6i0">45</a> We might say we are “illusionists” about continuous visual fields in humans. Similarly, I think some core features of consciousness are illusions, and the job of cognitive scientists is not to explain how those features are “real,” but rather to explain why they <em>seem to us</em> to be real (even though they&#8217;re not). For example, it <em>seems to us</em> that our conscious experiences have “intrinsic” properties <em>beyond</em> that which could ever be captured by a functional, mechanistic account of consciousness. I agree that our experiences <em>seem to us</em> to have this property, but I think this “seeming” is simply mistaken. Consciousness (as defined <a href="#Defined">above</a>) is real, of course. There is “something it is like” to be us, and I doubt there it is “something it is like” to be a chess-playing computer, and I think the difference is morally important. I just think our intuitions mislead us about some of the <em>properties</em> of this “something it&#8217;s like”-ness. (For elaborations on these points, see <a href="#AppendixF">Appendix F</a>.)</p>
<p>Finally, I assume <a name="fuzzy" id="fuzzy"></a><strong>fuzziness</strong> about consciousness.<a class="see-footnote" id="footnoteref46_8kntxbz" title="Unlike physicalism, functionalism, and illusionism, &quot;fuzziness&quot; is not a standard term.  " href="#footnote46_8kntxbz">46</a> I assume there will <em>not</em> be a clear dividing line between systems that have <em>no conscious experience at all</em> and things that have <em>any conscious experience whatsoever</em>, just as there isn&#8217;t a clear dividing line between systems which do and don&#8217;t implement various forms of “attention,” “memory,” “self-modeling,” and so on. As a consequence, “wondering whether it is ‘probable’ that all mammals have [consciousness] thus begins to look like wondering whether or not any birds are <em>wise</em> or reptiles have <em>gumption</em>: a case of overworking a term from folk psychology that has [lost] its utility along with its hard edges.”<a class="see-footnote" id="footnoteref47_fz2gjqm" title="Dennett (1995).  " href="#footnote47_fz2gjqm">47</a> As the scientific study of “consciousness” proceeds, I expect our naive concept of consciousness to break down into a variety of different capacities, dispositions, representations, and so on, each of which will vary along many different dimensions. As that happens, we&#8217;ll be better able to talk about which features we morally care about and why, and there won&#8217;t be much utility to arguing about “where to draw the line” between which things are and aren&#8217;t “conscious.” But, given that we currently lack such a detailed decomposition of “consciousness,” I reluctantly organize this report around the concept of consciousness and the question of “which beings are conscious,” while pleading with the reader to remember that the line between what is and isn&#8217;t “conscious” is (I claim) extremely “fuzzy.” (For more on the fuzziness of consciousness, see <a href="#AppendixG">Appendix G</a>.)</p>
<p>My assumptions of physicalism and functionalism are quite confident, but probably don&#8217;t affect my conclusions about the distribution of consciousness very much anyway, except to make radical panpsychism less plausible. My assumption of illusionism is also quite confident, at least about <em>human</em> consciousness, but I&#8217;m not sure it implies much about the distribution question (see <a href="#AppendixF">Appendix F</a>). My assumption of fuzziness is moderately confident, and implies that the distribution question is difficult even to formulate, let alone answer, though I&#8217;m not sure it directly implies much about how extensive we should expect “consciousness” to be.</p>
<p>As with any similarly-sized set of assumptions about consciousness (see <a href="#AppendixZ5">Appendix Z.5</a>), my own set of assumptions is controversial, and endorsed only by a small minority of consciousness researchers.<a class="see-footnote" id="footnoteref48_raanpdr" title="As also mentioned in Appendix Z.5, the 2009 PhilPapers Survey found that among &quot;Target Faculty,&quot; 56.5% of respondents accepted or leaned toward physicalism about the mind, 27.1% of respondents accepted or leaned toward non-physicalism about the mind, and 16.4% of respondents gave an &quot;Other&quot; response.  The PhilPapers Survey did not ask about functionalism directly. But, it seems to be a widely held understanding that the vast majority of philosophers of mind, but not all of them, are functionalists. Some example quotes are given below, in chronological order.  Block (1978):  The functionalist approach to the philosophy of mind is increasingly popular; indeed, it may now be dominant (Armstrong, 1968; Block &amp; Fodor, 1972; Field, 1975; Fodor, 1965, 1968a; Grice, 1975; Harman, 1973; Lewis, 1971, 1972; Locke, 1968; Lycan, 1974; Nelson, 1969, 1975; Putnam, 1966, 1967, 1970, 1975a; Pitcher, 1971; Sellars, 1968; Shoemaker, 1975; Smart, 1971; Wiggins, 1975).  Churchland (1988), ch. 2:  As this book is written, functionalism is probably the most widely held theory of mind among philosophers, cognitive psychologists, and artificial intelligence researchers.  Ryder (1996):   Most theories of consciousness are functional theories, as functionalism is something of a &quot;received view&quot; among materialists.  Macphail (1998), p. 213:  The idea that consciousness is a product of functional organization lies at the heart of what is now the most widely held materialist account of the mind-body problem — so widely held that it has been claimed [by Searle (1992), p. 7] that functionalism now constitutes a virtual orthodoxy among psychologists and philosophers.  Gray (2004), p. vii, says &quot;Today's dominant view is functionalism,&quot; though interestingly Gray defines functionalism as &quot;the doctrine that states of consciousness can be identified with sets of functional (input-output) relationships that hold between a behaving organism and the environment in which it behaves, which is closer to how I might define behaviorism. In my sense of the term, functionalism need not be defined with respect to input-output relationships that hold between a behaving organism and its environment — see e.g. my comments on consciousness inessentialism.  Kim (2010), ch. 5:  In 1967 Hilary Putnam published a paper… [that] ushered in functionalism, which has since been a highly influential — arguably the dominant — position on the nature of mind.  Mandik (2013), p. 122:  Functionalism is the most popular current position on the mind–body problem…  Reggia (2013):  It is probably the case that the vast majority of individuals investigating the philosophical and scientific basis of consciousness today, including those developing computer models of consciousness, are functionalists…  Heil (2013), p. 87:  These days functionalism dominates the landscape in the philosophy of mind, in cognitive science, and in psychology… When basic tenets of functionalism are put to non-philosophers, the response is, often enough, &quot;Well, that's obvious, isn't it?&quot;  Nevertheless, functionalism is debated heavily within philosophy. Those arguments are well-covered elsewhere: see e.g. chapter 6 of Weisberg (2014); Block (2007a); Levin (2013); Tye (2015); Polger &amp; Shapiro (2016).  Note that some of the theories I consider &quot;functionalist&quot; are sometimes called &quot;eliminativist.&quot; See my comments on eliminativism in Appendix Z.6.  I'm not aware of surveys indicating how common illusionist approaches are, though Frankish (2016a) remarks that:  The topic of this special issue is the view that phenomenal consciousness (in the philosophers' sense) is an illusion — a view I call illusionism. This view is not a new one: the first wave of identity theorists favoured it, and it currently has powerful and eloquent defenders, including Daniel Dennett, Nicholas Humphrey, Derk Pereboom, and Georges Rey. However, it is widely regarded as a marginal position, and there is no sustained interdisciplinary research programme devoted to developing, testing, and applying illusionist ideas. I think the time is ripe for such a programme. For a quarter of a century at least, the dominant physicalist approach to consciousness has been a realist one. Phenomenal properties, it is said, are physical, or physically realized, but their physical nature is not revealed to us by the concepts we apply to them in introspection. This strategy is looking tired, however. Its weaknesses are becoming evident…, and some of its leading advocates have now abandoned it. It is doubtful that phenomenal realism can be bought so cheaply, and physicalists may have to accept that it is out of their price range. Perhaps phenomenal concepts don't simply fail to represent their objects as physical but misrepresent them as phenomenal, and phenomenality is an introspective illusion…  I don't know how widespread my &quot;fuzziness&quot; assumption is.  " href="#footnote48_raanpdr">48</a> In particular, though, I should emphasize that illusionism seems to be endorsed by a small number of theorists. I&#8217;m not sure what to make of this, given that illusionism seems to <em>me</em> to be “the obvious default theory of consciousness,” as Daniel Dennett argues.<a class="see-footnote" id="footnoteref49_s8nfpwn" title="Dennett (2016a).  " href="#footnote49_s8nfpwn">49</a> In any case, the debates about the fundamental nature of consciousness are well-covered elsewhere,<a class="see-footnote" id="footnoteref50_k6nig23" title="Classic sources and contemporary overviews include Chalmers (1996), Carruthers (2000), Frankish (2005), Weisberg (2014), Carruthers &amp; Schier (2014), several chapters of Velmans &amp; Schneider (2007), and several chapters of McLaughlin et al. (2009). On illusionism, see Volume 23, Numbers 11-12 of the Journal of Consciousness Studies.  " href="#footnote50_k6nig23">50</a> and I won&#8217;t repeat them here.</p>
<p><a name="eliminativism" id="eliminativism"></a>A quick note about “eliminativism”: the physical processes which instantiate consciousness could turn out be so different from our naive guesses about their nature that, for pragmatic reasons, we might choose to stop using the concept of “consciousness,” just as we stopped using the concept of “<a href="https://en.wikipedia.org/wiki/Phlogiston_theory">phlogiston</a>.” Or, we might find a collection of processes that are similar enough to those presumed by our naive concept of consciousness that we choose to preserve the concept of “consciousness” and simply revise our definition of it, as happened when we eventually decided to identify “life” with a particular set of low-level biological features (homeostasis, cellular organization, metabolism, reproduction, <a href="https://en.wikipedia.org/wiki/Life#Biology">etc.</a>) even though life turned out not to be explained by any <em><a href="https://en.wikipedia.org/wiki/%C3%89lan_vital">Élan vital</a></em> or supernatural soul, as many people had intuitively supposed.<a class="see-footnote" id="footnoteref51_43r0rif" title="It might also be helpful to consider historical cases of conceptual revision in which it was commonly thought that some view could be ruled out a priori, but in fact that supposedly ruled-out view is now the mainstream scientific view on the topic, e.g. perhaps &quot;space&quot; and &quot;time&quot; in the face of special relativity, or the idea of individually identifiable particles in the face of quantum mechanics.  " href="#footnote51_43r0rif">51</a> In other words, I&#8217;m not trying to take a strong position on “eliminativism” about consciousness here — I see that as a pragmatic issue to be decided later (see <a href="#AppendixZ6">Appendix Z.6</a>). For now, I think it&#8217;s easiest to talk about “consciousness,” “qualia,” and so on as truly existing phenomena that can be defined by example as <a href="#Defined">above</a>, despite being very “fuzzy.”</p>
<p><span style="float: right;">[<a href="/node/858/edit/10">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h2 id="DistributionQuestion">Specific efforts to sharpen my views about the distribution question</h2>
<p>Now we turn to the key question: What is the likely distribution of phenomenal consciousness — as <a href="#Defined">defined by example</a> — across different taxa? (I call this the “distribution question.”)</p>
<p>Note that in this report, I&#8217;ll use “taxa” very broadly to mean “classes of systems,” including:</p>
<ul><li>Phylogenetic taxa, such as “primates,” “fishes,” “rainbow trout,” “plants,” and “bacteria.”</li>
<li>Subsets of phylogenetic taxa, such as “humans in a <a href="https://en.wikipedia.org/wiki/Persistent_vegetative_state">persistent vegetative state</a>” and “<a href="https://en.wikipedia.org/wiki/Anencephaly">anencephalic infants</a>.”</li>
<li>Biological sub-systems, such as “human enteric nervous systems” and “non-dominant brain hemispheres of <a href="https://en.wikipedia.org/wiki/Split-brain">split-brain</a> patients.”</li>
<li>Classes of computer software and/or hardware, such as “deep reinforcement learning agents,” “industrial robots,” “versions of Microsoft Windows,” and “such-and-such <a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit">application-specific integrated circuit</a>.”</li>
</ul><p>In the academic literature on the distribution question, the three most common argumentative strategies I&#8217;ve seen are:<a class="see-footnote" id="footnoteref52_2zri1sx" title="Many sources employ a mix of both strategies. Example sources that seem to primarily use an &quot;apply a theory&quot; strategy include Carruthers (1989), Dennett (1995), ch. 8 of Tye (2000), Merker (2005), and Barron &amp; Klein (2016). Example sources that seem to primarily use a &quot;potentially consciousness-indicating features&quot; strategy include ch. 4 of Smith &amp; Boyd (1991), Bateson (1991), Beshkar (2008), Braithwaite (2010), Varner (2012), Sneddon et al. (2014), Tye (2016), and perhaps Arrables (2010). Note that most of my examples of the second kind aim to assess the likelihood of a taxon's capacity for conscious pain in particular. But of course a capacity for conscious pain presumes a capacity for consciousness.  " href="#footnote52_2zri1sx">52</a></p>
<ol><li><em>Theory</em>: Assume a particular theory of consciousness, then consider whether a specific taxon is likely to be conscious <em>if</em> that theory is true. [<a href="#Theories">More</a>]</li>
<li><em>Potentially consciousness-indicating features</em>: Rather than relying on a specific theory of consciousness, instead suggest a list of behavioral and neurobiological/architectural features which intuitively suggest a taxon <em>might</em> be conscious. Then, check how many of those potentially consciousness-indicating features (PCIFs) are possessed by a given taxon. If the taxon possesses all or nearly all the  PCIFs, conclude that its members are probably conscious. If the taxon possesses very few of the proposed PCIFs, conclude that its members probably are not conscious. [<a href="#PCIFs">More</a>]</li>
<li><em>Necessary or sufficient conditions</em>: Another approach is to argue that some feature is likely <em>necessary</em> for consciousness (e.g. a neocortex), or that some feature is likely <em>sufficient</em> for consciousness (e.g. mirror self-recognition), without relying on any particular <em>theory</em> of consciousness. If successful, such arguments might not give us a detailed picture of which systems are and aren&#8217;t conscious, but they might allow us to conclude that some particular taxa either are or aren&#8217;t conscious. [<a href="#Necessary">More</a>]</li>
</ol><p>Below, I consider each of these approaches in turn, and then I consider various big-picture considerations that “pull” toward or away from a “consciousness is rare” conclusion (<a href="#BigPicture">here</a>).</p>
<p><span style="float: right;">[<a href="/node/858/edit/11">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="Theories">Theories of consciousness</h3>
<p>I briefly familiarized myself with several physicalist functionalist theories of consciousness, listed in <a href="#AppendixZ1">Appendix Z.1</a>. Overall, my sense is that the current state of our scientific knowledge is such that it is difficult to tell whether any currently proposed theory of consciousness is promising. My impression from the literature I&#8217;ve read, and from the conversations I&#8217;ve had, is that many (perhaps most) consciousness researchers agree,<a class="see-footnote" id="footnoteref53_fxfztt5" title="I'm not aware of a poll that asked this question of consciousness researchers, but I provide a few supporting sources below. Obviously this is not sufficient to prove that my impression is true. Instead my aim in this footnote is to point to a few example sources that left me with my current impressions.  Naturally, mysterians agree that no currently proposed theory of consciousness is clearly promising. See e.g. Pinker (2007); McGinn (2004); Rowlands (2001).  Many of those who write about the methodological difficulties of consciousness science also seem to share my general impression, e.g. Irvine (2013), the authors of several chapters in Miller (2015), and the authors of several chapters in Overgaard (2015).  As another example, here is a passage from a review article on recent progress in consciousness science, written by several leaders in the field (Boly et al. 2013):  In order to consolidate the results of many relevant experiments that have been conducted within a single conceptual framework, theories of consciousness must become more precise and generate experimentally testable predictions. Accomplishing this requires both additional conceptual work from theorists and greater knowledge of brain architecture and neural computations relevant to consciousness, in order to guide and constraint theory development… Overall, theoretical developments will help move from simple correlation between neural events and conscious level and content, toward causal and explanatory accounts that show how specific neural mechanisms give rise to specific aspects or dimensions of conscious phenomenology…  Valerie Hardcastle, in chapter 12 of Sinnott-Armstrong (2016), is especially blunt:  I shall begin by stating what I believe to be obvious: We do not know what consciousness is…  …we do not have a good definition for consciousness, we do not know what the relevant psychological attributes of consciousness are, and we have no idea what the neural correlates for consciousness are either. We are not clear on what is sufficient for consciousness, and we at best have an incomplete list of what is necessary. We do not understand the relationship between alertness and awareness, if there is one, nor do we understand the connection between cognitive processing and consciousness, if there is one. At best, we can point to some things that some people believe index some aspects of consciousness. But by the standards of contemporary science and medicine, that is not pointing to very much at all.  See also Katz (2013) and Burkeman (2015).  " href="#footnote53_fxfztt5">53</a> even though some of the most <em>well-known</em> consciousness researchers are well-known precisely because they have put forward specific theories they see as promising. But if most researchers agreed with their optimism, I would expect theories of consciousness to have been winnowed over the last couple decades, rather than continuing to proliferate,<a class="see-footnote" id="footnoteref54_0ghkbws" title="To test this hypothesis, one could enumerate theories of consciousness matching some criteria, and then check the year of &quot;first peer-reviewed defense of the theory&quot; (or similar) for each theory. I have not done this, but my impression is that most consciousness researchers would agree that theories of consciousness have proliferated greatly over the last couple decades.  I'll quote just one example, from Shevlin (2016), pp. 191: &quot;The last two decades have witnessed an explosion in the variety of theories of consciousness…&quot;  " href="#footnote54_0ghkbws">54</a> under a huge variety of metaphysical and methodological assumptions, as they currently do. (In other words, consciousness studies seems to be in what Thomas Kuhn called a “pre-paradigmatic stage of development.”<a class="see-footnote" id="footnoteref55_hmlbe4b" title="Björn Merker expressed this point to me, in an August 2016 email, this way (quoted with permission):  Consciousness theory currently labors in what is obviously a pre-paradigmatic stage of development. In this typically protracted prehistory of a science, competing schools in a nascent field find themselves in disagreement over fundamentals.  As described [by Thomas Kuhn], at this stage in the history of a science each competing school builds its system from its own first principles, occasionally metaphysical, in reliance on a rich array of observations and arguments but without criteria for assessing their relative significance either within or across schools. None of the schools is therefore able to take its fundamentals for granted, and each is forced to constantly reiterate a complex system of facts and interpretations, essentially &quot;from scratch.&quot; Argument tends to be interminable when even first principles are in dispute.  Kuhn's description of the pre-paradigmatic stage of a nascent science applies rather literally to the current state of consciousness theory. It features a disparate array of competing proposals regarding the nature of consciousness, its scope, and genesis, with no agreement on first principles underlying analysis and interpretation. Thus, at one extreme consciousness is seriously proposed to be an intrinsic property of this universe itself on a par with mass, charge and space-time [David Chalmers], and at the other it is construed as a function or product of human language [Euan Macphail]. A field in which such diversity of fundamental commitments regarding its very subject matter can be taken seriously obviously has not yet arrived at the shared paradigm within which normal science, in Kuhn's sense, proceeds to solve puzzles.  For Kuhn's account of the &quot;pre-paradigmatic stage of development,&quot; see chapter 2 of Kuhn (2012). (The first edition of Kuhn's book was published in 1962.)  Or, here is a contemporary summary, from Bird (2011):  Kuhn describes an immature science, in what he sometimes calls its ‘pre-paradigm' period, as lacking consensus. Competing schools of thought possess differing procedures, theories, even metaphysical presuppositions. Consequently there is little opportunity for collective progress. Even localized progress by a particular school is made difficult, since much intellectual energy is put into arguing over the fundamentals with other schools instead of developing a research tradition. However, progress is not impossible, and one school may make a breakthrough whereby the shared problems of the competing schools are solved in a particularly impressive fashion. This success draws away adherents from the other schools, and a widespread consensus is formed around the new puzzle-solutions.  On the state of consciousness studies, see also e.g. Metzinger (2003), p. 116:  …there is yet no single, unified and paradigmatic theory of consciousness in existence which could serve as an object for constructive criticism and as a backdrop against which new attempts could be formulated. Consciousness research is still in a preparadigmatic stage.  But this assessment is not universal. Bill Faw, in his entry &quot;Consciousness, modern scientific study of&quot; on pp. 182-188 of Bayne et al. (2009), writes:  To use Kuhn's term, we might think of the period 1980–94 as representing the transition from the pre-paradigm stage of consciousness science to a normal science stage… The second half of the period — from 1994 to 2008 — constitutes what we might think of as an early phase of normal consciousness science.  " href="#footnote55_hmlbe4b">55</a>) </p>
<p>One might also argue about the distribution question not from the perspective of theories of <em>how consciousness works</em>, but from theories of <em>how consciousness evolved</em> (see the list in <a href="#AppendixZ5">Appendix Z.5</a>). Unfortunately, I didn&#8217;t find any of these theories any more convincing than currently available theories of how consciousness works.<a class="see-footnote" id="footnoteref56_wcgn2zx" title="I also found that it was often difficult for me to understand what, exactly, the authors of these theories are claiming, what evidence they think would falsify their theories, which aspects of their theories were intended as claims about consciousness in humans (or primates) rather than as claims about consciousness in general, and which aspects of their theories were intended as claims about scientific explanation as opposed to expressions about which types of processes they intuitively morally value.  " href="#footnote56_wcgn2zx">56</a></p>
<p>Given the unconvincing-to-me nature of current theories of consciousness (see also <a href="#AppendixB">Appendix B</a>), I decided to pursue investigation strategies that do not require me to put much emphasis on any specific theories of consciousness, starting with the “potentially consciousness-indicating features” strategy described below.</p>
<p><span style="float: right;">[<a href="/node/858/edit/12">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="PCIFs">Potentially consciousness-indicating features (PCIFs)</h3>
<p><span style="float: right;">[<a href="/node/858/edit/13">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="PCIFsWork">How PCIF arguments work</h4>
<p>The first theory-agnostic approach to the distribution question that I examined was the approach of “arguments by analogy” or, as I call them, “arguments about potentially consciousness-indicating features (PCIFs).”<a class="see-footnote" id="footnoteref57_rikx6ql" title="See also Mitchell (2005).  " href="#footnote57_rikx6ql">57</a></p>
<p>As <a href="https://global.oup.com/academic/product/personhood-ethics-and-animal-cognition-9780199758784?cc=us&amp;lang=en&amp;">Varner (2012)</a> explains, analogy-driven arguments appeal to the fact that because things <em>P</em> and <em>Q</em> share many “seemingly relevant” features (<em>a</em>, <em>b</em>, <em>c</em>, …<em>n</em>), and we know that <em>P</em> has some additional property <em>x</em>, we should infer that <em>Q</em> probably has property <em>x</em>, too.</p>
<p>After all, it is by such an analogy that I believe other humans are conscious. I cannot directly observe that my mother is conscious, but she talks about consciousness like I do, she reacts to stimuli like I do, she has a brain that is virtually identical to my own in form and function and evolutionary history, and so on. And since I know <em>I</em> am conscious, I conclude that my mother is conscious as well.<a class="see-footnote" id="footnoteref58_oagnhoz" title="For a more detailed discussion of the argument by analogy from my own consciousness to that of other humans, see Tye (2016), ch. 4.  " href="#footnote58_oagnhoz">58</a> The analogy between myself and a chimpanzee is weaker than that between myself and my mother, but it is, we might say, “fairly strong.” The analogy between myself and a pig is weaker still. The analogies between myself and a fish are even weaker but, some argue, still strong enough that we should put some substantial probability on fish consciousness.</p>
<p>One problem with analogy-driven arguments, and one reason they are difficult to fully separate from theory-driven arguments, is this: to decide how salient a given analogy between two organisms is, we need some “guiding theory” about what consciousness is, or what its function is. <a href="https://global.oup.com/academic/product/personhood-ethics-and-animal-cognition-9780199758784?cc=us&amp;lang=en&amp;">Varner (2012)</a> explains:<a class="see-footnote" id="footnoteref59_iahqmss" title="Pages 114-115.  " href="#footnote59_iahqmss">59</a></p>
<blockquote><p>[The] point about needing such a “guiding theory” can be illustrated with this obviously bad argument by analogy:</p>
<ol><li>Both turkeys (<em>P</em>) and cattle (<em>Q</em>) are animals, they are warm blooded, they have limited stereoscopic vision, and they are eaten by humans (<em>a</em>, <em>b</em>, <em>c</em>, …, and <em>n</em>).</li>
<li>Turkeys are known to hatch from eggs (<em>x</em>).</li>
<li>So probably cattle hatch from eggs, too.</li>
</ol><p>One could come up with more and more analogies to list (e.g., turkeys and cattle both have hearts, they have lungs, they have bones, etc., etc.). The above argument is weak, not because of the number of analogies considered, but because it ignores a crucial <em>disanalogy</em>: that cattle are mammals, whereas turkeys are birds, and we have very different theories about how the two are conceived, and how they develop through to birth and hatching, respectively. Another way of putting the point would be to say that the listed analogies are irrelevant because we have a “guiding theory” about the various ways in which reproduction occurs, and within that theory the analogies listed above are all irrelevant…</p>
<p>So in assessing an argument by analogy, we do not just look at the raw number of analogies cited. Rather, we look at both how salient are the various analogies cited and whether there are any relevant disanalogies, and we determine how salient various comparisons are by reference to a “guiding theory.”</p></blockquote>
<p>Unfortunately, as explained <a href="#Theories">above</a>, it isn&#8217;t clear to me what our guiding theory about consciousness should be. Because of this, I present below a table that includes an unusually wide variety of PCIFs that I have seen suggested in the literature, along with a few of my own. From this initial table, one can use one&#8217;s own guiding theories to discard or de-emphasize various PCIFs (rows), perhaps temporarily, to see what doing so seems to suggest about the likely distribution of consciousness.</p>
<p>Another worry is that one&#8217;s choices about which PCIFs to include, and which taxa to check for those PCIFs, can bias the conclusions of such an exercise.<a class="see-footnote" id="footnoteref60_qmtdo2r" title="For example, if I wanted to argue in favor of fish consciousness, I could present a table like this:           Potentially consciousness-indicating feature     True of a human?     True of a fish?           1. Forms and uses mental representations     Yes     Yes           2. Associates a current mental state with a memory     Yes     Yes           3. Can process emotions with a certain part of the brain     Yes     Yes           4. Can alter its view of an aversive situation depending on context     Yes     Yes           5. Can consider possible actions and ponder their consequences     Yes     Yes       But if I wanted to nudge you toward thinking that higher primates are conscious and fishes are not, and I knew that you were already inclined to think laptops aren't conscious, I could instead present the following table:           Potentially consciousness-indicating feature     True of a human?     True of a chimpanzee?     True of a fish?     True of a laptop?           1. Forms and uses mental representations     Yes     Yes     Yes     Yes           2. Associates a current mental state with a memory     Yes     Yes     Yes     Yes           3. Can process emotions with a certain part of the brain     Yes     Yes     Yes     No           4. Can alter its view of an aversive situation depending on context     Yes     Yes     Yes     Yes           5. Can consider possible actions and ponder their consequences     Yes     Yes     Yes     Yes           6. Has a neocortex     Yes     Yes     No     No           7. Passes the mirror self-recognition test     Yes     Yes     No     No           8. Engages in complex social politics     Yes     Yes     No     No       Note that my first example table is adapted from Braithwaite (2010)'s summary (at the end of chapter 4) of her case for fish consciousness, but it should not be attributed to her, since Braithwaite's argument is more nuanced than what I've put in my example PCIFs table.  Braithwaite summarizes her case for fish consciousness in the paragraph below. I've added the numbered PCIFs from my example table to illustrate the similarities:  So pulling the different threads together, fish really do appear to possess key traits associated with consciousness. Their ability to form and use mental representations indicates fish have some degree of access consciousness [PCIF #1]. They can consider a current mental state and associate it with a memory [PCIF #2]. Having an area of the brain specifically associated with processing emotion [PCIF #3] and evidence that they alter their view of an aversive situation depending on context [PCIF #4] suggests that fish have some form of phenomenal consciousness: they are sentient. This leaves monitoring and self consciousness, which I argue is in part what the eel and the grouper are doing: considering their actions and pondering the consequences [PCIF #5]. The grouper is clearly deciding it has no chance to get the prey itself and so swims off to get the eel. The eel is deciding that an easy meal is on offer. On balance then, fish have a capacity for some forms of consciousness, and so I conclude that they therefore have the mental capacity to feel pain. I suspect that what they experience will be different and simpler than the experiences we associate with pain and suffering, but I see no evidence to deny them these abilities, and quite a bit which argues that they will suffer from noxious stimuli.  In the second example table, I compare fishes and laptops according to these PCIFs (plus a few others), but note that Braithwaite explicitly denies that laptops have consciousness (Droege &amp; Braithwaite 2015).  " href="#footnote60_qmtdo2r">60</a> To mitigate this problem, my table below is unusually comprehensive with respect to both PCIFs and taxa. As a result, I could not afford the time to fill out most cells in the table, and thus my conclusions about the utility and implications of this approach (<a href="#PCIFsOverall">below</a>) are limited.</p>
<p><span style="float: right;">[<a href="/node/858/edit/14">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="PCIFsTable">A large (and incomplete) table of PCIFs and taxa</h4>
<p>The taxa represented in the table below were selected either (a) for comparison purposes (e.g. human, bacteria), or (b) because they are killed or harmed in great numbers by human activity and are thus plausible targets of welfare interventions <em>if</em> they are thought of as moral patients (e.g. chickens, fishes), or (c) a mix of both. To represent each very <em>broad</em> taxon of interest (e.g. fishes, insects), I chose a representative sub-taxon that has been especially well-studied (e.g. rainbow trout, common fruit fly). More details on the taxa and PCIFs I chose, and why I chose them, are provided in a footnote.<a class="see-footnote" id="footnoteref61_gpfyzpk" title="Each row in my PCIFs table is meant to report on the status of that row's PCIF in normally-functioning, adult members of each taxon, except where that designation has no meaning.  Details on why I chose each taxon:           Taxon     Why include this taxon?           Human (Homo sapiens sapiens)     For comparison.           Chimpanzee (Pan troglodytes)     For comparison: it's the non-human species I'm most confident is conscious, given how closely related it is to humans, and how sophisticated its behavior and cognition appear to be.           Cow (Bos taurus)     I wanted to include a heavily-consumed mammal that seems much less cognitively sophisticated than a chimpanzee.           Chicken (Gallus gallus domesticus)     I wanted to include a bird species, and the chicken is the most heavily-consumed.           Rainbow trout (Oncorhynchus mykiss)     I wanted to include a fish species. Among those fish species that are heavily-consumed, the rainbow trout is one of the most well-studied with respect to PCIFs, especially pain-related PCIFs. I could have chosen a subspecies, but unfortunately many studies of Oncorhynchus mykiss do not specify which subspecies was examined.           Gazami crab (Portunus trituberculatus)     I wanted to include a decapod species. Among decapods, some shrimp and crayfish species might be harvested in greater numbers than any crab species, but crabs have thus far been more thoroughly studied with respect to their likelihood of consciousness, largely due to the work of Robert W. Elwood. The Gazami crab is one of the most heavily-consumed species of crab.           Common fruit fly (Drosophila melanogaster)     I wanted to include an insect species, and the common fruit fly is perhaps the most-studied insect.           E. coli     I wanted to include a single-celled species of bacteria for comparison purposes, and E. coli is among the most-studied species of bacteria.           Function sometimes executed non-consciously in humans?     For comparison; see explanation here.           Adult human enteric nervous system (ENS)     For comparison; I wanted to include a biological sub-system that most people think of as non-conscious.      My explanation for choosing each PCIF in the table is given in the footnote which appears in the first cell of each row of the table, except in cases where the reason for a PCIFs inclusion seems sufficiently obvious to me (e.g. brain mass), or in some cases where I did not take the time to say anything at all about a PCIF beyond listing it.  " href="#footnote61_gpfyzpk">61</a></p>
<p><a name="nonconsciously" id="nonconsciously"></a>One column below — “Function sometimes executed non-consciously in humans?” — requires special explanation. Many behavioral and neurofunctional PCIFs can be executed by humans <em>either</em> consciously or non-consciously. In fact, <em>most</em> cognitive processing in humans seems to occur non-consciously, and humans sometimes engage in fairly sophisticated behaviors without conscious awareness of them, as in (it is often argued) cases of sleepwalking, or when someone daydreams while driving a familiar route, or in cases of absence seizures involving various “automatisms” like this one described by Antonio Damasio:<a class="see-footnote" id="footnoteref62_t8x094m" title="Damasio (1999), p. 6. Absence seizures involving automatisms are called &quot;complex&quot; absence seizures, and are more common than &quot;simple&quot; absence seizures (i.e. without automatisms). For more on absence automatisms, see e.g. Penry &amp; Dreifuss (1969); Arzimanoglou &amp; Ostrowsky-Coste (2010).  " href="#footnote62_t8x094m">62</a></p>
<blockquote><p>…a man sat across from me… [and] we talked quietly. Suddenly the man stopped, in midsentence, and his face lost animation; his mouth froze, still open, and his eyes became vacuously fixed on some point on the wall behind me. For a few seconds he remained motionless. I spoke his name but there was no reply. Then he began to move a little, he smacked his lips, his eyes shifted to the table between us, he seemed to see a cup of coffee and a small metal vase of flowers; he must have, because he picked up the cup and drank from it. I spoke to him again and again he did not reply. He touched the vase. I asked him what was going on, and he did not reply, his face had no expression. He did not look at me. Now, he rose to his feet and I was nervous; I did not know what to expect. I called his name and he did not reply. When would this end? Now he turned around and walked slowly to the door. I got up and called him again. He stopped, he looked at me, and some expression returned to his face — he looked perplexed. I called him again, and he said, “What?”</p>
<p>For a brief period, which seemed like ages, this man suffered from an impairment of consciousness. Neurologically speaking, he had an absence seizure followed by an absence automatism, two among the many manifestations of epilepsy…</p></blockquote>
<p>If such PCIFs are observed in humans either with <em>or without</em> consciousness, then perhaps the case for them as being indicative of consciousness in other taxa is less strong than one might think:<a class="see-footnote" id="footnoteref63_fhcnhj6" title="LeDoux (2015), ch. 6, makes the point this way:  One strategy used to explore consciousness in animals assumes that if an organism can solve complex problems behaviorally, it has complex mental capacities and therefore mental state consciousness. But this approach conflates cognitive capacities with consciousness, which we've seen are not the same. Animals are not, as Descartes characterized them, simple beast machines that only react reflexively to the world. They use internal (cognitive) processing of external events to help them pursue goals, make decisions, and solve problems. But because the human brain can often carry out these same tasks nonconsciously, the mere existence of such cognitive capacities in animals can't be used as evidence that consciousness was involved.  Similarly, here is Tononi &amp; Koch (2015):  …the lessons learnt from studying the behavioural… and neuronal correlates of consciousness in people must make us cautious about inferring its presence in creatures very different from us, no matter how sophisticated their behaviour and how complicated their brain. Humans can perform complex behaviours—recognizing whether a scene is congruous or incongruous, controlling the size, orientation and strength of how one's finger should grip an object, doing simple arithmetic, detecting the meaning of words or rapid keyboard typing—in a seemingly non-conscious manner [61–66]. When a bee navigates a maze, does it do so like when we consciously deliberate whether to turn right or left, or rather like when we type on a keyboard?  Dawkins (2015) puts it this way:  There are several reasons [to be cautious about inferences from behavior or physiology to consciousness]. First, we know from our own experience that the three components of human emotion (autonomic/behavioral/cognitive) do not necessarily correlate with each other (Oatley &amp; Jenkins, 1996). Sometimes, for example, strong subjective emotions occur with no obvious autonomic changes, as when someone experiences a rapid switch from excitement to fear on a roller coaster. This does not mean that the change in emotional experience has no physiological basis. It just means that it is probably due to a subtle change in brain state rather than the obvious autonomic changes that are what are usually referred to as physiological (autonomic) measures of emotion…  Second, there is increasing evidence that much more human behavior than we had realized takes place without consciousness at all. Many complex tasks in humans, such as driving a car, playing a musical instrument, or even breathing can be carried out either consciously or unconsciously (Blackmore, 2012; Paul, Harding, &amp; Mendl, 2005; Rolls, 2014; Weiskrantz, 2003). Some human patients with certain sorts of brain damage can successfully reach out and touch objects in front of them but then say they are not conscious of having seen them at all (Weiskrantz, 2003). They are simultaneously blind (as far as their verbal reports go) but also sighted (unconsciously guided reaching). For much of what we humans do there appears to be multiple routes to the same behavior, only some of which reach consciousness (Rolls, 2014). But if the same action (e.g., breathing or touching an object) can occur in humans through either an unconscious or conscious pathway, the argument that if the behavior of another animal is similar to that of a human, that animal must be conscious (der Waal, 2005) is seriously weakened. An animal could be doing the same behavior as a human using his or her unconscious circuits (McPhail, 1998). Unconscious mechanisms explain much more of human behavior than previously thought and may also underlie much animal behavior (Shettleworth, 2010b). Many of the more complex aspects of animal behavior, such as corvid re-caching, that had previously thought to involve awareness can be mimicked by relatively simple computer programs without a theory of mind (van der Vaart, Verbrugge, &amp; Hemelrijk, 2012). In fact, a recent trend in comparative psychology has been away from emphasizing the complexity of animal behavior and toward emphasizing the simplicity of human behavior (Shettleworth, 2010b).  Humans can even have unconscious emotions and changes of emotional state that they are completely unaware of (Morris, Ohman, &amp; Dolan, 1998; Berridge &amp; Winkielman, 2003; Sato &amp; Aoki, 2006). This has important implications for our interpretation of animal emotions, because if we can have unconscious emotions, then the fact that animals behave 'like us' says much less about their consciousness or otherwise than we might think (Dawkins, 2001b, 2012).  " href="#footnote63_fhcnhj6">63</a> e.g. are fishes conscious of their behavior, or are they continuously “sleepwalking”?<a class="see-footnote" id="footnoteref64_u4ncf9g" title="For this column, I distinguish &quot;conscious&quot; and &quot;non-conscious&quot; processes in the normal way they are discussed in the psychological and neuroscientific literature, and thus I temporarily set aside the possibility of &quot;hidden qualia&quot; (see Appendix H). However, in the full analysis, this possibility must be considered: it is possible that many of the cognitive processes normally described by psychologists and neuroscientists as &quot;unconscious&quot; actually instantiate phenomenally conscious experience, but not for the &quot;self&quot; who can report experiences to an external observer.  " href="#footnote64_u4ncf9g">64</a></p>
<p>In the table below, a cell is left blank if I didn&#8217;t take the time to investigate, or in some cases even think about, what its value should be, or if I investigated briefly but couldn&#8217;t find a clear value for the cell. A cell&#8217;s value is “n/a” when a PCIF is not applicable to that taxon, and it is “unavailable” when I&#8217;m fairly confident the relevant data has not (as of December 2016) been collected. In some cases, data are not available for my taxon of choice, but I guess or estimate the value of that cell from data available for a <em>related</em> taxon (e.g. a closely related species), and in cases where this leaves me with substantial uncertainty about the appropriate value for that cell, I indicate my extra uncertainty with a question mark, an “approximately” tilde symbol (“~”) for scalar data, or both. To be clear: a question mark does not necessarily indicate that <em>domain experts</em> are uncertain about the appropriate value for that cell of the table; it merely means that <em>I</em> am substantially uncertain, given the very few sources I happened to skim. Sources and reasoning for the value in each cell are given in the footnote immediately following each row&#8217;s PCIF.</p>
<p>The values of the cells in this table have not been vetted by anyone, let alone by a domain expert. In many cases, I populated a cell with a value drawn from a single study, without reading the study carefully or trying hard to ensure I was interpreting it correctly. Hence, the contents of this table should be interpreted as a set of tentative estimates and guesses, collected hastily by a non-expert.<a class="see-footnote" id="footnoteref65_tzy2f5o" title="Moreover, I wouldn't be surprised if many of the studies cited in this section turn out to not &quot;hold up&quot; very well upon closer scrutiny; see Appendix Z.8.  " href="#footnote65_tzy2f5o">65</a></p>
<p>Because the table dI am also grateful to several people for helping me find someoesn&#8217;t fit on the page, the table must be scrolled horizontally and vertically to view all its contents.</p>
<table class="vertical-scroll"><tr><th>Potentially consciousness-indicating feature</th>
<th>Human</th>
<th>Chimpanzee</th>
<th>Cow</th>
<th>Chicken</th>
<th>Rainbow trout</th>
<th>Gazami crab</th>
<th>Common fruit fly</th>
<th><em>E. coli</em></th>
<th>Function sometimes executed non-consciously in humans?</th>
<th>Human enteric nervous system</th>
</tr><tr><td>Last common ancestor with humans (Mya)<a class="see-footnote" id="footnoteref66_5k1ohmn" title="I included &quot;years since last common ancestor with humans&quot; as a PCIF because it is a relatively theory-agnostic measure of &quot;similarity to humans.&quot;  My source for &quot;years since last common ancestor with humans&quot; was the website TimeTree, which compiles estimates from a variety of published sources. Here are the specific pages from which I drew my numbers (in August 2016): chimpanzees, cows, chickens, rainbow trout, gazami crab, common fruit fly, E. coli.  " href="#footnote66_5k1ohmn">66</a></td>
<td>n/a</td>
<td>6.6</td>
<td>97.5</td>
<td>320.5</td>
<td>429.6</td>
<td>847</td>
<td>847</td>
<td>4290</td>
<td>n/a</td>
<td>n/a</td>
</tr><tr><td><strong>Category: Neurobiological features</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Adult average brain mass (g)<a class="see-footnote" id="footnoteref67_fg2ko4z" title="My sources of brain mass estimates are: Olkowicz et al. (2016) for humans, Herndon et al. (1999) for chimpanzees, Ballarin et al. (2016) for cows, and Sangiao-Alvarellos et al. (2004) for rainbow trout. For chickens I computed the average brain mass across the 80 domestic chickens (from 8 breeds) summarized in table 1 of Rehkämper et al. (2003).  " href="#footnote67_fg2ko4z">67</a></td>
<td>1509</td>
<td>385</td>
<td>480.5</td>
<td>3.5</td>
<td>0.2</td>
<td></td>
<td></td>
<td>n/a</td>
<td>n/a</td>
<td></td>
</tr><tr><td>Neurons in brain (millions)<a class="see-footnote" id="footnoteref68_xdth79j" title="For humans, see Olkowicz et al. (2016). For chickens I just used Olkowicz et al. (2016)'s estimate for the red junglefowl, which is the same species but a different subspecies from the domestic chicken. For the human ENS, I used the midpoint of Furness et al. (2014)'s estimate of &quot;200-600 million neurons.&quot;   As far as I know, no one has yet counted the number of neurons in the brains of chimpanzees, cows, rainbow trout, or gazami crabs. My source for an estimate of neurons in the brain of the common fruit fly is Strausfeld (2012), p. 80.  " href="#footnote68_xdth79j">68</a></td>
<td>86060</td>
<td>unavailable</td>
<td>unavailable</td>
<td>~221</td>
<td>unavailable</td>
<td>unavailable</td>
<td>0.12</td>
<td>n/a</td>
<td>n/a</td>
<td>400</td>
</tr><tr><td>Neurons in pallium (millions)<a class="see-footnote" id="footnoteref69_b7pwudm" title="For humans, see Olkowicz et al. (2016). For chickens I just used Olkowicz et al. (2016)'s estimate for the red junglefowl, which is the same species but a different subspecies from the domestic chicken.  As far as I know, no one has yet counted the number of pallial neurons in chimpanzees, cows, and rainbow trout.  " href="#footnote69_b7pwudm">69</a></td>
<td>16340</td>
<td>unavailable</td>
<td>unavailable</td>
<td>~60.7</td>
<td>unavailable</td>
<td>n/a</td>
<td>n/a</td>
<td>n/a</td>
<td>n/a</td>
<td>n/a</td>
</tr><tr><td>Encephalization quotient<a class="see-footnote" id="footnoteref70_293uira" title="It's my impression that encephalization quotient is quickly falling out of favor as an important predictor of higher cognitive capacities — see e.g. Herculano-Houzel (2011, 2016), Deaner et al. (2007), and MacLean et al. (2014). Nevertheless, I include here the numbers collected in table 1 of Roth &amp; Dickie (2005). Where that table lists a range, I used the midpoint of that range.  " href="#footnote70_293uira">70</a></td>
<td>7.6</td>
<td>2.35</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n/a</td>
<td>n/a</td>
<td>n/a</td>
</tr><tr><td>Has a neocortex<a class="see-footnote" id="footnoteref71_p6nuce7" title="Typically, all mammals are considered to have a neocortex (Liu et al. 2011), but there is some terminological debate about whether any non-mammals should be considered to have a &quot;neocortex&quot; (e.g. see Jarvis et al. 2005; Reiner et al. 2004).  For example arguments that a neocortex (or some structure performing similar functions) might be required for consciousness, see my later section on that debate.  " href="#footnote71_p6nuce7">71</a></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td></td>
<td></td>
<td>No</td>
<td>No</td>
<td>n/a</td>
<td>n/a</td>
<td>n/a</td>
</tr><tr><td><strong>Category: Nociceptive features</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Has nociceptors<a class="see-footnote" id="footnoteref72_q8ueafl" title="See Appendix D.      " href="#footnote72_q8ueafl">72</a></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes?</td>
<td>Yes</td>
<td>Yes?</td>
<td>n/a</td>
<td>Yes?</td>
</tr><tr><td>Has neural nociceptors<a class="see-footnote" id="footnoteref73_8du00au" title="See Appendix D.      " href="#footnote73_8du00au">73</a></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes?</td>
<td>Yes</td>
<td>No</td>
<td>n/a</td>
<td>Yes?</td>
</tr><tr><td>Nociceptive reflexes<a class="see-footnote" id="footnoteref74_ue9lc28" title="By nociceptive reflexes I mean &quot;movement away from noxious stimuli.&quot; In general, see Sneddon et al. (2014). On rainbow trout in particular, see e.g. Chervova et al. (1994).  On nociceptive reflexes without conscious experience, see e.g. Crook &amp; Walters (2011):  Nociceptive reflexes and nociceptive plasticity can occur without conscious, emotional experience because these responses are expressed not only in the simplest animals but also in reduced preparations, such as spinalized animals [Clarke and Harris (2001); Egger (1978)] and snail ganglia [Walters et al. (1983)]. Similarly, in human patients nociceptive reflexes can occur without conscious awareness below a level of complete spinal transection [Finnerup and Jensen (2004)].  " href="#footnote74_ue9lc28">74</a></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes?</td>
<td>Yes?</td>
<td>Yes?</td>
<td>Yes</td>
<td></td>
</tr><tr><td>Physiological responses to nociception or handling<a class="see-footnote" id="footnoteref75_lj82flm" title="By &quot;physiological responses&quot; I have in mind Sneddon et al. (2014)'s &quot;one or a combination of the following: change in respiration, heart rate or hormonal levels (e.g. cortisol in some vertebrates).&quot; That paper and Sneddon (2015) are my central sources for the values I put in the cells of this row. For debate about the interpretation of some physiological responses to nociception in fishes, see Rose et al. (2014).  " href="#footnote75_lj82flm">75</a></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td></td>
<td></td>
<td>n/a</td>
<td></td>
<td>n/a</td>
</tr><tr><td>Long-term alteration in behavior to avoid noxious stimuli<a class="see-footnote" id="footnoteref76_hpizn44" title="This PCIF is ill-defined and I did not investigate it, but see e.g. the discussions in Sneddon et al. (2014).  " href="#footnote76_hpizn44">76</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Taste aversion learning<a class="see-footnote" id="footnoteref77_o9flb1k" title="I have not investigated this PCIF, but some potentially relevant sources include Parker (2003); Riley &amp; Freeman (2004); Reilly &amp; Schachtman (2008).  " href="#footnote77_o9flb1k">77</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Protective behavior (e.g. wound guarding, limping, rubbing, licking)<a class="see-footnote" id="footnoteref78_b27x760" title="My central source for this PCIF is Sneddon et al. (2014).   " href="#footnote78_b27x760">78</a></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes?</td>
<td>Yes?</td>
<td>Yes?</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Nociceptive reflexes or avoidant behaviors reduced by analgesics<a class="see-footnote" id="footnoteref79_0x8k75c" title="My central source for this PCIF is Sneddon et al. (2014).   " href="#footnote79_0x8k75c">79</a></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Self-administration of analgesia<a class="see-footnote" id="footnoteref80_bfojwx0" title="My central source for this PCIF is Sneddon et al. (2014).   " href="#footnote80_bfojwx0">80</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td>Yes</td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Will pay a cost to access analgesia<a class="see-footnote" id="footnoteref81_ap3oo92" title="My central source for this PCIF is Sneddon et al. (2014).   " href="#footnote81_ap3oo92">81</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Selective attention to noxious stimuli over other concurrent events<a class="see-footnote" id="footnoteref82_qybsp9f" title="I did not investigate this PCIF. For fishes, see Sneddon et al. (2014).  " href="#footnote82_qybsp9f">82</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Pain-relief learning<a class="see-footnote" id="footnoteref83_m5flqcx" title="My central source for this PCIF is Gerber et al. (2014).  " href="#footnote83_m5flqcx">83</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
</tr><tr><td><strong>Category: Other behavioral/cognitive features</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Reports details of conscious experiences to scientists<a class="see-footnote" id="footnoteref84_m8zodey" title="This is usually taken to be the most important PCIF, but it is typically thought to be prone to many false negatives: i.e. there are likely systems that are conscious but simply do not have the faculties needed to describe their conscious experiences to human scientists.  I should note that one might argue that some monkeys &quot;report&quot; some &quot;detail&quot; about their conscious experience in binocular rivalry studies, as Bayne (2010), p. 97, mentions:  …the science of consciousness draws on data from creatures whose ability to produce any kind of reports is questionable.  In an influential set of experiments designed to identify the neural correlates of visual consciousness, Logothetis and colleagues examined the neural responses of rhesus monkeys to binocular rivalry… The monkeys were first trained to press bars in response to various images — horizontal and vertical gratings, for example — and then presented with rivalrous stimuli. As expected, their responses closely modelled those of human observers to the same stimuli. The question that concerns us here is not what this research tells us about the neural correlates of visual experience, but what we should say about the monkeys' button-presses. Logothetis and colleagues describe the monkeys as reporting their mental states, but I would want to resist this interpretation. It seems to me that there is little reason to suppose that the monkeys were producing reports of any kind let alone introspective reports. Arguably, to report that such-and-such is the case one has to conceive of… one's behaviour as likely to bring about a particular belief in the mind of one's audience — indeed, as likely to bring this belief about in virtue of the fact that one's audience appreciates that one's behavior carries the relevant informational content — and I know of no good reason to believe that the monkeys conceived of their button-presses in these terms.  Does it follow that we have no grounds for thinking that the monkeys were experiencing binocular rivalry? Not at all; in fact, I think the monkeys' button-presses qualify as very good evidence for the claim that they had rivalrous experiences. However, their button-presses constitute evidence of consciousness not because they were reports of any kind but because they were intentional actions…  " href="#footnote84_m8zodey">84</a></td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr><tr><td>Cross-species measures of general cognitive ability<a class="see-footnote" id="footnoteref85_12x1hpe" title="My sense is that these are not yet well-developed enough to serve as a quantitative PCIF, but they may become useful for that purpose within a decade or two. For reviews, see Burkart et al. (forthcoming); Hernandez-Orallo (2017); Kabadayi et al. (2016).  " href="#footnote85_12x1hpe">85</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Plastic behavior<a class="see-footnote" id="footnoteref86_wn6dc8w" title="Ng (1995) makes the case for plastic behavior as a strong indicator of consciousness, and points to Bunge (1980), p. 45 for a definition of &quot;plasticity&quot;:      The ability of the [central nervous system] to change either its composition or its organization (structure), and consequently some of its functions (activities), even in the presence of a (roughly) constant environment, is called plasticity (cf. Paillard, 1976). Plasticity seems to be characteristic of the associative cerebral cortex from birth to senility, to the point that this system has been characterized as &quot;the organ capable of forming new functional organs&quot;… In psychological terms, plasticity is the ability to learn and unlearn. From a monistic perspective learning is activating neural systems not previously engaged in the task in question, presumably by establishing or reinforcing certain synaptic connections.  " href="#footnote86_wn6dc8w">86</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Detour behaviors<a class="see-footnote" id="footnoteref87_s73619h" title="Rial et al. (2008) propose detour behaviors as a PCIF:  The detour behaviour represents the ability of an animal to reach a goal by moving round an interposed obstacle with temporal loss of sensorial contact… The acquisition of &quot;object constancy&quot; in the human child, i.e., the ability to understand that an object temporally hidden is the same after being retrieved, has received considerable attention… Similarly, the detour behaviour requires the maintenance of a memory of the location of a disappeared object, that is, an internal representation of the environment and the production of a &quot;mental&quot; experiment as the animal should construct a complex motor trajectory in advance to the final behavioural performance… Looking at comparative and phylogenetic studies on the detour behaviour, numerous examples have been described in mammals. In birds, it has been convincingly demonstrated in chickens, quails and in herring gulls, but not in canaries [Vallortigara 2000]…  I did not check whether the detour behavior has been observed in other animals besides humans and chickens.  " href="#footnote87_s73619h">87</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Play behaviors<a class="see-footnote" id="footnoteref88_ha6u9u0" title="Rial et al. (2008) propose play behaviors as a PCIF:  …play shows several traits indicative of consciousness. Besides of being an onerous activity, play seems to be always pleasant. The only explanation for the play paradox lies in considering that the expenditure of energy must have a wide variation in hedonic value, from rather unpleasant to extremely pleasurable, that is, it shows a wide range of alliesthesia. An animal confronted with the possibility of playing should rank the costs and the benefits of each alternative and its final decision will aim at maximizing pleasure. Therefore, the presence of play should be a sign of consciousness.  Overview sources on animal play include Burghardt (2005); Balcombe (2006), ch. 4; Graham &amp; Burghardt (2010); Held &amp; Špinka (2011).  According to Graham &amp; Burghardt (2010), &quot;play is well-developed in primates, rodents, carnivorans, ungulates, elephants, and cetaceans,&quot; and according to figure 1 has been observed in several other taxa as well, including birds and ray-finned fishes. I have filled in the cells of this row accordingly.  " href="#footnote88_ha6u9u0">88</a></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes?</td>
<td>Yes?</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n/a</td>
</tr><tr><td>Grief behaviors<a class="see-footnote" id="footnoteref89_bzy2plq" title="I have not investigated this PCIF, but some potentially relevant sources include King (2013); King (2016) and the replies in that issue of Animal Sentience; Preti (2007, 2011).  " href="#footnote89_bzy2plq">89</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Goal-directed behavior<a class="see-footnote" id="footnoteref90_x2wjfqn" title="For overviews, see e.g. Verschure et al. (2014); Dickinson (2011); Trestman (2012).  Also, it is perhaps worth combating a pervasive anecdote used to suggest that insect behavior is rigid rather than adaptive and (at least sometimes) goal-directed. I refer to what Keijzer (2012) calls &quot;the sphex story&quot;:  The Sphex story is an anecdote about a female digger wasp that at first sight seems to act quite intelligently, but subsequently is shown to be a mere automaton that can be made to repeat herself endlessly. Dennett and Hofstadter made this story well known and widely influential within the cognitive sciences, where it is regularly used as evidence that insect behavior is highly rigid…  Here is the version [of the anecdote] that became a classic of cognitive science…: &quot;When the time comes for egg laying, the wasp Sphex builds a burrow for the purpose and seeks a cricket which she stings in such a way as to paralyze but not kill it. She drags the cricket into the burrow, lays her eggs alongside, closes the burrow, then flies away, never to return. In due course, the eggs hatch and the wasp grubs feed off the paralyzed cricket, which has not decayed, having been kept in the wasp equivalent of a deep freeze. To the human mind, such an elaborately organized and seemingly purposeful routine conveys a convincing flavor of logic and thoughtfulness—until more details are examined. For example, the wasp's routine is to bring the paralyzed cricket to the burrow, leave it on the threshold, go inside to see that all is well, emerge, and then drag the cricket in. If, while the wasp is inside making her preliminary inspection, the cricket is moved a few inches away, the wasp, on emerging from the burrow, will bring the cricket back to the threshold, but not inside, and will then repeat the preparatory procedure of entering the burrow to see that everything is all right. If again the cricket is removed a few inches while the wasp is inside, once again the wasp will move the cricket up to the threshold and re-enter the burrow for a final check. The wasp never thinks of pulling the cricket straight in. On one occasion this procedure was repeated forty times, always with the same result.&quot; [Wooldridge (1963), pp. 82–83.]  The message is clear and simple. Behavior that seems to be strikingly intelligent is actually the result of a straightforward mechanical setup that involves a strict and rigid sequencing of environmental triggers to regulate the several steps involved. The insect is not at all aware of what it is doing and its internal processes are in this sense very different from the characteristics of human cognition. Hofstadter even coined the term 'sphexish' to refer to such an unknowing and mechanical form of &quot;seeming intelligence,&quot; and set it as &quot;totally opposite to what we feel we are all about, particularly when we talk about our own consciousness&quot; (1985, p. 529). Dennett (1984) used this notion to refer to the possibility that we might be sphexish ourselves, only less obviously so, and investigated possible implications for free will. The general idea here is that, if this rigidity of behavior is true for insects as a fundamental property that can be uncovered under the right circumstances, then the same should apply to the more complex but not intrinsically different case of human beings.  …[But] looking at this history, there are several striking features. First and foremost, digger wasps very often do not repeat themselves endlessly when the cricket test is done. After a few trials many wasps take the cricket into their burrow without the visit. Second, in certain cases there are ecological and practical reasons for repeating the visit. Third, the cricket test focuses on an extremely minor component of digger wasp behavior, which has since its discovery been completely swamped by many other findings that provide a very different general picture of the mind of the digger wasp.  …  [One example study is] a wonderfully sophisticated and extensive report on the cricket test derives from a five year study done by Jane Brockmann (1985). The cricket test was only one aspect of this study, discussed under the name of &quot;prey-retrieval behavior.&quot; First she discusses six natural reasons why the prey of Sphex ichneumoneus may be missing when the wasp reappears from the nest. Subsequently she describes the results of the cricket test performed systematically on 31 wasps. For each wasp, she used 15 different places for repositioning the prey, positioned at four different distances (2, 4, 6, and 8 cm) from the entrance, spread in four right-angled directions, the 16th position being the place where the wasp left her prey herself. Brockmann placed the prey at each of the 15 non-standard positions in random order, and then finished by placing it in the normal position, from which the wasp always drew it in. Twelve wasps came to the end of the full procedure, repeating the visit fifteen times. Ten wasps drew the prey in from another position, breaking the loop. Of the remainder, five gave up searching for their missing prey, while four did not finish for other reasons. In a retest with fourteen wasps, four wasps remained stuck in their loop, while five broke out if it (Brockmann, 1985, pp. 639–641). In her discussion, where she also takes into account many other findings concerning the provisioning behavior of the great golden digger, Brockmann says: &quot;Although the behavior generally follows one scheme, there are many situations that arise and the wasps behave in an adaptive manner towards each. . . . The fixity of repeatedly repositioning and re-entering the nest is almost certainly an adaptive response to prey that can easily become lodged in the nest if pulled in backwards.&quot; (1985, p. 651)  And as a final concluding remark: &quot;The adaptable provisioning behavior of Sphex ichneumoneus would be surprising to anyone who viewed insect behavior as stereotyped and fixed. The versatility of individuals extended to all phases of their behavior, from the habitats in which they hunted, to the types of prey captured, to the behavior used in getting the prey into the brood cell. Where responses show stereotypy, such as in repeated prey retrievals, there is an obvious, adaptive explanation. I suspect that long-term studies of known individuals in other species of insects would similarly reveal the same kind of adaptive behavioral versatility.&quot; (Brockmann, 1985, p. 652)  See also e.g. Strausfeld (2012), pp. 307-308, and Mallinson (2016).  " href="#footnote90_x2wjfqn">90</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Mirror self-recognition<a class="see-footnote" id="footnoteref91_ioa2k3x" title="Descriptions of this test can be found on Wikipedia's mirror test article, in Anderson &amp; Gallup Jr. (2015), and especially in Gallup Jr. et al. (2011). There are many interpretation subtleties here, but from the few sources I've read, Dr. Gallup Jr. seems to be reasoning about these subtleties in a roughly reasonable way. For an example claim of mirror self-recognition in a robot, see Takeno (2012). For a more recent journalistic overview, see Yong (2017).          Note that body self-recognition and &quot;conceptual&quot; self-recognition might be quite different functions, and thus evidence for the presence of one might not be strong evidence for the presence of the other, as explained by Lieberman (2013), pp. 185-186:      For forty years we have taken mirror self-recognition as a decisive sign of self-awareness in others, but the truth is more complicated. In Cartesian terms, this test focuses on the recognition of our body as our body…  &#9;…In an fMRI study, participants were shown adjectives, such as polite and talkative. For some of the trials, participants had to judge whether the adjective described George W. Bush, who was the U.S. president at the time. On other trials, participants had to judge whether the adjectives described themselves. The critical analysis examined whether there were any regions of the brain that were more active when people judged the applicability of an adjective to themselves as opposed to George Bush. There were only two regions of the brain whose activity followed this pattern.  &#9;Just as in the mirror self-recognition studies, there was activity in the prefrontal cortex and parietal cortex. But unlike the mirror self-recognition studies, these activations were present in the medial prefrontal cortex (MPFC) and the precuneus — on the midline of the brain where the two hemispheres meet, rather than on the lateral surface of the brain near the skull… In other words, recognizing yourself in the mirror and thinking about yourself conceptually rely on very different neural circuits. Seeing yourself and knowing yourself are two different things…  &#9;…this distinction clarifies what the mirror self-recognition test tells us about the animals that can pass it. Chimps, dolphins, and elephants all have some sense of their corporeal identity, that the body they see in the mirror is their body. However, the fMRI data suggests that passing this test does not imply that these animals engage in self-reflection the same way that we do, reflecting on whether we possess a particular personality trait or wondering what will become of us in ten years. It does not imply that these animals reflection the wisdom of their past decisions. And it certainly does not imply that these animals come to have a conceptual sense of self through introspective contemplation.  " href="#footnote91_ioa2k3x">91</a></td>
<td>Yes</td>
<td>Yes</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>n/a</td>
<td></td>
<td>n/a</td>
</tr><tr><td>Mental time-travel / episodic memory</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Distinct sleep/wake states<a class="see-footnote" id="footnoteref92_b26i214" title="For the relation between sleep and phenomenal consciousness in animals, see e.g. the discussion in Allen (2013), pp. 30-32. On the distribution of sleep across the animal kingdom, see Siegel (2008).  " href="#footnote92_b26i214">92</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Advanced social politics</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Uncertainty monitoring<a class="see-footnote" id="footnoteref93_1k420qs" title="My primary source for this PCIF is Smith &amp; Washburn (2005). I concluded that chimpanzees &quot;probably?&quot; exhibit uncertainty monitoring, because uncertainty monitoring has been observed in rhesus monkeys.      " href="#footnote93_1k420qs">93</a></td>
<td>Yes</td>
<td>probably?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Intentional deception</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Teaching others<a class="see-footnote" id="footnoteref94_p3actwm" title="For example see Loukola et al. (2017).      " href="#footnote94_p3actwm">94</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Abstract language capabilities<a class="see-footnote" id="footnoteref95_zd29s2z" title="I have not investigated this PCIF, but some potentially relevant sources include Botha &amp; Everaert (2013); Fitch (2010); Anderson (2004).  " href="#footnote95_zd29s2z">95</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Intentional agency<a class="see-footnote" id="footnoteref96_4tm6khe" title="Here, I have in mind the arguments of Bayne (2013) concerning agency as a mark of phenomenal consciousness.  " href="#footnote96_4tm6khe">96</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Understands pointing at distant objects</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Non-associationist learning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Selective attention (outside a nociceptive context)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Tool use<a class="see-footnote" id="footnoteref97_c883i2t" title="There are many kinds of tool use, and it's unclear which kinds are most indicative of phenomenal consciousness. In the foreword to Shumaker et al. (2011), Gordon M. Burghardt succinctly illustrates the diversity of animal tool use:  Ground squirrels kick sand into the faces of venomous snakes to deter attacks. Ant lions engage in a similar behavior in their sand pits to incapacitate prey. Degus (small rodents) use rakes to access food, an ability shared with many birds and non-human primates. Some mice set out markers to aid in finding their way home. Birds use small food items to bait fish, but crocodiles have turned the tables, using fish to attract birds, which they then attack. New Caledonian crows sometimes travel with a toolkit of proven implements for probing for food (including lizards in crevices). Crabs use all sorts of objects, animate and inanimate, to affix to themselves or to the shells they inhabit, for camouflage against predators. Apes are able to use tools of all kinds in both captivity and the wild. Through observation and practice they crack open nuts, apply herbal medications, open locks and doors, use sticks to stir liquids, saw wood, and even dig with a shovel. In fact, while tools are mostly used in foraging for food, they also are employed in many other contexts, such as to deter predators, facilitate courtship and copulation, mark territories, and intimidate competitors of their own species.  In the book's Introduction, the authors further illustrate the difficulty of deciding what should and shouldn't count as &quot;tool use&quot; by listing 53 observed animal behaviors that different definitions classify differently. After surveying the strengths and weaknesses of several proposed definitions, they opt for the following definition of tool use:  Our present definition of tool use is: The external employment of an unattached or manipulable attached environmental object to alter more efficiently the form, position, or condition of another object, another organism, or the user itself, when the user holds and directly manipulates the tool during or prior to use and is responsible for the proper and effective orientation of the tool.  Perhaps more useful is table 1.1, in which the authors describe 26 &quot;modes&quot; of tool use and manufacture. Below are just a few example rows, quoted directly from table 1.1:          Name of use mode     Function     Comments            Throw     Create or augment signal value of social display; amplify mechanical force; extend user's reach     Propel an object through open space. Can be aimed or unaimed. The object is propelled by the user's own energy.            Prop and Climb, Balance and Climb, Bridge, Reposition     Extend user's reach by expanding accessible three-dimensional space; bodily comfort     Prop and Climb: Place and stabilize an elongate object vertically or diagonally against another object or surface, and then move up or climb up the object. Distal end of propped object touches the other object or surface. Stable.  Balance and Climb: Place an elongate object vertically and then move up or climb up the object. The distal end of the balanced object does not touch another object or surface. Unstable.  Bridge: Place an elongate object or organism over water or open space such that each end rests on a surface on opposite sides of the water or spatial gap. User locomotes on the subject. Stable.  Reposition: Relocate and climb on an object or organism. Includes rafting (placing a buoyant object on water to support user's weight).            Symbolize     Abstract or represent reality     Carry, keep, or trade an object that represents another object, another organism, or a psychological state.             Detach     Structural modification of an object or an existing tool by the user or a conspecific so that the object/tool serves, or serves more effectively, as a tool     Remove the eventual tool from a fixed connection to the substrate or another object.             Add, Combine     As above [for 'Detach']     Join or connect two or more objects to make one tool that is held or directly manipulated in its entirety during its eventual use.       Most of the rest of the book, then, catalogues published observations of these various modes of tool use, organized by taxa such as &quot;insects,&quot; &quot;crustaceans,&quot; &quot;fish,&quot; &quot;birds,&quot; &quot;rodents,&quot; &quot;cetaceans,&quot; &quot;old world monkeys,&quot; &quot;gibbons,&quot; &quot;chimpanzees,&quot; etc. In table 7.1, they organize observed cases of tool use by mode and taxon.  In the end, I decided not to choose one or more modes of tool use for inclusion in my table of PCIFs and taxa, but future creators of similar tables might want to. For example, perhaps &quot;Symbolize&quot; is a particularly consciousness-informative mode of animal tool use.  Another useful recent source on animal tool use is Sanz et al. (2013).  " href="#footnote97_c883i2t">97</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Can spontaneously plan for future days without reference to current motivational state<a class="see-footnote" id="footnoteref98_hidxbd1" title="What I have in mind is the kind of planning for the future exhibited by western scrub-jays in Raby et al. (2007):  Knowledge of and planning for the future is a complex skill that is considered by many to be uniquely human. We are not born with it; children develop a sense of the future at around the age of two and some planning ability by only the age of four to five. According to the Bischof-Köhler hypothesis, only humans can dissociate themselves from their current motivation and take action for future needs: other animals are incapable of anticipating future needs, and any future-oriented behaviours they exhibit are either fixed action patterns or cued by their current motivational state. The experiments described here test whether a member of the corvid family, the western scrub-jay (Aphelocoma californica), plans for the future. We show that the jays make provision for a future need, both by preferentially caching food in a place in which they have learned that they will be hungry the following morning and by differentially storing a particular food in a place in which that type of food will not be available the next morning. Previous studies have shown that, in accord with the Bischof-Koöhler hypothesis, rats and pigeons may solve tasks by encoding the future but only over very short time scales. Although some primates and corvids take actions now that are based on their future consequences, these have not been shown to be selected with reference to future motivational states, or without extensive reinforcement of the anticipatory act. The results described here suggest that the jays can spontaneously plan for tomorrow without reference to their current motivational state, thereby challenging the idea that this is a uniquely human ability.  " href="#footnote98_hidxbd1">98</a></td>
<td>Yes</td>
<td></td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td></td>
<td></td>
<td></td>
</tr><tr><td>Can take into account another&#8217;s spatial perspective<a class="see-footnote" id="footnoteref99_u1dk56i" title="I have not investigated this PCIF, but see section 9 of David DeGrazia's &quot;Self-awareness in animals,&quot; which is chapter 11 in Lurz (2009).  " href="#footnote99_u1dk56i">99</a></td>
<td>Yes</td>
<td>Yes?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td>unavailable?</td>
<td></td>
<td>n/a</td>
<td></td>
<td>n/a</td>
</tr><tr><td>Theory of mind<a class="see-footnote" id="footnoteref100_mw9ggkb" title="I have not investigated this PCIF, but see e.g. Kaminski (2016).  " href="#footnote100_mw9ggkb">100</a></td>
<td>Yes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr></table><p>A fuller examination of the PCIFs approach, which I don&#8217;t conduct here, would involve (1) explaining these PCIFs in some detail, (2) cataloging and explaining their presence or absence (or scalar value) for a wide variety of taxa, (3) arguing for some set of “weights” representing how strongly each of these PCIFs indicate consciousness and why, with some PCIFs perhaps being assigned ~0 weight, and (4) arguing for some resulting substantive conclusions about the likely distribution of consciousness.</p>
<p><span style="float: right;">[<a href="/node/858/edit/15">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="PCIFsOverall">My overall thoughts on PCIF arguments</h4>
<p>Given that my table of PCIFs and taxa is so incomplete, not much can be concluded from it concerning the distribution question. However, my investigation into analogy-driven arguments, and my incomplete attempt to construct my own table of analogies, left me with some impressions I will now share (but not defend).</p>
<p>First, I think that analogy-driven arguments about the distribution of consciousness typically draw from far too narrow a range of taxa and PCIFs. In particular, it seems to me that analogy-driven arguments, as they are typically used, do not take seriously enough the following points:</p>
<ol><li>Many commonly-used PCIFs are executed both with <em>and</em> without conscious awareness in humans (e.g. at different times), and are thus not particularly compelling evidence for the presence of consciousness in non-humans.<a class="see-footnote" id="footnoteref101_rrftpdm" title="For discussions and debates about the relatively sophisticated behavior controlled by unconscious (or at least unconscious-to-us) processes in humans, see e.g. appendix 3 of Shevlin (2016); Prinz (2015); Bargh &amp; Morsella (2010); Gigerenzer (2007); ch. 15 of Macchi et al. (2016); Hassin (2013); Newell &amp; Shanks (2014); Goodale &amp; Milner (2013); Weiskrantz (2008); Shepherd (2015); Kihlstrom (2013); de Gelder et al. (2002). See also my section on cortex-required views and the appendices it links to. &#9; &#9;However, we must be careful not to exaggerate the powers of the unconscious human mind. For example, several results in this area have fared poorly in psychology's &quot;replication crisis.&quot; (See also Appendix Z.8.  &#9;" href="#footnote101_rrftpdm">101</a></li>
<li>Many commonly-used PCIFs are possessed by biological subsystems which are typically thought to be non-conscious, for example the enteric nervous system and the spinal cord.<a class="see-footnote" id="footnoteref102_y677dat" title="For an example discussion involving learning by the rat spinal cord, see Allen et al. (2009). On the enteric nervous system, see Young (2012), Wood (2011), and Rao &amp; Gershon (2016). On the autonomic nervous system more generally, see Ryder (1996).  &#9;" href="#footnote102_y677dat">102</a></li>
<li>Many commonly-used PCIFs are possessed by simple, short computer programs, or in other cases by more complicated programs in widespread use (such as <a href="https://en.wikipedia.org/wiki/Microsoft_Windows">Microsoft Windows</a> or <a href="http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html">DQNs</a>). Yet, these programs are typically thought to be non-conscious, even by functionalists.<a class="see-footnote" id="footnoteref103_edkz5na" title="See e.g. my example extensions to MESH: Hero here, Herzog et al. (2007), and Table 1 of Liu &amp; Schubert (2010).  &#9;" href="#footnote103_edkz5na">103</a></li>
<li>Many commonly-used PCIFs are possessed by plants and bacteria and other very “simple” organisms, which are typically thought to be non-conscious.<a class="see-footnote" id="footnoteref104_ziib6gi" title="On bacteria, see Bray (2011) and Lyon (2015). On plants, see Smith (2016).  &#9;" href="#footnote104_ziib6gi">104</a> For example, a neuron-less slime mold can store memories, transfer learned behaviors to conspecifics, escape traps, and solve mazes.<a class="see-footnote" id="footnoteref105_brks0qd" title="See Yong (2016).  &#9;" href="#footnote105_brks0qd">105</a></li>
<li>Analogy-driven arguments typically make use of a very short list of PCIFs, and a very short list of taxa. Including more taxa and PCIFs would, I think, give a more balanced picture of the situation.</li>
</ol><p>Second, I think analogy-driven arguments about consciousness too rarely stress the general point that “functionally similar behavior, such as communicating, recognizing neighbors, or way finding, may be accomplished in different ways by different kinds of animals.”<a class="see-footnote" id="footnoteref106_ptpepma" title="This quote is from Shettleworth (2009), p. 5, which cites Dyer (1994) as an example.  " href="#footnote106_ptpepma">106</a> This holds true for software as well<a class="see-footnote" id="footnoteref107_smtyq6n" title="Dawkins (2012) gives the following example:  Security cameras are sensitive to movement and respond appropriately by switching on a light, sounding an alarm, or even ringing a police station, but most of us don't worry too much about whether or not they are conscious, despite our tendency to describe them in anthropomorphic terms ('Don't do that or the camera will think you are an intruder').  We know that what a surveillance camera does is very simple. It can detect movement and it can then respond in a totally automated way to raise the alarm and even to summon the police. We also know that if we looked out of the window and saw a strange man running across the lawn at night brandishing a gun, we would perform a similar task of alerting the police but we would do it in a completely different, conscious way. The end result is the same, but with a different way of getting there. One is the totally unconscious activation of a phone line, the other has the full panoply of conscious recognition of the presence of an intruder, followed by the experience of fear at what he might do, and then the conscious action of telephoning the police and explaining to them rationally what is happening.  This simple example shows why identifying where there is consciousness is so difficult. There is clearly a spectrum of mechanisms for producing a similar outcome that has security cameras at one end and ourselves peering into the night at the other. Where on this spectrum are we to put, say, slugs? Fish? Chimpanzees? Plants? The fact that so many of the attributes of consciousness, such as the ability to respond to stimuli and choose an appropriate action, can be mimicked by relatively simple machines shows that it is not necessary to feel or experience anything in order to have adaptive, appropriate behaviour. A few simple sensors, a bit of programming, and an electrically powered output of a sort we are all familiar with and you can do a lot of routine, everyday behaviour. Consciousness just isn't necessary.  " href="#footnote107_smtyq6n">107</a> — consider the many different algorithms that can be used to <a href="https://en.wikipedia.org/wiki/Sorting_algorithm">sort information</a>, or implement a <a href="https://en.wikipedia.org/wiki/Shared_memory">shared memory system</a>, or make complex decisions,<a class="see-footnote" id="footnoteref108_kin0rxb" title="See e.g. ch. 17 of Russell &amp; Norvig (2009).  " href="#footnote108_kin0rxb">108</a> or learn from data.<a class="see-footnote" id="footnoteref109_n4kplu0" title="See e.g. Goodfellow et al. (2016).  " href="#footnote109_n4kplu0">109</a> Clearly, many behavioral PCIFs can be accomplished by many different means, and for any given behavioral PCIF, it may be the case that it is achieved with the help of conscious awareness in some cases, and without conscious awareness in other cases.</p>
<p><span style="float: right;">[<a href="/node/858/edit/16">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="Necessary">Hunting for necessary or sufficient conditions</h3>
<p>How else might we learn something about the distribution question, without putting much weight on any single, specific theory of consciousness?</p>
<p>One possibility is to argue that some structure or capacity is likely <em>necessary</em> for consciousness — without relying much on any particular <em>theory</em> of consciousness — and then show that this structure or capacity is present for some taxa and not others. This wouldn&#8217;t necessarily prove which taxa <em>are</em> conscious, but it would tell us something about which ones <em>aren&#8217;t</em>.</p>
<p>Another possibility is to argue that some structure or capacity is likely <em>sufficient</em> for consciousness, and then show that this structure or capacity is present for some taxa and not others. This wouldn&#8217;t say much about which taxa <em>aren&#8217;t</em> conscious, but it would tell us something about which ones <em>are</em>.</p>
<p>(Technically, all potential necessary or sufficient conditions are just PCIFs, but with a different “strength” to their indication of consciousness.<a class="see-footnote" id="footnoteref110_kbc2ws6" title="Intuitively, we might rate PCIFs on a &quot;strength of indication&quot; scale from -1 to 1, such that:   A score of -1 means that if a system exhibits that PCIF, then the system is definitely not conscious. A score of 0 means that if a system exhibits that PCIF, this doesn't indicate the presence or absence of consciousness in that system at all. A score of 1 means that if a system exhibits that PCIF, then the system is definitely conscious.   Using this rating system, a true &quot;sufficient&quot; condition of consciousness could be scored as 1, or nearly that high. A property found to be totally irrelevant to whether or not a system is conscious, such as whether the most common English term for it includes the letter g, could be scored as 0. A true necessary condition of consciousness might be scored across a wide range of values, depending on the degree to which it is also a sufficient condition of consciousness. But the inverse of a true necessary condition of counsciousness would be scored as -1.  This scoring system would have to be extended to accomodate PCIFs that are scalar rather than binary, such as number of neurons. In such cases, the strength of indication would (perhaps) typically be a monotonic function, but probably not a linear function — i.e. more neurons is always more consciousness-indicating, all else equal, but strength of indication changes more between 100 neurons and 1 million and one hundred neurons than it does between 100 billion neurons and 100 billion and one million neurons, even though the difference between the two is one million neurons in both cases.  " href="#footnote110_kbc2ws6">110</a> I discuss them separately in this report mainly for organizational reasons.)</p>
<p><a name="substantive" id="substantive"></a>Of course, we&#8217;d want such necessary or sufficient conditions to be “substantive.” For example, I think there&#8217;s a pretty strong case that information processing of some sort is a necessary condition for consciousness, but this doesn&#8217;t tell me much about distribution of consciousness: even bacteria process information. I also think there&#8217;s a pretty strong case that “human neurobiology plus detailed self-report of conscious experience” should be seen as sufficient evidence for consciousness, but again this doesn&#8217;t tell me anything novel or interesting about the distribution question.</p>
<p>I assume that at this stage of scientific progress we cannot definitely <em>prove</em> a “substantive” necessary or sufficient condition for consciousness, but can we make a “moderately strong argument” for some such necessary or sufficient condition? If so, that wouldn&#8217;t <em>settle</em> the matter, and it might not convince most readers, but it might at least allow <em>me</em> to noticeably shift my probabilities concerning which taxa are and aren&#8217;t likely to be conscious.</p>
<p>Below I consider the case for just one proposed necessary or sufficient condition for consciousness.<a class="see-footnote" id="footnoteref111_b76uipi" title="See also notes from my conversation with David Chalmers.  " href="#footnote111_b76uipi">111</a> There are other candidates I could have investigated,<a class="see-footnote" id="footnoteref112_zkwat0a" title="For example, language of a certain sort is sometimes argued to be a necessary condition for phenomenal consciousness. See, for example, the sources cited in (the endnotes for) this passage from ch. 6 of LeDoux (2015):  As part of our daily lives we use language to label and describe our perceptions, memories, thoughts, beliefs, desires, and feelings. As we've seen, this capacity to talk about our inner states makes it relatively easy for us to study human consciousness scientifically. But the contribution of language goes far beyond simply providing a tool for assessing consciousness. Language, Daniel Dennett says, lays down tracks on which thoughts can travel. Many other philosophers of mind and scientists have argued for a strong relation between language and consciousness.  " href="#footnote112_zkwat0a">112</a> but decided not to at this time.</p>
<p><span style="float: right;">[<a href="/node/858/edit/17">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="Cortex">Is a cortex required for consciousness?</h4>
<p>One commonly-proposed necessary condition for phenomenal consciousness is <em>possession of a cortex</em>, or sometimes possession of a <em>neocortex</em>, or possession of a specific part of the neocortex such as the <em>association cortex</em>. Collectively, I&#8217;ll refer to these as “cortex-required views” (CRVs). Below, I report my findings about the credibility of CRVs.</p>
<p>Even sources arguing <em>against</em> CRVs often acknowledge that, for many years, it has been commonly believed by cognitive scientists and medical doctors that the cortex is the organ of consciousness in humans,<a class="see-footnote" id="footnoteref113_cwcppm7" title="For example, Merker (2007):  Few [cognitive scientists] or neuroscientists would today object to the assertion that &quot;cortex is the organ of consciousness.&quot;  And Devor et al. (2014):  …there has never been much doubt among neuroscientists or neurologists that the neural process that constitutes pain perception, as well as other forms of conscious experience, occurs in the cerebral cortex…  The dogma of a cortical seat of pain and consciousness is convenient. First, until recently, the presence of a 'flat' EEG (a marker of lost cortical function) was a key criterion for 'brain death' and a diagnostic basis for the ethical harvesting of vital organs for transplantation … conventional reasoning in the neurological community arrives at the conclusion that pain experience is absent, and indeed impossible, in the PVS [persistent vegetative state] patient [because] it is assumed that all conscious perception, including pain, resides in the cortex, and in the PVS patient cortical function is absent, [therefore] it follows that the PVS patient is incapable of feeling pain.  Similarly, Mallatt &amp; Feinberg (2016), who argue for an ancient origin of consciousness, say:  Most investigators have for centuries located consciousness in the cerebral cortex. To this day, the dominant paradigm in consciousness studies is that primary consciousness of mapped mental images in mammals comes from the cerebral cortex or from interactions between the cortex and the thalamus, not from the superior colliculus/tectum as Merker claims. We agree with the dominant paradigm for mammals because so many of the neural correlates of mammalian exteroceptive consciousness are in this corticothalamic system (Koch, Massimini, Boly, &amp; Tononi, 2016). Medical neuroimaging and brain-lesion studies strongly support cortical consciousness when the results are interpreted in the most direct and straightforward way: damage to the cortex leads to loss of some sensory consciousness (Boly et al., 2013; Feinberg, 2009). Destruction of the visual, occipital, cortex causes blindness in primates. In his argument for tectal instead of cortical consciousness, Merker (2007) said that these loss phenomena are more complex than they appear, that the cortical damage actually inhibits the conscious role of the superior colliculus, etc. However, his interpretation is less parsimonious and therefore it requires extraordinary counterevidence to be believed. The indirect counterevidence that Merker provided — on the &quot;Sprague effect&quot; (p. 67) and on the cortex projecting to the superior colliculus (p. 76) — does not seem definitive enough to topple the dominant view of cortical consciousness in mammals.  Tye (2016), who likewise argues against CRVs, notes that (pp. 79-80):  The claim that in humans pain and other experiences require a neocortex is widely accepted. For example, the American Academy of Neurology asserts (1989):  Neurologically, being awake but unaware is the result of a functioning brainstem and the total loss of cerebral cortical functioning… Pain and suffering are attributes of consciousness requiring cerebral cortical functioning.  The Medical Task Force on Anencephaly (1990) says much the same thing in connection with congenital cases:  Infants with anencephaly, lacking functioning cerebral cortex are permanently unconscious… The suffering associated with noxious stimuli (pain) is a cerebral interpretation of the stimuli; therefore, infants with anencephaly presumably cannot suffer. (pp. 671-672)    " href="#footnote113_cwcppm7">113</a> though it&#8217;s not clear whether they would have also endorsed the much stronger claim that a cortex is required for consciousness <em>in general</em>. However, some experts have recently lost confidence that the cortex is required for consciousness (even just in humans), for several reasons (which I discuss below).<a class="see-footnote" id="footnoteref114_286c995" title="For some interesting historical context, see Thompson (1993). Ward (2011) also covers some of the history very briefly:  The search for the neural correlates of consciousness (NCC) has been intense and productive in the past two decades since Crick and Koch (1990) focused attention on this project… Recent work has emphasized the importance of the thalamo-cortical system of the brain in generating conscious awareness. Within this system three possibilities for the critical brain activity most closely associated with consciousness have been proposed: it occurs primarily within the cortex (e.g., Crick &amp; Koch, 2003; Romijn, 2002), it occurs in the entire system of thalamo-cortical loops (e.g., Edelman &amp; Tononi, 2000; John, 2001, 2002; Llinás, Ribary, Contreras, &amp; Pedroarena, 1998), it occurs primarily within the thalamus (e.g., Penfield, 1975)… In this paper I discuss the third possibility… that the neural activity most closely associated with primary consciousness occurs primarily in the thalamus…  The proposal that the thalamus is a particularly important locus in the brain involved in generating consciousness also is not new. As Newman (1995) pointed out, views of the locus of the neural correlate of conscious awareness have oscillated around three foci for many years: the cerebral cortex, the reticular activating system, and the thalamus. Penfield (e.g., 1975, p. 19) was perhaps the most extreme proponent of the subcortical view, asserting that &quot;The indispensable substratum of consciousness lies outside the cerebral cortex, probably in the diencephalon (the higher brainstem).&quot; The thalamus is a major component of the diencephalon (it also includes the epithalamus and the hypothalamus). Penfield's ideas were founded upon several major evidential bases: (1) the results of stimulating the brain, especially the temporal lobe, with low-intensity direct current electricity; (2) the results of surgery for epilepsy in which various chunks of brain, especially temporal lobe, were removed; and (3) the structural and functional anatomy of the brain that was known in the early 1970s. More recently, several authors have argued that the thalamic reticular nucleus (TRN) plays a role in consciousness by modulating the local 40-Hz oscillations observed in various parts of the brain via its inhibitory inputs to the dorsal thalamic nuclei (Min, 2010; Newman, 1995). Indeed, abolishing inhibitory interactions among the neurons of the TRN dramatically increases absenceepilepsy-like, low-frequency synchronous oscillations in the dorsal thalamic nuclei (Huntsman, Porcello, Homanics, DeLorey, &amp; Huguenard, 1999), indicating that such inhibition might play a major role in preventing the neural hyper-synchrony that characterizes epilepsy and its accompanying state of unconsciousness. Moreover, the TRN has been implicated in producing the unconscious state seen in absence epilepsy, presumably playing the role of strongly inhibiting thalamic neuron activity under the influence of cortical excitation (e.g., Steriade, 2005).  Another vociferous recent proponent of localizing the critical NCC for the state of consciousness in the thalamus has been Joseph Bogen, the surgeon who developed the split brain operation… Bogen argued that the intralaminar nuclei of the thalamus form the essential substrate of the state of phenomenal consciousness. His argument is complex, but critical to it is the fact that there are only two places in the CNS where very small bilateral lesions (those involving less than 1 g of neural tissue) abolish the state of consciousness: in the mesencephalic reticular formation and in the intralaminar nuclei of the thalamus. Moreover, the intralaminar nuclei are connected to much of the rest of the brain through diffuse reciprocal connections, making this a candidate for a central clearinghouse or modulator of cortical and subcortical activity.  Another proponent of a subcortical locus for a substrate of phenomenal awareness is Merker (2007). He updated Penfield and Jasper's (1954) centrencephalic system proposal and reviewed extensive evidence that the top of the brainstem, and the superior colliculus in particular, forms a system that integrates motivation, the sensory world, and body capabilities to accomplish goal-directed action. His arguments rely on the extensive convergence of inputs from pretty much the entire brain into this region, including to and from parts of the thalamus. Indeed, the thalamus plays an important integrative role in his theory, although the theory does not specify it to be the substrate of consciousness… Among other important facts discussed by Merker (2007) is the remarkable observation that in over 750 operations to cure epilepsy, during which the patient was not anesthetized, Penfield and Jasper (1954) never once observed even an interruption in the continuity, let alone cessation, of a patient's consciousness as they removed large chunks of cortex, sometimes even an entire hemisphere. Interestingly, in his later book Penfield (1975) identified the diencephalon with the &quot;highest brain mechanism&quot; that is directly responsible for consciousness. The part investigated thoroughly by Merker, on the other hand, was termed by Penfield &quot;the brain's computer&quot; and was said to be responsible for sensory-motor integration, as updated and extended by Merker (2007). Both mechanisms acting together were thought to be necessary to explain human behavior because the diencephalon has privileged access to frontal and temporal areas of the cortex, whereas the older system just below this area at the roof of the brainstem has privileged access to sensory and motor mechanisms. It is thus likely that the entire upper brain stem plays a crucial role in human behavior…  " href="#footnote114_286c995">114</a> </p>
<p>In this section, I describe some of the evidence used to argue for and against a variety of cortex-required views. As with the table of PCIFs and taxa <a href="#PCIFsTable">above</a>, please keep in mind that I am not an expert on the topics reviewed below, and my own understanding of these topics is based on a quick and shallow reading of various overview books and articles, plus a small number of primary studies.<a class="see-footnote" id="footnoteref115_uqzwnug" title="Also as above, I wouldn't be surprised if many of the studies cited in this section turn out to not &quot;hold up&quot; very well upon closer scrutiny; see Appendix Z.8.  " href="#footnote115_uqzwnug">115</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/18">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="ProCRV">Arguments for cortex-required views</h5>
<p>For decades, much (but not all) of the medical literature and the bioethics literature more-or-less assumed one or another CRV, at least in the case of humans, without much argument.<a class="see-footnote" id="footnoteref116_wbolmbm" title="One example is Veatch (1975), who identifies conscious experience with the neocortex but doesn't cite any evidence in support of this connection. Similarly, Bartlett &amp; Younger (1988) and Rich (1997) repeatedly assert that consciousness requires cortical function, but they don't argue for, or cite evidential support for, that assertion.  Another example is Green &amp; Wikler (1980), though they do qualify their statement with &quot;arguably&quot;:  …it does not follow from our argument that all humans lacking the substrate of consciousness are dead. Anencephalic infants are lacking at birth the cortical material necessary for the development of cognitive functioning and, arguably, consciousness. Still, due to possession of a functioning brain stem, they may have spontaneous breathing and heartbeat, and a good suck.  Even the more recent DeGrazia (2016) seems to assume a CRV, at least for humans (and again, without argument), e.g. in statements such as &quot;…the cerebrum, the primary vehicle of conscious awareness…&quot; and &quot;in a permanent (irreversible) vegetative state (PVS), while the higher brain is extensively damaged, causing irretrievable loss of consciousness, the brainstem is largely intact…&quot; and &quot;Consider… anencephalic infants, who are born without cerebral hemispheres and never have the capacity for consciousness…&quot;   However, one caveat about my discussion of CRVs is that different authors appeal to slightly different definitions of &quot;consciousness,&quot; and so it is not always the case that the authors I cite explicitly argued for or against the view that a cortex is required for &quot;consciousness&quot; as defined above. Still, these are arguments are sometimes used by others to make claims about the dependence or non-dependence of consciousness (as defined above) on a cortex, and certainly the arguments could easily be adapted to make such claims.  Note also that &quot;many… court decisions have relied on the presumption that consciousness is permanently lost in the persistent vegetative state, and have assumed that physicians can reliably make this diagnosis [Cranford &amp; Smith (1987); Smith (1988)]&quot; (Truog &amp; Fackler 1992).  Another example is Tononi et al. (2016), which seems to assume that at least human consciousness requires cortical function:  Consciousness depends on the integrity of certain brain regions and the particular content of an experience depends on the activity of neurons in parts of the cerebral cortex. However, despite increasingly refined clinical and experimental studies, a proper understanding of the relationship between consciousness and the brain has yet to be established. For example, it is not known why the cortex supports consciousness when the cerebellum does not, despite having four times as many neurons, or why consciousness fades during deep sleep while the cerebral cortex remains active. There are also many other difficult questions about consciousness. Are patients with a functional island of cortex surrounded by widespread damage conscious, and if so, of what?…  " href="#footnote116_wbolmbm">116</a> In those sources which <em>argue</em> for a CRV,<a class="see-footnote" id="footnoteref117_8an0xpp" title="Rose (2002):  Extensive evidence demonstrates that our capacity for conscious awareness of our experiences and of our own existence depends on the functions of this expansive, specialized neocortex. This evidence has come from diverse sources such as clinical neuropsychology (Kolb and Whishaw, 1995), neurology (Young et al., 1998; Laureys et al., 1999, 2000a-c), neurosurgery (Kihlstrom et al., 1999), functional brain imaging (Dolan, 2000; Laureys et al., 1999, 2000a-c), electrophysiology (Libet, 1999) and cognitive neuroscience (Guzeldere et al., 2000; Merikle and Daneman, 2000; Preuss, 2000). A strong case has been made that it is mainly those cortical regions that have achieved such massive expansion in humans that are most centrally involved in the production of consciousness (Edelman and Tononi, 2000; Laureys et al., 1999, 2000a-c).  …The evidence that the neocortex is critical for conscious awareness applies to both types of consciousness [&quot;primary&quot; consciousness and &quot;higher-order&quot; consciousness]. Evidence showing that neocortex is the foundation for consciousness also has led to an equally important conclusion: that we are unaware of the perpetual neural activity that is confined to subcortical regions of the central nervous system, including cerebral regions beneath the neocortex as well as the brainstem and spinal cord (Dolan, 2000; Güzeldere et al., 2000; Jouvet, 1969; Kihlstrom et al., 1999; Treede et al., 1999).  …From the clinical perspective, primary consciousness is defined by: (1) sustained awareness of the environment in a way that is appropriate and meaningful, (2) ability to immediately follow commands to perform novel actions, and (3) exhibiting verbal or nonverbal communication indicating awareness of the ongoing interaction… Thus, reflexive or other stereotyped responses to sensory stimuli are excluded by this definition. Primary consciousness appears to depend greatly on the functional integrity of several cortical regions of the cerebral hemispheres especially the &quot;association areas&quot; of the frontal, temporal, and parietal lobes (Laureys et al., 1999, 2000a-c). Primary consciousness also requires the operation of subcortical support systems such as the brainstem reticular formation and the thalamus that enable a working condition of the cortex. However, in the absence of cortical operations, activity limited to these subcortical systems cannot generate consciousness (Kandel et al., 2000; Laureys et al., 1999, 2000a; Young et al., 1998). Wakefulness is not evidence of consciousness because it can exist in situations where consciousness is absent (Laureys et al., 2000a-c). Dysfunction of the more lateral or posterior cortical regions does not eliminate primary consciousness unless this dysfunction is very anatomically extensive (Young et al., 1998).  …Diverse, converging lines of evidence have shown that consciousness is a product of an activated state in a broad, distributed expanse of neocortex. Most critical are regions of &quot;association&quot; or homotypical cortex (Laureys et al., 1999, 2000a-c; Mountcastle, 1998), which are not specialized for sensory or motor function and which comprise the vast majority of human neocortex. In fact, activity confined to regions of sensory (heterotypical) cortex is inadequate for consciousness (Koch and Crick, 2000; Lamme and Roelfsema, 2000; Laureys et al., 2000a,b; Libet, 1997; Rees et al., 2000).  About a decade later, Rose et al. (2014) added:  The neural basis of consciousness was reviewed and applied to the problem of fish pain by Rose (2002)… Subsequent research has further substantiated and refined the fundamental principles identified earlier, that, the existence of all the previously described forms of consciousness [primary consciousness and higher-order consciousness] depends on neocortex, particularly frontoparietal ‘association' cortex in distinction from primary or secondary sensory or motor cortex (Laureys and Boly 2007; Amting et al. 2010; Vanhaudenhuyse et al. 2012). Primary consciousness also requires supporting operation of subcortical systems including (i) the brainstem reticular formation to enable a working condition of the cortex and (ii) interactions between the cortex and thalamus as well as cortex and basal ganglia structures (Edelman and Tononi 2000; Laureys et al. 1999, 2000a,b,c)… Human neocortex, the six-layered cortex that is unique to mammals, has specialized functional regions of sensory and motor processing, but activity confined to these regions is insufficient for consciousness (Koch and Crick 2000; Lamme and Roelfsma 2000; Laureys et al. 2000a,b; Rees et al. 2000). Although neocortex is usually identified as the critical substrate for consciousness, a critical role for some regions of mesocortex, particularly the cingulate gyrus, is well established. Mesocortical structures have fewer than six layers, but like neocortex, are unique to mammalian brains and highly interconnected with neocortex. The cingulate gyrus, in concert with neocortex, is particularly important for conscious awareness of the emotional aspect of pain (Vogt et al. 2003), other dimensions of emotional feelings (Amting et al. 2010) and self-awareness (Vanhaudenhuyse et al. 2012).  Building on these earlier articles by Rose, Key (2015) argues:  What is so unique about the cortex that enables inner mental states? First, the cortex is parcellated into discrete anatomically structures or cortical areas that process information related to specific functions. It is estimated that there are about 200 cortical areas in humans (Kaas 2012). For instance, the cortical visual system consists of over a dozen distinct regions with diverse subfunctions that are strongly interconnected by reciprocal axon pathways. One of the defining features of these subregions is that they become simultaneously active. Both recurrent activity and binding of neural activity across cortical regions are believed to be essential prerequisites for the subjective experience of vision (Sillito et al. 2006; Pollen 2011; Koivisto and Silvanto 2012). It has been shown that when neural processing of recurrent signalling from higher cortical regions entering the V1 visual cortex is perturbed by transcranial magnetic stimulation, the subjective awareness of a visual stimulus is disrupted (Koivisto et al. 2010, 2011; Jacobs et al. 2012; Railo and Koivisto 2012; Avanzini et al. 2013).  The subregionalisation of the neocortex also allows the formation of spatial maps of the sensory world, such as those associated with the representations of the surface of the body or the visual field. These topographical maps are important for the multiscale processing of sensory information (Kaas 1997; Thivierge and Marcus 2007). Variation in the size of the maps alters the sensitivity of responses to stimuli while spatial segregation of neurons responding to selective parts of a stimulus allows for finer perceptual discrimination. Painful and non-painful somatosensory stimuli are topographically mapped to overlying regions in the primary somatosensory cortex (SI) in humans (Mancini et al. 2012). These results are consistent with the known point-to-point topography from the body surface to SI (called somatotopy) that underlies spatial acuity. However, by using high resolution mapping in the squirrel monkey SI (sub-millimetre level) it was revealed that there were slight differences in the localisation of different somatosensory modalities (Chen et al. 2001). This slight physical separation of cortical neurons responding to different peripheral stimuli suggests that differences in the subjective quality of somatosensory sensations may arise as early as in SI. Somatotopic maps for painful stimuli are also present in the human SII and insular cortices (Baumgartner et al. 2010). Interestingly, different qualities of painful stimuli (such as heat and pinprick) are more distinctly mapped topographically to different regions of SII and the insular cortex than in SI. Similarly, painful and non-painful stimuli are mapped to separate regions in human SII (Torquati et al. 2005). This separation of cortical processing of heat and tactile stimuli within different cortical areas has also been observed in non-human primates (Chen et al. 2011). These multiple neural maps suggests that SII and the insular cortex play important roles in discriminating differences in the subjective quality of somatosensory stimuli, particularly painful from non-painful (Tommerdahl et al. 1996; Baumgartner et al. 2010; Chen et al. 2011; Mazzola et al. 2012). This idea is supported by evidence from direct electrical stimulation of discrete areas in the human insular cortex (Afif et al. 2010).  Second, the cortex is a laminated structure that enables the efficient processing and integration of different types of neural information by unique subpopulations of neurons (Schubert et al. 2007; Maier et al. 2010; Larkum 2013). Lamination appears to facilitate complex wiring patterns during development. If two populations of neurons were randomly distributed within a specific brain region and incoming axons were required to synapse with only one subpopulation, then those axons would need to rely on stochastic and hence error-prone searching to complete wiring. On the other hand, when similar neurons are partitioned together in a single lamina then a small set of molecular cues is able to guide axons with high precision to their appropriate post-synaptic target. Two principal afferent inputs (from the neocortex itself, and the thalamus) enter the neocortex and separately innervate distinct layers (Nieuwenhuys 1994). The main thalamic fibres terminate densely in layer IV (called the granular layer) while the neocortical fibres innervate different pyramidal neurons in layers I–III (supragranular layers) (Opris 2013). By selectively ablating Pax6, a developmentally significant patterning gene, in the cortex of mice it is possible to disrupt the laminar organisation of this structure (Tuoc et al. 2009). This altered cortical layering causes neurological deficits that are similar to those observed in humans with Pax6 haploinsufficiency (Tuoc et al. 2009) and provides strong experimental evidence of the importance of lamination to cortical function. A number of human brain disorders involve defects in cortical lamination that are detrimental to brain function (Guerrini et al. 2008; Guerrini and Parrini 2010; Bozzi et al. 2012).  Third, lamination facilitates the economical establishment of microcircuitry between neurons processing different properties of the stimulus. A vertical canonical microcircuit is established which leads to the emergence of functionally interconnected columns and minicolumns of neurons (Mountcastle 1997). For example, a hexagonal column in the primate somatosensory cortex is about 400 µm in width and contains populations of neurons that respond to the same stimulus (e.g. light touch or joint stimulation) arising from a specific topographical zone of the body. Columns can be associated with processing information related to a specific function (e.g. &quot;visual tracking&quot; and &quot;arm reach&quot; columns in the parietal cortex; Kass 2012). Each column itself consists of minicolumns (80–100 neurons) that are ~30–50 µm in diameter and interconnected by short-range horizontal processes (Buxhoeveden and Casanova 2002). While columns are most clearly distinguished in the sensory and motor cortices of primates, minicolumns appear to be ubiquitous in all animals with a neocortex (Kaas 2012). Minicolumns have a small receptive field within the larger receptive field of the column. The correlated activity in the fine-scale networks of minicolumns produces concentrated bursts of neural activity that may enable the cortex to transmit signals in the face of background noise (Ohiorhenuan et al. 2010). The function of the cortex seems to depend on the ability of canonical circuitry within the minicolumns to rapidly switch from feedforward to feedback processing between layers. During learned tasks in responses to cues in the awake monkey, information flows from layer 4 to layer 2/3 and then down to layer 5 in a feedforward loop in the temporal neocortex (Takeuchi et al. 2011; Bastos et al. 2012). This is followed shortly afterwards by a feedback loop from layer 5 to layer 2/3. Correlated firing of layer 2/3 and layer 5 neurons in minicolumns occurs during decision making in the monkey prefrontal cortex, an area responsible for executive control in primates (Opris et al. 2012). The accuracy of error-prone tasks was increased when layer 5 neurons were artificially stimulated by activity recorded during successful task execution. These results provide evidence for the role of the minicolumn as the fundamental processing unit of the neocortex associated with higher order behaviour (Bastos et al. 2012; Opris et al. 2012).  In summary, the unique morphology of the mammalian cortex facilitates multiscale processing of sensory information. Initially there is course scaling at the level of gross anatomical cortical regions specialising, for example, in processing of visual or somatosensory information. Some of these regions are then topographically mapped in order to preserve spatial relationships and facilitate selective processing of specific sensory features. Importantly, to preserve the holistic quality of a sensory stimulus, these subregions are strongly interconnected via axon pathways that create synchronized re-entrant loops of neural activity. Cortical regions are laminated which supports finer scale sensitivity in the processing of specific features. Finally, canonical microcircuits (minicolumns) bridge across layers to enhance signal contrast (Casanova 2010). Local connectivity between minicolumns enables the lowest level of stimulus binding that contributes to the holistic nature of the stimulus (Buxhoeveden and Casanova 2002).  I propose that only animals possessing the above neuroanatomical features (i.e. discrete cortical sensory regions, topographical maps, multiple cortical layers, columns/minicolumns and strong local and long-range interconnections), or their functionally analogous counterparts, have the necessary morphological prerequisites for experiencing subjective inner mental states such as pain.  For much more on this, see Key (2016) and the many replies to it in the same issue of Animal Sentience. See also the brief article &quot;An Argument in Defense of Fishing&quot; by Michael LaChat on pp. 20-21 in volume 21, issue 7 (1996) of Fisheries.  An earlier defense of a CRV (at least, for the human case) was mounted by Puccetti (1998):   If… the neocortical surface is itself selectively destroyed… that is sufficient to obliterate all conscious functions.  The midbrain is of course just the top of the brain stem, where the superior and inferior colliculi trigger orientating reflexes related, respectively, to sources of visual and auditory stimuli: such reflexive responses do not require conscious mediation, as we all know from finding ourselves turned towards an abrupt movement in the peripheral visual field, or in the direction of a sudden sound, before such stimuli register in consciousness. If a brain structure does its job unconsciously, then there is no reason to think its integrity in a comatose patient is evidence of residual conscious functions. Similarly with the cerebellum, which pre orchestrates complex bodily movements, and under therapeutic electrode stimulation does not yield clear sensations [3]. The cerebellum probably also stores learned subroutines of behavior, like swimming or typing: precisely the kinds of things you do better when not concentrating on them.  …[Douglas N. Walton's] statement [that &quot;the pupillary reflex could, for all we know, indicate some presence of feeling or sensation even if the higher cognitive faculties are absent&quot;]… reeks of superstition. As we all know, when the doctor flashes his penlight on the eye, we do not feel the pupil contract, then expand again when he turns the light off. If not, then why in the world does Walton suppose that a deeply comatose patient feels anything in the same testing situation? The whole point of evolving reflexes like this, especially in large brained animals that do little peripheral but lots of central information processing, is to shunt quick-response mechanisms away from the cerebrum so that the animal can make appropriate initial responses to stimuli before registering them consciously. If one could keep an excised human eye alive in vitro and provoke the pupillary reflex, the way slices of rat hippocampus have been stimulated to threshold for neuronal excitation, would Walton argue that the isolated eye might feel something as its pupil contracts?  …One thing I feel reasonably confident in stating is that sensations are not experienced without recruitment of populations of neurons in the grey matter on the cerebral cortical surface. And it is easy to see why this is so: the phylogenetic novelty of neocortex is due to brain expansion in primates beginning about 50 million years ago to accommodate increasing intelligence, for where else could new cell layers appear but on the outer surface of the brain [9]? That being the case, sensation migrated there as well, and although deeper structures certainly contribute complexly to the sentient input, this is not transduced as sensation until, at a minimum, some 104 neurons are provoked to discharge on the surface of at least one cerebral hemisphere at the same time [16]. It is also plain why the contribution of subcortical mechanisms to this input does not itself implicate conscious perception. If it did, we would have sensations in seriatum: a baseball leaving the pitcher's hand would be seen as arriving by the hitter several times in succession as neural impulses course from retina to optic chiasm to geniculate body through the optic radiation to primary visual cortex in the occipital lobe. From an evolutionary viewpoint, that would be a recipe for disaster.  …What Walton is doing is confusing the normally necessary contribution of subcortical mechanisms to sensation with the sufficient condition of neocortical functions. In the case of the primary visual system in man this is indisputable: destruction of Brodmann's area 17 alone, say by shrapnel wounds, brings permanent total blindness [7]; whereas a peripherally blind person with intact visual cortex can be induced to experience visual sensations by direct electrode stimulation of that grey matter [2].  …  [Another of Walton's points] alludes to findings by Lober (reported in [12]), that some people recovered from infantile hydrocephaly, thus growing up with severely reduced cerebral hemispheres, can nevertheless function well: an example being that of a university student, IQ 126, who gained first class honors in mathematics. This Walton takes to be evidence that the neocortex is neither the sole seat of consciousness nor, perhaps, crucial to the return of conscious functions.  One wants to scream aloud a commonplace of clinical psychopathology: When neural plasticity enters the picture, all bets are off! The neural plasticity of the infant brain allows a lot less than the normal quantity of grey matter to take over a wide range of functions that are usually diffused in greater brain space. This is strikingly and uncontroversially demonstrated in complete hemispherectomy for infantile hemiplegia, where control of the whole body (except for distal finger movements in the arm contralateral to the missing half brain) is found in adulthood [1]. Furthermore, as Epstein has said (quoted in [12]), hydrocephalus is principally a disease of the white matter of the brain (the cerebral ventricles, swelled by overproduction of cerebrospinal fluid, disrupt the axons of association fibers around them). It is precisely the sparing of nerve cells in the grey matter, even in severe cases of hydrocephalus, that explains the retention of conscious functions and high-performance IQs.  Tye (2016) argues against CRVs, but he presents some of the case for CRVs this way (pp. 78-79):  In humans, in standard cases, the sensory aspect of pain is generated by activity in the primary and secondary somatosensory cortices of the parietal lobe (SI and SII). The unpleasantness of pain — what we might call its &quot;felt badness&quot; — is closely tied to activity in the anterior cingulate cortex (ACC). Human functional imaging studies show that there is a significant correlation between the felt badness of pain and ACC activation… Furhter, when subjects feel pain from a stimulus that is held constant and a hypnotic suggestion is used to increase or decrease subjective unpleasantness, a correlation is found in regional cerebral blood flow in ACC but not in the primary somatosensory cortex… Also, patients with ACC lesions say that their pain is &quot;less bothersome.&quot;  If regions of SI and SII are damaged but not ACC, what results is an unpleasant sensation that is not pain. For example, a laser was used to deliver a thermal stimulus to a fifty-seven-year-old man with most of his SI damaged as a result of a stroke… When the stimulus significantly above normal threshold on his right hand was delivered to his left hand, the man reported an unpleasant sensation, but he denied that it was a case of pain. [Tye's footnote here says: &quot;Asked to classify his sensation from a list of terms that included 'hot,' 'burning,' and 'pain,' the patient picked none.&quot;]  It appears, then, that the painfulness of pain in humans is based on activity in two different neural regions: the somatosensory cortex (comprised of SI and SII) and ACC.  Some animals, such as fish, lack a neocortex. So they lack these regions. This neurophysiological difference, it might be said, makes a crucial difference. A related thought is that the causal story for animals lacking a neocortex that lies behind this behavior… cannot be reconciled with the story for humans. So we aren't entitled to infer a common cause [i.e. consciousness], even given common behavior. The neurophysiological difference between the nonhuman animals and humans defeats the explanation of behavior that [appeals] to pain.  Baars et al. (2003) is focused on the human case, and reviews several lines of evidence suggesting that frontoparietal association cortex could be especially critical for consciousness. Their summary reads:  …several lines of evidence suggest that [frontoparietal association areas] could have a special relationship with consciousness, even though they do not support the contents of sensory experience. (i) Conscious stimulation in the waking state leads to frontoparietal activation, but unconscious input does not; (ii) in unconscious states, sensory stimulation activates only sensory cortex, but not frontoparietal regions; (iii) the conscious resting state shows high frontoparietal metabolism compared with outward-directed cognitive tasks; and (iv) four causally very different unconscious states show marked metabolic decrements in the same areas.  " href="#footnote117_8an0xpp">117</a> I typically see two types of arguments:</p>
<ol><li>For multiple types of cognitive processing (visual processing, emotional processing, etc.), we have neuroimaging evidence and other evidence showing that we are consciously aware of activity occuring in (some regions of) the cortex, but we are <em>not</em> aware of activity occuring outside (those regions of) the cortex.</li>
<li>In cases where (certain kinds of) cortical operations are destroyed or severely disrupted, conscious experience seems to be abolished.</li>
</ol><p>According to James Rose<a class="see-footnote" id="footnoteref118_d4l61h9" title="See notes from my conversation with James Rose.  " href="#footnote118_d4l61h9">118</a> and some other proponents of one or another CRV, the case for CRVs about consciousness comes not from any <em>one</em> line of evidence, but from several converging lines of evidence of types (1) and (2), all of which somewhat-independently suggest that conscious processing must be subserved by certain regions of the cortex, whereas unconscious processing can be subserved by other regions. <em>If</em> this is true, this could provide a very suggestive case in favor of some kind of CRV about consciousness.</p>
<p>Unfortunately, I did not have the time to survey several different lines of evidence to check whether they converged in favor of some kind of CRV. Instead, I examine below just <em>one</em> of these lines of evidence — concerning conscious and unconscious vision — in order to illustrate how the case for a CRV could be constructed, if other lines of evidence showed a similar pattern of results.</p>
<p><span style="float: right;">[<a href="/node/858/edit/19">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="UnconsciousVision">Unconscious vision</h5>
<p>Probably the dominant<a class="see-footnote" id="footnoteref119_zf3kxq7" title="For example, Freud et al. (2016), despite being critics of the view, write:  The cortical visual system is almost universally thought to be segregated into two anatomically and functionally distinct pathways: a ventral occipitotemporal pathway that subserves object perception, and a dorsal occiptoparietal pathway that subserves object localization and visually guided action.  Similarly, another paper critical of the view, Cardoso-Leite &amp; Gorea (2010), describe Goodale &amp; Milner's &quot;two streams&quot; account as &quot;the most widespread account&quot; of &quot;how a physical stimulus can lead to a motor response, with or without an accompanying conscious experience.&quot;  " href="#footnote119_zf3kxq7">119</a> (but still contested) theory of human visual processing holds that most human visual processing occurs in two largely (but not entirely) separate streams of processing. According to this theory, the ventral stream, also known “vision for perception,” serves to recognize and identify objects and people, and typically leads to conscious visual experience. The dorsal stream, also known as “vision for action,” serves to locate objects precisely and interact with them, but is not part of conscious experience.<a class="see-footnote" id="footnoteref120_acdxds0" title="This finding directly contradicts our intuitive &quot;assumption of experience-based control&quot; Clark (2001).  " href="#footnote120_acdxds0">120</a> Below, I summarize what this theory says, but I don&#8217;t summarize the evidence in favor of the theory. I summarize that evidence in <a href="#AppendixC">Appendix C</a>.</p>
<p>These two streams are thought to be supported by different regions of the cortex, as shown below:<a class="see-footnote" id="footnoteref121_qtlnu1m" title="This image is File:1424 Visual Streams.jpg from Wikimedia Commons, and is licensed under the Creative Commons Attribution 3.0 Unported license.  " href="#footnote121_qtlnu1m">121</a></p>
<p><a href="https://commons.wikimedia.org/wiki/File:1424_Visual_Streams.jpg"><img style="display: block; margin-left: auto; margin-right: auto;" src="/files/Research/Moral_Patienthood/Two_visual_streams.jpg"   alt="Two streams of visual processing" align="middle" /></a></p>
<p>In primates, most visual information from the retina passes through the lateral geniculate nucleus (LGN) in the thalamus on its way to the primary visual cortex (V1) in the occiptal lobe at the back of the skull.<a class="see-footnote" id="footnoteref122_if4m0fi" title="Some additional visual information travels not through the LGN but instead to the superior colliculus, then to the pulvinar, and then joins the dorsal stream of visual processing in the posterior parietal cortex. See Snowden et al. (2012), ch. 11. In addition to these two pathways, there are several other retinal projections that are less well-studied (see Milner &amp; Goodale 2006, section 1.1).  " href="#footnote122_if4m0fi">122</a> From there, visual information is passed from V1 to two separate streams of the processing. The ventral stream leads into the inferotemporal cortex in the temporal lobe, while the dorsal stream leads into the posterior parietal cortex in the parietal lobe. (The dorsal stream also receives substantial input from several subcortical structures in addition to its inputs from V1, whereas the the ventral stream depends almost entirely on inputs from V1.<a class="see-footnote" id="footnoteref123_1mgps01" title="Milner &amp; Goodale (2006), p. 67:  …both the dorsal and ventral streams diverge anatomically from the primary visual cortex, but, as we noted in Chapter 2, the dorsal stream also has substantial inputs from several subcortical visual structures in addition to the input from V1… In contrast, the ventral stream appears to depend on V1 almost entirely for its visual inputs.  " href="#footnote123_1mgps01">123</a>)</p>
<p>To illustrate how these systems are thought to interact, consider an analogy to the remote control of a robot in a distant or hostile environment (e.g. Mars):<a class="see-footnote" id="footnoteref124_gqxhs16" title="This quote from Goodale &amp; Milner (2013), ch. 9.  Campbell (2002), pp. 55-56, offers a different analogy, that of a heat-seeking missile:  …conscious attention is what defines the target of processing for the visuomotor system, and thereby ensures that the object you intend to act on is the very same as the object with which the visuomotor system becomes engaged… This whole procedure may work even though your experience of the location of the object is not particularly accurate. If you see a penny in a mirror without realizing that you are seeing it in a mirror, you may use its apparent location in verifying that it is brown, and [in grasping] your hand correctly to pick it up, even though your experience of its location is not actually correct. There is an obvious analogy with the behaviour of a heat-seeking missile. Once the thing is launched, it sets the parameters for action on its target in its own way; but to have it reach the target you want, you have to have it roughly pointed in the right direction before it begins, so that it has actually locked on to the intended object.  " href="#footnote124_gqxhs16">124</a></p>
<blockquote><p>In tele-assistance, a human operator identifies and “flags” the goal object, such as an interesting rock on the surface of Mars, and then uses a symbolic language to communicate with a semi-autonomous robot that actually picks up the rock.</p>
<p>A robot working with tele-assistance is much more flexible than a completely autonomous robot… Autonomous robots work well in situations such as an automobile assembly line, where the tasks they have to perform are highly constrained and well specified… But autonomous robots… [cannot] cope with events that its programmers [have] not anticipated…</p>
<p>At present, the only way to make sure that the robot does the right thing in unforeseen circumstances is to have a human operator somewhere in the loop. One way to do this is to have the movements or instructions of the human operator… simply reproduced in a one-to-one fashion by the robot… [but this setup] cannot cope well with sudden changes in scale (on the video monitor) or with a significant delay between the communicated action and feedback from that action [as with a Mars robot]. This is where tele-assistance comes into its own.</p>
<p>In tele-assistance the human operator doesn&#8217;t have to worry about the real metrics of the workspace or the timing of the movements made by the robot; instead, the human operator has the job of identifying a goal and specifying an action toward that goal in general terms. Once this information is communicated to the semi-autonomous robot, the robot can use its on-board range finders and other sensing devices to work out the required movements for achieving the specified goal. In short, tele-assistance combines the flexibility of tele-operation with the precision of autonomous robotic control.</p>
<p>…[By analogy,] the perceptual systems in the ventral stream, along with their associated memory and other higher-level cognitive systems in the brain, do a job rather like that of the human operator in tele-assistance. They identify different objects in the scene, using a representational system that is rich and detailed but not metrically precise. When a particular goal object has been flagged, dedicated visuomotor networks in the dorsal stream, in conjunction with output systems elsewhere in the brain… are activated to perform the desired motor act. In other words, dorsal stream networks, with their precise egocentric coding of the location, size, orientation, and shape of the goal object, are like the robotic component of tele-assistance. Both systems have to work together in the production of purposive behavior — one system to help select the goal object from the visual array, the other to carry out the required metrical computations for the goal-directed action.</p></blockquote>
<p> </p>
<p><em>If</em> something like this account is true — and it might not be; see <a href="#AppendixC">Appendix C</a> — then it could be argued to fit with a certain kind of CRV, according to which some parts of the cortex — those which include the ventral stream but not the dorsal stream — are required for conscious experience (at least in humans).<a class="see-footnote" id="footnoteref125_xj5d5gq" title="Of course, vision neuroscience could still support some kind of CRV even if this &quot;two streams&quot; account turns out to be false. For example, even if the &quot;two streams&quot; account is wrong, it could still be the case that all subcortical visual processing is unconscious. E.g., the pupillary light reflex is controlled via subcortical visual processing, and we are not consciously aware of our own pupil dilation (Dragoi 2016).  Also, there are several other lines of evidence (besides those reviewed in Appendix C) that could be used to argue for unconscious vision of various kinds, supported by different parts of the brain. For example see the literatures on backward masking, binocular rivalry, and blindsight, which I don't review here. On backward masking, see Breitmeyer &amp; Ogmen (2006). On binocular rivalry, see Miller (2015). On blindsight, see Cowey (2004).  " href="#footnote125_xj5d5gq">125</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/20">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="ProCRVsSummary">Suggested other lines of evidence for CRVs</h5>
<p>On its own, this theory of conscious and unconscious vision is not very suggestive, but if several different types of cognitive processing tell a similar story — with all of them seeming to depend on certain areas of the cortex for <em>conscious</em> processing, with unconscious processing occuring elsewhere in the brain — then this could add up to a suggestive argument for some kind of CRV.</p>
<p>Here are some other bodies of evidence that could (but very well might not) turn out to collectively suggest some sort of CRV about consciousness:</p>
<ul><li>Preliminary evidence suggests there may be multiple processing streams for other sense modalities, too, but I haven&#8217;t checked whether this evidence is compatible with CRVs.<a class="see-footnote" id="footnoteref126_p75ycxb" title="Goodale &amp; Milner (2013), ch. 7, summarize some of this evidence briefly:  The somatosensory (touch) system seems to have an organization remarkably similar to that of the visual system, with two distinct representations of the body existing in the brain, one for the guidance of action and the other for perception and memory. This model, proposed by [Dijkerman &amp; De Haan (2007)], is supported not only by neuroscience evidence, but by the fact that there are somatosensory illusions that fool our perception without fooling actions based on the same form of bodily sensation. Dijkerman and De Haan call the perceptual representation of the body (which is vulnerable to illusions) our &quot;body image&quot; and the metrical representation that guides action our &quot;body schema.&quot;  One illusion to which the body image is prone is the so-called &quot;rubber hand illusion.&quot; This illusion is induced by having a person rest one of their arms on a table, but hidden from sight, with an artificial arm made of rubber lying alongside it. The experimenter proceeds to stroke the person's arm while simultaneously stroking the artificial arm in full view. The result is that the person gains a strong impression that the stroking sensations are located not where their real arm is really is, but as if shifted in space to where the artificial arm is lying. Yet when tested in a reaching task with the affected arm, they do not make erroneously long or short movements based on where they wrongly sense the starting point of their arm to be. Instead, their actions are guided on the basis of the true location of the arm, independently of their perceptions—presumably on the basis of the veridical body schema.  …There are even reports of brain-damaged individuals who have a somatosensory equivalent of blindsight — Yves Rossetti calls this phenomenon &quot;numbsense.&quot; Such patients are able to locate a touch on an arm which has completely lost the conscious sense of touch, by pointing with the other arm while blindfolded. Rossetti reports that his patient was amazed that she could do this. Even more dramatically…, Rossetti found that this numbsense ability evaporated to chance guessing when the patient was asked to delay two to three seconds before making her pointing response. It may be then that while the body schema may survive brain damage that disables the body image, it is constantly reinventing itself, with each reinvention having only a transient lifetime before being lost or replaced. Just like the dorsal visual stream.  …There is recent evidence to suggest that the auditory system too may be divided into perception and action pathways. Steve Lomber… has recently shown [Lomber &amp; Malhotra (2008)] that when one region of auditory cortex in the cat is temporarily inactivated by local cooling, the cat has no problem turning its head and body toward the sounds but cannot recognize differences in the patterns of those sounds, whereas when another quite separate area is cooled the cat can tell the sound patterns apart but can no longer turn towards them. These findings in the auditory system — and the work discussed earlier on the organization of the somatosensory system — suggest that a division of labor between perceiving objects and acting on them could be a ubiquitous feature of sensory systems in the mammalian cerebral cortex.  " href="#footnote126_p75ycxb">126</a></li>
<li>There is definitely “unconscious pain” (technically, unconscious <em>nociception</em>), but I haven&#8217;t checked whether the evidence is CRVs-compatible. (See my linked sources on this in <a href="#AppendixD">Appendix D</a>.)</li>
<li>There are both conscious and unconscious aspects to our emotional responses, but I haven&#8217;t checked whether the relevant evidence is CRVs-compatible. (See <a href="#AppendixZ4">Appendix Z.4</a>.)</li>
<li>Likewise, there are both conscious and unconscious aspects of (human) learning and memory,<a class="see-footnote" id="footnoteref127_b4j2gpg" title="E.g. see Kihlstrom (2013).  " href="#footnote127_b4j2gpg">127</a> but I haven&#8217;t checked whether the relevant evidence is CRVs-compatible.</li>
<li>According to <a href="http://www.sciencedirect.com/science/article/pii/S1364661305002998">Laureys (2005)</a>, patients in a persistent vegetative state (PVS), who are presumed to be unconscious, show greatly reduced activity in the associative cortices, and also show disrupted cortico-cortical and thalamo-cortical connectivity. Laureys also says that recovery from PVS is accompanied by restored connectivity of some of these thalamo-cortical pathways.<a class="see-footnote" id="footnoteref128_1o876te" title="Laureys (2005):  Voxel-based statistical analyses have sought to identify regions showing metabolic dysfunction in the vegetative state as compared with the conscious resting state in healthy controls. These studies have identified a metabolic dysfunction, not in one brain region but in a wide frontoparietal network encompassing the polymodal associative cortices…  …Awareness seems not to be exclusively related to activity in the frontoparietal network, but equally important is the relation of awareness to the functional connectivity within this network, and with the thalami. ‘Functional disconnections' in long-range cortico–cortical (between latero-frontal and midline-posterior areas) and cortico–thalamo–cortical (between non-specific thalamic nuclei and lateral and medial frontal cortices) pathways have been identified in the vegetative state [6,9]. Moreover, recovery is accompanied by a functional restoration of the frontoparietal network [7] and some of its cortico–thalamo–cortical connections [9]. In addition to measuring resting brain function and connectivity, recent neuroimaging studies have identified brain areas that still show activation during external stimulation in vegetative patients.  " href="#footnote128_1o876te">128</a></li>
<li>The mechanism by which general anesthetics abolish consciousness in humans isn&#8217;t well-understood, but at least one live hypothesis is that (at least some) general anesthetics abolish consciousness primarily by disrupting cortical functioning. If true, perhaps this account would lend some support to some CRVs.<a class="see-footnote" id="footnoteref129_oj0bt5e" title="See e.g. pp. 421-425 of Tononi et al. (2015a).  " href="#footnote129_oj0bt5e">129</a></li>
<li>Coma states, in which consciousness is typically assumed to be absent, seem to be especially associated with extensive cortical damage.<a class="see-footnote" id="footnoteref130_naw48pr" title="See e.g. pp. 425-426 of Tononi et al. (2015a).  " href="#footnote130_naw48pr">130</a></li>
</ul><p><span style="float: right;">[<a href="/node/858/edit/21">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="ProCRVSummary">Overall thoughts on arguments for CRVs</h5>
<p>I have not taken the time to assess the case for CRVs about consciousness. I can see how such a case could be made, if multiple lines of evidence about a variety of cognitive functions aligned with the suggestive evidence concerning the neural substrates of conscious vs. unconscious vision. On the other hand, my guess is that <em>if</em> I investigated these additional lines of evidence, I would find the following:</p>
<ol><li>I expect I would find that the evidence base on these other topics is less well-developed than the evidence base concerning conscious vs. unconscious vision, since vision neuroscience seems to be the most “developed” area within cognitive neuroscience.</li>
<li>I expect I would find that the evidence concerning which areas of the brain subserve specifically <em>conscious</em> processing of each type would be unclear, and subject to considerable expert debate.</li>
</ol><p>Overall, then, my sense is that the case <em>for</em> CRVs about consciousness is currently weak or at least equivocal, though I can <em>imagine</em> how the case could turn out to be strong in the future, after much more evidence is collected in several different subdomains of neurology and cognitive neuroscience.</p>
<p><span style="float: right;">[<a href="/node/858/edit/22">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="AntiCRV">Arguments against cortex-required views</h5>
<p>One influential paper, <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=1007572&amp;fileId=s0140525x07000891">Merker (2007)</a>, pointed to several pieces of evidence that seem, to some people, to argue against CRVs. One piece of evidence is the seemingly-conscious behavior of hydranencephalic children,<a class="see-footnote" id="footnoteref131_0dsb744" title="One might wonder why Merker does not discuss cases of a related condition hydrocephalus, for example the famous patient described by John Lorber as &quot;a young student… who has an IQ of 126 [and who] gained a first-class honors degree in mathematics&quot; and yet who &quot;has virtually no brain&quot; (Lewin 1980). See also Jackson &amp; Lorber (1984)'s report of one patient with a brain volume 56% of the normal volume, yet possessing &quot;a first class degree in mathematics and… [an] IQ of 130&quot; (this might be the same patient). (For another relatively dramatic case, see Feuillet et al. 2007.)  Merker describes hydrocephalus as a &quot;far more benign&quot; condition than hydranencephaly, because &quot;cortical tissue is is compressed by enlarging ventricles but is present in anatomically distorted form [Sutton et al. (1980)].&quot; Merker seems to be saying that hydranencephaly can result in far more dramatic loss of cortical tissue than hydrocephalus does, and from the literature I've skimmed, that seems to be correct.  " href="#footnote131_0dsb744">131</a> whose cerebral hemispheres are almost entirely missing and replaced by cerebrospinal fluid filling that part of the skull:</p>
<blockquote><p>These children are not only awake and often alert, but show responsiveness to their surroundings in the form of emotional or orienting reactions to environmental events…, most readily to sounds, but also to salient visual stimuli… They express pleasure by smiling and laughter, and aversion by “fussing,” arching of the back and crying (in many gradations), their faces being animated by these emotional states. A familiar adult can employ this responsiveness to build up play sequences predictably progressing from smiling, through giggling, to laughter and great excitement on the part of the child. The children respond differentially to the voice and initiatives of familiars, and show preferences for certain situations and stimuli over others, such as a specific familiar toy, tune, or video program, and apparently can even come to expect their regular presence in the course of recurrent daily routines.</p>
<p>…some of these children may even take behavioral initiatives within the severe limitations of their motor disabilities, in the form of instrumental behaviors such as making noise by kicking trinkets hanging in a special frame constructed for the purpose (“little room”), or activating favorite toys by switches, presumably based upon associative learning of the connection between actions and their effects… The children are, moreover, subject to the seizures of absence epilepsy. Parents recognize these lapses of accessibility in their children, commenting on them in terms such as “she is off talking with the angels,” and parents have no trouble recognizing when their child “is back.”</p></blockquote>
<p>In a later survey of 108 primary caregivers of hydranencephalic children (<a href="http://onlinelibrary.wiley.com/doi/10.1111/apa.12718/full">Aleman &amp; Merker 2014</a>), 94% of respondents said they thought their child could feel pain, and 88% said their child takes turns with the caregiver during play activities.</p>
<p>However, these findings are not a certain refutation of CRVs, for at least three reasons. First, hydranencephalic children cannot provide verbal reports of conscious experiences (if they have any). Second, it is typically the case that hydranencephaly allows for small portions of the cortex to develop, which might subserve conscious experience. Third, there is the matter of plasticity: perhaps consciousness <em>normally</em> requires certain regions of the cortex, but in cases of hydranencephaly, other regions are able to support conscious functions. Nevertheless, these observations of hydranencephalic children are suggestive to many people that CRVs cannot be right.</p>
<p>Another line of evidence against CRVs comes from isolated case studies in which conscious experience remains despite extensive cortical damage.<a class="see-footnote" id="footnoteref132_cja3z8z" title="Damasio et al. (2013); Feinstein et al. (2016); Starr et al. (2009); Philippi et al. (2012). If one considers some PVS patients to be &quot;reporting&quot; conscious experience as detected by neuroimaging alone (Owen 2013; Klein 2015), these cases might serve as additional examples of this phenomenon.  " href="#footnote132_cja3z8z">132</a> But once again these cases are not a definitive refutation of CRVs, because “extensive cortical damage” is not the same as “complete destruction of the cortex,” and also because of the issue of plasticity mentioned above.</p>
<p><a name="Mike" id="Mike"></a>Or, consider the case of <a href="https://en.wikipedia.org/wiki/Mike_the_Headless_Chicken">Mike the headless chicken</a>. On September 10, 1945, a Colorado farmer named Lloyd Olsen decapitated a chicken named “Mike.” The axe removed most of Mike&#8217;s head, but left intact the jugular vein, most of the brain stem, and one ear. Mike got back up and began to strut around as normal. He survived another 18 months, being fed with milk and water from an eyedropper, as well as small amounts of grit (to help with digestion) and small grains of corn, dropped straight into the exposed esophagus. Mike&#8217;s behavior was reported to be basically normal, albeit without sight. For example, according to various reports, he tried to crow (which made a gurgling sound), he could hear and respond to other chickens, and he tried to preen himself (which didn&#8217;t accomplish much without a beak). He was taken on tour, photographed for dozens of magazines and newspapers, and examined by researchers at the University of Utah.<a class="see-footnote" id="footnoteref133_uq4024a" title="For these and other details on Mike, see the following sources:   Teri Thomas' The Official Mike the Headless Chicken Book (2000). The book was previously available online here, and has an Amazon page (with no copies available when I checked) here. I was able to order the book, in September 2016, by calling the Fruita Community Center at 970-858-0360. The story &quot;Headless Rooster: Beheaded chicken lives normally after freak decapitation by ax&quot; on pp. 53-54 of the Oct 22, 1945 issue of Life magazine. Lambert &amp; Kinsley (2004), pp. 83-84. The history page on the website for the annual Mike the Headless Chicken Festival in Fruita, Colorado. Crew (2014). A 2015 BBC Magazine article, &quot;The chicken that lived for 18 months without a head.&quot; The 2001 PBS documentary The Natural History of the Chicken, directed by Mark Lewis. The section on Mike, including interviews with some of the people who witnessed Mike post-decapitation, appears from 34:14-41:30.   Note that while some popular sources report that Mike was killed specifically to serve as that night's dinner, Lloyd Olsen's great-grandson Troy Waters disputes this (unimportant) detail, claiming instead that Lloyd and his wife &quot;were actually slaughtering, oh, 40 or 50 of them that day&quot; (Thomas 2000, p. 1).  Here are a few additional details from Thomas (2000):  Q: Did Mike ever try to mate after he was beheaded?  A: No. Mike was only about four and a half months old when he was beheaded, and that's not old enough for roosters to mate. Usually they have to be about a year old for that. When Mike was beheaded, although his body continued to develop and gain weight, he didn't mature in the traditional sense…  Q: Did Mike suffer from pain or discomfort?  A: Officers from several humane societies examined Mike on several occasions, and declared him to be free from suffering. According to one, account, he had lost the part of his brain that would have caused him to feel pain. That seems to have been verified by the fact that his owners had to tape up his feet to keep him from instinctively scratching his neck where his head would have been. The sharp spurs on his feet would have damaged his exposed neck if they hadn't been taped up.  However… Troy Waters says that post-decapitation Mike was more docile than most chickens, and that most of the time he just lay in his straw-filled apple box. Lloyd and Hope had to prod him to get him to flap his wings and walk around for the tourists. He may have been depressed… or just in a fowl mood. [p. 21]  …  According to numerous accounts, Mike's domeless existence was studied extensively by scientists and students at the University of Utah back when he was alive…  However, apparently that research is lost…, and no one at the University of Utah was able to find anything about it, even after extensive searches done of their archives at the request of the Fruita Chamber of Commerce last year, and PBS this year (2000). By the time I called, the librarians had heard all about the story of Mike, and they explained to me that the University of Utah had changed the structure of its life sciences department since the 1940s… Any records of Mike were simply lost, if they ever existed in the first place. [p. 60]  I located only one other source on consciousness which mentions Mike the headless chicken: Leisman &amp; Koch (2009).  " href="#footnote133_uq4024a">133</a></p>
<p>For those who endorse a CRV, Mike could be seen as providing further evidence that a wide variety of behaviors can be produced without any conscious experience. For those who reject CRVs, Mike could be seen as evidence that the brain stem alone can be sufficient for consciousness.</p>
<p>Another problem remains. Even if it could be proved that particular cortical structures are required to produce conscious experiences in humans, this wouldn&#8217;t prove that other animals can&#8217;t be phenomenally conscious via other brain structures. For example, it might be the case that once the cortex evolved in mammals, some functions critical to consciousness “migrated” from subcortical structures to cortical ones.<a class="see-footnote" id="footnoteref134_kxfyy2r" title="Mallatt &amp; Feinberg (2016):  Our own solution to the problem raised by the convincing evidence for cortical consciousness is that the consciousness shifted from the tectum to the enlarging cerebral cortex when mammals evolved from their reptile-like ancestors.  " href="#footnote134_kxfyy2r">134</a> To answer this question, we&#8217;d need to have a highly developed theory of how consciousness works in general, and not just evidence about its necessary substrates in humans.</p>
<p>Several authors have summarized additional arguments against CRVs,<a class="see-footnote" id="footnoteref135_ugatktu" title="See especially Merker (2007), including the response commentaries, and also Devor et al. (2014) and Merker (2016).  Barron &amp; Klein (2016) are especially succinct in their case against CRVs:  There is now considerable evidence that, in humans, subjective experience can exist in the absence of self-reflexive consciousness, and that the two are supported by different neural structures. Midbrain structures, rather than cortex, seem to be especially important. [Merker (2005, 2007), Parvizi &amp; Damasio (2001), Damasio &amp; Carvalho (2013), and Mashour &amp; Alkire (2013)] have all argued that the integrated structures of the vertebrate midbrain are sufficient to support the capacity for subjective experience.  [Merker (2007)] notes that subjective experience is remarkably sensitive to damage to midbrain structures. Conversely, there is evidence of preserved consciousness even in patients who lack a cortex [Merker (2005)]. Further, although cortical damage can have profound effects on the contents of consciousness, damage to any portion of the cortex alone can spare the basic capacity for subjective experience [Damasio et al. (2013); Philippi et al. (2012); Herbet et al. (2014); Kapur et al. (1994); Friedman-Hill et al. (1995); Damasio &amp; Hosen (1983)]. Cortical damage alone can have profound effects on the contents of consciousness, but even massive cortical damage seems to spare subjective experience itself [Merker (2007); Damasio et al. (2013); Philippi et al. (2012)]. Indeed, there is evidence of residual conscious awareness in patients with severe cortical damage who are otherwise unresponsive to the world, suggesting that preserved subcortical structures may continue to support subjective experience [Owen et al. (2002); Klein &amp; Hohwy (2015)]. Although the mechanism of anesthetic action is still debated [Hudetz (2012)], there is increasing evidence that the effect of anesthetics depends on the disconnection of cortical circuits from subcortical structures rather than on their direct cortical activity [Alkire et al. (2008); Gili et al. (2013)]. Anesthetics [Gyulai et al. (1996)] or electrical stimulation [Herbet et al. (2014)], which affect cortical midline structures without affecting subcortical structures, do not abolish consciousness; they instead produce unresponsive but conscious dreamlike states. Conversely, emergence from anesthesia [Mashour &amp; Alkire (2013); Långsjö et al. (2012)] and coma/vegetative state [Schiff (2010)] are predicted by the reengagement of subcortical structures and reintegration of those structures with cortical circuits. Other authors have noted the powerful subcortical effect of drugs, endogenous peptides, and direct stimulation on primitive motivational states [Panksepp (2008); Mashour &amp; Alkire (2013); Denton (2006)].  In sum, there is good evidence that subcortical structures underlie the basic capacity for subjective experience in humans. This is not to say that the cortex is unimportant for conscious experience, of course. Rather, the proposal is that subcortical structures support the basic capacity for experience, the detailed contents of which might be elaborated by or otherwise depend upon cortical structures [Merker (2013)].  For a reply to Barron &amp; Klein on these points, see Allen-Hermanson (2016). For a counter-reply, and additional clarifications of Klein &amp; Barron's case against CRVs, see Klein &amp; Barron (2016).  Machado (2007), ch. 3, provides another fairly succinct case against CRVs, in the context of his discussion of the neurological grounds for pronouncing a medical patient &quot;dead.&quot; In the excerpt below, I have left out Machado's many citations, to avoid cluttering the text:  Any full account of death should include three distinct elements: the definition of death, the criterion (anatomical substratum) of brain death, and the tests to prove that the criterion has been satisfied. Undoubtedly, the term ‘criterion' for referring to the anatomical substratum introduces confusion in this discussion, because protocols of tests (clinical and instrumental) for brain diagnosis are called ‘diagnostic criteria' or ‘sets of diagnostic criteria'. Therefore, I will use the term ‘anatomical substratum' instead of criterion.  During the last decades, three main brain-oriented formulations of death have been discussed: whole brain, brainstem death and higher brain standards… The whole brain criterion refers to the irreversible cessation of all intracranial structure functions. It has been accepted by society mainly for practical reasons…  The brainstem standard was adopted in several Commonwealth countries. Pallis emphasized that the capacity for consciousness and respiration are two hallmarks of life of the human being, and that brainstem death predicts an inescapable asystole. However, a physiopathological review of consciousness generation will provide a basis for not accepting Pallis' definition of death…  The higher brain formulation springs largely from consideration of the persistent vegetative state (PVS), and has been mainly defended by philosophers. The higher brain theorists have defined human death as the ‘the loss of consciousness' (definition), related to the irreversible destruction of the neocortex (anatomical substratum).  I will demonstrate in this chapter that consciousness does not bear a simple one-to-one relationship with higher or lower brain structures and that, consequently, the higher brain view is wrong, because the definition (consciousness) does not harmonize with the anatomical substratum (neocortex)…  …Two physiological components control conscious behavior: arousal and awareness… Arousal represents a group of behavioral changes that occurs when a person awakens from sleep or transits to a state of alertness… Awareness, also known as content of consciousness, represents the sum of cognitive and affective mental functions, and denotes the knowledge of one's existence, and the recognition of the internal and external worlds…  In summary, a human being's state of consciousness reflects both his or her level of arousal that depends on subcortical arousal-energizing systems and, the sum of the cognitive, affective, and other higher brain functions (content of consciousness or awareness), related to &quot;complex physical and psychologic mechanisms by which limbic systems and the cerebrum enrich and individualize human consciousness.&quot; Therefore, I will use the term arousal when referring to those subcortical arousal-energizing systems, and awareness, to denote the sum of those complex brain functions, related to limbic and cerebrum levels…  …Awareness is thought to be dependent upon the functional integrity of the cerebral cortex and its subcortical connections; each of its many parts are located, to some extent, in anatomically defined regions of the brain…  …Shewmon has discussed some examples of clear participation of subcortical structures in awareness. Experimental animals with complete decortication have been shown to be capable of complex interactions with the environment, which is evidence of some awareness. In lesions of the somatosensory cortex an evident loss of tactile, vibration and joint position sense is observed; nonetheless, conscious experience of pain and temperature is preserved, mediated by subcortical structures, probably the thalamus. This author also commented that two hydranencephalic patients (&quot;prenatal destruction of the cerebral hemispheres with intact skull and scalp&quot;) unquestionably manifested conscious behavior. These two cases are examples of the brainstem &quot;plasticity&quot; in newborns. Clinical and experimental evidence convincingly suggests that the brainstem of newborns is potentially capable of much more complex integrative functioning. This includes some functions commonly considered to be cortical, even in animals. Based on these subjects, the potential presence of some primitive form of awareness in anencephalics, and the possibility of subjective feeling of pain, has been suggested. Thus, according to Shewmon &quot;the human brainstem and diencephalon, in the absence of cerebral cortex, can mediate consciousness and purposeful interaction with the environment.&quot;  …PVS [persistent vegetative state] provides an anatomic-functional model in which arousal is preserved and awareness is apparently lacking. Therefore, it has been suggested that both components of consciousness [i.e., arousal and awareness] &quot;are mediated by distinct anatomic, neurochemical and/or physiological systems.&quot; Nonetheless, the potential plasticity of the brain has demonstrated that subcortical structures could mediate awareness, even with the complete absence of the cerebral cortex. Cases that have undergone hemispherectomy have shown clear signs of neuroplasticity. Austin and Grant reported 3 cases that had undergone total hemispherectomy (comprising cortex, white matter and basal ganglia), who continued speaking and were aware of their environment during the operation, done under local anesthesia…  Thus, awareness is not only related to the function of the neocortex (although it is of primary importance), but also to complex physical and psychological mechanisms, due to the interrelation of the ARAS [ascending reticular activating system], limbic system, and the cerebrum…  …we cannot simply differentiate and locate arousal as a function of the ARAS, and awareness as a function of the cerebral cortex. Substantial interconnections among the brainstem, subcortical structures and the neocortex, are essential for subserving and integrating both components of human consciousness.  The above considerations lead one to conclude that there is no single anatomical place of the brain &quot;necessary and sufficient for consciousness.&quot;  …  Can we deny the existence of internal awareness in PVS, because these patients apparently seem to be disconnected from the external world? The subjective dimension of awareness is philosophically impossible to test, but physiologically it seems conceivable that subjective awareness might continue. Karen Ann Quinlan's brain showed severe damage of the thalamus, with the cerebral hemispheres relatively spared, and other authors have reported similar findings…  …Thus, in PVS cases it is impossible to deny a possible preservation of internal awareness. According to the neuropathological pattern, either subcortical structures could provide internal awareness, or some remaining activating pathways projecting to the cerebral cortex without relaying through the thalamus could stimulate the cerebral cortex. As consciousness is based on anatomy and physiology throughout the brain, it is impossible to classify a PVS case as dead. The brain is severely damaged, but not fully and irreversibly destroyed.  In ch. 7, Machado adds:  The perceptions of pain and suffering are conscious experiences; unconsciousness, by definition, precludes these experiences. The Multi-Society Task Force on PVS concluded that PVS patients are unconscious, and they &quot;cannot experience pain and suffering.&quot; However, it is important to argue that the pain response in newborns does not involve the cerebral cortex, which is one of the primary loci of damage in PVS. It logically follows that PVS patients have the same potential to experience pain and suffering as newborns. This argument also implies that brute animals cannot experience pain since they are not self-conscious. Howsepian considered this to be &quot;at best counterintuitive and at worst patently false.&quot;  " href="#footnote135_ugatktu">135</a> but I don&#8217;t find any of them to be even moderately conclusive. I do, however, think all this is sufficient to conclude that the case <em>for</em> CRVs is unconvincing. Hence, I don&#8217;t think there is even a “moderately strong” case for the cortex as a necessary condition for phenomenal consciousness (in humans and animals). But, I could imagine the case becoming stronger (or weaker) with further research.</p>
<p><span style="float: right;">[<a href="/node/858/edit/23">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="BigPicture">Big-picture considerations that pull toward or away from “consciousness is rare”</h3>
<p>Given that (1) I lack a satisfying theory of consciousness, (2) I don&#8217;t know which PCIFs are <em>actually</em> consciousness-indicating, and that (3) I haven&#8217;t found any convincing and substantive necessary or sufficient conditions for consciousness, my views about the distribution of consciousness at this point seem to be quite sensitive to how heavily I weight various “big picture considerations.” I explain four of these below.</p>
<p>Putting substantial weight on some of these considerations pulls me toward a “consciousness is rare” conclusion, whereas putting more weight on other considerations pulls me toward a “consciousness is extensive” conclusion.</p>
<p><span style="float: right;">[<a href="/node/858/edit/24">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="Inessentialism">Consciousness inessentialism</h4>
<p>How did artificial intelligence (AI) researchers build machines that outperform many or most humans (and in some cases <em>all</em> humans) at intelligent tasks such as playing <a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol">Go</a> or <em><a href="https://arxiv.org/abs/1609.05521">DOOM</a></em>, <a href="https://en.wikipedia.org/wiki/Autonomous_car">driving cars</a>, <a href="http://openreview.net/pdf?id=BkjLkSqxg">reading lips</a>, and <a href="http://www.nature.com/news/deep-learning-boosts-google-translate-tool-1.20696">translating texts between languages</a>? They did not do it by figuring out how consciousness works, even though consciousness might be required for how <em>we</em> do those things. In my experience, most AI researchers don&#8217;t think they&#8217;ll need to understand consciousness to successfully automate other impressive feats of human intelligence, either,<a class="see-footnote" id="footnoteref136_gmnjwnc" title="I haven't seen a poll on this question, this is just the sense I get from reading AI papers and talking to AI researchers, especially those in the important AI subfield of deep learning.  " href="#footnote136_gmnjwnc">136</a> and that fits with my intuitions as well (though I won&#8217;t argue the point here).</p>
<p>That said, AI scientists might produce consciousness <em>as a side effect</em> of trying to automate certain intelligent behaviors, without first understanding how consciousness works, just as AI researchers at Google DeepMind produced a game-playing AI that <a href="https://www.youtube.com/watch?v=nwx96e7qck0">learned to exploit</a> the “tunneling” strategy in <em>Breakout!</em>, even though the AI programmers didn&#8217;t know about that strategy themselves, and didn&#8217;t specifically write the AI to use it. Perhaps it is even the case that certain intelligent behaviors can <em>only</em> be achieved with the participation of conscious experience, even if the designers don&#8217;t need to understand consciousness themselves to produce a machine capable of exhibiting those intelligent behaviors.</p>
<p>My own intuitions lean the other way, though. I think it&#8217;s plausible that, “for any intelligent activity <em>i</em>, performed in any cognitive domain <em>d</em>, even if <em>we</em> do <em>i</em> with conscious accompaniments, <em>i</em> can in principle be done without these conscious accompaniments.” <a href="https://mitpress.mit.edu/books/consciousness-reconsidered">Flanagan (1992)</a> called this view “conscious inessentialism,”<a class="see-footnote" id="footnoteref137_kl263u6" title="Flanagan (1992), p. 5. Note that Flanagan rejects consciousness inessentialism (p. 6).  " href="#footnote137_kl263u6">137</a> but I think it is more properly called “conscious<em>ness</em> inessentialism.”<a class="see-footnote" id="footnoteref138_so64bbr" title="I'm not the only one to prefer this term. E.g. Rose &amp; Dietrich (2009) write that &quot;['conscious essentialism'] should probably be 'consciousness inessentialism' since it is a thesis about consciousness…&quot;  " href="#footnote138_so64bbr">138</a></p>
<p>Defined this way, consciousness inessentialism is not the same as <a href="http://plato.stanford.edu/entries/epiphenomenalism/">epiphenomenalism</a> about consciousness, nor does it require that one think <a href="http://plato.stanford.edu/entries/zombies/">philosophical zombies</a> are empirically possible. (Indeed, I reject both those views.) Instead, consciousness inessentialism merely requires that it be possible in principle for a system to generate the same input-output behavior as a human (or some other conscious system), without that system being conscious.</p>
<p>To illustrate this view, <a href="http://lesswrong.com/lw/pa/gazp_vs_glut/">imagine</a> replacing a human brain with a giant lookup table:</p>
<blockquote><p>A Giant Lookup Table… is when you implement a function as a giant table of inputs and outputs, usually to save on runtime computation. If my program needs to know the multiplicative product of two inputs between 1 and 100, I can write a multiplication algorithm that computes each time the function is called, or I can precompute a Giant Lookup Table with 10,000 entries and two indices. There are times when you do want to do this, though not for multiplication — times when you&#8217;re going to reuse the function a lot and it doesn&#8217;t have many possible inputs; or when clock cycles are cheap while you&#8217;re initializing, but very expensive while executing.</p>
<p>Giant Lookup Tables [GLUTs] get very large, very fast. A GLUT of all possible twenty-ply conversations with ten words per remark, using only 850-word Basic English, would require 7.6 * 10<sup>585</sup> entries.</p>
<p>Replacing a human brain with a Giant Lookup Table of all possible sense inputs and motor outputs (relative to some fine-grained digitization scheme) would require an <em>unreasonably large amount</em> of memory storage.  But “in principle”… it could be done.</p>
<p>The GLUT is not a [philosophical] zombie… because it is microphysically dissimilar to a human.</p></blockquote>
<p>A GLUT of a human brain is not physically possible because it is too large to fit inside the observable universe, let alone inside a human skull, but it illustrates the idea of consciousness inessentialism: <em>if</em> it was possible, for example via a <a href="http://www.sciencedirect.com/science/article/pii/S0096300305008404">hypercomputer</a>, a GLUT would exhibit all the same behavior as a human — including talking about consciousness, writing articles about consciousness, and so on — without (I claim) being conscious.</p>
<p>Can we imagine a <em>physically possible</em> system that would exhibit all human input-output behavior without consciousness? Unfortunately, answering that question seems to depend on knowing how consciousness works. But, I think the answer might very well turn out to be “yes,” largely because, as every programmer knows, there are almost always many, many ways to write any given computer program (defined in terms of its input-output behavior), and those different ways of writing the program typically use different internal sequences of information processing and different internal representations. In most contexts, what separates a “good” programmer from a mediocre one is not that the good programmer can find a way to write a program satisfying some needed input-output behavior while the mediocre programmer cannot; rather, what separates them is that the good programmer can write the needed program using particular sequences of information processing and internal representations that (1) are easy to understand and debug, (2) are modular and thus easy to modify and extend, (3) are especially computationally efficient, and so on.</p>
<p>Similarly, if consciousness is instantiated by some kinds of sequences of information processing and internal representations but not others, then it seems likely to me that there are many cognitive algorithms that could give rise to my input-output behavior <em>without</em> the <em>particular</em> sequences of information processing and internal representations that instantiate consciousness. (Again, remember that as with the GLUT example, this does not imply epiphenomenalism, nor the physical possibility of zombies.)</p>
<p>For example, suppose (for the sake of illustration) that consciousness is <em>only</em> instantiated in a human brain if, among other necessary conditions, some module <em>A</em> shares information <em>I</em> with some module <em>B</em> in <em>I</em>’s “natural” form. Afterward, module <em>B</em> performs additional computations on <em>I</em>, and passes along the result to module <em>C</em>, which eventually leads to verbal reports and stored memories of conscious experience. But now, suppose that my brain is rewired such that module <em>A</em> encrypts <em>I</em> before passing it to <em>B</em>, and <em>B</em> knows how to perform the requisite computations on <em>I</em> via <a href="https://en.wikipedia.org/wiki/Homomorphic_encryption#Fully_homomorphic_encryption">fully homomorphic encryption</a>, but <em>B</em> doesn&#8217;t know how to decrypt the encrypted version of <em>I</em>. Next, <em>B</em> passes the result to module <em>C</em> which, as a result of the aforementioned rewiring, <em>does</em> know how to decrypt the encrypted version of <em>I</em>, and pass it along further so that it eventually results in verbal reports and stored memories of conscious experience. In this situation (with the hypothesized “rewiring”), the same input-output behavior as before is always observed, even my verbal reports about conscious experience, but consciousness is never instantiated inside my brain, because module <em>B</em> never sees information <em>I</em> in its natural form.<a class="see-footnote" id="footnoteref139_z3aj0a5" title="This is not an argument against functionalism. In the example given here, the original version of my brain and the post-rewiring version of my brain are different functions (at the level that, by hypothesis, matters for consciousness), even though they result in the same input-output behavior at the level of my global behavior (modulo some extra energy expenditure by the version of my brain that has to undertake the additional computational work of encrypting and decrypting I, and performing computations on an encrypted form of I using fully homomorphic encryption).  For more on fully homomorphic encryption, see Armknecht et al. (2015).  In case you're curious: yes, fully homomorphic encryption is possible within a machine learning context. See e.g. Jiang et al. (2016).  " href="#footnote139_z3aj0a5">139</a></p>
<p>Of course, it seems unlikely in the extreme that the human brain implements fully homomorphic encryption — this is just an illustration of the general principle that there are many ways to compute a quite sophisticated <em>behavioral</em> function, and it&#8217;s plausible that not <em>all</em> of those methods <em>also</em> compute a function that is sufficient for <em>consciousness</em>.</p>
<p>Unfortunately, it&#8217;s still unclear whether there is any system <em>of a reasonable scale</em> that would replicate my behavior without any conscious experience. That seems to depend in part on whether the computations necessary for human consciousness are highly specific (akin to those of a specific, small-scale <a href="https://en.wikipedia.org/wiki/Device_driver">device driver</a> but not <em>other</em> device drivers), or whether consciousness is a result of relatively broad, general kinds of information processing (<em>a la</em> <a href="https://en.wikipedia.org/wiki/Global_Workspace_Theory">global workspace theory</a>). In the former case, one might imagine relatively small (but highly unlikely to have evolved) tweaks to my brain that would result in identical behavior without conscious experience. In the latter case, this is more difficult to imagine, at least without vastly more computational resources, i.e. the vast computational resources required to execute a large fraction of my brain&#8217;s total information processing under fully homomorphic encryption.</p>
<p>Moreover, even if I&#8217;m right about consciousness inessentialism, I also think it&#8217;s quite plausible that, as a matter of contingent fact, many animals <em>do</em> have conscious experiences (of some sort) accompanying some or all of their sophisticated behaviors. In fact, at least for the animals with relatively similar brains to ours (primates, and probably all mammals), it seems more reasonable than not to assume they <em>do</em> have conscious experiences (at least, before consulting additional evidence), simply because <em>we</em> have conscious experiences, and we share a not-so-distant common ancestor with those animals,<a class="see-footnote" id="footnoteref140_jtpq751" title="As Rachels (1990), p. 131, put it:  Descartes's view [about the strong difference between human and animal minds] was extreme, even for his own time, and despite its wide influence most thinkers did not share it. Nevertheless, it was a possible view then, in a way that it is not possible now. The reason Descartes's view of animals is not possible today - the reason his view seems so obviously wrong to us-is that between him and us came Darwin. Once we see the other animals as our kin, we have little choice but to see their condition as analogous to our own. Darwin stressed that, in an important sense, their nervous systems, their behaviours, their cries, aTe our nervous systems, our behaviours, and our cries, with only a little modification. They are our common property because we inherited them from the same ancestors. Not knowing this, Descartes was free to postulate far greater differences between humans and non-humans than is possible for us.  I think this statement of the point is much too strong, though. Yes, we must take seriously the fact that our brains are on a continuum with those of other animals, but this fact alone cannot tell us which specific cognitive functions we share with specific other species, and which ones we do not. One could make the same argument as Rachels does for mirror self-recognition, and this argument would fail to correctly predict that chimpanzees exhibit mirror self-recognition and gorillas do not (Anderson &amp; Gallup Jr. 2015). Darwin alone cannot answer the specific questions of comparative psychology — that's what the fields of comparative psychology and ethology are for.  " href="#footnote140_jtpq751">140</a> and their brains seem similar to ours in many ways (see <a href="#AppendixE">Appendix E</a>).</p>
<p>Still, if you find consciousness inessentialism as plausible as I do, then you can&#8217;t take for granted that if an animal exhibits certain sophisticated behaviors, it must be conscious.<a class="see-footnote" id="footnoteref141_si35ttz" title="Compare to Dennett (2017)'s extended discussion of &quot;competence without comprehension.&quot; One could make a similar (but not identical) case for &quot;competence without consciousness.&quot;  " href="#footnote141_si35ttz">141</a> On the other hand, if you find consciousness inessentialism highly <em>implausible</em>, then perhaps at least some sophisticated behaviors should be taken (by you) to be very strong evidence of consciousness. In this way, putting more weight on consciousness inessentialism should shift one&#8217;s view in the direction of “consciousness might be rare,” whereas putting little weight on consciousness inessentialism should shift one&#8217;s view in the direction of “consciousness might be widespread.”</p>
<p><span style="float: right;">[<a href="/node/858/edit/25">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="ComplexityConsciousness">The complexity of consciousness</h4>
<p>How complex is consciousness? That is, how many “components” are needed, and how precisely must they be organized, for a conscious experience to be instantiated? The simpler consciousness is, the more extensive it should be (all else equal), for the same reason that both “unicellular life” and “muliticellular life” are rarer than “life”, and for the same reason that instances of both “Microsoft Windows” and “Mac OS” are rarer than instances of “personal computer operating systems.”</p>
<p>To illustrate this point, I&#8217;ll survey some families of theories of consicousness which differ in how complex they take consciousness to be.</p>
<p><span style="float: right;">[<a href="/node/858/edit/26">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="Panpsychism">The complexity of panpsychism</h5>
<p>Panpsychism posits that consciousness is a fundamental feature of reality, e.g. a fundamental property in physics.<a class="see-footnote" id="footnoteref142_let6tpy" title="See e.g. Seager &amp; Allen-Hermanson (2010); Chalmers (2015); ch. 4 of Weisberg (2014).  " href="#footnote142_let6tpy">142</a> This, of course, is as simple as consciousness could possibly be. Other approaches presume consciousness to be substantially more complicated than panpsychism does, and therefore less ubiquitous.</p>
<p><span style="float: right;">[<a href="/node/858/edit/27">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="FirstOrder">The complexity of first-order representationalism</h5>
<p>First-order representationalism (FOR) posits a more complex account of consciousness, one relying on the notion of <em>representation</em>.<a class="see-footnote" id="footnoteref143_urgkkgu" title="For more on current debates about representation in the philosophy of mind, see e.g. Rey (2015).  " href="#footnote143_urgkkgu">143</a></p>
<p>What is a representation? It&#8217;s a thing that <em>carries information</em> about something else. An image of a flower carries information about a flower. The sentence “The flower smells good” carries information about a flower — specifically, the information that it smells good. Perhaps a nociceptive signal represents, to some brain module, that there is tissue damage of a certain sort occurring at some location on the body. There can also be representations that carry information about things that don&#8217;t exist, such as Luke Skywalker. If a representation mischaracterizes the thing it is about in some important way, we say that it is <em>misrepresenting</em> its target.</p>
<p>Representational theories of consciousness, then, say that if a system does the right kind of representing, then that system is conscious. To illustrate how FOR theories work, I&#8217;ll summarize one particular FOR theory: Michael Tye&#8217;s PANIC theory.<a class="see-footnote" id="footnoteref144_jgpt26b" title="See Tye (2000), chs. 3-8, especially section 3.4. For some updates to Tye's theory, see Tye (2009a). For an overview of FOR theories in general, see Weisberg (2014), ch. 7. For a brief introduction to representational theories of mind, see Tye (2009b).  " href="#footnote144_jgpt26b">144</a></p>
<p>Tye claims that a mental state is phenomenally conscious if it has some Poised, Abstract, Nonconceptual, Intentional Content (PANIC). Let&#8217;s unpack that.</p>
<p>To simplify just a bit, “intentional content” is just a phrase that (in philosophy) means “representational content” or “representational information.” What about the other three terms?</p>
<ul><li><em>Poised</em>: Conscious representational contents, unlike unconscious representational contents, must be suitably “poised” to play a certain kind of functional role. Specifically, they are poised to impact beliefs and desires. E.g. conscious perception of an apple can change your belief about whether there are apples in your house, and the conscious feeling of hunger can create a desire to eat.</li>
<li><em>Abstract</em>: Conscious representational contents are representations of “general features or properties” rather than “concrete objects or surfaces,” e.g. because in hallucinatory experiences, “no concrete objects need be present at all,” and because under some circumstances two different objects can “look exactly alike phenomenally.”<a class="see-footnote" id="footnoteref145_x12g9ta" title="Tye (2000), p. 62.  " href="#footnote145_x12g9ta">145</a></li>
<li><em>Nonconceptual</em>: The representational contents of consciousness are more detailed than anything we have words or concepts for. E.g. you can consciously perceive millions of distinct colors, but you don&#8217;t have separate concepts for red<sub>17</sub> and red<sub>18</sub>, even though you can tell them apart when they are placed next to each other.</li>
</ul><p>So according to Tye, conscious experiences have poised, abstract, nonconceptual, representational contents. If a representation is missing one of these properties, then it isn&#8217;t conscious. For example, consider how Tye explains the consistency of his PANIC theory of consciousness with the phenomenon of blindsight:<a class="see-footnote" id="footnoteref146_rdz5ijz" title="Tye (2000), pp. 62-63.  " href="#footnote146_rdz5ijz">146</a></p>
<blockquote><p>…given a suitable elucidation of the “poised” condition, blindsight poses no threat to [my theory]. Blindsight subjects are people who have large blind areas or scotoma in their visual fields due to brain damage… They deny that they can see anything at all in their blind areas, and yet, when forced to guess, they produce correct responses with respect to a range of simple stimuli (for example, whether an X or an O is present, whether the stimulus is moving, where the stimulus is in the blind field).</p>
<p>If their reports are to be taken at face value, blindsight subjects… have no phenomenal consciousness in the blind region. What is missing, on the PANIC theory, is the presence of appropriately poised, nonconceptual, representational states. There are nonconceptual states, no doubt representationally impoverished, that make a cognitive difference in blindsight subjects. For some information from the blind field does reach the cognitive centers and controls their guessing behavior. But there is no complete, unified representation of the visual field, the content of which is poised to make a direct difference in beliefs. Blindsight subjects do not believe their guesses. The cognitive processes at play in these subjects are not belief-forming at all.</p></blockquote>
<p>(To me, saying that the blindsight subject&#8217;s guesses don&#8217;t count as “beliefs” seems to use a somewhat gerrymandered definition of “belief,” but let&#8217;s set that aside for now.)</p>
<p>On Tye&#8217;s theory of consciousness (and other FOR theories), consciousness is a <em>much</em> more specific, complicated, and rare sort of thing than it is on a panpsychist view. If conscious states are states with PANIC, then we are unlikely to find them in carbon dioxide molecules, stars, and rocks, as the panpsychist claims. Nevertheless the PANIC theory seems to imply that consciousness might be relatively extensive <em>within the animal kingdom</em>. When Tye applies his own theory to the distribution question, he concludes that even some insects are clearly conscious.<a class="see-footnote" id="footnoteref147_31nnhuz" title="Tye (2000), ch. 8:  States with PANIC are nonconceptual states that track certain features, internal or external, under optimal conditions (and thereby represent those features). They are also states that stand ready and available to make a direct difference to beliefs and desires. It follows that creatures that are incapable of reasoning, of changing their behavior in light of assessments they make, based upon information provided to them by sensory stimulation of one sort or another, are not phenomenally conscious…  Consider, to begin with, the case of plants. There are many different sorts of plant behavior… [but] the behavior of plants is inflexible. It is genetically determined and, therefore, not modifiable by learning… [Plants] neither acquire beliefs and change them in light of things that happen to them nor do they have any desires… [Plants exhibit] no goal-directed behavior, no purpose, nothing that is the result of any learning, no desire for [e.g.] water. Plants, then, are not subject to any PANIC states… [and thus are] not phenomenally conscious.  …What about caterpillars? …Different kinds of caterpillars show different sorts of behavior upon hatching… Some, for example, eat the shells of the eggs from which they emerge; others crawl away from their cells immediately. But there is no clear reason to suppose that caterpillars are anything more than stimulus-response devices. They have a very limited range of behaviors available to them, each of which is automatically triggered at the appropriate time by the appropriate stimulus. Consider, for example, their sensitivity to light. Caterpillars have two eyes, one on each side of the head. Given equal light on both eyes, they move straight ahead. But given more light on one of the eyes, that side of the body toward the direction of most intense light, which is why caterpillars climb trees all the way to the top; the light there is strongest. Shift the light to the bottom of the tree, and the caterpillar will go down, not up, as it usually does, even if it means starving to death. Remove one of its eyes, and it will travel in a circle without ever changing its route.  Once one is made aware of these facts, there seems no more reason intuitively to attribute phenomenal consciousness to a caterpillar on the basis of how it moves than to an automatic door. The latter responds in a fixed, mechanical way to the presence of pressure on a plate in the floor or ground in front of it, just as the former responds mechanically to the presence of light. No learning, no variation in behavior with changed circumstances, no reasoned assessment occurs… Caterpillars, then, do not support states with PANIC any more than plants do.  …I come finally to the case of honey bees. There are many examples of sophisticated honey bee behavior. Bee colonies take on odors, primarily as a result of the food contained in the hives. These odors, which vary from hive to hive, are absorbed by the fur on the bees, and guards, placed at the entrance to the hive, learn to use it to check whether incoming bees are intruders or members of the colony. Scouts fly out from the hive each spring in search of a cavity suitable for a new hive. They use the sun as their main guide but they also rely upon landmarks. Upon returning, they dance to show bees in the hive what they have discovered. Their dance requires them to remember how the sun moves relative to the positions of the landmarks, enabling them to communicate the position of the cavity correctly. Recruit bees must learn what the dancers are telling them. This demands that they form some sort of cognitive map involving the landmarks. Scouts back from their trips attend to the dances of other scouts and then go out again to visit the different cavities. With their later return, they dance again. Eventually, the dances agree and the colony moves as one to the chosen spot…  Of course, some of this is preprogrammed. Bees choose neither to dance nor how to navigate; these activities are instinctive. But, equally clearly, in the above examples, the bees learn and use facts about their environments as they go along…  …[Another] example is provided by an experiment in which bees were shown some sugar solution on a plate near the hive. Then every five minutes or so, the plate was moved away so that the distance from the hive increased by one quarter. Initially, with the plate only four inches away, it was moved just one inch. But later when the food was four hundred feet away, the plate was removed another 100 feet. Amazingly, the bees caught on to this procedure and began to anticipate where the sugar would be next by flying there and waiting for the plate to arrive!  There seems to be ample evidence, then, that honey bees make decisions about how to behave in response to how things look, taste, and smell. They use the information their senses give them to identify things, to find their way around, to survive. They learn what to do in many cases as the situation demands. Their behavior is sometimes flexible and goal-driven. They are, therefore, the subjects of states with PANIC… [and thus] are phenomenally conscious…  " href="#footnote147_31nnhuz">147</a> Personally, I think a PANIC theory of consciousness also seems to imply that some webcams and many common software programs are also conscious, though I suspect Tye disagrees that his theory has that implication.</p>
<p><span style="float: right;">[<a href="/node/858/edit/28">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="HigherOrder">The complexity of higher-order approaches</h5>
<p>So-called “higher-order” approaches to consciousness typically posit an even more complex account of consciounsess, relative to FOR theories like PANIC. <a href="http://plato.stanford.edu/entries/consciousness-higher/">Carruthers (2016)</a> explains:<a class="see-footnote" id="footnoteref148_r3j301m" title="Carruthers was, for decades, a leading defender of higher-order approaches to consciousness, but has recently (Carruthers 2017) recanted, and now defends a first-order view.  " href="#footnote148_r3j301m">148</a></p>
<blockquote><p>According to first-order views, phenomenal consciousness consists in analog or fine-grained contents that are available to the first-order processes that guide thought and action. So a phenomenally-conscious percept of red, for example, consists in a state with the analog content red which is tokened in such a way as to feed into thoughts about red, or into actions that are in one way or another guided by redness…</p>
<p>The main motivation behind higher-order theories of consciousness… derives from the belief that all (or at least most) mental-state types admit of both conscious and unconscious varieties. Almost everyone now accepts, for example, …that beliefs and desires can be activated unconsciously. (Think, here, of the way in which problems can apparently become resolved during sleep, or while one&#8217;s attention is directed to other tasks. Notice, too, that appeals to unconscious intentional states are now routine in cognitive science.) And then if we ask what makes the difference between a conscious and an unconscious mental state, one natural answer is that conscious states are states that we are aware of… That is to say, these are states that are the objects of some sort of higher-order representation…</p></blockquote>
<p>As a further example, consider the case of unconscious vision, discussed <a href="#UnconsciousVision">here</a>. Visual processing in the dorsal stream seems to satisfy something very close to Tye&#8217;s PANIC criteria,<a class="see-footnote" id="footnoteref149_1unnkft" title="Technically, if Tye's &quot;poised&quot; criterion requires that conscious contents be poised to affect &quot;belief&quot; or &quot;desire&quot; (rather than behavior more generally), then dorsal stream processing might not satisfy Tye's PANIC theory, but a very similar first-order theory would.  " href="#footnote149_1unnkft">149</a> and yet these processes are unconscious (as far as anyone can tell). Hence the suggestion that more is required — specifically, that some “higher-order” processing is required. For example, perhaps to become <em>conscious</em> of visual processing, some circuits of the brain need to represent parts of that processing <em>as being attended-to by the self</em>, or something like that.</p>
<p>It&#8217;s easy to see, then, why higher-order theories will tend to be more complex than FOR theories. Basically, higher-order theories tend to assume FOR-style information processing, but they say that some <em>additional</em> processing is required in order for consciousness to occur. If higher-order theories are right, then (all else equal) we should expect consciousness to be rarer than if lower-order theories are right.</p>
<p><span style="float: right;">[<a href="/node/858/edit/29">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="IllusionistTheories">The complexity of illusionist theories</h5>
<p>What about <a href="#illusionism">illusionist</a> theories? As of today, most illusionist theories seem to be at least as complex as higher-order theories tend to be. For example:</p>
<ul><li>Dennett&#8217;s illusionist theory of consciousness makes use not just of (e.g.) global broadcasting, but also of culturally learned memes (<a href="http://books.wwnorton.com/books/detail.aspx?ID=4294992671">Dennett 2017</a>). Dennett seems to suggest that as a consequence, consciousness might be limited to humans, or perhaps apes.</li>
<li>Graziano&#8217;s illusionist theory (<a href="https://global.oup.com/academic/product/consciousness-and-the-social-brain-9780199928644?cc=us&amp;lang=en&amp;">Graziano 2013</a>) assumes not just integrated information, higher-order representations, and a “global workspace,” but also an internal model of one&#8217;s attentional processes, an internal model of the self, and a set of ways in which these models must interact in order to instantiate a conscious experience.</li>
<li>I list other illusionist theories in a footnote.<a class="see-footnote" id="footnoteref150_jzt34jw" title="Frankish (2016b) lists some (strong) illusionist theories in footnote 2, reproduced below with links to the cited papers:  Defenders of illusionist positions (under various names) include Dennett (1988; 1991; 2005), Hall (2007), Humphrey (2011), Pereboom (2011), Rey (1992; 1995; 2007), and Tartaglia (2013). As Tartaglia notes, Place and Smart also denied the existence of phenomenal properties, which Place described as ‘mythological’ (Place, 1956, p. 49; Smart, 1959, p. 151).  Arguably, one might also cite Chen et al. (2016), Sloman &amp; Chrisley (2016), Kammerer (2016), and perhaps some ancient Buddhist philosophers (Garfield 2016).  " href="#footnote150_jzt34jw">150</a></li>
</ul><p>However, it&#8217;s not clear to me that illusionist theories of consciousness <em>necessarily</em> imply that consciousness is complex; this could be an historical accident.<a class="see-footnote" id="footnoteref151_csl6j1i" title="See also notes from my conversation with Keith Frankish.  " href="#footnote151_csl6j1i">151</a> Perhaps in the future illusionists will put forward compelling illusionist theories which do not imply a particularly complex account of consciousness.<a class="see-footnote" id="footnoteref152_ql90di2" title="For example, Carruthers (2017) is a somewhat illusionist account, and might (in the end) be relatively simple:  …what Carruthers &amp; Veillet (2011) proposed is that phenomenal consciousness can be operationalized as whatever gives rise to the &quot;hard problems&quot; of consciousness… That is, a given type of content can qualify as phenomenally conscious if and only if it seems ineffable, one can seemingly imagine zombie characters who lack it, one can imagine what-Mary-didn't-know scenarios for it, and so on. For the very notion of phenomenal consciousness seems constitutively tied to these issues. If there is a kind of state or a kind of content for which none of these problems arise, then what would be the point of describing it as phenomenally conscious nonetheless? And conversely, if there is a novel type of content not previously considered in this context for which hard-problem thought-experiments can readily be generated, then that would surely be sufficient to qualify it as phenomenally conscious.  Once phenomenal consciousness is operationalized as whatever gives rise to hard-problem thought-experiments, however, it should be obvious that the initial challenge to first-order representationalism collapses. The reason why nonconceptual contents made available to central thought processes are phenomenally conscious, whereas those that are not so available are not, is simply that without thought one cannot have a thought-experiment. Only those nonconceptual contents available [via global broadcasting] to central thought are ones that will seem to slip through one's fingers when one attempts to describe them (that is, be ineffable), only they can give rise to inversion and zombie thought-experiments, and so on. This is because those thought-experiments depend on a distinctively first-personal way of thinking of the experiences in question. This is possible if the experiences thought about are themselves available to the systems that generate and entertain such thoughts, but not otherwise. Experiences that are used for online guidance of action, for example, cannot give rise to zombie thought-experiments for the simple reason that they are not available for us to think about in a first-person way, as this experience or something of the sort. They can only be thought about third-personally, as the experience that guides my hand when I grasp the cup, or whatever.  There is simply no need, then, to propose that dual higher-order / first-order nonconceptual contents are necessary in order for globally broadcast experiences to acquire a subjective dimension and be like something to undergo. Once possession of such a dimension / possession of phenomenal consciousness is operationalized as whatever gives rise to hard-problem thought-experiments, then the mere fact of global broadcasting provides the required explanation. For it is nonconceptual content made available to central thought processes, and which is thus available to be thought about in a distinctively first-personal way, that grounds those thought-experiments.  If we suppose that this explanation on behalf of the first-order theorist is correct, however, then what should be said about phenomenally conscious experience in nonhuman animals? Presumably no animals have the conceptual resources to engage in hard-problem-type thought-experiments. (Indeed, the same may be true of many humans.) Does that mean that their experiences aren’t phenomenally conscious ones? Surely not. For giving rise to hard-problem thought-experiments is not supposed to be constitutive of phenomenal consciousness. Rather, it provides a theory-neutral way of delimiting the class of phenomenally conscious states in ourselves: roughly, phenomenally conscious states are the ones that are especially philosophically challenging or puzzling. Instead (according to first-order representationalism), what constitutes phenomenal consciousness is being a globally broadcast nonconceptual state. And there is plenty of reason to think that many species of animal (perhaps all vertebrates) have states of that general kind…  Seen from this perspective, indeed, there isn’t any deep issue about the phenomenally consciousness status of animal experience. Once we have established that an animal has a similar cognitive architecture to ourselves, with globally broadcast nonconceptual states that are made available to a range of different belief-forming, affect-generating, and executive decision-making systems, then there is simply no further question whether its experiences are really like something for the animal, or whether its experiences genuinely possess a subjective — felt — dimension. For there is no further property that needs to be added in order to render an experience phenomenally conscious. All that needs to be shown is that the animal possesses states of the same kind that we identify as phenomenally conscious (that is, which give rise to hard-problem thought-experiments) in ourselves.  Indeed, from this perspective it also emerges that there isn’t really a deep divide between creatures capable of phenomenal consciousness and ones that aren’t. For instance, we know that bees have structured belief-like states that guide them in the service of multiple goals, informed by perceptual input from a number of different sense-modalities… So they seem to possess simple minds… But suppose it turns out that bees nevertheless lack globally broadcast perceptual states. This might be because different types of perceptual content are made available only to specific decision-making systems, for example. Perhaps no perceptual states are broadcast to most such systems simultaneously. In which case they lack phenomenal consciousness, according to an account that identifies the latter with globally broadcast nonconceptual content. But so what? This doesn’t mean that bees are all &quot;dark on the inside&quot; or anything of the sort. Nor does it mean that there is any point in phylogeny when some special type of experience (one that is intrinsically like something to undergo) appears on the scene. Indeed, the question of when, precisely, phenomenal consciousness emerged in phylogeny makes no sense, from this perspective.  All that can be said is that there are a variety of kinds of nonconceptual perceptual state across creatures, some of which are available to inform more systems and some of which are available to inform fewer. These states thus differ in their functional roles, and some of these roles are more similar than others to the states in ourselves that give rise to hard-problem thought-experiments, that is all. Nothing special, or magical, or especially significant happened in evolution when global-broadcasting architectures first emerged on the scene. It was just more of the same, but somewhat differently organized.  " href="#footnote152_ql90di2">152</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/30">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="ComplexityOverview">So, how complex will consciousness turn out to be?</h5>
<p>Much of the academic debate over the complexity of consciousness and the distribution question has taken place in the context of the debate between first-order and higher-order approaches to consciousness, and experts seem to agree that higher-order theories imply a less-extensive distribution of consciousness than first-order theories do. If I assume this framing for the debate, I generally find myself more sympathetic with higher-order theories (for the usual reason summarized by Carruthers <a href="#HigherOrder">above</a>), though I think there are <em>some</em> reasons to take a first-order (or at least “lower-order”<a class="see-footnote" id="footnoteref153_c9d3srf" title="By &quot;lower-order,&quot; I have in mind accounts of consciousness that are less complex than typical higher-order accounts, but which are more complex than typical first-order accounts.  " href="#footnote153_c9d3srf">153</a>) approach as a serious possibility (see <a href="#AppendixH">Appendix H</a>).</p>
<p>However, I think the first-order / higher-order dichotomy is a very limiting way to argue about theories of consciousness, the complexity of consciousness, and the distribution question. I would much rather see these debates transition to being debates about proposed (and at least partially <em>coded</em>) cognitive architectures — architectures which don&#8217;t neatly fit into the first-order / higher-order dichotomy. (I say more about this <a href="#MoreSatisfying">here</a>.)</p>
<p><span style="float: right;">[<a href="/node/858/edit/31">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="EarlyProgress">Early scientific progress tends to lead to more complicated models of phenomena</h5>
<p>I conclude this section with one more thought on the complexity of consciousness and the distribution question.</p>
<p>My impression is that early scientific progress on a then-mysterious phenomenon tends to make our models of that phenomenon increasingly complicated (except in fundamental physics, and I strongly doubt that consciousness is a feature of fundamental physics<a class="see-footnote" id="footnoteref154_5xzemj7" title="One interpretation of &quot;consciousness is a feature of fundamental physics&quot; (i.e. panpsychism) seems clearly false, as there are no &quot;spare degrees of freedom&quot; in which an additional &quot;consciousness&quot; property could be &quot;hiding&quot; in the Standard Model (Wilczek 2008, p. 164, calls it the &quot;Core Theory&quot;), and the weight of evidence supporting the Standard Model is enormous. Physicist Sean Carroll argues this point in chapter 42 of Carroll (2016):  Unlike brains, which are complicated and hard to explain, elementary particles such as photons are extraordinarily simple, and therefore relatively easy to study and understand. Physicists talk about different kinds of particles having different &quot;degrees of freedom&quot; — essentially, the number of different kinds of such particles that there are. An electron, for example, has two degrees of freedom. It has both electric charge and spin, but the electric charge can take on only one value (–1), while the spin comes in two possibilities: clockwise or counterclockwise. One times two is two, for two total degrees of freedom. An up quark, by contrast, has six degrees of freedom; like an electron, it has a fixed charge and two possible ways of spinning, but it also has three possible &quot;colors,&quot; and one times two times three is six. Photons have an electric charge fixed at zero, but they do have two possible spin states, so they have two degrees of freedom just like electrons do.  We could interpret the [proposal that consciousness is a fundamental property of the universe] in the most direct way possible, as introducing new degrees of freedom for each elementary particle. In addition to spinning clockwise or counterclockwise, a photon could be in one of (let's say) two [states of consciousness]. Call them &quot;happy&quot; and &quot;sad,&quot; although the labels are more poetic than authentic.  This overly literal version of panpsychism cannot possibly be true. One of the most basic things we know about the Core Theory is exactly how many degrees of freedom each particle has. Recall the Feynman diagrams from [a previous chapter], describing particles scattering off of one another by exchanging other particles. Each diagram corresponds to a number that we can compute, the total contribution of that particular process to the end result, such as two electrons scattering off of each other by exchanging photons. Those numbers have been experimentally tested to exquisite precision, and the Core Theory has passed with flying colors.  A crucial ingredient in calculating these processes is the number of degrees of freedom associated with each particle. If photons had some hidden degrees of freedom that we didn't know about, they would alter all of the predictions we make for any scattering experiment that involves such photons, and all of our predictions would be contradicted by the data. That doesn't happen. So we can state unambiguously that photons do not come in &quot;happy&quot; and &quot;sad&quot; varieties, or any other manner of mental properties that act like physical degrees of freedom.  Advocates of panpsychism would probably not go as far as to imagine that mental properties play roles similar to true physical degrees of freedom, so that the preceding argument wouldn't dissuade them. Otherwise these new properties would just be ordinary physical properties.  That leaves us in a position very similar to the zombie discussion: we posit new mental properties, and then insist that they have no observable physical effects [e.g. in particle scattering experiments]. What would the world be like if we replaced &quot;protoconscious photons&quot; with &quot;zombie photons&quot; lacking such mental properties? As far as the behavior of physical matter is concerned, including what you say when you talk or write or communicate nonverbally with your romantic partner, the zombie-photon world would be exactly the same as the world where photons have mental properties.  A good Bayesian can therefore conclude that the zombie-photon world is the one we actually live in. We simply don't gain anything by attributing the features of consciousness to individual particles. Doing so is not a useful way of talking about the world; it buys us no new insight or predictive power. All it does is add a layer of metaphysical complication onto a description that is already perfectly successful.  Consciousness seems to be an intrinsically collective phenomenon, a way of talking about the behavior of complex systems with the capacity for representing themselves and the world within their inner states. Just because it is here full-blown in our contemporary universe doesn't mean that there was always some trace of it from the very start. Some things just come into being as the universe evolves and entropy and complexity grow: galaxies, planets, organisms, consciousness.  Perhaps because of this, panpsychists typically locate consciousness within physics in other ways, for example by positing that in addition to the usual relational-causal properties of the Standard Model (that best explain the results of our experiments), there is also an &quot;intrinsic character&quot; to physical things, and this intrinsic character is the ground of conscious experience (in either a &quot;basic&quot; or &quot;emergent&quot; way). I confess that to me, this sort of move by various panpsychists/non-reductivists about consciousness seems just as unmotivated as substance dualism does. (For an overview of some views of this kind, see ch. 4 of Weisberg 2014.)  " href="#footnote154_5xzemj7">154</a>). If this pattern holds true for the scientific study of consciousness, too, then consciousness could turn out to be a relatively complicated phenomenon, and this would push me in the direction of thinking that consciousness can be found in fewer systems than if consciousness turned out to be simple.</p>
<p>Consider the case of recognizing oneself in the mirror. From the inside, it <em>feels</em> as though this is a fairly simple process: I see myself in the mirror, and I immediately recognize that it&#8217;s me. When I introspect about this process, I don&#8217;t detect any “steps” to the process. I don&#8217;t detect a series of complicated manipulations of data. Moreover, anyone trying to construct a theory of how mirror self-recognition works, before the age of computers and algorithms, wouldn&#8217;t have known enough about how mirror self-recognition <em>might</em> work to propose <em>any</em> complicated theory for it. But of course we now know — both from the neuroscientific study of vision and from attempts to build machine vision systems that can recognize objects (including themselves)<a class="see-footnote" id="footnoteref155_fssic5d" title="On the neuroscience of human vision, see Snowden et al. (2012), Schiller &amp; Tehovnik (2015), and Zhao (2016). Progress on machine vision is so rapid that anything I cite will be out-of-date within weeks or months, but see e.g. the readings for the University of Toronto's &quot;Deep Learning in Computer Vision&quot; Winter 2016 course. For a brief overview of mirror self-recognition in animals, see Anderson &amp; Gallup Jr. (2015). For an example claim of mirror self-recognition in a robot, see Takeno (2012).  " href="#footnote155_fssic5d">155</a> — that this process of mirror self-detection is actually very complicated, and very few computational systems can execute it. Indeed, while chimpanzees seem to recognize themselves in mirrors, even gorillas do not. Mirror self-recognition is a highly specific and complex computational process, and is very rare in computational systems.</p>
<p>That said, complexity doesn&#8217;t <em>necessarily</em> imply rarity. Consider again the case of “life,” which turned out to be both more complicated <em>and</em> more extensive than we had once supposed.<a class="see-footnote" id="footnoteref156_lum47k4" title="I should note that there has been considerable debate about the aptness of the analogy between the study of life and the study of consciousness. I won't take up that debate here, but see e.g. Chalmers (1996), ch. 3:  It is interesting to see how a typical high-level property — such as life, say — evades the arguments put forward in the case of consciousness. First, it is straightforwardly inconceivable that there could be a physical replica of a living creature that was not itself alive. Perhaps a problem might arise due to context-dependent properties (would a replica that forms randomly in a swamp be alive, or be human?), but fixing environmental facts eliminates even that possibility. Second, there is no &quot;inverted life&quot; possibility analogous to the inverted spectrum. Third, when one knows all the physical facts about an organism (and possibly about its environment), one has enough material to know all the biological facts. Fourth, there is no epistemic asymmetry with life; facts about life in others are as accessible, in principle, as facts about life in ourselves. Fifth, the concept of life is plausibly analyzable in functional terms: to be alive is roughly to possess certain capacities to adapt, reproduce, and metabolize. As a general point, most high-level phenomena come down to matters of physical structure and function, and we have good reason to believe that structural and functional properties are logically supervenient on the physical.  …All this notwithstanding, a common reaction to the sort of argument I have given is to reply that a vitalist about life might have said the same things. For example, a vitalist might have claimed that it is logically possible that a physical replica of me might not be alive, in order to establish that life cannot be reductively explained. And a vitalist might have argued that life is a further fact, not explained by any account of the physical facts. But the vitalist would have been wrong. By analogy, might not the opponent of reductive explanation for consciousness also be wrong?  I think this reaction misplaces the source of vitalist objections. Vitalism was mostly driven by doubt about whether physical mechanisms could perform all the complex functions associated with life: adaptive behavior, reproduction, and the like. At the time, very little was known about the enormous sophistication of biochemical mechanisms, so this sort of doubt was quite natural. But implicit in these very doubts is the conceptual point that when it comes to explaining life, it is the performance of various functions that needs to be explained. Indeed, it is notable that as physical explanation of the relevant functions gradually appeared, vitalist doubts mostly melted away. With consciousness, by contrast, the problem persists even when the various functions are explained.  Presented with a full physical account showing how physical processes perform the relevant functions, a reasonable vitalist would concede that life has been explained. There is not even conceptual room for the performance of these functions without life. Perhaps some ultrastrong vitalist would deny even this, claiming that something is left out by a functional account of life—the vital spirit, perhaps. But the obvious rejoinder is that unlike experience, the vital spirit is not something we have independent reason to believe in. Insofar as there was ever any reason to believe in it, it was as an explanatory construct — &quot;We must have such a thing in order to be able to do such amazing stuff.&quot; But as an explanatory construct, the vital spirit can be eliminated when we find a better explanation of how the functions are performed. Conscious experience, by contrast, forces itself on one as an explanandum and cannot be eliminated so easily.  One reason a vitalist might think something is left out of a functional explanation of life is precisely that nothing in a physical account explains why there is something it is like to be alive. Perhaps some element of belief in a &quot;vital spirit&quot; was tied to the phenomena of one's inner life. Many have perceived a link between the concepts of life and experience, and even today it seems reasonable to say that one of the things that needs to be explained about life is the fact that many living creatures are conscious. But the existence of this sort of vitalist doubt is of no comfort to the proponent of reductive explanation of consciousness, as it is a doubt that has never been overturned.  " href="#footnote156_lum47k4">156</a></p>
<p>Back when life was fairly mysterious to us, any <em>specific proposed theory</em> of life was almost unavoidably <em>simple</em>. When we know so little about something that it is “mysterious” to us, we often don&#8217;t even know enough detail to <em>propose</em> a specific and complicated model, so we either propose specific and <em>simple</em> models, or we say “it&#8217;s probably complex, but I can&#8217;t propose any specific theory of that complexity at this time.”</p>
<p>So for example in the case of life, Xaviet Bichat proposed in 1801 that living things are distinguished by certain “vital properties,” which he described as fundamental properties of the universe alongside gravity.<a class="see-footnote" id="footnoteref157_0subqz5" title="Bechtel &amp; Richardson (1998):  The role of vitalism in physiology is exemplified in the work of the French anatomist Xavier Bichat (1771-1802). Bichat analysed living systems into parts, identifying twenty-one distinct kinds of tissue, and explaining the behaviour of organisms in terms of the properties of these tissues. He characterized the different tissues in terms of their 'vital properties', as forms of 'sensibility' and 'contractility'. Bichat thought the sensibility and contractility of each tissue type constituted the limit to decomposing living matter into its parts. These vital properties preclude identifying life with any physical or chemical phenomenon because the behaviour of living tissues is irregular and contrary to forces exhibited by their inorganic constituents. Insofar as living matter maintains itself in the face of ordinary physical and chemical processes that would destroy it, Bichat thought it could not be explained in terms of those forces. He therefore allowed that there are additional fundamental forces in nature that are on a par with those Newton ascribed to all matter: 'To create the universe God endowed matter with gravity, elasticity, affinity… and furthermore one portion received as its share sensibility and contractility' (Bichat 1801, vol. 1: xxxvii). These are vital properties of living tissues.  " href="#footnote157_0subqz5">157</a> Some of Bichat&#8217;s contemporaries instead argued that organic and inorganic processes differed in complexity rather than in fundamental properties or substances, but in my understanding, they couldn&#8217;t propose specific complicated theories of life — all they could say was that life would probably turn out to be complicated,<a class="see-footnote" id="footnoteref158_aga64z0" title="For example here is Bechtel &amp; Richardson (1998) again:  …chemists in the early nineteenth century hoped to explain many of the reactions found in living organisms. Organic compounds are apparently formed only in living organisms, and thus appear to be products of vital activity. The physiological chemists of the early nineteenth century set out to show, contrary to initial appearances, that these products are the results of chemical processes. Jacob Berzelius (1779-1848) argued that chemistry could account for all of the reactions occurring within living organisms, and that organic and inorganic processes differ only in complexity. 'There is,' he said, 'no special force exclusively the property of living matter which may be called a vital force' (1836).  " href="#footnote158_aga64z0">158</a> as I am saying for consciousness.</p>
<p>Of course, early scientific progress on life revealed that life <em>is</em>, in fact, fairly complex: it is made of many parts, interacting in particular, complicated ways that were impossible to predict in detail prior to their discovery.</p>
<p>This pattern — that early scientific progress tends to lead to more complicated models of phenomena — seems especially true of behavioral and cognitive phenomena. In early theorizing about these phenomena, there were some who proposed specific and relatively simple models of a given phenomenon, and there were those who said “It&#8217;s probably complicated, but I don&#8217;t know enough detail to specify a complicated model,” and as far as I know, these phenomena have always turned out to be more complicated than any simple, specific model we could propose early on. (Of course, the early, simple, false models were often <em>useful</em> for guiding scientific progress,<a class="see-footnote" id="footnoteref159_f06lrnt" title="See e.g. Wimsatt (1976).  " href="#footnote159_f06lrnt">159</a> but that is different from saying that the early, simple models have tended to be <em>correct</em>.)</p>
<p>If this story is right — and I&#8217;m not sure it is, as I&#8217;m not an historian of science — then it should not be a surprise that most currently proposed and highly <em>specific</em> theories are quite simple,<a class="see-footnote" id="footnoteref160_tiy75a1" title="E.g. they seem to be satisfiable by a very small neural network (Herzog et al. 2007), or more generally a very short computer program.  " href="#footnote160_tiy75a1">160</a> and thus imply a relatively extensive distribution of consciousness. That is what early specific models always look like, and if we consider the history of scientific progress on other behavioral and cognitive mechanisms, consciousness seems likely to turn out to be a good deal more complicated than our early specific models suggest.</p>
<p>Of course, as the example of life shows, we might discover that consciousness is highly complex and yet still surprisingly extensive.<a class="see-footnote" id="footnoteref161_1ls15d3" title="One way this might happen is that we might conclude that consciousness is complex, but that &quot;consciousness&quot; properly refers to a highly disjunctive collection of a great many different complex processes.  " href="#footnote161_1ls15d3">161</a> Or perhaps it will be complex and rare (like mirror self-recognition).</p>
<p><span style="float: right;">[<a href="/node/858/edit/32">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="SophisticatedBehaviors">We continue to find that many sophisticated behaviors are more extensive than we once thought</h4>
<p>Turn to almost any chapter of a recent textbook on animal cognition,<a class="see-footnote" id="footnoteref162_l3wm9wi" title="For example Shettleworth (2009), Vonk &amp; Shackelford (2012), Pearce (2008), Wynne &amp; Udell (2013), Olmstead &amp; Kuhlmeier (2015), Cheng (2016), or Menzel &amp; Fischer (2011).  See also task-specific and taxa-specific books on animal cognition, such as:  &#9;Shumaker et al. (2011) and Sanz et al. (2013) on tool use &#9;Whitehead &amp; Rendell (2014) on culture among dolphins and whales &#9;Brown et al. (2011) and Balcombe (2016) on fish cognition and behavior &#9;Emery (2016) and Ackerman (2016) on birds   (Some of these are popular books rather than academic books.)  " href="#footnote162_l3wm9wi">162</a> check the publication years of the cited primary studies, and you&#8217;ll find an account that could be summarized like this: “A few decades ago, we didn&#8217;t know that [some animal taxa] exhibited [some sophisticated behavior], suggesting they may have [some sophisticated cognitive capacity]. Today, we have observed several examples.” In <em>this</em> sense, at least, it seems true that “research constantly moves forward, and the tendency of research is to extend the number of animals that might be able to suffer, not decrease it.”<a class="see-footnote" id="footnoteref163_3li9kek" title="From James Rachels' chapter &quot;The Basic Argument for Vegetarianism&quot; in Sapontzis (2004), p. 78.  " href="#footnote163_3li9kek">163</a> This is my third “big-picture consideration” concerning the distribution of consciousness.</p>
<p>Consider, for example, these (relatively) recent discoveries:<a class="see-footnote" id="footnoteref164_4r6m2sj" title="See also this Quora thread: What is the most intelligent thing a non-human animal has done?  " href="#footnote164_4r6m2sj">164</a></p>
<ul><li>Several forms of tool use and tool manufacture by insects and other invertebrates<a class="see-footnote" id="footnoteref165_ut0l6rb" title="See Shumaker et al. (2011), ch. 2, for an overview. Below are some examples. Note that the type of animal tool behavior is capitalized, e.g. Baiting and Inserting.  &#9;&#9;McMahan… described Baiting of prey by the neotropical assassin bug (Salyavata variegata), which uses previously captured termite carcasses to capture additional termites. By holding and shaking the carcass over the nest's entrance hole, the assassin bug lures a termite into attempting to retrieve the carcass for its own consumption. Once a termite grasps the lure, the assassin bug slowly pulls the termite out of the nest entrance toward itself. Once the termite is within reach, the assassin bug quickly kills and eats its new victim. McMahan… reported that the assassin bug, if not disturbed, will repeat this process an average of seven or eight times. The author once saw an assassin bug capture thirty-one termites in this manner over the course of three hours.  &#9;&#9;…  &#9;&#9;Some female digger wasps Insert small twigs into nest burrows they have closed with soil and Probe with them… This behavior may settle and pack the soil and provide the female with sensory information regarding the adequacy of the closure.  &#9;&#9;…  &#9;&#9;Boxer crabs, also known as &quot;pom-pom crabs&quot;… Detach small anemones from the substrate and Brandish or Wave one in each cheliped [claw]… The crab moves with its chelipeds extended and waving. If the crab is mechanically disturbed, the chelipeds with the anemones are directed at the source of irritation. This behavior would presumably facilitate the discharge of stinging nematocysts by the anemones toward the threat. However, the crab's use of anemones is not limited to protection or defense. If food is placed near the oral disc of the anemone, the crab immediately seizes the food with its anterior ambulatory appendages. Thus any food ensnared by the anemone in its own tentacles is apt to be appropriated by the crab. The crab also removes debris adhering to the body of the anemone and ingests the edible bits.  &#9;&#9;…  &#9;&#9;Finn, Tregenza, and Norman (2009) reported defensive tool use by the veined octopus (Amphioctopus marginatus). The octopuses frequently carried coconut shell halves and, when threatened, assembled them into a shelter by aligning the two halves of the coconut and hiding inside. These authors argued that the behavior is significant, as the octopuses carry the shells for future use as a shelter, despite the immediate energetic and locomotor costs. During travel, the octopus carries the shells under its body, in a form of locomotion termed &quot;stilt walking,&quot; which the researchers described as &quot;ungainly and clearly less efficient than unencumbered locomotion&quot;…  &#9;" href="#footnote165_ut0l6rb">165</a></li>
<li>Complex food preparation by a wild dolphin<a class="see-footnote" id="footnoteref166_1w5cnpo" title="Finn et al. (2009).  &#9;" href="#footnote166_1w5cnpo">166</a></li>
<li>The famous feats of <a href="https://en.wikipedia.org/wiki/Alex_(parrot)">Alex the parrot</a>, <a href="https://en.wikipedia.org/wiki/Koko_(gorilla)">Koko the gorilla</a>, <a href="https://en.wikipedia.org/wiki/Washoe_(chimpanzee)">Washoe the chimpanzee</a>, and others</li>
<li>Fish using transitive inference to learn their social rank<a class="see-footnote" id="footnoteref167_mcyfo7e" title="Grosenick et al. (2007).  &#9;" href="#footnote167_mcyfo7e">167</a></li>
<li>Fairly advanced puzzle-solving by New Caledonian crows<a class="see-footnote" id="footnoteref168_1aossj7" title="See e.g. Jelbert et al. (2014) and the resources on this page by the Behavioural Ecology Research Group at the University of Oxford.  &#9;" href="#footnote168_1aossj7">168</a></li>
<li>Western scrub-jays planning for future days without reference to their current motivational states<a class="see-footnote" id="footnoteref169_k4g7nfh" title="See previous footnote.  &#9;" href="#footnote169_k4g7nfh">169</a></li>
</ul><p>What accounts for this trend? No doubt, part of the explanation is normal scientific progress. After all, there are a lot of species, and a lot of possible behaviors to observe and test for, so perhaps in many cases we just “hadn&#8217;t gotten around to it” until recently.</p>
<p>But, some of the explanation may have to do with changing academic subcultures, as <a href="http://books.wwnorton.com/books/detail.aspx?ID=4294990752">de Waal (2016)</a> relates (in the prologue):</p>
<blockquote><p>For most of the last century, science was overly cautious and skeptical about the intelligence of animals. Attributing intentions and emotions to animals was seen as naïve “folk” nonsense. We, the scientists, knew better! We never went in for any of this “my dog is jealous” stuff, or “my cat knows what she wants,” let alone anything more complicated, such as that animals might reflect on the past or feel one another&#8217;s pain. Students of animal behavior either didn&#8217;t care about cognition or actively opposed the whole notion. Most didn&#8217;t want to touch the topic with a ten-foot pole. Fortunately, there were exceptions — and I will make sure to dwell on those, since I love the history of my field — but the two dominant schools of thought viewed animals as either stimulus-response machines out to obtain rewards and avoid punishment or as robots genetically endowed with useful instincts. While each school fought the other and deemed it too narrow, they shared a fundamentally mechanistic outlook: there was no need to worry about the internal lives of animals, and anyone who did was anthropomorphic, romantic, or unscientific.</p>
<p>Did we have to go through this bleak period? In earlier days, the thinking was noticeably more liberal. Charles Darwin wrote extensively about human and animal emotions, and many a scientist in the nineteenth century was eager to find higher intelligence in animals. It remains a mystery why these efforts were temporarily suspended, and why we voluntarily hung a millstone around the neck of biology… But times are changing. Everyone must have noticed the avalanche of knowledge emerging over the last few decades, diffused rapidly over the Internet. Almost every week there is a new finding regarding sophisticated animal cognition, often with compelling videos to back it up. We hear that rats may regret their own decisions, that crows manufacture tools, that octopuses recognize human faces, and that special neurons allow monkeys to learn from each other&#8217;s mistakes. We speak openly about culture in animals and about their empathy and friendships. Nothing is off limits anymore, not even the rationality that was once considered humanity&#8217;s trademark.</p></blockquote>
<p>In some cases, early reports of seemingly sophisticated animal behaviors have failed to replicate or have been found to have alternate explanations,<a class="see-footnote" id="footnoteref170_iyj0jdb" title="The study of mirror self-recognition provides some examples (Anderson &amp; Gallup Jr. 2015):  …the results of any study must be independently replicated by other scientists in order for the findings to be considered reliable. The demonstration of mirror self-recognition in chimpanzees, orangutans and humans has been replicated many times by different investigators all over the world… In contrast, the track record for claims of self-recognition in other species has not been encouraging. Single published reports of mirror self-recognition in one elephant that failed on a re-test (Plotnik et al. 2006), one dolphin (Reiss and Marino 2001), and two magpies (Prior et al. 2008) have yet to be replicated. Indeed, recent evidence with other corvids suggests that apparent instances of mirror self-recognition by magpies may be an artifact of tactile cues (Soler et al. 2014). And in the case of cottontop tamarins (Hauser et al. 1995) an attempt to replicate the original positive results completely failed (Hauser et al. 2001).  On scientific replication in general, see also Appendix Z.8.  " href="#footnote170_iyj0jdb">170</a> but the overall trend is nevertheless clear, and I have every reason to think the overall trend will continue: the more we look, the more we&#8217;ll find that a wide variety of animals, including many simple animals, engages in fairly sophisticated behaviors. The question is: Exactly which behaviors will eventually be observed in which taxa, and how indicative of consciousness are those behaviors?</p>
<p><span style="float: right;">[<a href="/node/858/edit/33">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="Anthropomorphism">Rampant anthropomorphism</h4>
<p>My fourth “big-picture consideration” is this: We humans are nearly-incorrigible <a href="https://en.wikipedia.org/wiki/Anthropomorphism">anthropomorphizers</a>. We seem to be hard-wired to attribute human-like cognitive traits and emotions to non-humans, including animals, robots, chatbots, inanimate objects, and even simple geometric shapes. Indeed, after extensive study of the behavior unicellular organisms, the microbiologist H.S. Jennings was convinced that (e.g.) if an amoeba was large enough that humans came into regular contact with it, we would assume it is conscious for the same reasons we instinctively assume a dog is conscious.<a class="see-footnote" id="footnoteref171_ytqbxfg" title="Jennings (1906), pp. 335-337:  All that we have said thus far in the present chapter is independent of the question whether there exist in the lower organisms such subjective accompaniments of behavior as we find in ourselves, and which we call consciousness. We have asked merely whether there exist in the lower organisms objective phenomena of a character similar to what we find in the behavior of man. To this question we have been compelled to give an affirmative answer. So far as objective evidence goes, there is no difference in kind, but a complete continuity between the behavior of lower and of higher organisms.  Has this any bearing on the question of the existence of consciousness in lower animals? It is clear that objective evidence cannot give a demonstration either of the existence or of the non-existence of consciousness, for consciousness is precisely that which cannot be perceived objectively. No statement concerning consciousness in animals is open to verification or refutation by observation and experiment. There are no processes in the behavior of organisms that are not as readily conceivable without supposing them to be accompanied by consciousness as with it.  But the question is sometimes proposed: Is the behavior of lower organisms of the character which we should &quot;naturally&quot; expect and appreciate if they did have conscious states, of undifferentiated character, and acted under similar conscious states in a parallel way to man? Or is their behavior of such a character that it does not suggest to the observer the existence of consciousness?  If one thinks these questions through for such an organism as Paramecium, with all its limitations of sensitiveness and movement, it appears to the writer that an affirmative answer must be given to the first of the above questions, and a negative one to the second. Suppose that this animal were conscious to such an extent as its limitations seem to permit. Suppose that it could feel a certain degree of pain when injured; that it received certain sensations from alkali, others from acids, others from solid bodies, etc., — would it not be natural for it to act as it does? That is, can we not, through our consciousness, appreciate its drawing away from things that hurt it, its trial of the environment when the conditions are bad, its attempting to move forward in various directions, till it finds one where the conditions are not bad, and the like? To the writer it seems that we can; that Paramecium in this behavior makes such an impression that one involuntarily recognizes it as a little subject acting in ways analogous to our own. Still stronger, perhaps, is this impression when observing an Amoeba obtaining food… The writer is thoroughly convinced, after long study of the behavior of this organism, that if Amoeba were a large animal, so as to come within the everyday experience of human beings, its behavior would at once call forth the attribution to it of states of pleasure and pain, of hunger, desire, and the like, on precisely the same basis as we attribute these things to the dog. This natural recognition is exactly what Munsterberg (1900) has emphasized as the test of a subject. In conducting objective investigations we train ourselves to suppress this impression, but thorough investigation tends to restore it stronger than at first.  Of a character somewhat similar to that last mentioned is another test that has been proposed as a basis for deciding as to the consciousness of animals. This is the satisfactoriness or usefulness of the concept of consciousness in the given case. We do not usually attribute consciousness to a stone, because this would not assist us in understanding or controlling the behavior of the stone. Practically indeed it would lead us much astray in dealing with such an object. On the other hand, we usually do attribute consciousness to the dog, because this is useful; it enables us practically to appreciate, foresee, and control its actions much more readily than we could otherwise do so. If Amoeba were so large as to come within our everyday ken, I believe it beyond question that we should find similar attribution to it of certain states of consciousness a practical assistance in foreseeing and controlling its behavior. Amoeba is a beast of prey, and gives the impression of being controlled by the same elemental impulses as higher beasts of prey. If it were as large as a whale, it is quite conceivable that occasions might arise when the attribution to it of the elemental states of consciousness might save the unsophisticated human being from the destruction that would result from the lack of such attribution. In such a case, then, the attribution of consciousness would be satisfactory and useful. In a small way this is still true for the investigator who wishes to appreciate and predict the behavior of Amoeba under his microscope.  But such impressions and suggestions of course do not demonstrate the existence of consciousness in lower organisms. Anv belief on this matter can be held without conflict with the objective facts. All that experiment and observation can do is to show us whether the behavior of lower organisms is objectively similar to the behavior that in man is accompanied by consciousness. If this question is answered in the affirmative, as the facts seem to require, and if we further hold, as is commonly held, that man and the lower organisms are subdivisions of the same substance, then it may perhaps be said that objective investigation is as favorable to the view of the general distribution of consciousness throughout animals as it could well be. But the problem as to the actual existence of consciousness outside of the self is an indeterminate one; no increase of objective knowledge can ever solve it. Opinions on this subject must then be largely dominated by general philosophical considerations, drawn from other fields.  " href="#footnote171_ytqbxfg">171</a> As cognitive scientist Dan Sperber put it, “Attribution of mental states is to humans as echolocation is to the bat.”<a class="see-footnote" id="footnoteref172_cbk3b1l" title="From a paper presented at a conference on Darwin and the Human Sciences, at the London School of Economics (1993). Quoted in Baron-Cohen (1995), p. 4.  " href="#footnote172_cbk3b1l">172</a></p>
<p>Of course, a general warning about anthropomorphism is no substitute for reading through a great many examples of (false) anthropomorphisms, from <a href="https://en.wikipedia.org/wiki/Clever_Hans">Clever Hans</a> onward, which you can find in the sources I list in a footnote.<a class="see-footnote" id="footnoteref173_uai65o2" title="On anthropomorphism in general, see Hutson (2012), ch. 6; Guthrie (1993); Horowitz (2010); Epley et al. (2007); Epley (2011); Waytz et al. (2012); Urquiza-Haas &amp; Kotrschal (2015).  Shettleworth (2009), ch. 1, provides a summary of the debates over anthropomorphism in the study of animal cognition and behavior:  Crows [in Davis, California] crack walnuts by dropping them from heights of 5–10 meters or more onto sidewalks, roads, and parking lots. Occasionally they drop walnuts in front of approaching cars, as if using the cars to crush the nuts for them. Do crows intentionally use cars as nutcrackers? Some of the citizens of Davis, as well as some professional biologists (Maple 1974, in Cristol et al. 1997) were convinced that they do, at least until a team of young biologists at UC Davis put this anecdote to the test (Cristol et al. 1997). They reasoned that if crows were using cars as tools, the birds would be more likely to drop nuts onto the road when cars were coming than when the road was empty. Furthermore, if a crow was standing in the road with an uncracked walnut as a car approached, it should leave the nut in the road to be crushed rather than carry it away.  Cristol and his collaborators watched crows feeding on walnuts and recorded how likely the birds were to leave an uncracked walnut in the road when cars were approaching and when the road was empty. They found no support for the notion that crows were using automobiles as nutcrackers…   …The people in Davis and elsewhere (Nihei 1995; Caffrey 2001) who saw nutcracking as an expression of clever crows' ability to reason and plan were engaging in an anthropomorphism that is common even among professional students of animal behavior (…Kennedy 1992; Wynne 2007a, 2007b). As we will see, such thinking can be a fertile source of ideas, but research often reveals that simple processes apparently quite unlike explicit reasoning are doing surprisingly complex jobs…  …  …some of Darwin's [early] supporters… set out to collect anecdotes appearing to prove animals could think and solve problems the way people do. Their approach was not just anthropocentric but frankly anthropomorphic, explaining animals' apparently clever problem solving in terms of human-like thinking and reasoning. But as we have seen in the case of the nutcracking crows, just because an animal's behavior looks to the casual observer like what a person would do in a similar-appearing situation does not mean it can be explained in the same way. Such reasoning based on analogy between humans and other animals must be tested with experiments that take into account alternative hypotheses (Heyes 2008).  Fortunately for progress in understanding animal cognition, critics of extreme anthropomorphism were not slow to appear. E.L. Thorndike's (1911/1970) pioneering experiments on how animals solve simple physical problems showed that gradual learning by trial and error was more common than human-like insight and planning (Galef 1998). C. Lloyd Morgan also observed animals in a systematic way but is now best known for stating a principle commonly taken as forbidding unsupported anthropomorphism. What Morgan (1894) called his Canon states, &quot;In no case may we interpret an action as the outcome of the exercise of a higher psychical faculty, if it can be interpreted as the outcome of the exercise of one which stands lower in the psychological scale.&quot; Morgan's Canon is clearly not without problems (Sober 2005). What is the &quot;psychological scale&quot;? Don't &quot;higher&quot; and &quot;lower&quot; assume the phylogenetic scale? In contemporary practice &quot;lower&quot; usually means associative learning, that is, classical and instrumental conditioning or untrained species-specific responses. &quot;Higher&quot; is reasoning, planning, insight, in short any cognitive process other than associative learning.  For an example of how Morgan's Canon might be applied today, suppose… that crows had been found to drop nuts in front of cars more than on the empty road. An obvious &quot;simple&quot; explanation is that they had been reinforced more often when dropping a nut when a car was coming than when the road was empty and thereby had learned to discriminate these two situations. A &quot;higher,&quot; anthropomorphic, explanation might be that having seen fallen nuts crushed by cars the insightful crows reasoned that they could drop the nuts themselves. The contrast between these explanations suggests a straightforward test: observe naive crows to see if the discrimination between approaching cars and empty roads develops gradually (supporting the &quot;simple&quot; explanation) or appears suddenly, without any previous trial and error (supporting the &quot;higher&quot; explanation). Unfortunately, competing explanations do not always make such readily discriminable predictions about observable behavior. Even when they do, experiments designed to pit them against each other may not yield clear results. Then agnosticism may be the most defensible policy (Sober 2005).  In practice, the field of comparative cognition as it has developed in the past 30–40 years has a very strong bias in favor of &quot;simple&quot; mechanisms (Sober 2001; Wasserman and Zentall 2006a). The burden of proof is generally on anyone wishing to explain behavior in terms of processes other than associative learning and/or species-typical perceptual and response biases. To many, anthropomorphism is a dirty word in scientific study of animal cognition (Mitchell 2005; Wynne 2007a, 2007b). But dismissing anthropomorphism altogether is not necessarily the best way forward. &quot;Anthropodenial&quot; (de Waal 1999) may also be a sin. After all, if other species share common ancestors with us, then we share an a priori unspecifiable number of biological processes with any species one cares to name. Thus in some ways, as Morgan apparently thought (Sober 2005), the simplest account of any behavior is arguably the anthropomorphic one, that behavior analogous to ours is the product of a similar cognitive process. Note, however, that &quot;simple&quot; has shifted here from the cognitive process to the explanation (Karin-D'Arcy 2005), from &quot;simpler for them&quot; to &quot;simpler for us&quot; (Heyes 1998).  Where do these considerations leave Morgan's Canon? A reasonable modern interpretation of the Canon (Sober 2005) is that a bias in favor of simple associative explanations is justified because basic conditioning mechanisms are widespread in the animal kingdom, having been found in every animal, from worms and fruitflies to primates, in which they have been sought (Papini 2008). Thus they may be evolutionarily very old, present in species ancestral to all present-day animals and reflecting adaptations to universal causal regularities in the world and/or fundamental properties of neural circuits. As species diverged, other mechanisms may have become available on some branches of the evolutionary tree, and it might be said to be the job of comparative psychologists to understand their distribution (Papini 2002).  But for such a project to make sense, it must be clear what is meant by associative explanations and what their limits are. Associative learning… is basically the learning that results from experiencing contingencies, or predictive relationships, between events. At the theoretical level, such experience in Pavlovian (stimulus-stimulus) or instrumental (response-stimulus) conditioning has traditionally been thought of as strengthening excitatory or inhibitory connections between event representations. Thus one might say that any cognitive performance that does not result from experience of contingencies between events and/or cannot be explained in terms of excitatory and/or inhibitory connections is nonassociative.  Path integration… is one example: an animal moving in a winding path from home implicitly integrates distance and direction information into a vector leading straight home. As another, on one view of conditioning… the flow of events in time is encoded as such and computed on to compare rates of food presentation during a signal and in its absence. Other nonassociative cognitive processes which might be (but rarely if ever have been) demonstrated in nonhumans include imitation, that is, storing a representation of an actor's behavior and later reproducing the behavior; insight; and any kind of reasoning or higher-order representations or computations on event representations. As we will see throughout the book, discriminating nonassociative &quot;higher&quot; processes from associative ones is seldom straightforward, in part because the learning resulting from associative procedures may have subtle and interesting cognitive content. In any case, the goal of comparative research should be understanding the cognitive mechanisms underlying animal behavior in their full variety and complexity rather than partitioning them into rational or nonassociative vs. associative (Papineau and Heyes 2006).  In conclusion, neither blanket anthropomorphism nor complete anthropodenial is the answer (Mitchell 2005). Evolutionary continuity justifies anthropomorphism as a source of hypotheses. When it comes to comparing human cognition with that of other species, it is most likely that — just as with our genes and other physical characters — we will find some processes shared with many other species, some with only a few, and some that are uniquely human. One of the most exciting aspects of contemporary research on comparative cognition is the increasing detail and subtlety in our picture of how other species' minds are both like and not like ours.  " href="#footnote173_uai65o2">173</a></p>
<p>The biologist and animal welfare advocate Marian Dawkins has expressed something close to my view on anthropomorphism, in her book <em>Why Animals Matter</em> (<a href="https://global.oup.com/academic/product/why-animals-matter-9780199747511?cc=us&amp;lang=en&amp;">2012</a>):</p>
<blockquote><p>Anthropomorphic interpretations may be the first ones to spring to mind and they may, for all we know, be correct. But there are usually other explanations, often many of them, and the real problem with anthropomorphism is that it discourages, or even disparages, a more rigorous exploration of these other explanations. Rampant anthropomorphism threatens the very basis of ethology by substituting anecdotes, loose analogies, and an ‘I just know what the animal is thinking so don&#8217;t bother me with science’ attitude to animal behaviour.</p>
<p>…</p>
<p>We need all the scepticism we can muster, precisely because we are all so susceptible to the temptation to anthropomorphize. If we don&#8217;t resist this temptation, we risk ending up being seriously wrong.</p></blockquote>
<p>My guess is that most people anthropomorphize animals far too quickly — including, by attributing consciousness to them<a class="see-footnote" id="footnoteref174_hu0qiip" title="Carruthers (1999) offers the following account, which I would guess is true of many incidents of attributing conscoiusness to animals:  [My view that most animals lack phenomenal consciousness] is highly controversial, of course… It also conflicts with a powerful common-sense intuition to the contrary. But I suggest that this intuition may well be illusory, and can easily be explained away. For notice that one important strategy we often adopt when attributing mental states to a subject, is to try imagining the world Jkom the subject's point of view, to see how things then seem. But when we do that, what we inevitably get are imaginings of conscious perceptions and thoughts, and of experiences with phenomenal feels to them. So, of course we naturally assume that the experiences of a cat will be like something, once we have got to the point of accepting (correctly, in my view) that the cat does have experiences. But this may merely reflect the fact that imaginings of perceptual states are always imaginings of conscious perceptual states, that is all. It may go no deeper than the fact that we have no idea how to imagine a non-conscious perception.  " href="#footnote174_hu0qiip">174</a> — and as such, a proper undercutting of these anthropomorphic tendencies should pull one&#8217;s views about the distribution of consciousness toward a “consciousness is rare” conclusion, relative to where one&#8217;s views were before.</p>
<p><span style="float: right;">[<a href="/node/858/edit/34">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h2 id="SummaryCurrentThinking">Summary of my current thinking about the distribution question</h2>
<p><span style="float: right;">[<a href="/node/858/edit/35">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="HighLevel">High-level summary</h3>
<p>Below is a high-level summary of my current thinking about the distribution-of-consciousness question (with each point numbered for ease of reference):</p>
<ol><li>Given that we <a href="#Theories">don&#8217;t yet</a> have a compelling theory of consciousness, and given that just about any behavior<a class="see-footnote" id="footnoteref175_smrg89h" title="The major possible exception, at least for reasonable-scale systems, is human-style verbal self-report of conscious experience.  " href="#footnote175_smrg89h">175</a> <em>could</em> (as far as I know) be accomplished with <em>or</em> without consciousness (<a href="#inessentialism">consciousness inessentialism</a>), it seems to me that we can&#8217;t know which <em>potentially</em> consciousness-indiciating features (<a href="#PCIFs">PCIFs</a>) are <em>actually</em> consciousness-indicating, except insofar as we continue to get evidence about how consciousness works from the best source of evidence about consciousness we have: human self-report.</li>
<li>Unfortunately, as far as I can tell, studies of human consciousness haven&#8217;t yet <em>confidently</em> identified any particular “<a href="#substantive">substantive</a>” PCIFs as necessary for consciousness, sufficient for consciousness, or strongly indicative of consciousness.</li>
<li>Still, there are <em>some</em> limits to my uncertainty about the distribution question, for reasons I give below…</li>
<li>As far as we know, the vast majority of human cognitive processing is unconscious, including a large amount of fairly complex, “sophisticated” processing. This suggests that consciousness is the result of some <em>particular</em> kinds of information processing, not just <em>any</em> information processing.</li>
<li>Assuming a relatively complex account of consciousness, it is hard to imagine how (e.g.) the 302 neurons of <a href="https://en.wikipedia.org/wiki/Caenorhabditis_elegans"><em>C. elegans</em></a> could support cognitive algorithms which instantiate consciousness. However, I <em>can</em> imagine how the ~100,000 neurons of the Gazami crab might support cognitive algorithms which instantiate consciousness. But I can <em>also</em> imagine it being the case that not even a chimpanzee happens to have the right <em>organization</em> of cognitive processing to have conscious experiences.</li>
<li>Given the uncertainties involved, it is hard for me to justify assigning a “probability of consciousness” lower than 5% to any creature with a neuron count at least a couple orders of magnitude larger than that of <em>C. elegans</em>, and it is hard for me to justify assigning a “probability of consciousness” higher than 95% to any non-human species, including chimpanzees. Indeed, I think I can make a sorta-plausible case for (e.g.) Gazami crab consciousness (even when assuming illusionism about consciousness), and I think I can make a sorta-plausible case for chimpanzee <em>non-consciousness</em>.</li>
<li>When introspecting about how I was intuitively assigning “probabilities of consciousness” (between 5% and 95%) to various species within (say) the “Gazami crabs to chimpanzees” range, it seemed that the four most important factors influencing my “wild guess” probabilities were:
<ol><li>evolutionary distance from humans (years since last common ancestor),</li>
<li>neuroanatomical similarity with humans (see <a href="#AppendixE">Appendix E</a>),</li>
<li>apparent cognitive-behavioral “sophistication” (advanced social politics, mirror self-recognition, abstract language capabilities, and some other PCIFs<a class="see-footnote" id="footnoteref176_udoxf1p" title="Other &quot;sophisticated&quot; PCIFs from my table of PCIFs might include, for example, intentional deception, teaching others, some forms of tool behavior, spontaneously planning for future days without reference to current motivational state, taking into account another's spatial perspective, play behaviors, and grief behaviors. Unfortunately, my ratings for &quot;apparent cognitive-behavioral sophistication&quot; below draw from much more information than I took the time to record in my table of PCIFs and taxa.  &#9;" href="#footnote176_udoxf1p">176</a>), and</li>
<li>total “processing power” (neurons, and maybe especially pallial neurons<a class="see-footnote" id="footnoteref177_hp33qax" title="For arguments about why the absolute number of pallial neurons might be especially indicative of &quot;higher&quot; cognitive functions (and thus perhaps consciousness), see Herculano-Houzel (2016, 2017).  &#9;" href="#footnote177_hp33qax">177</a>).</li>
</ol></li>
<li>But then, maybe I&#8217;m confused about consciousness at a fairly basic level, and consciousness <em>isn&#8217;t</em> at all complicated (see <a href="#AppendixH">Appendix H</a>), as a surprising number of scholars of consciousness currently think. I should give some weight to such views, nearly all of which would imply higher probabilities of consciousness for most animal taxa than more complex accounts of consciousness typically do.</li>
</ol><p>I should say a bit more about the four factors mentioned in (7). Each of these factors provide <em>very weak</em> evidence concerning the distribution question, and can be thought of as providing one component of a four-factor “theory-agnostic estimation process” for the presence of consciousness in an animal.<a class="see-footnote" id="footnoteref178_koor0n9" title="Technically, this four-factor approach isn't entirely theory-agnostic, but it is relatively theory agnostic.  " href="#footnote178_koor0n9">178</a></p>
<p>The reasoning behind the first two factors is this: Given that I know very little about consciousness beyond the fact that humans have it, and it is implemented by information processing in brains,<a class="see-footnote" id="footnoteref179_ixo2079" title="Here and in many other locations in this report, I should really say &quot;brains or ganglia or perhaps entire nervous systems,&quot; but instead I just say &quot;brains&quot; for brevity. For more on this, see Aronyosi (2013).  " href="#footnote179_ixo2079">179</a> then all else equal, creatures that are more similar to humans, especially in their brains, are more likely to be conscious.</p>
<p>The reasoning behind the third factor is twofold. First: in humans, consciousness seems to be especially (but not exclusively) associated with some of our most “sophisicated” behaviors, such as problem-solving and long-term planning. (For example, we have many cases of apparently unconscious simple nocifensive behaviors, but I am not aware of any cases of unconscious long-term logical planning.) Second, suppose we give each extant theory of consciousness a small bit of consideration. Some theories assume that consciousness requires only some very basic supporting functions (e.g. some neural information processing, a simple body schema, and some sensory inputs), whereas others assume that consciousness requires a fuller suite of supporting functions (e.g. a more complex self-model, long-term memory, and executive control over attentional mechanisms). As a result, the total number of theories which predict consciousness in an animal that exhibits both simple <em>and</em> “sophisticated” behaviors is much greater than the number of theories which predict consciousness in an animal that exhibits only simple behaviors.</p>
<p>The reasoning behind the fourth factor is just that a brain with more total processing power is (all else equal) more likely to be performing a greater variety of computations (some of which might be conscious), and is also more likely to be conscious if consciousness depends on a brain passing some threshold of repeated, recursive, or “integrated” computations.</p>
<p>Here is a table showing how the animals I ranked compare on these factors (according to my own quick, non-expert judgments):</p>
<table><tr><th></th>
<th>Evolutionary distance from humans</th>
<th>Neuroanatomical similarity with humans</th>
<th>Apparent cognitive-behavioral sophistication</th>
<th>Total processing power (neurons)</th>
</tr><tr><td>Humans (for comparison)</td>
<td>0</td>
<td>∞</td>
<td>Very high</td>
<td>86 billion<a class="see-footnote" id="footnoteref180_86gr0ue" title="See Olkowicz et al. (2016).    &#9;" href="#footnote180_86gr0ue">180</a></td>
</tr><tr><td>Chimpanzees</td>
<td>6.6 Mya</td>
<td>High</td>
<td>High</td>
<td>~6 billion??<a class="see-footnote" id="footnoteref181_eltm3le" title="As far as I know, the number of neurons in a chimpanzee brain has not been counted, but Herculano-Houzel (2016), figure 4.15, estimates the number at 6 billion (using the neuronal scaling rule discovered to hold for other primates).    &#9;" href="#footnote181_eltm3le">181</a></td>
</tr><tr><td>Cows</td>
<td>97.5 Mya</td>
<td>Moderate/high</td>
<td>Low</td>
<td>~3 billion??<a class="see-footnote" id="footnoteref182_im3s2gb" title="As far as I know, the number of neurons in a cow brain has not been counted, but Herculano-Houzel (2016), p. 75 says that, given the neuronal scaling rules discovered to hold for primates vs. other mammals, &quot;the chimpanzee can be expected to have at least twice as many neurons as a cow,&quot; and in figure 4.15, she estimates the number of neurons in a chimpanzee brain as 6 billion.    &#9;" href="#footnote182_im3s2gb">182</a></td>
</tr><tr><td>Chickens</td>
<td>320.5 Mya</td>
<td>Low/moderate</td>
<td>Low</td>
<td>~221 million</td>
</tr><tr><td>Rainbow trout</td>
<td>429.6 Mya</td>
<td>Low/moderate</td>
<td>Low</td>
<td>~12 million??<a class="see-footnote" id="footnoteref183_2uaaho2" title="This very rough guess was derived by comparing the average brain mass of chickens and rainbow trout, and by (unrealistically) assuming that chicken brains and rainbow trout briains follow the same neuronal scaling rule.    &#9;" href="#footnote183_2uaaho2">183</a></td>
</tr><tr><td>Common fruit flies</td>
<td>847 Mya</td>
<td>Very low</td>
<td>Very low</td>
<td>0.12 million</td>
</tr><tr><td>Gazami crabs</td>
<td>847 Mya</td>
<td>Very low</td>
<td>Very low</td>
<td>0.1 million??<a class="see-footnote" id="footnoteref184_glzdg6u" title="I couldn't locate the number of neurons for a Gazami crab, so I simply guessed it might be similar to the number of neurons in a lobster. Wikipedia's list of animals by number of neurons says a lobster has 100,000 neurons. After I found this number, I saw that Tye (2016), p. 156, also claims that &quot;Crabs have… around a hundred thousand [neurons]&quot;, though Tye doesn't provide a source for this claim.    &#9;" href="#footnote184_glzdg6u">184</a></td>
</tr></table><p>But let me be clear about my process: I did <em>not</em> decide on some particular combination rule for these four factors, assign values to each factor for each species, and then compute a resulting probability of consciousness for each taxon. Instead, I used my intuitions to generate my probabilities, then reflected on what factors seemed to be affecting my intuitive probabilities, and then filled out this table. However, once I created this table the first time, I continued to reflect on how much I think such weak sources of evidence <em>should</em> be affecting my probabilities, and my probabilities shifted around a bit as a result.</p>
<p>Given the uncertainties involved, and given how ad-hoc and unjustified the reasoning process described in this section is, and given that consciousness is likely a “<a href="#fuzzy">fuzzy</a>” concept, it might seem irresponsible or downright silly to say “There&#8217;s an X% chance that chickens are ‘conscious,’ a Y% chance that rainbow trout are ‘conscious,’ and a Z% chance that the <a href="https://www.tesla.com/autopilot">Tesla Autopilot algorithms</a> are ‘conscious.’” </p>
<p>Nevertheless, I will make some such statements in the next section, for the following reasons:</p>
<ul><li>As <a href="http://plato.stanford.edu/entries/probability-interpret/#SubPro">subjective Bayesians</a> would point out, my ongoing decisions imply that I already implicitly assign (something like) probabilities to consciousness-related or moral patienthood-related statements. I treat rocks differently than fishes, and fishes differently than humans. Also, there are some bets on this topic I would take, and some I would not. For example, given a suitably specified arbiter (e.g. a well-conducted poll of relevant leading scientists, taken 40 years from now), if someone wanted to bet me, at 100-to-1 odds, that no fishes are “conscious” (as defined by a plurality of relevant leading scientists, 40 years from now), I would take the bet.</li>
<li>Even if my probabilities have no principled justification, and even if they aren&#8217;t straightforwardly action-guiding (see below), putting “made-up” numbers on my beliefs makes it easier for others to productively disagree with my conclusions, and argue against them.</li>
</ul><p><span style="float: right;">[<a href="/node/858/edit/36">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="Probabilities">My current probabilities</h3>
<p>Below, I list some of my current probabilities for the possession of consciousness by normally-functioning adult members of several different animal taxa, and also for the possession of consciousness by an example AI program: DeepMind&#8217;s <a href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a>.</p>
<p>I always assigned a lower probability to “consciousness as loosely defined by example <a href="#Defined">above</a>” than I did to “consciousness of a sort I intuitively morally care about,” since I suspect the latter will end up being a slightly (but perhaps not hugely) broader concept than the former, since the former is defined with reference to the human example even though it is typically meant to apply substantially beyond it.</p>
<table><tr><th></th>
<th>Probability of consciousness as loosely defined by example above</th>
<th>Probability of consciousness of a sort I intuitively morally care about</th>
</tr><tr><td>Humans</td>
<td>&gt;99%</td>
<td>&gt;99%</td>
</tr><tr><td>Chimpanzees</td>
<td>85%</td>
<td>90%</td>
</tr><tr><td>Cows</td>
<td>75%</td>
<td>80%</td>
</tr><tr><td>Chickens</td>
<td>70%</td>
<td>80%</td>
</tr><tr><td>Rainbow trout</td>
<td>60%</td>
<td>75%</td>
</tr><tr><td>Common fruit flies</td>
<td>15%</td>
<td>30%</td>
</tr><tr><td>Gazami crabs</td>
<td>10%</td>
<td>30%</td>
</tr><tr><td><a href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a><a class="see-footnote" id="footnoteref185_gq1l4ky" title="At the time of the 2016 match with Lee Sedol.    &#9;" href="#footnote185_gq1l4ky">185</a></td>
<td>&lt;5%</td>
<td>&lt;5%</td>
</tr></table><p>My very low probability for AlphaGo consciousness is obviously not informed by most of the reasoning that informs my probabilies for animal species. AlphaGo has no evolutionary continuity with humans, it has no neuroanatomical similarity with humans (except for AlphaGo&#8217;s “neurons,” which are similar to human neurons only in a very abstract way), and its level of “cognitive-behavioral sophistication” is essentially “none” except for the very narrow task at which it is specifically programmed to excel (playing Go). Also, unlike with animal brains, I <em>can</em> trace, to a large extent, what AlphaGo is doing, and I don&#8217;t think it does anything that looks to me like it could instantiate consciousness (e.g. on an illusionist account of consciousness). Nevertheless, I feel I must admit <em>some</em> non-negligible probability that AlphaGo is conscious, given how many scholars of consciousness endorse views that seem to imply AlphaGo is conscious. (Though even if AlphaGo <em>is</em> conscious, it might have negligible <a href="#AppendixZ7">moral weight</a>.)</p>
<p>Please keep in mind that I don&#8217;t think this report <em>argues</em> for my stated probabilities. Rather, this report surveys the kinds of evidence and argument that have been brought to bear on the distribution question, reports some of my impressions about those bodies of evidence and argument, and then reports what my own intuitive probabilities seem to be at this time. Below, I try — to a limited degree — to explore why I self-report these probabilities rather than others, but of course, I have limited introspective access to the reasons why my brain has produced these probabilities rather than others.<a class="see-footnote" id="footnoteref186_uw349um" title="There is a vast literature on the limits of introspection. See e.g. Wilson (2004); Carruthers (2011).  " href="#footnote186_uw349um">186</a> I assume the evidence and argument I&#8217;ve surveyed here has a substantial impact on my current probabilities, but I do not think my brain even remotely approximates an ideal Bayesian integrator of evidence (at this scale, anyway), and I do not think my brief and shallow survey of such a wide range of complicated evidence (from fields in which I have little to no expertise) <em>argues</em> for the probabilities I&#8217;ve given here. Successfully arguing for <em>any</em> set of probabilities about the distribution of consciousness would, I think, require a much larger effort than I have undertaken here.</p>
<p>Also, remember that whichever of these taxa turn out to <em>actually</em> be “conscious,” they could vary by <em>several orders of magnitude</em> in moral weight. In particular, I suspect the arthropods on this list, <em>if</em> they are conscious, might be several orders of magnitude lower in moral weight (given <em>my</em> moral judgments) than (e.g.) most mammals, given the factors listed briefly in <a href="#AppendixZ7">Appendix Z.7</a>. (But that is just a hunch; I haven&#8217;t yet thought about moral weight much at all.)</p>
<p><span style="float: right;">[<a href="/node/858/edit/37">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="Why_these_probabilities">Why these probabilities?</h3>
<p>It is difficult to justify, or even to explain, why I give these probabilities and not others, beyond what I&#8217;ve already said above. My hope is that the rest of this report gives <em>some</em> insight into why I report these probabilities, but there is no clear weighted combination rule for synthesizing the many different kinds of argument and evidence I survey above, let alone the many considerations that are affecting my judgments but which I did not have time to explain in this report, and (in some cases) that I don&#8217;t even know about myself. Nevertheless, I offer a few additional explanatory comments below.</p>
<p>My guess is that to most consciousness-interested laypeople, the most surprising facts about the probabilities I state above will be that my probability of chimpanzee consciousness is so <em>low</em>, and that my probability of Gazami crab consciousness is so <em>high</em>. In a sense, these choices may simply reflect my view that, as <a href="https://global.oup.com/academic/product/why-animals-matter-9780199747511?cc=us&amp;lang=en&amp;">Dawkins (2012)</a> put it, “consciousness is harder [to understand] than you think”<a class="see-footnote" id="footnoteref187_12nrmor" title="Page 44.  " href="#footnote187_12nrmor">187</a> — i.e., that I&#8217;m unusually <em>uncertain</em> about my attributions of consciousness, which pulls the probability of consciousness for a wide range of taxa closer to some kind of intuitive “total ignorance prior” around 50%.<a class="see-footnote" id="footnoteref188_t4j2e06" title="In Bayesian statistics, an &quot;ignorance prior&quot; is a probability distribution for which equal probability is assigned to all possibilities — i.e., when one is still &quot;ignorant&quot; of all (or nearly all) the relevant evidence, before the probability distribution is updated by the observation of some Bayesian evidence. See e.g. Jaynes (2003), ch. 12.  " href="#footnote188_t4j2e06">188</a></p>
<p>What might experts in animal consciousness think of my probabilities? My guess is that most of them would think that my probabilities are too low, at least for the mammalian taxa, and probably for all the animal taxa I listed except for the arthropods. If that&#8217;s right, then my guess is that our disagreements are largely explained by (1) my greater uncertainty about ~all attributions of consciousness, and (2) selection effects on the field of animal consciousness studies. (If you don&#8217;t think it&#8217;s likely that many animals are conscious, you&#8217;re unlikely to devote a large fraction of your career to studying the topic!)<a class="see-footnote" id="footnoteref189_l5prlg4" title="Panpsychists, of course, will think that all my probabilities are too low. This difference of opinion likely traces back to the &quot;metaphysical&quot; debates about consciousness that I mostly skipped over in this report (but, see here).  " href="#footnote189_l5prlg4">189</a></p>
<p><a name="scientificskepticism" id="scientificskepticism"></a>I should say a bit more about why I might be less confident in “~all attributions of consciousness” than most experts in consciousness are. In part, this may be a result of the fact that, in my experience, I seem to be more skeptical of published scientific findings than most working scientists and philosophers are. Hence, whereas some people are convinced by (for example) the large, diverse, and cohesive body of evidence for global neuronal workspace theory assembled in <a href="http://www.penguin.com/book/consciousness-and-the-brain-by-stanislas-dehaene/9780143126263">Dehaene (2014)</a>, I read a book like that and think “Well, based on my experience, I bet many of the cited studies would fail to hold up under attempted replication, or even under close scrutiny of the methods used, and I&#8217;m not sure how much that would affect the overall conclusions.”<a class="see-footnote" id="footnoteref190_eqrzdks" title="I also have reservations about the lack of precision and comprehensiveness with which GNWT, as currently formulated, explains the explananda of consciousness (see Appendix B).  " href="#footnote190_eqrzdks">190</a> I could be wrong, of course, and I haven&#8217;t scrutinized the studies cited in Dehaene&#8217;s book myself; I&#8217;m just making a prediction based on the base rate for how often studies of various kinds fail to “hold up” upon closer inspection (by me), or upon attempted replication. I don&#8217;t defend my general skepticism of published studies here, but I list some example sources of my skepticism in <a href="#AppendixZ8">Appendix Z.8</a>.</p>
<p>In any case, heightened skepticism of published studies — e.g. studies offered as support for some theory of consciousness, or for the presence of some cognitive or behavioral feature in some animal taxon — will tend to pull one&#8217;s views closer to a “total ignorance” prior, relative to the views of someone who takes the published studies more seriously.</p>
<p><span style="float: right;">[<a href="/node/858/edit/38">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="Acting">Acting on my probabilities</h3>
<p>Should one take action based on such made-up, poorly-justified probabilities? I&#8217;m genuinely unsure. There are many different kind of uncertainty, and I&#8217;m not sure how to act given uncertainty of this kind.<a class="see-footnote" id="footnoteref191_c674xjk" title="We've wrestled with related issues before, but do not feel satisfied. See Why we can’t take expected value estimates literally (even when they’re unbiased), Modeling Extreme Model Uncertainty, Sequence thinking vs. cluster thinking, and section 2 of Technical and Philosophical Questions That Might Affect Our Grantmaking. Various formal models have been proposed for acting under various kinds of &quot;radical&quot; uncertainty — e.g. Jaynes (2003, ch. 18) and Jøsang (2016) — but I haven’t studied them closely enough to endorse any of them in particular. See also the papers cited in Romeijn &amp; Roy (2014).  " href="#footnote191_c674xjk">191</a> (We hope to write more about this issue in the future.)</p>
<p><span style="float: right;">[<a href="/node/858/edit/39">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="MindChanged">How my mind changed during this investigation</h3>
<p>First, a note on how my mind did <em>not</em> change during this investigation. By the time I began this investigation, I had already found persuasive my <a href="#Nature">four key assumptions about the nature of consciousness</a>: physicalism, functionalism, illusionism, and fuzziness. During this investigation I studied the arguments for and against these views more deeply than I had in the past, and came away more convinced of them than I was before. Perhaps that it because the arguments for these views are stronger than the arguments against them, or perhaps it is because I am roughly just as subject to confirmation bias as nearly all people seem to be (including those who, like me, know about confirmation bias and actively try to mitigate it).<a class="see-footnote" id="footnoteref192_0z4srcj" title="See e.g. the literature on the &quot;sophistication effect,&quot; as described in Yudkowksy's &quot;Knowing About Biases Can Hurt People&quot; and papers such as Achen &amp; Bartels (2006), and also the literature on differences between intelligence and rationality (e.g. Stanovich 2016).  " href="#footnote192_0z4srcj">192</a> In any case: as you consider how to update your own views based on this report, keep in mind that I <em>began</em> this investigation as a physicalist functionalist illusionist who thought consciousness was likely a very fuzzy concept, and I remain such.</p>
<p>During the first few months of this investigation, I raised my probability that a very wide range of animals might be conscious. However, this had more to do with a “negative” discovery than a “positive” one, in the following sense: Before I began this investigation, I hadn&#8217;t studied consciousness much, and I held out some hope that there would turn out to be compelling reasons to “draw lines” at certain points in phylogeny, for example between animals which do and don&#8217;t have a cortex, and that I could justify a relatively sharp drop in probability of consciousness for species falling “below” those lines. But, as mentioned above, I eventually lost hope that there would (at this time) be compelling arguments for drawing any such lines in phylogeny (short of having a nervous system at all). Hence, my probability of a species being conscious now fades gradually as my “four factors” decrease, with no particularly “sharp” drops in probability anywhere in phylogeny.</p>
<p>A few months into the investigation, I began to elicit my own intuitive probabilities about the possession of consciousness by several different animal taxa. I did this to get a sense of how my opinions were changing during the investigation, and perhaps also to harness a single-person “wisdom of crowds” effect (though, I don&#8217;t think this worked very well).<a class="see-footnote" id="footnoteref193_xbe07ur" title="This is sometimes called &quot;the crowd within&quot; effect. See Vul &amp; Pashler (2008); Steegen et al. (2014).  " href="#footnote193_xbe07ur">193</a> Between July and October, my probabilities changed very little (see footnote for details<a class="see-footnote" id="footnoteref194_i3lni4c" title="I elicited my probabilities via a Google Form (screenshot; spreadsheet). The form walked me through these steps:   First, it led me through two anti-anchoring exercises meant to minimize the effect of my earlier estimates on my current estimate (see below). Second, the form asked me to give my probability of consciousness (of a sort that I would morally care about, given my current moral judgments) for each of the following animal taxa: chimpanzees, cows, chickens, rainbow trout, gazami crabs, and common fruit flies.   My probabilities concerning the distribution question didn't change very much, as the graph of my 6 self-elicitations shows:    The anti-anchoring exercise the form prompted me to engage in worked as follows:   First, it prompted me to invent a written justification for a randomly-selected probability of consciousness in a randomly-chosen animal taxon. (Randomization was done using random.org.) Second, the form prompted me to &quot;take a deep breath,&quot; and then name a musical artist I hadn't heard for a while but would like to hear again soon.   " href="#footnote194_i3lni4c">194</a>). Then, in January 2017, I finally got around to investigating the arguments for “hidden qualia” (and thus for the plausibility of relatively simple accounts of consciousness; see <a href="#AppendixH">Appendix H</a>), and this moved my probabilities upward a bit, especially for “consciousness of a sort I intuitively morally care about.”</p>
<p>There are some other things on which my views shifted noticeably as a result of this investigation:</p>
<ul><li>During the investigation, I became more confident in <a href="#illusionism">illusionism</a>.</li>
<li>During the investigation, I became less optimistic that “philosophical” arguments will contribute much to our understanding of the distribution question <em>on the present margin</em>, relative to scientific work, such as the scientific work which would feed into my four-factor “theory-agnostic estimation process” described <a href="#HighLevel">above</a>, that which could contribute toward progress on theories of consciousness (see <a href="#AppendixB">Appendix B</a>), and that which can provide the raw data that can be used in arguments about whether specific taxa are conscious (such as those in <a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye 2016</a>, chs. 5-9).<a class="see-footnote" id="footnoteref195_mdwo0rh" title="Here's an intuition pump: Did philosophical argumentation in the 17th and 18th centuries contribute much to the improvement of our understanding of &quot;life,&quot; or was such progress made almost exclusively by scientific means? See also Baars &amp; McGovern (1993). &#9; &#9;This said, it's not the case that I see no role for philosophical argument. Rather, I think that on the present margin, &quot;scientific&quot; work is needed more urgently than &quot;philosophical&quot; work (though, the line between the two is fuzzy). I could see my intuition about this changing on a different margin. For example, it may be that substantial philosophical work will be invaluable once we have collected more scientific data than we have now.  &#9;" href="#footnote195_mdwo0rh">195</a> On the other hand, I do see a strong need (on the present margin) for computational modeling of theories of consciousness, and for scientific work to be targeted at testing different hypotheses in a philosophically careful way,<a class="see-footnote" id="footnoteref196_w3swqmf" title="For example, it could be informative for philosophers of consciousness to collaborate with neurologists in interviewing patients suffering from auto-activation deficit (see here) about the details of their conscious experiences at different stages in the progression of their symptoms, in a way that might test different hypotheses about consciousness. It could also be informative for philosophers to collaborate with scientists in the design of experiments aimed at empirically distinguishing the hypotheses put forward in Block (2007b) (and commentaries) to explain the results of the experiments described therein.  &#9;" href="#footnote196_w3swqmf">196</a> and some philosophers are likely well-positioned to do some of this work, regardless of how well it resembles “traditional” philosophical argumentation.</li>
<li>During the investigation, it became clear to me that I think too much professional effort is being spent on different schools of thought arguing with each other, and not enough effort spent on schools of thought ignoring each other and making as much progress as they can <em>on their own assumptions</em> to see what those assumptions can lead to. For example, I would like to see more books and articles similar to <a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>, <a href="http://www.penguin.com/book/consciousness-and-the-brain-by-stanislas-dehaene/9780143126263">Dehaene (2014)</a>, and <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00010">Kammererer (2016)</a>.<a class="see-footnote" id="footnoteref197_0poxrtw" title="After writing this paragraph, I discovered that Jesse Prinz expressed a similar view in a 2014 interview with the Moscow Center for Consciousness Studies, starting around 54:27 in the published recording:  &#9;[In The Conscious Brain] I dedicate very little time to the debate between different metaphysical positions [on consciousness]. So rather than arguing for materialism, I presuppose it. Where most of what's been written about consciousness by philosophers in recent decades has focused on the metaphysical debate, I spend just one chapter on it, and that's not a very thorough discussion of the literature, and in that sense I ignored it, and intentionally. I felt that while there are very profound difficult problems… there are all these other problems that are extremely exciting and important, and we've dedicated so much time to these metaphysical problems that we've neglected the others…   &#9;" href="#footnote197_0poxrtw">197</a></li>
<li>When I began this investigation, I felt fundamentally <em>confused</em> about consciousness in a way that I did not, for example, feel confused about many other classically confusing phenomena, for example free will or quantum mechanics. I couldn&#8217;t <em>grok</em> how any set of cognitive algorithms could ever “add up to” the phenomenality of phenomenal consciousness, though I assumed, via a “<a href="http://lesswrong.com/lw/7e5/the_cognitive_science_of_rationality/">system 2 override</a>” of my dualistic intuitions, that <em>somehow</em>, some set of cognitive algorithms <em>must</em> add up to phenomenal consciousness.<a class="see-footnote" id="footnoteref198_nfdx9oa" title="In other words, consciousness has, for me, &quot;clicked into place&quot; in the way described by Dennett (2017), ch. 1:  &#9;One cold, starry night over thirty years ago, I stood with some of my Tufts students looking up at the sky while my friend, the philosopher of science, Paul Churchland instructed us how to see the plane of the ecliptic, that is, to look at the other visible planets in the sky and picture them, and ourselves, as wheeling around the sun all on the same invisible plane. It helps to tip your head just so and remind yourself of where the sun must be, way back behind you. Suddenly, the orientation clicks into place and shazam, you see it! Of course we all knew for years that this was the situation of our planet in the solar system, but until Paul made us see it, it was a rather inert piece of knowledge.  &#9;" href="#footnote198_nfdx9oa">198</a> Now, having spent so much time trying to both solve and <a href="http://lesswrong.com/lw/of/dissolving_the_question/">dissolve</a> the perplexities of consciousness, I no longer feel confused about them in <em>that</em> way. Of course, I still don&#8217;t know which cognitive systems are conscious, and I don&#8217;t know which cognitive-behavioral evidence is most indicative of consciousness, and so on — but the puzzle of consciousness now feels to me more like the puzzle of how different cognitive systems achieve different sorts of long-term hierarchical planning, or the puzzle of how different cognitive systems achieve different sorts of metacognition (see <a href="#IllusionistRealistDisagree">here</a>). This loss of confusion might be <em>mistaken</em>, of course; perhaps I ought to feel more confused than I now do!</li>
</ul><p><span style="float: right;">[<a href="/node/858/edit/40">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="SomeOutputs">Some outputs from this investigation</h3>
<p>The first key output from the investigation is my stated set of probabilities, but — as mentioned above — I&#8217;m not sure they&#8217;re of much value for decision-making at this point.</p>
<p>Another key output of this investigation is a partial map of which activities might give me greater clarity about the distribution of consciousness (see the next section).</p>
<p>A third key output from this investigation is that we decided (months ago) to begin investigating possible grants targeting fish welfare. This is largely due to my failure to find any compelling reason to “draw lines” in phylogeny (see previous section). As such, I could find little justification for suggesting that there is a knowably large difference between the probability of chicken consciousness and the probability of fish consciousness. Furthermore, humans harm and kill many, many more fishes than chickens, and some fish welfare interventions appear to be relatively cheap. (We&#8217;ll write more about this later.)</p>
<p>Of course, this decision to investigate possible fish welfare grants could later be shown to have been unwise, even if the Open Philanthropy Project assumes my personal probabilities of consciousness in different taxa, and even if those probabilities don&#8217;t change. For example, I have yet to examine other potential criteria for moral patienthood besides consciousness, and I have not yet examined the question of moral weight (see <a href="#WhyWeCare">above</a>). The question of moral weight, especially, could eventually undermine the case for fish welfare grants, even if the case for chicken welfare grants remains robust.</p>
<p><span style="float: right;">[<a href="/node/858/edit/41">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h2 id="FutureInvestigations">Potential future investigations</h2>
<p><span style="float: right;">[<a href="/node/858/edit/42">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="ThingsNotDone">Things I considered doing, but didn&#8217;t, due to time constraints</h3>
<p>There are many things I considered doing to reduce my own uncertainty about the likely distribution of morally-relevant consciousness, but which I ended up not doing, due to time constraints. I may do some of these things in the future.<a class="see-footnote" id="footnoteref199_gidslz9" title="This list does not include projects outside the scope of &quot;What is the likely distribution of morally-relevant phenomenal consciousness?&quot;, for example projects related to other criteria for moral patienthood, or projects related specifically to moral weight.  " href="#footnote199_gidslz9">199</a> In no particular order:</p>
<ul><li>I&#8217;d like to speak to consciousness experts about, and think through more thoroughly, which potentially fundable projects seem as though they&#8217;d shed the most light on the likely distribution of morally-relevant consciousness.</li>
<li>I&#8217;d like to get more feedback on this report from long-time “consciousness experts” of various kinds.</li>
<li>I&#8217;d like to think through more carefully whether my four-factor “theory-agnostic estimation process” described <a href="#HighLevel">above</a> makes sense given my current state of ignorance, get help from some ethologists and comparative neuroanatomists to improve the “accuracy” of my ratings for “neuroanatomical similarity with humans” and “apparent cognitive-behavioral sophistication,” and explore what my updated factors and ratings suggest about the distribution question.</li>
<li>As mentioned <a href="#consciousprogram">elsewhere</a>, I&#8217;d like to work with a more experienced programmer to sketch a toy program that I think <em>might</em> be conscious if elaborated, coded fully, and run. Then, I&#8217;d like to adjust the details of its programming so that it more closely matches my own first-person data<a class="see-footnote" id="footnoteref200_lhf80zs" title="Yes, contra Dennett, I take my &quot;first-person data&quot; to be part of what needs to be explained. My reasons for favoring this view over Dennett's &quot;heterophenomenology&quot; are basically the same as Chalmers' in Chalmers (2010), pp. 52-58.  &#9;" href="#footnote200_lhf80zs">200</a> and the data gained from others’ self-reports of conscious experience (e.g. in experiments and in brain damage cases), and then check how my intuitions about the program&#8217;s moral patienthood respond to various tweaks to its design. I would especially like to think more carefully about algorithms that might instantiate conscious “pain” or “pleasure,” and how they might be dissociated from behavior. We have begun to collaborate with a programmer on such a project, but at this time we&#8217;re not sure how much effort we will put into it at this time.</li>
<li>I&#8217;d like to collect others’ moral intuitions, and their explanations of those intuitions, with respect to many cases I have also considered, possibly including different versions of the <em>MESH: Hero</em> program described <a href="#Hero">here</a> (or something like it).</li>
<li>I&#8217;d like to check my moral intuitions against many more cases — including those proposed by philosophers,<a class="see-footnote" id="footnoteref201_qbypqdc" title="Variations on Searle's Chinese Room, Block's Chinese Nation, etc. as well as more recent hypothetical minds such as the computational &quot;Mary&quot; from Yetter-Chappell (2015).  &#9;" href="#footnote201_qbypqdc">201</a> and further extensions of the <em>MESH: Hero</em> exercise I started <a href="#Hero">elsewhere</a> — and, when making my moral judgments about each case and each version of the program, expend more effort to more closely approximate the “extreme effort” version of <a href="#AppendixA">my process for making moral judgments</a> than I did for this report.</li>
<li>I&#8217;d like to research and make the best case I can for insect consciousness, and also research and make the best case I can for chimpanzee <em>non</em>-consciousness, so as to test my intuition that plausible cases can be made for both hypotheses.<a class="see-footnote" id="footnoteref202_9yzm540" title="I'm not satisfied with any of the cases for insect consciousness or chimpanzee non-consciousness that I've seen so far, so I can't just link to sources that I think make a good case for either, but I think I could piece together a good case for either from a variety of arguments and evidence I've come across via disparate sources.  &#9;" href="#footnote202_9yzm540">202</a></li>
<li>I&#8217;d like to more closely examine current popular theories (including computational models) of consciousness, and write down what I do and don&#8217;t find to be satisfying about them, in a much more thorough and less hand-waving way than I do in <a href="#AppendixB">Appendix B</a>. In particular, I&#8217;d like to consider evaluate <a href="http://www.penguin.com/book/consciousness-and-the-brain-by-stanislas-dehaene/9780143126263">Dehaene (2014)</a> more closely, as it seems to have been convincing to a decent number of theorists who previously endorsed other theories, e.g. see <a href="http://faculty.philosophy.umd.edu/pcarruthers/Carruthers%20recants.pdf">Carruthers (2017)</a>.</li>
<li>I&#8217;d like to more closely study the potential and limits of current methods for studying consciousness in humans, e.g. the validity of different self-report schemes,<a class="see-footnote" id="footnoteref203_3i00qds" title="In some fields, self-report is considered so unreliable that it is avoided — but in consciousness studies, it's the best we've got! &#9; &#9;I listed some sources relevant to the reliability of self-report measures in my earlier report on behavioral treatments for insomnia:  &#9;Sources that provide theoretical considerations and non-systematic evidence in favor of substantial a priori concern about the accuracy of self-report measures include Stone et al. (1999); Groves et al. (2009), especially section 7.3; Stalans (2012); Schwarz et al. (2008). Broad (and in some cases, systematic) empirical reviews (or unusually large-scale primary studies) comparing self-report measures to &quot;gold standard&quot; objective measures include Bryant et al. (2014); Gorber et al. (2007); Prince et al. (2008); Bhandari &amp; Wagner (2006); Gorber et al. (2009); Adamo et al. (2009); Kowalski et al. (2012); Kuncel et al. (2005); Bound et al. (2001); Meyer et al. (2009); Barnow &amp; Greenberg (2014). Finally, one cherry-picked primary study I found disheartening with regard to the accuracy of self-report was Suziedelyte &amp; Johar (2013). Please keep in mind that this is only a preliminary list of sources: I have not evaluated any of them closely, they may be unrepresentative of the literature on self-report as a whole, and I can imagine having a different impression of the typical accuracy of self-report measures if and when I complete [a separate investigation] on the accuracy of self-report measures… For those interested in the topic, I list some additional general sources I found useful, again without comment or argument at this time: Stone et al. (2007); Smith (2011); Fernandez-Ballesteros &amp; Botella (2007); Donaldson &amp; Grant-Vallone (2002); Thomas &amp; Frankenberg (2002); Chan (2009); Streiner (2008); Fayers &amp;  Machin (2016), ch. 19.  &#9;" href="#footnote203_3i00qds">203</a> interpretations of neuroimaging data,<a class="see-footnote" id="footnoteref204_43qeaxc" title="See e.g. Klein (2010).  &#9;" href="#footnote204_43qeaxc">204</a> and the feasibility of different strategies for making progress toward a satisfying theory of consciousness via triangulation of data coming from “the phenomenological reports of patients, psychological testing at the cognitive/behavioral level, and neurophysiological and neuroanatomical findings.”<a class="see-footnote" id="footnoteref205_0lwpnk7" title="The quoted phrase is taken from Güzeldere et al. (2000):  &#9;Many [researchers] have labored to develop various constructive explanatory accounts of consciousness. This group can be characterized by a common ontological denominator, say, a commitment to a materialist/naturalist framework; but here, too, we find differences of opinion. Consciousness is explained in terms of causal/functional roles (Lewis, 1966; Lycan, 1987), representational properties (Dretske, 1995; Tye, 1995), emergent biological properties (Flanagan, 1992; Searle, 1992), higher-order mental states (Armstrong, 1980; Rosenthal, 1997), or computer-related metaphors (Dennett, 1991), or a combination of these.  &#9;Within this latter group, we find a certain group of philosophers characterized by an emerging commitment to a particular methodological strategy, a commitment shared by some psychologists and neuroscientists interested in explaining the nature and function of our subjective experiences. Research into the nature and function of consciousness has made some recent advances, especially in the field of cognitive neuroscience, on the basis of a triangulation of data coming from the phenomenological reports of patients, psychological testing at the cognitive/behavioral level, and neurophysiological and neuroanatomical findings. Churchland (1986) calls this strategy for studying the mind the &quot;co-evolutionary strategy&quot;; and Shallice (1988), Dennett (1978, 1991), and Flanagan (1985, 1991, 1992) each promote co-evolutionary methodologies that attempt to bring into equilibrium the phenomenological, the psychological, and the neurobiological in understanding the mind.  &#9;" href="#footnote205_0lwpnk7">205</a></li>
<li>I&#8217;d like to make the exposition of my views on consciousness and moral patienthood more thoroughly-argued and better-explained, so that others can more easily and productively engage with them.</li>
<li>I&#8217;d like to more thoroughly investigate the current state of the arguments about what the human unconscious can and can&#8217;t do.</li>
<li>I&#8217;d like to expand on my “<a href="#BigPicture">big picture considerations</a>” and think through more carefully and thoroughly what I should think about them, and what they imply about the distribution question, and which other “big picture considerations” seem most important (that I haven&#8217;t already listed).</li>
<li>I&#8217;d like to more closely examine the arguments concerning “hidden qualia” (see <a href="#AppendixH">Appendix H</a>).</li>
<li>I&#8217;d like to more carefully examine current theories about how consciousness evolved.</li>
<li>I&#8217;d like to think more about what my intuitions about consciousness suggest about consciousness during early human development, current AI systems, and other potential moral patients besides non-human animals, since this report mostly focused on animals.</li>
<li>I&#8217;d like to study arguments and evidence about the unity of consciousness more closely.<a class="see-footnote" id="footnoteref206_5o03dye" title="See e.g. Bayne (2010); Bennett &amp; Hill (2014).  &#9;" href="#footnote206_5o03dye">206</a></li>
<li>I&#8217;d like to study the arguments for and against illusionism more closely, and consider in more depth how illusionism and other approaches should affect my views on the distribution question.</li>
<li>I&#8217;d like to think more about “valenced” experience (e.g. pain and pleasure), and how it might interact with “basic” consciousness and behavior.</li>
<li>I&#8217;d like to get a better sense of the likely robustness / reproducibility of empirical work on human consciousness, given the general concerns I outline in <a href-="">Appendix Z.8</a>.</li>
</ul><p><span style="float: right;">[<a href="/node/858/edit/43">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="ProjectsForOthers">Projects that others could conduct</h3>
<p>During this investigation, I came to think of progress on the “distrubution of morally relevant consciousness” question as occuring on three “fronts”:</p>
<ol><li>Progress on <em>theories of consciousness</em>: If we can arrive at a convincing theory of consciousness, or at least a convincing theory of <em>human</em> consciousness, then we can apply that theory to the distribution question. This is the most obvious way forward, and the way science usually works.</li>
<li>Progress on our <em>best theory-agnostic guess</em> about the distribution of consciousness: Assuming we are several decades away from having a convincing theory of consciousness, what should our “best theory-agnostic guess” about the distribution question be in the meantime? Should it be derived from something like a better-developed version of the four-factor approach I described <a href="#HighLevel">above</a>? Which other factors should be added, and what is our best guess for the value of each variable in that model? Etc.</li>
<li>Progress on our <em>moral judgments</em> about moral patienthood: How do different judges’ moral intuitions respond to different first-person and third-person cases of possible moral patienthood? Do those intuitions change when people temporarily adopt <a href="#AppendixA">my approach</a>, after engaging in <a href="/blog/efforts-improve-accuracy-our-judgments-and-forecasts">some training for forecasting accuracy</a>? Do we know enough about how moral intuitions vary over time and in different contexts to say much about which moral intuitions we should see as “legitimate,” and which ones we shouldn&#8217;t, when thinking about which beings are moral patients? Etc.</li>
</ol><p>Below are some projects that seem like they&#8217;d be useful, organized by the “front” it is advancing.</p>
<p><span style="float: right;">[<a href="/node/858/edit/44">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="TheoryProjects">Projects related to theories of consciousness</h4>
<ol><li>Personally, I&#8217;m most optimistic about <a href="#illusionism">illusionist</a> theories of consciousness, so it could be useful for current illusionists to gather and discuss how develop their theories further, especially in collaboration with computer programmers, along the lines described <a href="#MoreSatisfying">here</a>.</li>
<li>Of course, it could also be useful to engage in similar projects to better-develop other types of theories.</li>
<li>It could be useful to produce a large reference work on “the explananda of human consciousness.” Each chapter would summarize what is currently known about (self-reported) human conscious experience under various conditions. For example there could be chapters on auto-activation deficit, various sensory agnosias, lucid dreams, absence seizures, pain asymbolia, blindsight, split-brain patients, locked-in syndrome, masking studies on healthy subjects, and many other “natural” or experimentally manipulated conditions. Ideally, each chapter would be co-authored by multiple subject-matter experts, including experts who disagree about the interpretation of the primary studies, and would survey expert disagreement about how to interpret those studies. It might also be best if each chapter explained many of the primary studies in some detail, with frank acknowledgment of their design limitations. This reference work could be updated every 5-10 years, and (I hope) would make it much easier for conscoiusness theorists to understand the full body of evidence with which a theory of human consicousness should be consistent.</li>
<li>It could be useful for a “neutral” party (not an advocate of one of the major theories of consciousness) to summarize each major existing theory of consciousness in a fair but critical way, list the predictions they seem to make (according to the theory as stated, not necessarily according to each theory&#8217;s advocates, and with a focus on predictions for which the “ground truth” is not already known but could be tested by future studies<a class="see-footnote" id="footnoteref207_8re11e1" title="Here is Dennett's account (Dennett 1994) of a prediction he made, using his theory of consciousness, which had not been tested at the time:  &#9;&#9;On the last page of Consciousness Explained, I described an experiment with eye-trackers that had not been done and predicted the result. The experiment has since been done, by John Grimes at the Beckmann Institute in Champaign Urbana [Grimes 1996], and the results were much more powerful than I had dared hope. I had inserted lots of safety nets (I was worried about luminance boundaries and the like — an entirely gratuitous worry as it turns out). Grimes showed subjects high-resolution color photographs on a computer screen and told the subjects to study them carefully, since they would be tested on the details. (The subjects were hence highly motivated, like Betsy, to notice, detect, discriminate, or judge whatever it was they were seeing.) They were also told that there might be a change in the pictures while they were studying them (for ten seconds each). If they ever saw (yes, &quot;saw,&quot; the ordinary word) a change, they were to press the button in front of them — even if they could not say (or judge, or discriminate) what the change was. So the subjects were even alerted to be on the lookout for sudden changes. Then when the experiment began, an eye-tracker monitored their eye movements and during a randomly chosen saccade changed some large and obvious feature in each picture. (Some people think I must be saying that this feature was changed, and then changed back, during the saccade. No. The change was accomplished during the saccade, and the picture remained changed thereafter.) Did the subjects press the button, indicating they had seen a change? Usually not; it depended on how large the change was. Grimes, like me, had expected the effect to be rather weak, so he began with minor, discreet changes in the background. Nobody ever pressed the button, so he began getting more and more outrageous. For instance, in a picture of two cowboys sitting on a bench, Grimes exchanged their heads during the saccade and still, most subjects didn't press the button! In an aerial photograph of a bright blue crater lake, the lake suddenly turned jet black — and half the subjects were oblivious to the change, in spite of the fact that this is a portrait of the lake. (What about the half that did notice the change? They had apparently done what Betsy did when she saw the thimble in the epistemic sense: noted, judged, identified, the lake as blue.)  &#9;&#9;What does this show? It shows that your brain doesn't bother keeping a record of what was flitting across your retinas (or your visual cortex), even for the fraction of a second that elapses from one saccade to the next. So little record is kept that if a major change is made during a saccade — during the changing of the guards, you might say — the difference between the scene thereafter and the scene a fraction of a second earlier, though immense, is typically not just unidentifiable; it is undetectable. The earlier information is just about as evanescent as the image on the wall in the camera obscura. Only details that were epistemically seen trigger the alarm when they are subsequently changed. If we follow Dretske's usage, however, we must nevertheless insist that, for whatever it is worth, the changes in the before and after scenes were not just visible to you; you saw them, though of course you yourself are utterly clueless about what the changes were, or even that there were changes.  &#9;" href="#footnote207_8re11e1">207</a>), and critically examine how well those predictions match the available data. They could also argue for some list of key consciousness explananda (“things to be explained”), and critically examine how thoroughly and precisely each major theory explains those explananda. Ideally, the author(s) of this synthesis would collaborate with both advocates and critics of these theories to help ensure they are interpreting the theories, and the relevant evidence, accurately.</li>
<li>Given my general study quality concerns (see <a href="#AppendixZ8">Appendix Z.8</a>), it could be useful to try to improve study quality standards for the types of studies are used to support theories of (human) consciousness, for example by organizing a conference attended both by experimentalists studying human consciousness and by experts in study robustness / replicability.</li>
</ol><p>For a higher-level overview of scientific work that can contribute to the development of more satisfying theories of consciousness, see <a href="http://philpapers.org/rec/CHAHCW/">Chalmers (2004)</a>.</p>
<p><span style="float: right;">[<a href="/node/858/edit/45">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="GuessingProjects">Projects related to theory-agnostic guesses about the distribution of consciousness</h4>
<ol><li>It could be helpful for researchers to collect counts of neurons (especially pallial neurons) for a much wider variety of species, since “processing power” is probably the cheapest of the four factors contributing to my “theory-agnostic estimation process” to collect additional data on — except perhaps for “evolutionary distance from humans,” which is already <a href="http://www.timetree.org/">widely measured</a>. (Note: I&#8217;d rather not measure brain masses alone, because neuronal scaling rules vary widely across different taxa.<a class="see-footnote" id="footnoteref208_hhjaa2f" title="See Olkowicz et al. (2016).  &#9;" href="#footnote208_hhjaa2f">208</a>)</li>
<li>It could be useful for a group of neuroscientists, ethologists, and other relevant experts to collaborate on a large reference book that collects data about a long list of PCIFs in a wide variety of taxa, organized similar to how <a href="https://jhupbooks.press.jhu.edu/content/animal-tool-behavior">Shumaker et al. (2011)</a> organizes different types of animal tool behavior by taxon.<a class="see-footnote" id="footnoteref209_aoha24p" title="For more detail, see this footnote.  &#9;" href="#footnote209_aoha24p">209</a> Ideally, the book would explain how each PCIF and taxon was chosen and defined, fairly characterize any ongoing expert debates about whether those PCIFs should have been chosen and how they should be defined, and also fairly characterize ongoing expert debates about the absence, presence, or scalar value of each PCIF in each taxon. Besides its contribution to “theory-agnostic guesses” about the distribution question, such a book would also make it easier to construct and critique theories of consciousness, by gathering lots of relevant data across disparate fields into one place.</li>
<li><em>After</em> project (2) is completed, it could be useful for several different consciousness experts to make their own extended arguments about which PCIFs should be considered most strongly consciousness-indicating, and what those conclusions imply about the distribution question.</li>
<li>It could be helpful for someone to write a detailed analysis of the case for and against 3-5 potential (non-obvious and substantive) necessary or sufficient conditions for consciousness, along the lines of (a more thorough version of) my analysis of the case for and against “cortex-required views” <a href="#Cortex">above</a>. Two additional potential necessary conditions that could be examined in this way are (1) language and (2) a bilaterally symmetric nervous system.</li>
<li>It could be useful for someone to conduct a high-response-rate survey of a wide variety of “consciousness experts,” asking a variety of questions about phenomenal consciousness, consciousness-derived moral patienthood, and their guesses about the distribution of each.<a class="see-footnote" id="footnoteref210_eem9sl2" title="I haven't seen many surveys of experts on consciousness, but I'll list some related sources. McDermott (2007) reports the results of an informal survey of Fellows of the American Association for Artificial Intelligence from 2003. Miller (2000) didn't conduct a survey, but claimed that &quot;Almost every member of the American Philosophical Association would agree that all mammals are conscious, and that all conscious experience is of some moral significance.&quot; &#9; &#9;Some authors point to the Cambridge Declaration on Consciousness (2012) as evidence that there is now a scientific consensus that:  &#9;The neural substrates of emotions do not appear to be confined to cortical structures… Systems associated with affect are concentrated in subcortical regions where neural homologies [between humans and animals] abound… The absence of a neocortex does not appear to preclude an organism from experiencing affective states. Convergent evidence indicates that non-human animals have the neuroanatomical, neurochemical, and neurophysiological substrates of conscious states along with the capacity to exhibit intentional behaviors. Consequently, the weight of evidence indicates that humans are not unique in possessing the neurological substrates that generate consciousness. Nonhuman animals, including all mammals and birds, and many other creatures, including octopuses, also possess these neurological substrates.  &#9;However:  &#9; &#9;&#9;The document reads more like a political document than a scientific document. (See e.g. this commentary.) &#9;&#9;As far as I can tell, the declaration was signed by a small number of people, perhaps about 15 people, and thus hardly demonstrates a &quot;scientific consensus.&quot; &#9;&#9;Several of the signers of the declaration have since written scientific papers that seem to treat cortex-required views as a live possibility, e.g. Koch et al. (2016) and Laureys et al. (2015), p. 427. &#9;  &#9;For a much lengthier critique of the Cambridge Declaration on Consciousness, which I only partially agree with and which is posted to a creationism blog, see here.  &#9;" href="#footnote210_eem9sl2">210</a></li>
</ol><p><span style="float: right;">[<a href="/node/858/edit/46">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="PatienthoodProjects">Projects related to moral judgments about moral patienthood</h4>
<ol><li>It could be useful for several people to more-or-less independently try the “extreme effort” version of <a href="#AppendixA">my process for making moral judgments</a>, and publish the detailed results of this exercise for dozens of variations on dozens of cases. Ideally, each report would include a summary table of the author&#8217;s moral judgments with respect to each variation of each case, as in <a href="https://rucore.libraries.rutgers.edu/rutgers-lib/40469/">Beckstead (2013)</a>, chs. 4 &amp; 5.</li>
<li>It could be useful for a programmer to do something similar to my incomplete <em>MESH: Hero</em> exercise <a href="#Hero">here</a>, but with a new program written from scratch, and with many more (increasingly complicated) versions of it coded, and with the source code of every version released publicly. Then, the next step could be to gather various “consciousness experts” and moral philosophers at a conference and, over the course of a couple days, have the programmer walk them through how each (progressively more complex) version of the program works, answering questions as needed, and taking a silent electronic survey after each version is explained, so that the consciousness experts and moral philosophers can indicate for each version of the program whether they think it is “conscious,” whether they consider it a moral patient (assuming functionalism), and why. All survey responses could then be published (after being properly anonymized, as desired) and analyzed in various ways. After the conference participants have had several months to digest these results, a follow-up conference could feature public debates about whether specific versions of the program are moral patients or not, and why — again with the programmer present to answer any questions about exactly how the program works. In the event that no non-panpsychist participants think <em>any</em> version of the program is conscious or a moral patient (assuming functionalism), the project could shift to a focus on collecting detailed reasons and intuitions about <em>why</em> no versions of the program are conscious or have moral status, and what changes would be required to (maybe) make some version of the program that is conscious or has moral status.</li>
</ol><p><span style="float: right;">[<a href="/node/858/edit/47">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="AdditionalProjects">Additional thoughts on useful projects</h4>
<p>One project that doesn&#8217;t fit into the above categorization is that it could be useful for computer scientists try to address the triviality objection to functionalism<a class="see-footnote" id="footnoteref211_w07tqak" title="When put into computational terms, the &quot;triviality objection&quot; is explained by Searle (1992), pp. 208-209, in this way:  …the wall behind my back is right now implementing the Wordstar program, because there is some pattern of molecule movements that is isomorphic with the formal structure of Wordstar. But if the wall is implementing Wordstar, if it is a big enough wall it is implementing any program, including any program implemented in the brain.  Note that if this is a problem, it is seems to be a problem for functionalist accounts of anything, not just consciousness.   I will also quote the description of the problem from Drescher (2006), ch. 2:  …any message can be construed as a substitution-cipher encoding of any other message (of the same length), simply by contriving the appropriate key. The contrivance is easy: just align the corresponding letters of the two messages and for each pair of aligned letters, choose as the corresponding key-number however many alphabet steps are needed to get from the &quot;unencrypted&quot; letter to the &quot;encrypted&quot; letter…  Returning to the subject at hand, the point… is that construing physical events (such as brain activity) as representations (of external things such as flowers, and of other brain events themselves) leaves room for… mischievously creative interpretations…  …For example, we could pick up a random rock and construe it as playing a game of chess. We'd already need to have a detailed description of the series of internal states that a real chess-playing computer goes through in the course of a particular game. Then, we'd just point to as many atoms in the rock as we need and we'd stipulate, for each atom at each moment, that its state at that moment represents a particular constituent state of the chess-playing computer. That is, we'd build a translation table with entries like the following:  If the rock's atoms numbered 458,620,198,259,728 through 458,620,198,570,954 at time t are in such-and-such state (the state they were in fact in at t), that represents the chess-playing computer's transistor number 11,252,664,293 being in thus-and-such state at t (the state it was in fact in at t).  Of course, there'd be no uniformity to our interpretation scheme — the same state, exhibited by different atoms, would have an entirely different &quot;meaning&quot; in each case. Even the same state of the same atom would &quot;mean&quot; entirely different things at different times… And of course, we have no hope of writing down every entry of the mapping table — it's just too huge. Nonetheless, we can speak of the interpretation scheme that the hypothetical table implements…  Here, though, is a problem regarding [consciousness]. The problem is that we could likewise contrive a joke interpretation scheme according to which a rock is conscious. As with the joke chess-machine interpretation, we could (in principle) devise this scheme by taking a conscious entity — say, me — and recording all the states in its brain over a period of several minutes. We then map some portion of the rock onto some part of the brain. And we contrive a mapping function that, at each next moment, just asserts by fiat that the state of a given portion of the rock (the specific placement of individual atoms there, say) represents the next recorded state of the corresponding brain portion's state. Under this joke interpretation scheme, the rock undergoes the same sequence of conscious (and unconscious) thoughts over then next few minutes as I did during the few recorded minutes. Or, we could in principle create a different mapping table that attributes to the rock a series of thoughts and feelings all its own.  Of course, [Dennett's] intentional-stance test easily disqualifies such joke interpretations from being taken seriously (just as with the joke chess-machine interpretation). Still, there is a reason this joke interpretation poses a problem. The intentional stance only tells us what representation (if any) an external observer has reason to ascribe to an object of interest. And as noted above, a certain practicality follows from an intentional-stance-supported interpretation: it lets us predict that the correspondence in question would or will continue to exist under a reasonable range of circumstances. In contrast, it is of no more practical value to contrivedly ascribe a consciousness-implementing set of representations to a rock than it is to contrivedly ascribe a chess-implementing set of representations. In that sense, a rock is clearly no more engaged in conscious experience than it is engaged in chess playing.  But if we are asking whether an object — be it a person or a computer or a rock — is conscious, we are asking (at least in large measure) about the object's own point of view, about how (or whether at all) the object feels to itself, regardless of any external observer's perspective or the practical merits thereof. If… consciousness is just a particular kind of representational process, then why isn't it the case that at least as far as the rock itself is concerned, the rock possesses the same stream of consciousness as I do, by virtue of the (albeit impractical) joke interpretation?  On this topic, see also Putnam (1988), especially the appendix; Chrisley (1994); Godfrey-Smith (2009); Shagrir (2012); section 6 of Aaronson (2013); Chalmers (2011) and the replies cited in Chalmers (2012).  " href="#footnote211_w07tqak">211</a> — which in my view may be the most compelling objection to physicalist functionalism I&#8217;ve seen — via computational complexity theory, as <a href="https://books.google.com/books?id=2OPxCwAAQBAJ&amp;lpg=PA261&amp;ots=5thXpvIs7G&amp;lr=lang_en&amp;pg=PA261#v=onepage&amp;q&amp;f=false">Aaronson (2013)</a> suggests.<a class="see-footnote" id="footnoteref212_hebomkp" title="Aaronson (2013):  …consider a waterfall (though any other physical system with a large enough state space would do as well)… say, Niagara Falls. Being governed by laws of physics, the waterfall implements some mapping f from a set of possible initial states to a set of possible final states. If we accept that the laws of physics are reversible, then f must also be injective. Now suppose we restrict attention to some finite subset S of possible initial states, with |S| = n. Then f is just a one-to-one mapping from S to some output set T = f(S) with |T| = n. The &quot;crucial observation&quot; is now this: given any permutation σ from the set of integers {1, … , n} to itself, there is some way to label the elements of S and T by integers in {1, … , n}, such that we can interpret f as implementing σ. For example, if we let S = {s1, … , si} and f (si) = ti, then it suffices to label the initial state si by i and the final state ti by σ(i). But the permutation σ could have any &quot;semantics&quot; we like: it might represent a program for playing chess, or factoring integers, or simulating a different waterfall. Therefore &quot;mere computation&quot; cannot give rise to semantic meaning.  …To my mind… perhaps the easiest way to demolish the waterfall argument is through computational complexity considerations.  Indeed, suppose we actually wanted to use a waterfall to help us calculate chess moves. How would we do that? In complexity terms, what we want is a reduction from the chess problem to the waterfall-simulation problem. That is, we want an efficient algorithm that somehow encodes a chess position P into an initial state sp ∈ S of the waterfall, in such a way that a good move from P can be read out efficiently from the waterfall's corresponding final state, f(sp ∈ T. But what would such an algorithm look like? We cannot say for sure — certainly not without detailed knowledge about f (i.e., the physics of waterfalls), as well as the means by which the S and T elements are encoded as binary strings. But for any reasonable choice, it seems overwhelmingly likely that any reduction algorithm would just solve the chess problem itself, without using the waterfall in an essential way at all! A bit more precisely, I conjecture that, given any chess-playing algorithm A that accesses a &quot;waterfall oracle&quot; W, there is an equally good chess-playing algorithm A′, with similar time and space requirements, that does not access W. If this conjecture holds, then it gives us a perfectly observer-independent way to formalize our intuition that the &quot;semantics&quot; of waterfalls have nothing to do with chess.  …  In my view, there is an important lesson here for debates about computationalism. Suppose we want to claim, for example, that a computation that plays chess is &quot;equivalent&quot; to some other computation that simulates a waterfall. Then our claim is only non-vacuous if it's possible to exhibit the equivalence (i.e., give the reductions) within a model of computation that isn't itself powerful enough to solve the chess or waterfall problems.  " href="#footnote212_hebomkp">212</a></p>
<p>In addition to the specific projects listed above, basic “field-building” work is likely also valuable.<a class="see-footnote" id="footnoteref213_ncunq4o" title="See also the final section of the notes from my conversation with David Chalmers.  " href="#footnote213_ncunq4o">213</a> We&#8217;ll make faster progress on the likely distribution of phenomenal consciousness if there are a greater number of skilled researchers devoted to the problem than there are today. So far, the topic has been fairly neglected, though several recent books on the topic<a class="see-footnote" id="footnoteref214_hx5l2gk" title="For example Tye (2016), Godfrey-Smith (2016a), and Dennett (2017). (For a condensed account of some of the key theoretical ideas in Godfrey-Smith 2016a, see Godfrey-Smith 2016b.)  " href="#footnote214_hx5l2gk">214</a> may begin to help change that. On the “theory of consciousness” front, illusionist approaches seem especially neglected relative to how promising they seem (to me) to be. Efforts on the distribution question and illusionist approaches to consciousness could be expanded via workshops, conferences, post-doctoral positions, etc.</p>
<p>There are also many projects that I would likely suggest as high-priority if I knew more than I do now. I share Daniel Dennett&#8217;s intuition that perhaps the <em>most</em> promising path forward on the distribution question is to devise a theory focused on <em>human</em> consciousness — because humans are the taxon for which we can get the strongest evidence about consciousness and its character (self-report) — and then “look and see which features of that account apply to animals, and why.”<a class="see-footnote" id="footnoteref215_jsb7r8q" title="Dennett (1995), p. 700. The full quote is:  Lockwood says &quot;probably&quot; all birds are conscious, but maybe some of them — or even all of them — are rather like sleepwalkers! Or what about the idea that there could be unconscious pains (and that animal pain, though real, and — yes — morally important, was unconscious pain)? Maybe there is a certain amount of generous-minded delusion (which I once called the Beatrix Potter syndrome) in our bland mutual assurance that as Lockwood puts it, &quot;Pace Descartes, consciousness, thus construed, isn't remotely, on this planet, the monopoly of human beings.&quot;  How, though, could we ever explore these &quot;maybes&quot;? We could do so in a constructive, anchored way by first devising a theory that concentrated exclusively on human consciousness — the one variety about which we will brook no &quot;maybes&quot; or &quot;probablys&quot; — and then look and see which features of that account apply to which animals, and why. There is plenty of work to do…  " href="#footnote215_jsb7r8q">215</a> According to that approach, much of the most important work to be done on the distribution of consciousness will take the form of consciousness-related experiments conducted on humans. However, I&#8217;m not sure which specific studies I&#8217;d most like to see conducted, because I haven&#8217;t yet taken the time to deeply familiarize myself with the latest studies and methods of human consciousness research.</p>
<p><span style="float: right;">[<a href="/node/858/edit/48">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h2 id="appendices">Appendices</h2>
<p><span style="float: right;">[<a href="/node/858/edit/49">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="AppendixA">Appendix A. Elaborating my moral intuitions</h3>
<p>In this appendix, I describe my process for making moral judgments, and then report the outputs of that process for some particular cases, so as to further explain “where I&#8217;m coming from” on the topic of consciousness and moral patienthood.</p>
<p><span style="float: right;">[<a href="/node/858/edit/50">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="WhichKinds">Which kinds of consciousness-related processes do I morally care about?
</h4><p>Given <a href="#Metaethical">my metaethical approach</a>, when I make a “moral judgment” about something (e.g. about which kinds of beings are moral patients), I don&#8217;t conceive of myself as perceiving an objective moral truth, or coming to know an objective moral truth via a series of arguments. Nor do I conceive of myself as merely expressing my moral feelings as they stand today. Rather, I conceive of myself as making a <em>conditional forecast</em> about what my values <em>would be</em> if I underwent a certain “idealization” or “extrapolation” procedure (coming to know more true facts, having more time to consider moral arguments, etc.).</p>
<p>This metaethical approach begins to look a bit like something worthy of being called “moral realism” if you are optimistic that all members of a certain broad class of moral reasoners would converge on roughly the same values if all of them underwent a similar extrapolation procedure (one that was designed “sensibly” rather than designed merely to ensure convergence).<a class="see-footnote" id="footnoteref216_082gc3w" title="This general approach sometimes goes by names such as &quot;ideal advisor theory&quot; or, arguably, &quot;reflective equilibrium.&quot; Diverse sources explicating various extrapolation procedures (or fragments of extrapolation procedures) include: Rosati (1995); Daniels (2016); Campbell (2013); chapter 9 of Miller (2013); Muehlhauser &amp; Williamson (2013); Trout (2014); Yudkowsky's &quot;Extrapolated volition (normative moral theory)&quot; (2016); Baker (2016); Stanovich (2004), pp. 224-275; Stanovich (2013).   On the prospects for values convergence, see e.g. Sobel (1999); Döring and Andersen's 2009 unpublished manuscript &quot;Rationality, Convergence and Objectivity&quot;; Swanton (1996); the sources listed in footnote 19 of Egan (2012); section 5.1 of Sobel (2001); Dahlsgaard et al. (2005); Pinker (2011); Bicchieri &amp; Mercier (2014); Shermer (2015); Huemer (2016); Sobel (2017).  " href="#footnote216_082gc3w">216</a> I think there would be <em>some</em> values convergence among moral reasoners, but not enough for me to be expect that, say, everyone who reads this report within 5 years of its publication would, upon completing a “sensible” extrapolation procedure, converge on roughly the same values.<a class="see-footnote" id="footnoteref217_pdb4gwh" title="I am even more skeptical, of course, that visiting aliens or future artificial intelligence systems capable of comprehending this report would, upon completing such an extrapolation procedure, converge on the same values.  " href="#footnote217_pdb4gwh">217</a></p>
<p>Hence, in sharing my intuitions about moral patients below, I see no way to escape the limitation that they are merely <em>my</em> moral judgments. Nevertheless, I suspect many readers will feel that they have similar but not identical moral intuitions. Moreover, as mentioned earlier, I think that sharing my intuitions about moral patients is an important part of being clear about “where I&#8217;m coming from” on consciousness, especially since my moral intuitions no doubt affect my preliminary guesses about the distribution of consciousness even if I do not explicitly refer to my moral intuitions in justifying those guesses.</p>
<p><span style="float: right;">[<a href="/node/858/edit/51">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="ExtremeEffort">The “extreme effort” version of my process for making moral judgments</h4>
<p>To provide more detail on my (ideal) process for making moral judgments, I provide below a description of the “extreme effort” version of my process for making moral judgments. However, I should note that I very rarely engage all the time-consuming cognitive operations described below when making moral judgments, and I did <em>not</em> engage all of them when making the moral judgments reported in this appendix. Rather, I made those moral judgments after running a small subset of the processes described below — whichever processes intuitively seemed, in the moment and for each given case, as though they were likely to quickly and noticeably improve my approximation of the “extreme effort” process described below.</p>
<p>I expect most readers will want to skip to the next subsection, and not bother to read the bullet-points summary of my “extreme effort” process for making moral judgments described below. Nevertheless, here it is:</p>
<ul><li>I try to make the scenario I&#8217;m aiming to forecast as concrete as possible, so that my brain is able to treat it as a genuine forecasting challenge, akin to participating in a prediction market or forecasting tournament, rather than as a fantasy about which my brain feels “allowed” to make up whatever story feels nice, or signals my values to others, or achieves something else that isn&#8217;t <em>forecasting accuracy</em>.<a class="see-footnote" id="footnoteref218_ypw7cqh" title="For more on forecasting accuracy, see this blog post. My use of research on the psychological predictors of forecasting accuracy for the purposes of doing moral philosophy is one example of my support for the use of &quot;ameliorative psychology&quot; in philosophical practice — see e.g. Bishop &amp; Trout (2004, 2008).  " href="#footnote218_ypw7cqh">218</a> In my case, I concretize the extrapolation procedure as one involving a large population of copies of me who learn many true facts, consider many moral arguments, and undergo various other experiences, and then collectively advise me about what I should value and why.<a class="see-footnote" id="footnoteref219_5ysegta" title="Specifically, the scenario I try to imagine (and make conditional forecasts about) looks something like this:   In the distant future, I am non-destructively &quot;uploaded.&quot; In other words, my brain and some supporting cells are scanned (non-destructively) at a fine enough spatial and chemical resolution that, when this scan is combined with accurate models of how different cell types carry out their information-processing functions, one can create an executable computer model of my brain that matches my biological brain's input-output behavior almost exactly. This whole brain emulation (&quot;em&quot;) is then connected to a virtual world: computed inputs are fed to the em's (now virtual) signal transduction neurons for sight, sound, etc., and computed outputs from the em's virtual arm movements, speech, etc. are received by the virtual world, which computes appropriate changes to the virtual world in response. (I don't think anything remotely like this will ever happen, but as far as I know it is a physically possible world that can be described in some detail; for one attempt, see Hanson 2016.) Given functionalism, this &quot;em&quot; has the same memories, personality, and conscious experience that I have, though it experiences quite a shock when it awakens to a virtual world that might look and feel somewhat different from the &quot;real&quot; world. This initial em is copied thousands of times. Some of the copies interact inside the same virtual world, other copies are placed inside isolated virtual worlds. Then, these ems spend a very long time (a) collecting and generating arguments and evidence about morality and related topics, (b) undergoing various experiences, in varying orders, and reflecting on those experiences, (c) dialoguing with ems sourced from other biological humans who have different values than I do, and perhaps with sophisticated chat-bots meant to simulate the plausible reasoning of other types of people (from the past, or from other worlds) who were not available to be uploaded, and so on. They are able to do these things for a very long time because they and their virtual worlds are run at speeds thousands of times faster than my biological brain runs, allowing subjective eons to pass in mere months of &quot;objective&quot; time. Finally, at some time, the ems dialogue with each other about which values seem &quot;best,&quot; they engage in moral trade (Ord 2015), and they try to explain to me what values they think I should have and why. In the end, I am not forced to accept any of the values they then hold (collectively or individually), but I am able to come to much better-informed moral judgments than I could have without their input.   For more context on this sort of values extrapolation procedure, see Muehlhauser &amp; Williamson (2013).  " href="#footnote219_5ysegta">219</a></li>
<li>However, I also try to make forecasts I can actually check for accuracy, e.g. about what my moral judgment about various cases will be 2 months in the future.</li>
<li>When making these forecasts, I try to draw on the best research I&#8217;ve seen concerning how to make accurate estimates and forecasts. For example I try to “think like a fox, not like a hedgehog,” and I&#8217;ve engaged in several hours of probability calibration training, and some amount of forecasting training.<a class="see-footnote" id="footnoteref220_zqqyoi6" title="For more on forecasting &quot;best practices,&quot; see this blog post.  " href="#footnote220_zqqyoi6">220</a></li>
<li>Clearly, my current moral intuitions serve as one important source of evidence about what my extrapolated values might be. However, recent findings in moral psychology and related fields lead me to assign more evidential weight to some moral intuitions than to others. More generally, I interpret my current moral intuitions as data generated partly by my moral principles and partly by various “error processes” (e.g. a hard-wired disgust reaction to spiders, which I don&#8217;t endorse upon reflection). Doing so allows me to make use of some standard lessons from statistical curve-fitting when thinking about how much evidential weight to assign to particular moral intuitions.<a class="see-footnote" id="footnoteref221_raq9er2" title="Following Hanson (2002) and ch. 2 of Beckstead (2013), I consider my moral intuitions in the context of Bayesian curve-fitting. To explain, I'll quote Beckstead (2013) at some length:  Curve fitting is a problem frequently discussed in the philosophy of science. In the standard presentation, a scientist is given some data points, usually with an independent variable and a dependent variable, and is asked to predict the values of the dependent variable given other values of the independent variable. Typically, the data points are observations, such as &quot;measured height&quot; on a scale or &quot;reported income&quot; on a survey, rather than true values, such as height or income. Thus, in making predictions about additional data points, the scientist has to account for the possibility of error in the observations. By an error process I mean anything that makes the observed values of the data points differ from their true values. Error processes could arise from a faulty scale, failures of memory on the part of survey participants, bias on the part of the experimenter, or any number of other sources. While some treatments of this problem focus on predicting observations (such as measured height), I'm going to focus on predicting the true values (such as true height).  …For any consistent data set, it is possible to construct a curve that fits the data exactly… If the scientist chooses one of these polynomial curves for predictive purposes, the result will usually be overfitting, and the scientist will make worse predictions than he would have if he had chosen a curve that did not fit the data as well, but had other virtues, such as a straight line. On the other hand, always going with the simplest curve and giving no weight to the data leads to underfitting…  I intend to carry over our thinking about curve fitting in science to reflective equilibrium in moral philosophy, so I should note immediately that curve fitting is not limited to the case of two variables. When we must understand relationships between multiple variables, we can turn to multiple-dimensional spaces and fit planes (or hyperplanes) to our data points. Different axes might correspond to different considerations which seem relevant (such as total well-being, equality, number of people, fairness, etc.), and another axis could correspond to the value of the alternative, which we can assume is a function of the relevant considerations. Direct Bayesian updating on such data points would be impractical, but the philosophical issues will not be affected by these difficulties.  …On a Bayesian approach to this problem, the scientist would consider a number of different hypotheses about the relationship between the two variables, including both hypotheses about the phenomena (the relationship between X and Y) and hypotheses about the error process (the relationship between observed values of Y and true values of Y) that produces the observations…  …Lessons from the Bayesian approach to curve fitting apply to moral philosophy. Our moral intuitions are the data, and there are error processes that make our moral intuitions deviate from the truth. The complete moral theories under consideration are the hypotheses about the phenomena. (Here, I use &quot;theory&quot; broadly to include any complete set of possibilities about the moral truth. My use of the word &quot;theory&quot; does not assume that the truth about morality is simple, systematic, and neat rather than complex, circumstantial, and messy.) If we expect the error processes to be widespread and significant, we must rely on our priors more. If we expect the error processes to be, in addition, biased and correlated, then we will have to rely significantly on our priors even when we have a lot of intuitive data.  Beckstead then summarizes the framework with the following table (p. 32):     Science Moral Philosophy   Hypotheses about phenomena Different trajectories of a ball that has been dropped Moral theories (specific versions of utilitarianism, Kantianism, contractulaism, pluralistic deontology, etc.)   Hypotheses about error processes Our position measurements are accurate on average, and are within 1 inch 95% of the time (with normally distributed error) Different hypotheses about the causes of error in historical cases; cognitive and moral biases; different hypotheses about the biases that cause inconsistent judgments in important philosophical cases   Observations Recorded position of a ball at different times recorded with a certain clock Intuitions about particular cases or general principles, and any other relevant observations   Background theory The ball never bounces higher than the height it started at. The ball always moves along a continuous trajectory. Meta-ethical or normative background theory (or theories)    " href="#footnote221_raq9er2">221</a></li>
<li>As part of forecasting what my extrapolated values might be, I like to consider different processes and contexts that could generate alternate moral intuitions in moral reasoners both similar and dissimilar to my current self, and consider how I feel about the the “legitimacy” of those mechanisms as producers of moral intuitions. For example I ask myself questions such as “How might I feel about that practice if I was born into a world in which it was already commonplace?” and “How might I feel about that case if my built-in (and largely unconscious) processes for associative learning and imitative learning had been exposed to different life histories than my own?” and “How might I feel about that case if I had been born in a different century, or a different country, or with a greater propensity for clinical depression?” and “How might a moral reasoner on another planet feel about that case if it belonged to a more strongly <a href="https://en.wikipedia.org/wiki/R/K_selection_theory">r-selected species</a> (compared to humans) but had roughly human-like general reasoning ability?”<a class="see-footnote" id="footnoteref222_le00uu6" title="For more on this, see my conversation with Carl Shulman, O'Neill (2015), the literature on the evolution of moral values (e.g. de Waal et al. 2014; Sinnott-Armstrong &amp; Miller 2007; Joyce 2005), the literature on moral psychology (e.g. Doris 2010; Liao 2016; Christen et al. 2014; Sunstein 2005), the literature on how moral values vary between cultures and eras (e.g. see Flanagan 2016; Morris 2015; Friedman 2005; Prinz 2007, pp. 187-195), and the literature on moral thought experiments (e.g. Tittle 2004, ch. 7). See also Wilson (2016)'s comments on internal and external validity in ethical thought experiments, and Bakker (2017) on &quot;alien philosophy.&quot;  I do not read much fiction, but I suspect that some types of fiction — e.g. historical fiction, fantasy, and science fiction — can help readers to temporarily transport themselves into fully-realized alternate realities, in which readers can test how their moral intuitions differ when they are temporarily &quot;lost&quot; in an alternate world.  " href="#footnote222_le00uu6">222</a></li>
<li>Observable patterns in how people&#8217;s values change (seemingly) in response to components of my proposed extrapolation procedure (learning more facts, considering moral arguments, etc.) serve as another source of evidence about my extrapolated values. For example, the correlation between aggregate human knowledge and our “expanding circle of moral concern” (<a href="http://press.princeton.edu/titles/9434.html">Singer 2011</a>) might (very weakly) suggest that, if I continued to learn more true facts, my circle of moral concern would continue to expand. Unfortunately, such correlations are badly confounded, and might not provide much evidence at all with respect to my extrapolated values.<a class="see-footnote" id="footnoteref223_xcdq1ww" title="There are many sources which discuss how people's values seem to change along with (and perhaps in response to) components of my proposed extrapolation procedure, such as learning more facts, reasoning through more moral arguments, and dialoguing with others who have different values. See e.g. Inglehart &amp; Welzel (2010), Pinker (2011), Shermer (2015), and Buchanan &amp; Powell (2016).  " href="#footnote223_xcdq1ww">223</a></li>
<li>Personal facts about how my own values have evolved as I&#8217;ve learned more, considered moral arguments, and so on, serve as yet another source of evidence about my extrapolated values. Of course, these relations are likely confounded as well, and need to be interpreted with care.<a class="see-footnote" id="footnoteref224_8gwwydj" title="For example, as I've learned more, considered more moral arguments, and dialogued more with people who don't share my values, my moral values have become more &quot;secular-rational&quot; and &quot;self-expressive&quot; (Inglehart &amp; Welzel 2010), more geographically global, more extensive (e.g. throughout more of the animal kingdom), less person-affecting, and subject to greater moral uncertainty (MacAskill 2014).  " href="#footnote224_8gwwydj">224</a></li>
</ul><p><span style="float: right;">[<a href="/node/858/edit/52">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="MoralJudgments">My moral judgments about some particular cases</h4>
<p><span style="float: right;">[<a href="/node/858/edit/53">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="FirstPerson">My moral judgments about some first-person cases
</h5><p>What, then, do my moral intuitions say about some specific cases? I&#8217;ll start with some “first-person” cases that involve my own internal experiences. The next section will discuss some “third-person” cases, which I can only judge “from the outside,” by guessing about what those algorithms might feel like “from the inside.”</p>
<p>The starting point for my moral intuitions is my own phenomenal experience. The reason I don&#8217;t want others to suffer is that I know what it feels like when I cut my hand, or when I feel sad, or when my goals are thwarted, and I don&#8217;t want others to have experiences like that. Likewise, the reason I want others to flourish is that I know what it feels when I taste chocolate ice cream, or when I feel euphoric, or when I achieve my goals, and I <em>do</em> want others to have experiences like <em>that</em>.</p>
<p>What if I am injured, or my goals are thwarted, but I don&#8217;t have a subjective experience of that? <a href="#Defined">Earlier</a>, I gave the example of injuring myself while playing sports, but not noticing my injury (and its attendant pain) until 5 seconds after the injury occurred, when I exited my <a href="https://en.wikipedia.org/wiki/Flow_(psychology)">flow state</a>.</p>
<p>For example, when I played soccer and basketball as a teenager, I occasionally twisted my ankle or otherwise acquired a minor injury while chasing after the ball, and I didn&#8217;t realize I had hurt myself until after the play ended and I exited my <a href="https://en.wikipedia.org/wiki/Flow_(psychology)">flow state</a>. Had such a moment been caught on video, I suspect the video would show that I had been unconsciously favoring my hurt ankle while I continued to chase after the ball, even before I realized I was injured, and before I experienced any pain. So, what if a fish&#8217;s experience of nociception is like my “experience” of nociception <em>before</em> exiting the flow state?<a class="see-footnote" id="footnoteref225_dlfumws" title="Or, as Allen-Hermanson (2008) might put it: what if fishes are &quot;natural zombies,&quot; or &quot;naturally blindsighted&quot; about all their sensory and internal states?  " href="#footnote225_dlfumws">225</a> If that&#8217;s how it works, then I&#8217;m not sure I care about such fish “experiences,” for the same reason I don&#8217;t care about <em>my own</em> “experience” of nociception before I exited the flow state. (Of course, I care about the conscious pain that came <em>after</em>, and I care about the conscious experience of sadness at having to sit out the rest of the game as a result of my injury, but I don&#8217;t think I care about whatever nociception-related “experience” I had <em>during the 5 seconds before I exited the flow state</em>.)</p>
<p>Next, what if I was conscious, but there was no positive or negative “valence” to any part of my conscious experience? Suppose I was consciously aware of nociceptive signals, but they didn&#8217;t bother me at all, as pain asymbolics report.<a class="see-footnote" id="footnoteref226_rek4jj3" title="See Grahek (2007).  " href="#footnote226_rek4jj3">226</a> Suppose I was similarly aware of sensations that would normally be “positive,” but I didn&#8217;t experience them as either positive or negative, but rather experienced them as I experience neutral touch, for example how it feels when my fingers tap away at my keyboard as I write this sentence. Moreover, suppose I had goals, and I had the conscious experience of making plans that I predict would achieve those goals, and I consciously knew when I had achieved or not-achieved those goals, but I didn&#8217;t emotionally <em>care</em> whether I achieved them or not, I didn&#8217;t feel any happiness or disappointment upon achieving or not-achieving them, and so on. Would I consider such a conscious existence to have moral value? Here again, I&#8217;m unsure, but my guess is that I wouldn&#8217;t consider such conscious existence to have moral value. If fishes are conscious, but the character of their conscious experience is like this, then I&#8217;m not sure I care about fishes. (Keep in mind this is just an illustration: if fishes are conscious at all, then my guess is that they experience at least some nociceiption as unpleasant pain rather than as an unbothersome signal like the pain asymbolic does.)</p>
<p>This last example is similar to a thought experiment invented by Peter Carruthers, which I consider next.</p>
<p><span style="float: right;">[<a href="/node/858/edit/54">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="Phenumb">The case of Phenumb</h5>
<p><a href="http://www.tandfonline.com/doi/abs/10.1080/00048409912349231">Carruthers (1999)</a> presents an interesting intuition pump concerning consciousness and moral patienthood:</p>
<blockquote><p>Let us imagine, then, an example of a conscious, language-using, agent — I call him ‘Phenumb’ who is unusual only in that satisfactions and frustrations of his conscious desires take place without the normal sorts of distinctive phenomenology. So when he achieves a goal he does not experience any warm glow of success, or any feelings of satisfaction. And when he believes that he has failed to achieve a goal, he does not experience any pangs of regret or feelings of depression. Nevertheless, Phenumb has the full range of attitudes characteristic of conscious desire-achievement and desire-frustration. So when Phenumb achieves a goal he often comes to have the conscious belief that his desire has been satisfied, and he knows that the desire itself has been extinguished; moreover, he often believes (and asserts) that it was worthwhile for him to attempt to achieve that goal, and that the goal was a valuable one to have obtained. Similarly, when Phenumb fails to achieve a goal he often comes to believe that his desire has been frustrated, while he knows that the desire itself continues to exist (now in the form of a wish); and he often believes (and asserts) that it would have been worthwhile to achieve that goal, and that something valuable to him has now failed to come about.</p>
<p>Notice that Phenumb is not (or need not be) a zombie. That is, he need not be entirely lacking in phenomenal consciousness. On the contrary, his visual, auditory, and other experiences can have just the same phenomenological richness as our own; and his pains, too, can have felt qualities. What he lacks are just the phenomenal feelings associated with the satisfaction and frustration of desire. Perhaps this is because he is unable to perceive the effects of changed adrenaline levels on his nervous system, or something of the sort.</p>
<p>Is Phenumb an appropriate object of moral concern? I think it is obvious that he is. While it may be hard to imagine what it is like to be Phenumb, we have no difficulty identifying his goals and values, or in determining which of his projects are most important to him — after all, we can ask him! When Phenumb has been struggling to achieve a goal and fails, it seems appropriate to feel sympathy: not for what he now feels — since by hypothesis he feels nothing, or nothing relevant to sympathy — but rather for the intentional state which he now occupies, of dissatisfied desire. Similarly, when Phenumb is engaged in some project which he cannot complete alone, and begs our help, it seems appropriate that we should feel some impulse to assist him: not in order that he might experience any feeling of satisfaction — for we know by hypothesis that he will feel none — but simply that he might achieve a goal which is of importance to him. What the example reveals is that the psychological harmfulness of desire-frustration has nothing (or not much — see the next paragraph) to do with phenomenology, and everything (or almost everything) to do with thwarted agency.</p>
<p>The qualifications just expressed are necessary, because feelings of satisfaction are themselves often welcomed, and feelings of dissatisfaction are themselves usually unwanted. Since the feelings associated with desire-frustration are themselves usually unpleasant, there will, so to speak, be more desire-frustration taking place in a normal person than in Phenumb in any given case. For the normal person will have had frustrated both their world-directed desire and their desire for the absence of unpleasant feelings of dissatisfaction. But it remains true that the most basic, most fundamental, way in which desire-frustration is bad for, or harmful to, the agent has nothing to do with phenomenology.</p></blockquote>
<p>My initial intuitions agree with Carruthers, but upon <a href="#AppendixA">reflection</a>, I lean toward thinking that Phenumb is <em>not</em> a moral patient (at least, not via the character of his <em>consciousness</em>), so long as he does not have any sort of “valenced” or “affective” experiences. (Phenumb might, of course, be a moral patient via <a href="#ProposedCriteria">other criteria</a>.)</p>
<p>Carruthers suggests a reason why some people (like me) might have a different moral intuition about this case than he does:</p>
<blockquote><p>What emerges from the discussions of this paper is that we may easily fall prey to a cognitive illusion when considering the question of the harmfulness to an agent of non-conscious frustrations of desire. In fact, it is essentially the same cognitive illusion which makes it difficult for people to accept an account of mental-state consciousness which withholds conscious mental states from non-human animals. In both cases the illusion arises because we cannot consciously imagine a mental state which is unconscious and lacking any phenomenology. When we imagine the mental states of non-human animals we are necessarily led to imagine states which are phenomenological; this leads us to assert… that if non-human animals have any mental states at all…, then their mental states must be phenomenological ones. In the same way, when we try to allow the thought of non-phenomenological frustrations of desire to engage our sympathy we initially fail, precisely because any state which we can imagine, to form the content of the sympathy, is necessarily phenomenological; this leads us… to assert that if non-human animals do have only non-conscious mental states, then their states must be lacking in moral significance.</p>
<p>In both cases what goes wrong is that we mistake what is an essential feature of (conscious) imagination for something else — an essential feature of its objects, in the one case (hence claiming that animal mental states must be phenomenological); or for a necessary condition of the appropriateness of activities which normally employ imagination, in the other case (hence claiming that sympathy for non-conscious frustrations is necessarily inappropriate). Once these illusions have been eradicated, we see that there is nothing to stand in the way of the belief that the mental states of non-human animals are non-conscious ones, lacking in phenomenology. And we see that this conclusion is perfectly consistent with according full moral standing to the [non-conscious, according to Carruthers] sufferings and disappointments of non-human animals.</p></blockquote>
<p>It is interesting to consider the similarities between Carruthers’ fictional Phenumb and the real-life cases of auto-activation deficit (AAD) described in <a href="#FuzzinessAAD">Appendix Z.4</a>. These patients are (as far as we can tell) phenomenally conscious like normal humans are, but — at least during the period of time when their AAD symptoms are most acute — they report having approximately no affect or motivation about anything. For example, one patient “spent many days doing nothing, without initiative or motivation, but without getting bored. The patient described this state as “a blank in my mind’&#8201;” (<a href="http://jnnp.bmj.com/content/47/4/377.short">Laplane et al. 1984</a>).</p>
<p>Several case reports (see <a href="#FuzzinessAAD">Appendix Z.4</a>) describe AAD patients as being capable of playing games if prompted to do so. Suppose we could observe an AAD patient named Joan, an avid chess player. Next, suppose we prompted her to play a game of chess, waited until some point in the midgame, and then asked her why she had made her latest move. To pick a dramatic example, suppose her latest move was to take the opponent&#8217;s Queen with her Rook. Given the case reports I&#8217;ve read, it sounds as though Joan might very well be able (like Phenumb) to explain why her latest move was instrumentally useful for the goal of checkmating the opponent&#8217;s King. Moreover, she might be able to explain that, of course, her goal at the moment is to checkmate the opponent&#8217;s King, because that is the win condition for a chess game. But, if asked if she felt (to use Carruthers’ phrase) “a warm glow of success” as a result of taking the opponent&#8217;s Queen, it sounds (from the case reports) as though Joan would say she did not feel any such thing.<a class="see-footnote" id="footnoteref227_56yq829" title="It would be interesting to test my hypothesis on several subjects with AAD, for example the 13 patients of Leu-Semenescu et al. (2013), if they are still alive.  " href="#footnote227_56yq829">227</a></p>
<p>Or, suppose Joan had <em>her</em> Queen taken by the opponent&#8217;s Rook. If asked, perhaps she could report that this event reduced her chances of checkmating the opponent&#8217;s King, and that her goal (for the moment) was still to checkmate the opponent&#8217;s King. But, based on the AAD case reports I&#8217;ve seen, it seems that she would probably report that she felt no affective pang of disappointment or regret at the fact that her Queen had just been captured. Has anything morally negative happened to Joan, not counting her earlier and presumably very morally bad transition from neurotypical function to a condition of AAD? My intuitions say “no,” but perhaps Carruthers’ intuitions would say “yes.”</p>
<p>So as to more closely match Joan&#8217;s characteristics to Phenumb&#8217;s, we might also stipulate that Joan is a pain asymbolic (<a href="https://mitpress.mit.edu/books/feeling-pain-and-being-pain">Grahek 2007</a>) and also, let&#8217;s say, a “pleasure asymbolic.” Further, let&#8217;s stipulate that we can be absolutely certain Joan cannot recover from her conditions of AAD, pain asymbolia, and pleasure asymbolia. Is there now a moral good realized when Joan, say, wins a chess game or accomplishes some other goal? Part of me wants to say “Yes, of course! She has goals and aversions, and she can talk to you about them.” But upon further reflection, I&#8217;m not sure I should endorse those empathic impulses in the very strange case of Joan, and I&#8217;m not so sure I should think that moral good or harm is realized when Joan&#8217;s goals are realized or frustrated — though, it does seem clear that something very bad happened to her when she (let&#8217;s suppose) acquired some brain damage resulting in AAD, pain asymbolia, and pleasure asymbolia. </p>
<p><span style="float: right;">[<a href="/node/858/edit/55">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="ThirdPerson">My moral judgments about some third-person cases
</h5><p>It is difficult to state my moral intuitions about whether specific (brained) animals are moral patients or not, because <em>I don&#8217;t know what their brains are doing</em>. Neuroscientists know many things about how individual neurons work, and they are starting to learn a few things about how certain small populations of neurons work, and they can make some observations about how the brain works at the “macro” scale (e.g. via fMRI), but they don&#8217;t yet know <em>which particular algorithms</em> brains use to accomplish their tasks.<a class="see-footnote" id="footnoteref228_yntutgh" title="In other words, neuroscientists don't yet know much about what David Marr called the &quot;algorithmic level&quot; (Wikipedia).  Here is the explanation of Marr's levels of analysis from Bermudez (2014), p. 47:  Marr distinguishes three different levels for analyzing cognitive systems. The highest is the computational level. Here cognitive scientists analyze in very general terms the particular type of task that the system performs…  The guiding assumption here is that cognition is ultimately to be understood in terms of information processing, so that the job of individual cognitive systems is to transform one kind of information (say, the information coming into a cognitive system through its sensory systems) into another type of information (say, information about what type of objects there might be in the organism's immediate environment). A computational analysis identifies the information with which the cognitive system has to begin (the input to that system) and the information with which it needs to end up (the output from that system).  The next level down is what Marr calls the algorithmic level. The algorithmic level tells us how the cognitive system actually solves the specific information- processing task identified at the computational level. It tells us how the input information is transformed into the output information. It does this by giving algorithms that effect that transformation. An algorithmic level explanation takes the form of specifying detailed sets of information-processing instructions that will explain how, for example, information from the sensory systems about the distribution of light in the visual field is transformed into a representation of the three-dimensional environment around the perceiver.  In contrast, the principal task at the implementational level is to find a physical realization for the algorithm – that is to say, to identify physical structures that will realize the representational states over which the algorithm is defined and to find mechanisms at the neural level that can properly be described as computing the algorithm in question.  For a nice illustration of some reasons why it's so difficult for neuroscientists to study brain function at the algorithmic level given current tools, see Jonas &amp; Kording (2017).  " href="#footnote228_yntutgh">228</a></p>
<p>Hence, it is easier to state my moral intuitions about <em>computer programs</em>, especially when I have access to their source code, or at least have a rough sense of how they were coded. (As a <a href="#Nature">functionalist</a>, I believe that the right kind of computer program would be conscious, regardless of whether it was implemented via a brain or brain-like structure or implemented some other way.) In the course of reporting some of my moral intuitions, I will also try to illustrate the problematic vagueness of psychological terms (more on this below).</p>
<p>For example, consider the short program below, written in <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> (version 3).<a class="see-footnote" id="footnoteref229_hjrkux7" title="To run my Python code without needing to install any software, you can use an online Python sandbox such as this one from Tutorials Point. Or, you can install a full-featured Python IDE such as PyCharm (the Community edition is free).  " href="#footnote229_hjrkux7">229</a> My hope is that even non-programmers will be able to understand what the code below does, especially with the help of my comments. (In Python code, any text following a <code>#</code> symbol is a “comment,” which means it is there to be read by human readers of the source code, and is completely ignored by the interpreter or compiler program that translates the human-readable source code into bytecode for the computer to run. Thus, comments do not affect how the program runs.)</p>
<p>You may need to scroll horizontally to read all of the source code.</p>
<pre><code class="language-python"># increment_my_pain.py
my_pain = 0  	# Create a variable called my_pain, store the value 0 in it.
while True:  	# Execute the code below, in a continuous loop, forever.
	my_pain += 1  	# Increment my_pain by 1.
	print("My current pain level is " + str(my_pain) + ".")  	# Print current value of my_pain.
</code></pre><p>
If you compile and run this source code, it will continously increment the value of <code>my_pain</code> by 1, and print the value of <code>my_pain</code> to the screen after each increment, like this:</p>
<pre><code>My current pain level is 1.
My current pain level is 2.
My current pain level is 3.
</code></pre><p>
…and so on, until you kill the process, the computer runs out of memory and crashes, or the process hits some safeguard built into the browser or operating system from which you are running the program.</p>
<p>My moral intuitions are such that I do not care about this program, at all. This program does not experience pain. It does not “experience” anything. There is nothing that it “feels like” to be this program, running on my computer. It has no “phenomenal consciousness.”</p>
<p>To further illustrate why I don&#8217;t care about this program, consider running the following program instead:</p>
<pre><code class="language-python"># increment_my_pleasure.py
my_pleasure = 0
while True:
	my_pleasure += 1
	print("My current pleasure level is " + str(my_pleasure) + ".")
</code></pre><p>
Is the moral value of this program any different than that of <code>increment_my_pain.py</code>? I think not. The compiler doesn&#8217;t know what English speakers mean when we use the strings of letters “pleasure” and “pain.” In fact, if I didn&#8217;t hard-code the words “pleasure” and “pain” into the printing string of each program, the compiler would transform <code>increment_my_pain.py</code> and <code>increment_my_pleasure.py</code> into the <em>exact same bytecode</em>, which will run exactly the same on the same virtual machine.<a class="see-footnote" id="footnoteref230_9jppn2i" title="Python implementations vary; see here.  " href="#footnote230_9jppn2i">230</a></p>
<p>The same points hold true for a similar program using a nonsensical variable name:</p>
<pre><code class="language-python"># increment_flibbertygibbets.py
flibbertygibbets = 0
while True:
	flibbertygibbets += 1
	print("My current count of flibbertygibbets is " + str(flibbertygibbets) + ".")
</code></pre><p>
While this simple illustration is fairly uninformative and (I hope) uncontroversial, I do think that testing one&#8217;s moral intuitions against snippets of source code — or, against existing programs for which you have some idea of how they work — is a useful way to make progress on the questions of moral patienthood.<a class="see-footnote" id="footnoteref231_xbbr7z9" title="In general, I think lots of philosophical discussion and argument should be conducted using short and long snippets of source code, to improve the clarity and concreteness of those discusions. Steven Phillips calls this approach to philosophy &quot;executable philosophy&quot; (see also this incomplete draft of his thoughts on the subject). See also Yudkowsky's &quot;Executable Philosophy,&quot; which includes a similar recommendation about philosophical practice on a list which includes several other recommendations. (As far as I know, Phillips and Yudkowsky use this term independently of each other.)  One example of this approach being used in philosophy of consciousness is Brian Tomasik's &quot;A Simple Program to Illustrate the Hard Problem of Consciousness.&quot; To make his &quot;simple program&quot; more comprehensible to people who are not Python programmers, I added extensive comments to his code (and bumped the syntax to Python Version 3): see here.  For related but not identical ideas about philosophical methodology, see discussions on computational explanations and computational models in philosophy, e.g. Grim (2004); Rusanen &amp; Lappi (2016).  " href="#footnote231_xbbr7z9">231</a> Most discussions of the criteria for moral patienthood use vague psychological language such as “goals” or “experience,” which can be interpreted in many different ways. In contrast, computer code is precise.</p>
<p>To illustrate how problematic vague psychological language can be when discussing theories of consciousness and moral patienthood, I consider below how some computer programs could be said to qualify as conscious on some (perhaps not very charitable) interpretations of vague terms like “goals.”<a class="see-footnote" id="footnoteref232_5q8s5re" title="This &quot;short program argument&quot; is a generalization of Herzog et al. (2007)'s &quot;small network argument.&quot; It is also similar to some remarks in Rey (1983):  …it seems to me to be entirely feasible… to render an existing computing machine intentional by providing it with a program that would include the following:  1. The alphabet, formation, and transformation rules for quantified modal logic (the system's &quot;language of thought&quot;).  2. The axioms for your favorite inductive logic and/or abductive system of hypotheses, with a &quot;reasonable&quot; function for selecting among them on the basis of given input.  3. The axioms of your favorite decision theory, and some set of basic preferences.  4. Mechanical inputs, via sensory transducers, for Clauses 2 and 3.  5. Mechanical connections that permit the machine to realize its outputs (e.g., its &quot;most preferred&quot; basic act descriptions).  Any computer that functioned according to such a program would, I submit, realize significant Rational Regularities, complete with intensionality. Notice, for example, that it would be entirely appropriate — and probably unavoidable — for us to explain and predict its behavior and internal states on the basis of those regularities. It would be entirely reasonable, that is to say, for us to adopt toward it what Dennett (1971) has called the &quot;intentional stance.&quot;  …However clever a machine programmed with Clauses 1-5 might become, counting thereby as a thinking thing, surely it would not also count thereby as conscious. The program is just far too trivial. Moreover, we are already familiar with systems satisfying at least Clauses 1-5 that we also emphatically deny are conscious: there are all those unconscious neurotic systems postulated in so many of us by Freud, and all those surprisingly intelligent, but still unconscious, subsystems for perception and language postulated in us by contemporary cognitive psychology. (Some evidence of the cognitive richness of unconscious processing is provided by the interesting review of such material in Nisbett &amp; Wilson, 1977, but especially by such psycholinguistic experiments as that by Lackner &amp; Garrett, 1973, in which subliminal linguistic material provided to one ear biased subjects in their understanding of ambiguous sentences provided to the other ear.) In all of these cases we are, I submit, quite reasonably led to ascribe beliefs, preferences, and sometimes highly elaborate thought processes to a system on the basis of the Rational Regularities, despite the fact that the systems involved are often not the least bit &quot;conscious&quot; of any such mental activity at all. It is impossible to imagine these psychological theories getting anywhere without the ascription of unconscious content — and it is equally difficult to imagine any animals getting anywhere without the exploitation of it. Whatever consciousness will turn out to be, it will pretty certainly need to be distinguished from the thought processes we ascribe on the basis of the rational regularities.  How easily this point can be forgotten, neglected, or missed altogether is evidenced by the sorts of proposals about the nature of consciousness one finds in some of the recent psychobiological literature. The following seem to be representative:  Consciousness is usually defined by the ability: (1) to appreciate sensory information; (2) to react critically to it with thoughts or movements; (3) to permit the accumulation of memory traces. (Moruzzi, 1966)  Perceptions, memories, anticipatory organization, a combination of these factors into learning — all imply rudimentary consciousness. (Knapp, 1976)  Modern views… regard human conscious activity as consisting of a number of major components. These include the reception and processing (recoding) of information, with the selection of its most important elements and retention of the experience thus gained in the memory; enunciation of the task or formulation of an intention, with the preservation of the corresponding modes of activity, the creation of a pattern or model of the required action, and production of the appropriate program (plan) to control the selection of necessary actions; and finally the comparison of the results of the action with the original intention … with correction of the mistakes made. (Luria, 1978)  Consciousness is a process in which information about multiple individual modalities of sensation and perception is combined into a unified, multidimensional representation of the state of the system and its environment and is integrated with information about memories and the needs of the organism, generating emotional reactions and programs of behavior to adjust the organism to its environment. (John, 1976)  What I find astonishing about such proposals is that they are all more-or-less satisfiable by almost any information-processing system, for precisely what modern computational machinery is designed to do is to receive, process, unify, and retain information; create (or &quot;call&quot;) patterns, models, and subroutines to control its activity; and, by all means, to compare the results of its action with its original intention in order to adjust its behavior to its environment. This latter process is exactly what the &quot;feedback&quot; that Wiener (1954) built into his homing rocket was for! Certainly, most of the descriptions in these proposals are satisfied by any recent game-playing program (see, e.g., Berliner, 1980). And if it's genuine &quot;modalities,&quot; &quot;thoughts,&quot; &quot;intentions,&quot; &quot;perceptions,&quot; or &quot;representations&quot; that are wanted, then, as I've argued, supplementing the program with Clauses 1-5 will suffice, but without rendering anything a whit more conscious.  White (1991), ch. 6, summed up Rey's point like so:  …a survey of recent characterizations of consciousness by philosophers and psychologists reveals that most or all characterizations would be satisfied by information-processing devices that either exist now or would be trivial extensions of devices that exist.  See also Rey (1995; 2016).  " href="#footnote232_5q8s5re">232</a> (Hereafter on this point, I&#8217;ll just say “moral patienthood,” since it is a common view, and the one temporarily assumed for this report, that consciousness is sufficient for moral patienthood.)</p>
<p>I don&#8217;t know whether advocates of these theories would agree that the programs I point to below satisfy their verbal description of their favorite theory. My guess is that in most cases, they <em>wouldn&#8217;t</em> think these programs are conscious. But, it&#8217;s hard to know for sure, and theories of consciousness could be clarified by pointing to existing programs or snippets of code that do and don&#8217;t satisfy various components of these theories.<a class="see-footnote" id="footnoteref233_ozlx949" title="Even if they are not functionalists, they could still clarify their views by saying &quot;If I was a functionalist, then such-and-such computer program exhibits the kind of functional behavior and cognitive processing that I think would be sufficient for moral patienthood.&quot;  " href="#footnote233_ozlx949">233</a> Such an exercise would provide a clearer account of theories of consciousness than is possible using vague terms such as “goal” and “self-modeling.” </p>
<p>Consider, for example, the algorithm controlling Mario in the animation below:<a class="see-footnote" id="footnoteref234_tq5r1k9" title="For more details on the algorithm whose behavior is shown in the video, see the section on VI.A of Togelius et al. (2010).  " href="#footnote234_tq5r1k9">234</a></p>
<p><img style="display: block; margin-left: auto; margin-right: auto;" src="/files/Research/Moral_Patienthood/A-star_search_Mario_animation.gif" alt="Mario A* search" align="middle" /></p>
<p>In the <a href="https://www.youtube.com/watch?v=DlkMs4ZHHr8">full video</a>, Mario dodges bullets, avoids falling into pits, runs toward the goal at the end of the level, stomps on the heads of some enemies but “knows” to avoid doing so for other enemies (e.g. ones with spiky shells), kills other enemies by throwing fireballs at them, intelligently “chooses” between many possible paths through the level (indicated by the red lines), and more. Very sophisticated behavior! And yet it is all a consequence of a very simple search algorithm called A* search.</p>
<p>I won&#8217;t explain how the A* search algorithm works, but if you take the time to examine it — see Wikipedia&#8217;s article on <a href="https://en.wikipedia.org/wiki/A*_search_algorithm">A* search</a> for a general explanation, or <a href="https://github.com/RobinB/mario-astar-robinbaumgarten">Github</a> for the source code of this Mario-playing implementation — I suspect you&#8217;ll be left with the same intuition I have: that the algorithm controlling Mario has no conscious experience, and is not a moral patient.<a class="see-footnote" id="footnoteref235_ebg2chi" title="However, if you do think this algorithm is a moral patient — because it seems to have goals and aversions, is capable of planning (its path through the level), and so on — and you are some kind of utilitarian, then this may have some surprising implications. For example, suppose you think that this Mario-controlling algorithm is a moral patient, but only has a tiny fraction of the &quot;moral weight&quot; that a human has, such that when Mario reaches the goal at the end of the level, that has about 1/1000th as much positive moral value as when you have a single spoonful of ice cream. In that case, it might still be the case that the most morally valuable thing you could do per dollar — given your moral intuitions — is to run this Mario-controlling algorithm trillions of times using rented cloud computation from e.g. Amazon Web Services.  Similarly, if you (unlike me) have the intuition that today's reinforcement learning algorithms are moral patients, there are practical code modifications that could be made today to reduce the risk that these (very common) algorithms are instantiating negative phenomenal experiences: see Tomasik (2014), p. 17.  " href="#footnote235_ebg2chi">235</a> And yet, this Mario-controlling algorithm arguably exhibits many of the features that are often considered to be strong indicators of consciousness.</p>
<p>But these are just isolated cases, and there is a more systematic way we can examine our intuitions about moral patients, and explore the problematic vagueness of psychological terms, using computer code — or at least, using a rough description of code that we are confident experienced programmers could figure out how to write. We can start with a simple program, and then gradually add new features to the code, and consult our moral intuitions at each step along the way. That is the exercise I begin (but don&#8217;t finish) in the next section. Along the way, it will probably become clearer why I have a “fuzzy” view about consciousness. The next section probably also helps to illustrate what I find unsatisfying about all current theories of consciousness, a topic I discuss in more detail in <a href="#AppendixB">Appendix B</a>.</p>
<p><span style="float: right;">[<a href="/node/858/edit/56">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h5 id="Hero">My moral judgments, illustrated with the help of a simple game
</h5><p><a href="/files/Research/Moral_Patienthood/MESH_Hero_demonstration.gif"><img src="/files/Research/Moral_Patienthood/MESH_Hero_demonstration.gif" alt="MESH: Hero demonstration.gif" style="width:470px;height:306px;" align="right" /></a>Many years ago, I discovered a series of top-down puzzle games called <em><a href="http://www.kaser.com/mesh.html">MESH: Hero</a></em>. To get to the exit of each tile-based level, you must navigate the Hero character through the level, picking up items (e.g. keys), using those items (e.g. to open doors), avoiding obstacles and enemies (e.g. fire), and interacting with objects (e.g. pushing a slanted mirror in front of a laser so that laser beam is redirected and burns through an obstacle for you). Each time the player moves the Hero character by one tile, everything else in the game “progresses” one step, too — for example enemies move forward one step. (See animated screenshot.) </p>
<p>I wrote the code to add some additional interactive objects to the game,<a class="see-footnote" id="footnoteref236_08h5ofm" title="Search for &quot;SUPERHRO&quot; on this page.  " href="#footnote236_08h5ofm">236</a> so I have some idea of how the game works at a source-code level. To illustrate, I&#8217;ll describe what happens when the Hero is standing on a tile that is within the blast zone of the Bomb when it explodes. First, a message is passed to check whether the Hero object has a Shield in its inventory. If it does, nothing happens. If the Hero object does not have a Shield, then the Hero object is removed from the level and a new HeroDead object — which looks like the Hero lying down beneath a gravestone — is placed on the same tile.</p>
<p>Did anything morally bad happen, there? I think clearly not, for reasons pretty similar to why I don&#8217;t morally care about <code>increment_my_pain.py</code>. But, we can use this simplified setup to talk concretely — including with executable source code, if we want — about what we do and don&#8217;t intuitively morally care about.</p>
<p>In <em>MESH: Hero</em>, some enemies’ movements can be predicted using (roughly) Daniel Dennett&#8217;s <a href="https://en.wikipedia.org/wiki/Intentional_stance#Dennett.27s_three_levels">“physical stance”</a> (or perhaps his “design stance”). For example, at each time step (when the player moves), the Creeper — the pink object moving about in the animated screenshot — works like this: (1) If there is no obstacle one tile to the left, move one tile to the left, now facing that direction; (2) if there&#8217;s an obstacle to the left but no obstacle one tile straight ahead, move one tile straight ahead; (3) if there are obstacles to the left and straight ahead, but no obstacle one tile to the right, move one tile to the right and face that direction; (4) if there are obstacles to the left, straight ahead, and to the right, but not behind, move one space backward, now facing that direction; (5) if there are obstacles on all sides, do nothing.<a class="see-footnote" id="footnoteref237_jyqym4k" title="I might be mis-remembering the details of these algorithms, but these details don't matter much to my illustration.  " href="#footnote237_jyqym4k">237</a> Now: is the Creeper a moral patient? I think not.</p>
<p>Some other enemies can be predicted using (roughly) Dennett&#8217;s “intentional stance.” For example the Worm, in action, looks as though it wants to get to the Hero. (The Worm is the purple moving object in the animated screenshot.) At each time step, the Worm retrieves the current X/Y coordinates of itself and the Hero (in the level&#8217;s grid of tiles), then moves one tile closer to the Hero, so long as there isn&#8217;t an obstacle in the way. For example, let&#8217;s designate columns with letters and rows with numbers, and say that the Worm is on G5 and the Hero is on E3. In this case, the Worm will be facing diagonally toward the Hero, and will try to move to F4 (diagonal moves are allowed). But if there is an obstacle on F4, it will instead try to move one tile “right and forward” (to G4). But if there&#8217;s also an obstacle on G4, it will try to move “left and forward” (to F5). And if there are obstacles on all those tiles, it will stay put. Given that the Worm could be said to have a “goal” — to reach the same tile as the Hero — is the Worm a moral patient? My moral judgment is “no.”</p>
<p>I imagine you have these same intuitions. Now, let&#8217;s imagine adding new features to the game, and consider at each step whether our moral intuitions change.<a class="see-footnote" id="footnoteref238_lxi7ddo" title="The closest analogue of this exercise I've seen elsewhere is Rey (1983), though I discovered that article after writing a first draft of this section.  " href="#footnote238_lxi7ddo">238</a></p>
<p><strong><a name="MESH1" id="MESH1"></a>1. Planning Hero</strong>: Imagine the Hero object is programmed to <a href="https://en.wikipedia.org/wiki/Pathfinding">find its own path</a> through the levels. This could essentially work the same way a chess-playing computer does: the Hero object would be programmed with knowledge of how all the objects in the game work, and then it would search all possible “paths” the game would take — including e.g. picking up keys and using them on doors, how each Worm would move in response to each of the Hero&#8217;s possible moves, and so on — and find at least one path to the Exit.<a class="see-footnote" id="footnoteref239_4om8psz" title=" We also have to assume the game is deterministic, i.e. that it doesn't allow for random number generation. Off the top of my head, I can't recall whether this is true for MESH: Hero.  " href="#footnote239_4om8psz">239</a> The program could use <a href="https://en.wikipedia.org/wiki/A*_search_algorithm">A* search</a>, or <a href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning">alpha-beta pruning</a>, perhaps with some heuristic improvements. Alternately, the program could use a belief-desire-intention (BDI) architecture to enable its planning.<a class="see-footnote" id="footnoteref240_s29rzgm" title="Implementers of BDI systems can make a wide variety of choices about how to implement &quot;beliefs,&quot; &quot;desires,&quot; and &quot;intentions,&quot; how these relate to one another, and how they determine an agent's actions. For example, Davies et al. (2006), Palazzo et al. (2013), and Kim et al. (2014) make different choices about these things. Still, if an agent roughly fits the BDI architecture, it's more likely that Dennett's &quot;intentional stance&quot; could be used to interpret or predict its actions.  As an example, Daniel Dewey (a Program Officer for the Open Philanthropy Project) describes the Davies et al. (2006) BDI system with the following table:    &#9;Sources &#9;Beliefs &#9;Desires &#9;Intentions   &#9;Davies et al. (2006); Pokahr et al. (2005) &#9;Set of facts stored in an object-oriented style. &#9;&quot;Goals&quot; = &quot;concrete, momentary desires of an agent.&quot; May be used to make plans or not depending on the agent's believed context. Goals may be to execute certain actions, to reach certain states of the world, to reach certain internal states (e.g. the agent learns particular things), etc. May include &quot;subgoals&quot; created by plans to achieve other goals. &#9;A library of plan templates, added to a list of active plans when certain goals are active and certain beliefs are held. Plans are procedures that include templated external actions (e.g. moving the agent) and internal actions (manipulating beliefs, creating subgoals).    On BDI architectures in general, see e.g. Wikipedia and Wooldridge (2000).  " href="#footnote240_s29rzgm">240</a> Pathfinding or BDI-based versions of the Hero object would be even more tempting to interpret using Dennett&#8217;s “intentional stance” than the Worm is. <em>Now</em> is the Hero object a moral patient. (I think not.)</p>
<p>Does this version of the program satisfy any popular accounts of consciousness or moral patienthood? Again, it depends on how we interpret vague psychological terms. For example, Peter Carruthers argues that a being can be morally harmed by “the known or believed frustration of first-order desires,” and he is explicit that this does not require phenomenal consciousness.<a class="see-footnote" id="footnoteref241_dmzmagb" title="Carruthers (1999):  The conclusion C1 [that &quot;The mental states of non-human animals lack phenomenal feels&quot;]… generates, quite naturally, a further question…  Question 1: Given C1, ought we to conclude that sympathy (and other moral attitudes) towards the sufferings and disappointments of non-human animals is inappropriate?  In my [Carruthers (1992)], chapter 8, I argued tentatively for a positive answer to this question. But I am now not so sure. Indeed, the main burden of this paper is to demonstrate that there is a powerful case for answering Q1 in the negative…  …  I propose… to defend the following claim:  A6: Only subjective frustrations or thwartings of desire count as psychological harms, and are appropriate objects of sympathetic concern.  However, the sense of 'subjective' in A6 need not be… that of possessing phenomenological properties. Rather, the sense can be that of being believed in by the subject, on this account, a desire counts as being subjectively frustrated, in the relevant sense, if the subject believes that it has been frustrated, or believes that the desired state of affairs has not (and/or will not) come about. Then there would be nothing to stop a phenomenology-less frustration of desire from counting as subjective, and from constituting an appropriate object of moral concern. So we have a question:   Q2: Which is the appropriate notion of subjective to render A6 true? — (a) possessing phenomenology? or (b) being believed in by the subject?  If the answer to Q2 is (a), then animal frustrations and pains, in lacking phenomenology by C1, will not be appropriate objects of sympathy or concern. This would then require us to answer Q1 in the affirmative, and animals would, necessarily, be beyond the moral pale. However, if the answer to Q2 is (b), then there will be nothing in C1 and A6 together to rule out the appropriateness of moral concern for animals; and we shall then have answered Q1 in the negative.  It is important to see that desire-frustration can be characterised in a purely first-order way, without introducing into the account any higher-order belief concerning the existence of that desire… So, suppose that an animal has a strong desire to eat, and that this desire is now activated; suppose, too, that the animal is aware that it is not now eating; then that seems sufficient for its desire to be subjectively frustrated, despite the fact that the animal may be incapable of higher-order belief.  …  So putting A6 and Q2 together, in effect, we have the question:  Q3: What is bad or harmful, from the point of view of a sympathetic observer, about the frustration or thwarting of desire? — (a) the phenomenology associated with desire frustration? or (b) the fact of learning that the object of desire has not been achieved?  …  If my assumptions… are granted, then the main point is (at least tentatively) established: the most basic form of psychological harm, from the perspective of a sympathetic observer, consists in the known or believed frustration of first-order desires (which need not require that agents have knowledge that they have those desires — just knowledge of what states of affairs have come about). That is to say, the answer to Q3 is (b). So the proper object of sympathy, when we sympathise with what has happened to an agent, is the known (or believed) frustration of first-order desire. And it follows, then (given A1 and A2), that the non-conscious desires of non-human animals are at least possible, or appropriate, objects of moral sympathy and concern. (Whether they should then be objects of such concern is a further distinctively moral question, to be answered by considerations pertaining to ethical theory rather than to philosophical psychology.) And it emerges that the complete absence of phenomenology from the lives of most non-human animals, derived in C1, is of little or no direct relevance to ethics.  Carruthers (2004) develops this line of thinking further.  Several others have advocated views which might be interpreted as according moral patienthood to animals on account of their having preferences that can be satisfied or frustrated, regardless of whether they are also conscious. See e.g. Dawkins (2012), chs. 7-9, and the endorsement of that account by Rose (2016).  " href="#footnote241_dmzmagb">241</a> If the Hero object has an explicitly-programmed goal to reach the Exit object, and its (non-conscious) first-order desire to achieve this goal is frustrated (e.g. by obstacle or enemy objects), has the Hero object been harmed in a morally relevant way? I would guess Carruthers thinks the answer is “no,” but why? Why wouldn&#8217;t the algorithm I&#8217;ve described count as having a first-order desire? How would the program need to be different in order for it to have a first-order desire?</p>
<p>One might also wonder whether the Hero object in this version of the program satisfies (some interpretations of) the core Kantian criterion for moral patienthood, that of rational agency.<a class="see-footnote" id="footnoteref242_n0m1xww" title="For example Jaworska &amp; Tannenbaum (2013) write:  Historically, the most famous [account of moral status grounded in intellectual capacities] was given by Kant, according to whom autonomy, the capacity to set ends via practical reasoning, must be respected… and grounds the dignity of all rational beings… Beings without reason may be treated as a mere means…  Similarly, Dillon (2014) writes:  The most influential position on [the topic of respect for persons] is found in the moral philosophy of Immanuel Kant… Indeed, most contemporary discussions of respect for persons explicitly claim to rely on, develop, or challenge some aspect of Kant's ethics. Central to Kant's ethical theory is the claim that all persons are owed respect just because they are persons, that is, free rational beings. To be a person is to have a status and worth that is unlike that of any other kind of being: it is to be an end in itself with dignity. And the only response that is appropriate to such a being is respect. Respect (that is, moral recognition respect) is the acknowledgment in attitude and conduct of the dignity of persons as ends in themselves. Respect for such beings is not only appropriate but also morally and unconditionally required: the status and worth of persons is such that they must always be respected…  " href="#footnote242_n0m1xww">242</a> Given that this Hero object is capable of its own means-end reasoning, is it thus (to some Kantians) an “end in itself,” whose dignity must be respected? Again, I would guess the answer is “no,” but why? What counts as “rational agency,” if not the means-end reasoning of the Hero object described above? What computer program <em>would</em> count as exhibiting “rational agency,” if any?</p>
<p><strong>2. Partially observable environment</strong>: Suppose the Hero still uses a pathfinding algorithm to decide its next move, except that instead of having access to the current location and state of every object in the level, it only has access to the location and state of every object “within the Hero&#8217;s direct line of sight” — that is, not on the other side of a wall of some other opaque object, relative to the Hero&#8217;s position. Now the environment is only “partially observable.” In cases where a path to the Exit is not findable via the objects the Hero can “see,” the Hero object will systematically explore the space (via its modified pathfinding algorithm) until its built-up “knowledge” of the level is complete enough for its pathfinding algorithm to find a path to the Exit. Is the Hero object now a moral patient?</p>
<p><strong>3. Non-discrete movement and collision detection</strong>: Suppose that objects in the game “progress” not whenever the Hero moves, but once per second. (The Hero also has one opportunity to move per second.) Moreover, when objects move, they do not “jump” discretely from one tile to the next, but instead their location changes “continuously” (i.e. one pixel at a time; think of a pixel as the smallest possible area in a theory of physics that quantizes area, such as <a href="https://en.wikipedia.org/wiki/Loop_quantum_gravity">loop quantum gravity</a>) from the center of one tile to the center of the next tile. Let&#8217;s say tiles are 1000×1000 pixels (it&#8217;s now a very high-resolution game), and since objects move at one tile-width per second, that means they move one pixel per millisecond (ms). Now, instead of objects interacting by checking (at each time step) whether they are located on the same tile as another object, there is instead a <a href="https://en.wikipedia.org/wiki/Collision_detection">collision detection</a> algorithm run by every object to check whether another object has at least one pixel overlapping with one of its own pixels. Each object checks a ten-pixel-deep layer of pixels running around its outermost edge (let&#8217;s calls this layer the “skin” of each object), each millisecond. So e.g. if the Hero&#8217;s collision detection algorithm detects that a pixel on the Hero&#8217;s “face” is overlapping with a pixel of a Worm, then the Hero object is removed from the level and replaced with the HeroDead object immediately, without waiting until both the Hero and the Worm have completed their moves to the center of the same tile. Is the Hero object now a moral patient? (I still think not.)</p>
<p><strong>4. Nociception and nociceptive reflexes</strong>: Now, suppose we give the Hero object nociceptors. That is: 1/100th of the pixels in the Hero&#8217;s “skin” layer are designated as “nociceptors.” Once per ms, the Hero&#8217;s CheckNociception() function checks those pixels for collisions with the pixels of other objects, and if it detects such a “collision,” it runs the NociceptiveReflex() function, which moves the Hero “away” from that collision at a speed of 1 pixel per 0.5ms. By “away,” I mean that, for example, if the collision happened in a pre-defined region of the Hero&#8217;s skin layer that is sensibly called the “top-right” region, the Hero moves toward the center of the tile that is one tile down and left from the tile that the center of the Hero is currently within. Naturally, the Hero might fail to move in this direction because it detects an obstacle on that tile, in which case it will stay put. Or there might be a Worm or other enemy on that tile. In any case, another new function executed by the Hero object, CheckInjury(), runs a collision check for all pixels “inside” (closer to the center than) the skin layer, and if there are any such collisions detected, the Hero object is replaced with HeroDead. Is the Hero object a moral patient now? (My moral judgment remains “no.”)</p>
<p><strong>5. Health meter</strong>: Next, we give the Hero object an integer-type variable called SelfHealth, which initializes at 1000. When it reaches 0, the Hero object is replaced with the HeroDead object. Each collision detection in the Hero&#8217;s skin layer reduces the SelfHealth variable by 1, and each collision detection “inside” the Hero&#8217;s skin layer reduces the SelfHealth variable by 5. <em>Now</em> is the Hero a moral patient? (I still think “no.”)</p>
<p><strong>6. Nociception sent to a brain</strong>: Now, a new sub-object of the Hero, called Brain, is the object that can call the NociceptiveReflex() function. It also runs its own collision detection for a 50×50 box of pixels (the “brain pixels”) in the middle of the Hero&#8217;s “head,” and if it detects collisions with other “external” objects (e.g. a Worm) there, SelfHealth immediately goes to 0. Moreover, rather than a single Hero-wide CheckNociception() function checking for pixel collisions at each of the pixels designated “nociceptors,” each nociceptor is instead defined in the game as its own object, and it runs its own collision detection function. If a nociceptor detects a collision, it creates a new object called NociceptiveSignal, which thereafter moves at a speed of 1 pixel per 0.1ms toward the nearest of the “brain pixels.” If the Brain object&#8217;s CheckNociception() function detects a collision between a “brain pixel” and a NociceptiveSignal object (instead of with an “external” object like a Worm), then it executes the NociceptiveReflex() function, using data stored in the NociceptiveSignal object to determine which edge of the Hero to move “away” from. Is the Hero object, finally, a moral patient?</p>
<p>By now, the program I&#8217;m describing seems like it might satisfy several of the criteria that <a href="https://books.google.com/books?id=aMvonPqzu_cC&amp;printsec=frontcover">Braithwaite (2010)</a> uses to argue that fishes are conscious, including the presence of nociceptors, the transmission of nociceptive signals to a brain for central processing, the ability to use mental representations, a rudimentary form of self-modeling (e.g. via the SelfHealth variable, and via making plans to navigate the Hero object to the Exit while avoiding events that would cause the Hero object to be replaced with HeroDead), and so on.<a class="see-footnote" id="footnoteref243_1zasdnq" title="This isn't really a rebuttal against Braithwaite's argument for fish consciousness, because it is easy find details of her account that are technically not satisfied by the program I sketched here, even for the specific features I've listed above; rather, I sketched the program above and pointed to Braithwaite's account merely to illustrate the more general point I make in the next paragraph.  In any case, here is Braithwaite's summary of her case for fish consciousness, from Braithwaite (2010), ch. 4:  So pulling the different threads together, fish really do appear to possess key traits associated with consciousness. Their ability to form and use mental representations indicates fish have some degree of access consciousness. They can consider a current mental state and associate it with a memory. Having an area of the brain specifically associated with processing emotion and evidence that they alter their view of an aversive situation depending on context suggests that fish have some form of phenomenal consciousness: they are sentient. This leaves monitoring and self consciousness, which I argue is in part what the eel and the grouper are doing: considering their actions and pondering the consequences. The grouper is clearly deciding it has no chance to get the prey itself and so swims off to get the eel. The eel is deciding that an easy meal is on offer. On balance then, fish have a capacity for some forms of consciousness, and so I conclude that they therefore have the mental capacity to feel pain.  Braithwaite doesn't mention nociceptors or the transmission of nociceptive signalling for central processing in this quote, but it's clear from earlier sections of the book that these two features of fish neurobiology are critical to her confidence in conscious fish pain.  This version of MESH: Hero might also satisfy the criteria for having &quot;interests&quot; of the sort that Johnson (1993) argues are sufficient for moral status. Note that unlike some authors defending the moral status of plants and ecosystems, Johnson is explicit that his account might accord moral status to certain kinds of machines (pp. 145-146).  " href="#footnote243_1zasdnq">243</a> And yet, <em>I</em> don&#8217;t think this version of the Hero object is conscious, and I&#8217;d guess that Braithwaite would agree. But if <em>this</em> isn&#8217;t what Braithwaite means by “nociception,” “mental representations,” and so on, then what <em>does</em> she mean? What program <em>would</em> satisfy one or more of her indicators of consciousness?</p>
<p>I think this exercise can be continued, in tiny steps, until we&#8217;ve described a sophisticated 2D Hero object that seems to exhibit many commonly-endorsed criteria for (or indicators of) moral patienthood or consciousness.<a class="see-footnote" id="footnoteref244_t8gdzew" title="A much more satisfying, but also more costly to write, version of this exercise would involve doing the following:   Collect several dozen functionalist theories of consciousness and moral patienthood. Summarize their key features. Think of a basic program design that would allow you to chart an efficient course through as many of these theories of consciousness and moral patienthood as possible, merely by adding 1-5 new &quot;features&quot; for each updated version of the program. Write the code for each version of that program. Briefly describe each version of the program in order, and after each program version description, quote the theory or theories of consciousness or moral patienthood that now seem to be satisfied by the program.   " href="#footnote244_t8gdzew">244</a> Moreover, such sophisticated Hero objects could not just be <em>described</em>, but (I claim) programmed and run. And yet, when I carry out that exercise (in my head), I typically do not end up having the intuition that any of those versions of the <em>MESH: Hero</em> code — especially those described above — are conscious, or moral patients.</p>
<p>There are, however, two kinds of situations, encountered when continuing this exercise in my head, in which I begin to worry that the program I&#8217;m imagining might be a phenomenally conscious moral patient if it was coded and run.</p>
<p>First, I begin to worry about the Hero object&#8217;s moral patienthood when the program I&#8217;m imagining gets so complicated that I can no longer trace what it&#8217;s doing, e.g. if I control the Hero agent using a very large <a href="http://karpathy.github.io/2016/05/31/rl/">deep reinforcement learning agent</a> that has learned to navigate the game world via millions of play-throughs using only raw pixel data, or if I control the Hero object using a complicated candidate solution discovered via an <a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">evolutionary algorithm</a>.<a class="see-footnote" id="footnoteref245_nl9ryy5" title="I don't worry about just any &quot;large deep reinforcement learning agent&quot; or &quot;complicated candidate solution,&quot; of course. E.g. I might start to worry if I can't trace what the system is doing and it exhibits some highly sophisticated behavior that matches human conscious behavior in certain ways. On the other hand, I might not worry if a strong argument can be made that a given (large and complicated) system is essentially doing a very high-dimensional variant on linear regression, and is not engaging in e.g. dynamic control of memory and attention subsystems, as is the case for some deep learning agent architectures (see e.g. Marblestone et al. 2016).  " href="#footnote245_nl9ryy5">245</a></p>
<p><a name="consciousprogram" id="consciousprogram"></a>Second, I begin to worry about the Hero object&#8217;s moral patienthood when it begins to look like the details of my own phenomenal experience might be pretty fully captured by how the program I&#8217;m imagining works, and thus I start to worry it might be a moral patient precisely because I <em>can</em> trace what it&#8217;s doing. My approach assumes that phenomenal consciousness is how a certain kind of algorithm feels “from the inside,”<a class="see-footnote" id="footnoteref246_z57c5i3" title="I owe this phrase to Yudkowsky, &quot;How an Algorithm Feels from Inside.&quot;  " href="#footnote246_z57c5i3">246</a> and — fairly late in this investigation — I was able to piece together (in my head) a sketch of a program that, from the outside, looks to me like it <em>might</em>, with some elaboration and careful design beyond what I was able to sketch in my head, feel (from the inside) something like my own phenomenal experience feels to <em>me</em>. (Obviously, this conclusion is very speculative, and I don&#8217;t give it much weight, and I don&#8217;t make use of it in the rest of this report, but it is also quite different from my earlier state of understanding, under which <em>no</em> theory or algorithm I had read about or considered seemed to me like it might even come <em>close</em> to feeling from the inside like my own phenomenal experience feels to me.)</p>
<p>Unfortunately, it would require a very long report for me to explore and then explain what I think such a program looks like (given <em>my</em> intuitions), so for this report all I&#8217;ve done is pointed to some of the key inspirations for my intuitive, half-baked “maybe-conscious” program (my “GDSAK” account described <a href="#MoreSatisfying">here</a>). In the future, I hope to describe this program in some detail, and then show how my moral intuitions respond to various design tweaks, but we decided this exercise fell beyond the scope of this initial report on moral patienthood.</p>
<p>In any case, I hope I have explained at least a few things about how my moral intuitions work with respect to moral patienthood and consciousness, so that my readers have some sense of “where I&#8217;m coming from.”</p>
<p><span style="float: right;">[<a href="/node/858/edit/57">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="AppendixB">Appendix B. Toward a more satisfying theory of consciousness</h3>
<p>In this appendix, I describe some popular theories of (human) consciousness, explain the central reason why I find them unsatisfying, and conclude with some thoughts about how a more satisfying theory of consciousness could be constructed.</p>
<p>In short, I think even the most compelling extant theories of consciousness are, in the words of <a href="http://www.sciencedirect.com/science/article/pii/S1364661311001252">Dennett &amp; Cohen (2011)</a>:</p>
<blockquote><p>…merely the beginning, rather than the end, of the study of consciousness. There is still much work to be done…</p></blockquote>
<p>Neuroscientist Michael Graziano states the issue more vividly (and less charitably):<a class="see-footnote" id="footnoteref247_q5ohtp1" title="Graziano (2013), ch. 1.  " href="#footnote247_q5ohtp1">247</a></p>
<blockquote><p>I was in the audience watching a magic show. Per protocol a lady was standing in a tall wooden box, her smiling head sticking out of the top, while the magician stabbed swords through the middle.</p>
<p>A man sitting next to me whispered to his son, “Jimmy, how do you think they do <em>that</em>?”</p>
<p>The boy must have been about six or seven. Refusing to be impressed, he hissed back, “It&#8217;s <em>obvious</em>, Dad.”</p>
<p>“Really?” his father said. “You figured it out? What&#8217;s the trick?”</p>
<p>“The magician makes it happen that way,” the boy said.</p></blockquote>
<p>Graziano&#8217;s point is that “the magician makes it happen” is not much of an explanation. There is still much work to be done. Current theories of consciousness take a <em>few</em> steps toward explaining the details of our conscious experience, but at some point they end up saying “and then [such-and-such brain process] makes consciousness happen.” And I want to say: “Well, that <em>might</em> be right, but <em>how</em> do those processes make consciousness happen?”<a class="see-footnote" id="footnoteref248_y4hh2ok" title="Or, to be more accurate: How do such-and-such brain processes instantiate consciousness?  " href="#footnote248_y4hh2ok">248</a> Or in some cases, a theory of consciousness might not make <em>any</em> attempt to explain some important feature of consciousness, not even at the level of “[such-and-such brain process] makes it happen.”</p>
<p>A successful explanation of consciousness would show how the details of some theory predicts, with a fair amount of precision, the “explananda” (things to be explained) of consciousness — i.e., the specific features of consciousness that we know about from our own phenomenal experience and from (reliable, validated) cases of self-reported conscious experience (e.g. in experiments, or in brain lesion studies).</p>
<p>Current theories of consciousness, I think, do not “go far enough” — i.e., they don&#8217;t explain enough consciousness explananda, with enough precision — to be compelling (yet).<a class="see-footnote" id="footnoteref249_ps7566l" title="One way to think about this is from the perspective of &quot;inference to the best explanation&quot; or &quot;explanationism,&quot; according to which theories are judged by how well they perform on a list of common-sense &quot;explanatory virtues.&quot; Years ago, I collected the following list of commonly-endorsed explanatory virtues from philosophical defenders of inference to the best explanation:   Testability: better explanations render specific predictions that can be falsified or corroborated. Scope (aka &quot;comprehensiveness&quot; or &quot;consilience&quot;): better explanations explain more types of phenomena. Precision: better explanations explain phenomena with greater precision. Simplicity: better explanations make use of fewer claims, especially fewer as yet unsupported claims (&quot;lack of ad-hoc-ness&quot;). Mechanism: better explanations provide more information about underlying mechanisms. Unification: better explanations unify apparently disparate phenomena (also sometimes called &quot;consilience&quot;). Predictive novelty: better explanations don't just &quot;retrodict&quot; what we already know, but predict things we observe only after they are predicted. Analogy (aka &quot;fit with background knowledge&quot;): better explanations generally fit with what we already know with some certainty. Past explanatory success: better explanations fit within a tradition or trend with past explanatory success (e.g. astronomy, not astrology).   On this framework, a more precise way to state my core complaint about current theories of consciousness is that they are lacking in precision and scope.  (Of course, they may be lacking in other explanatory virtues, too.)  " href="#footnote249_ps7566l">249</a> Below, I elaborate this issue with respect to three popular theories of consciousness (for illustrative purposes): temporal binding theory, integrated information theory, and global workspace theory.<a class="see-footnote" id="footnoteref250_ssginck" title="One can compare my exercise to section 4 (&quot;Some case studies&quot;) from Chalmers (1995):  In the last few years, a number of works have addressed the problems of consciousness within the framework of cognitive science and neuroscience. This might suggest that the foregoing analysis is faulty, but in fact a close examination of the relevant work only lends the analysis further support. When we investigate just which aspects of consciousness these studies are aimed at and which aspects they end up explaining, we find that the ultimate target of explanation is always one of the easy problems. I illustrate this with two representative examples.  After explaining Crick &amp; Koch's temporal binding theory, Chalmers says:  The details of how this binding might be achieved are still poorly understood, but suppose that they can be worked out. What might the resulting theory explain? Clearly it might explain the binding of information, and perhaps it might yield a more general account of the integration of information in the brain. Crick and Koch also suggest that these oscillations activate the mechanisms of working memory, so that there may be an account of this and perhaps other forms of memory in the distance. The theory might eventually lead to a general account of how perceived information is bound and stored in memory for use by later processing.  Such a theory would be valuable, but it would tell us nothing about why the relevant contents are experienced. Crick and Koch suggest that these oscillations are the neural correlates of experience. This claim is arguable— does not binding also take place in the processing of unconscious information?— but even if it is accepted, the explanatory question remains: why do the oscillations give rise to experience? The only basis for an explanatory connection is the role they play in binding and storage, but the question of why binding and storage should themselves be accompanied by experience is never addressed. If we do not know why binding and storage should give rise to experience, telling a story about the oscillations cannot help us. Conversely, if we knew why binding and storage gave rise to experience, the neurophysiological details would be just the icing on the cake. Crick and Koch's theory gains its purchase by assuming a connection between binding and experience and so can do nothing to explain that link.  Chalmers then elaborates a similar complaint about some other theories.  One difference between Chalmers' complaint and mine is that, as an illusionist about consciousness who thus &quot;replaces the hard problem with the illusion problem&quot; (Frankish 2016b), one way to view my complaint is that it is the complaint that the theories of consciousness surveyed here fail to explain the illusions of conscious experience.  But really, my complaint is more general than this, and not dependent on illusionism in particular. If later I decide that (say) I am a Prinz-style realist about consciousness (Prinz 2016), the my core complaint will remain: simply, these theories do not explain enough consciousness explananda, with enough precision, to be satisfying.  " href="#footnote250_ssginck">250</a></p>
<p>It&#8217;s possible this “doesn&#8217;t go far enough” complaint would be largely accepted by the leading proponents of these theories, because (I would guess) none of them think they have described a “final” theory of consciousness, and (I would guess) all of them would admit there are many details yet to be filled in. This is, after all, a normal way to make progress in science: propose a simple model, use the model to make novel predictions, test those predictions, revise the model in response to experimental results, and so on. Nevertheless, in some cases the leading proponents of these theories write as though they have already put forward a <em>near</em>-final theory of consciousness, and I hope to illustrate below why I think we have “a long way to go,” even <em>if</em> these theories are “on the right track,” and then explain how I think we can do better (with a lot of hard work).</p>
<p><span style="float: right;">[<a href="/node/858/edit/58">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="TemporalBinding">Temporal binding theory</h4>
<p>Of the modern theories of consciousness, the first one <a href="https://global.oup.com/academic/product/consciousness-and-the-social-brain-9780199928644?cc=us&amp;lang=en&amp;">Graziano (2013)</a> complains about (ch. 1) is Francis Crick and Christof Koch&#8217;s temporal binding theory:</p>
<blockquote><p>[Crick and Koch] suggested that when the electrical signals in the brain oscillate they cause consciousness. The idea… goes something like this: the brain is composed of neurons that pass information among each other. Information is more efficiently linked from one neuron to another, and more efficiently maintained over short periods of time, if the electrical signals of neurons oscillate in synchrony. Therefore, consciousness might be caused by the electrical activity of many neurons oscillating together.</p>
<p>This theory has some plausibility. Maybe neuronal oscillations are a precondition for consciousness. But note that… the hypothesis is not truly an explanation of consciousness. It identifies a magician. Like the Hippocratic account, “The brain does it” (which is probably true)… this modern theory stipulates that “the oscillations in the brain do it.” We still don&#8217;t know how. Suppose that neuronal oscillations do actually enhance the reliability of information processing. That is impressive and on recent evidence apparently likely to be true. But by what logic does that enhanced information processing cause the inner experience? Why an inner feeling? Why should information in the brain — no matter how much its signal strength is boosted, improved, maintained, or integrated from brain site to brain site — become associated with any subjective experience at all? Why is it not just information without the add-on of awareness?</p></blockquote>
<p>I should note that Graziano is too harsh, here. Crick &amp; Koch (“C&amp;K”) make more of an effort to connect the details of their model to the explananda of consciousness than Graziano suggests. There is more to C&amp;K&#8217;s account than just “the oscillations in the brain do it.”<a class="see-footnote" id="footnoteref251_hkdzfs7" title="See Crick &amp; Koch (1990, 1998).  Note that they later abandoned their early theory of consciousness. Koch (2004), p. 46, writes:  Today, Francis [Crick] and I no longer think that synchronized firing is a sufficient condition for the [neural correlates of consciousness]. A functional role more in line with the data is that synchronization assists a nascent coalition  in its competetition with other nascent coalitions. As explained in Chapter 9, this occurs when you attend to an object or event. A neuronal substrate of this bias could be synchronized firing in certain frequency bands… Once a coalition has established itself as a winner and you are conscious of the associated attributes, the coalition may be bale to maintain itself without the assistance of synchrony, at least for a time. Thus, one might expect synchronized oscillations to occur in the early stages of perception, but not necessarily in later ones.  " href="#footnote251_hkdzfs7">251</a> But, in the end, I agree with Graziano that C&amp;K do not “go far enough” with their theory to make it satisfying. As Graziano says <a href="http://www.theatlantic.com/science/archive/2016/03/phlegm-theories-of-consciousness/472812/">elsewhere</a>:</p>
<blockquote><p>…the theory provides no mechanism that connects neuronal oscillations in the brain to a person being able to say, “Hey, I have a conscious experience!” You couldn&#8217;t give the theory to an engineer and have her understand, even in the foggiest way, how one thing leads to the other.</p></blockquote>
<p>I think this is a good test for theories of consciousness: If you described your theory of consciousness to a team of software engineers, machine learning experts, and roboticists, would they have a good idea of how they might, with several years of work, build a robot that functions according to your theory? And would you expect it to be phenomenally conscious?</p>
<p>For a similar attitude toward theories of consciousness, see also the (illusionist-friendly) introductory paragraph of <a href="http://link.springer.com/article/10.1007/s11023-012-9285-z">Molyneux (2012)</a>:</p>
<blockquote><p>…Instead of attempting to solve what appears unsolvable, an alternative reaction is to investigate why the problem seems so hard. In this way, Minsky (1965) hoped, we might at least explain why we are confused. Since a good way to explain something is often to build it, a good way to understand our confusion [about consciousness] may be to build a robot that thinks the way we do… I hope to show how, by attempting to build a smart self-reflective machine with intelligence comparable to our own, a robot with its own hard problem, one that resembles the problem of consciousness, may emerge.</p></blockquote>
<p><span style="float: right;">[<a href="/node/858/edit/59">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="IntegratedInformation">Integrated information theory</h4>
<p>Another popular theory of consciousness is Integrated Information Theory (IIT), according to which consciousness is equal to a measure of integrated information denoted Φ (“phi”). <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003588">Oizumi et al. (2014)</a> explains the basics:</p>
<blockquote><p>Integrated information theory (IIT) approaches the relationship between consciousness and its physical substrate by first identifying the fundamental properties of experience itself: existence, composition, information, integration, and exclusion. IIT then postulates that the physical substrate of consciousness must satisfy these very properties. We develop a detailed mathematical framework in which composition, information, integration, and exclusion are defined precisely and made operational. This allows us to establish to what extent simple systems of mechanisms, such as logic gates or neuron-like elements, can form complexes that can account for the fundamental properties of consciousness. Based on this principled approach, we show that IIT can explain many known facts about consciousness and the brain, leads to specific predictions, and allows us to infer, at least in principle, both the quantity and quality of consciousness for systems whose causal structure is known. For example, we show that some simple systems can be minimally conscious, some complicated systems can be unconscious, and two different systems can be functionally equivalent, yet one is conscious and the other one is not.</p></blockquote>
<p>I won&#8217;t explain IIT any further; see other sources for more detail.<a class="see-footnote" id="footnoteref252_z9xm36t" title="See Tononi (2004); Oizumi et al. (2014); Tononi (2015); Tononi et al. (2015b). I have not read these sources in full.   " href="#footnote252_z9xm36t">252</a> Instead, let me jump straight to my reservations about IIT.<a class="see-footnote" id="footnoteref253_sj60434" title="I am hardly an expert on IIT, so my criticisms could be misguided, but even if they are, I hope they will help to illustrate how I think about theories of consciousness.  " href="#footnote253_sj60434">253</a></p>
<p>I have many objections to IIT, for example that it predicts enormous quantitites of consciousness in simple systems for which we have no evidence of consciousness.<a class="see-footnote" id="footnoteref254_yxehgkp" title="For example, IIT predicts enormous quantities of consciousness in the &quot;trivially simple network&quot; of Seth et al. (2006) and the expander graph of Aaronson (2014a).  Tononi (2014) replied to the latter example by confirming that Aaronson's expander graph would be enormously conscious according to IIT, but then saying that we shouldn't trust our intuitions that the expander graph isn't enormously conscious:  [Aaronson's] main point that certain systems that are simple — in the sense that they are easy to describe — could have large values of PHI, [is correct]… Resorting to expander graphs is actually overkill. This is because systems that are even simpler to describe than expander graphs, for example a 2D lattice of identical logic gates (a &quot;grid&quot;) could also achieve very large values of PHI. So things are &quot;even worse&quot; for IIT… [Aaronson also] argues that some systems with high PHI may not only have a structure that is simple to describe, but they may perform computations that are also just as simple to describe, such as parity checks. In fact, the situation for IIT is actually &quot;worse&quot;, since it allows for a large 2D grid to be conscious even if it were doing nothing, with all gates switched off at a fixed point. Thus, if IIT can be invalidated by an expander graph doing not much at all, it can be invalidated all the more by a mere grid doing absolutely nothing…  …However, [Aaronson's] &quot;commonsense&quot; intuition that such simple systems cannot possibly be conscious is wrong and should be revised.  …it can be dangerous to rely too much on one's pre-theoretical intuitions, however strong they may seem. Examples in science are numerous, starting with the strong intuitions people once had that the earth must be still and the sun must revolve around it, or that the earth cannot be round because otherwise we would fall off. Concerning consciousness, the reliability of pre-theoretical intuitions is even worse, because different people often hold radically different ones…  Aaronson (2014b)'s responses to Tononi are roughly the same ones I would give (though, see also the additional exchanges between David Chalmers and Scott Aaronson on that post), so I won't repeat them here.   " href="#footnote254_yxehgkp">254</a> But here, I want to focus on the issue that runs throughout this section: IIT does not predict many consciousness explananda with much precision.</p>
<p>Graziano provides the following example:<a class="see-footnote" id="footnoteref255_tn23z6q" title="Graziano (2013), ch. 11.  In Graziano (2016), he makes his case against IIT this way:  [One] popular explanation of consciousness is the integrated information theory. Actually, there are several different theories that fit into this same general category. They share the underlying idea that consciousness is caused by linking together large amounts of information. It's one thing to process a few disconnected scraps of information. But when information is connected into vast brain-spanning webs, then, according to the proposal, subjective consciousness emerges.  I can't deny that information is integrated in the brain on a massive scale. Vast networks of information play a role in many brain functions. If you could de-integrate the information in the brain, a lot of basic functions would fail, probably including consciousness. And yet, as a specific explanation of consciousness, this one is definitely a phlegm theory.  Again, it flatters intuition. Most people have an intuition about consciousness as an integrated whole. Your various impressions and thoughts are somehow rolled together into a single inner you. That's the impression we get, anyway.  You see this same trope in science fiction: If you bundle enough information into a computer, creating a big enough connected mass of data, it'll wake up and start to act conscious, like Skynet. This appeal to our latent biases has given the integrated information theory tremendous currency. It's compelling to many respected figures in the field of neuroscience, and is one of the most popular current theories.  And yet it doesn't actually explain anything. What exactly is the mechanism that leads from integrated information in the brain to a person who ups and claims, &quot;Hey, I have a conscious experience of all that integrated information!&quot; There isn't one.  If you point a wavelength detector at the sky, it will compute that the sky is blue. If you build a machine that integrates the blueness of the sky with a lot of other information – the fact that the blue stuff is a sky, that it's above the earth, that it extends so far here and so far there – if the machine integrates a massive amount of information about that sky – what makes the machine claim that it has a subjective awareness of blue? Why doesn't it just have a bunch of integrated information, without the subjective awareness? The integration theory doesn't even try to explain. It flatters our intuitions while explaining nothing.  Some scholars retreat to the position that consciousness must be a primary property of information that cannot be explained. If information is present, so is a primordial, conscious experience of it. The more information that is integrated together, the richer the conscious experience. This type of thinking leads straight to a mystical theory called panpsychism, the claim that everything in the universe is conscious, each in its own way, since everything contains at least some information. Rocks, trees, rivers, stars. This theory is the ultimate in phlegm theories. It has enormous intuitive appeal to people who are prone to project consciousness onto the objects around them, but it explains absolutely nothing. One must simply accept consciousness as an elemental property and abandon all hope of understanding it.  " href="#footnote255_tn23z6q">255</a></p>
<blockquote><p>[One way to test IIT] would be to test whether human consciousness fades when integration in the brain is reduced. Tononi emphasizes the case of anesthesia. As a person is anesthetized, integration among the many parts of the brain slowly decreases, and so does consciousness… But even without doing the experiment, we already know what the result must be. As the brain degrades in its function, so does the integration among its various parts and so does the intensity of awareness. But so do most other functions. Even many unconscious processes in the brain depend on integration of information, and will degrade as integration deteriorates.</p>
<p>The underlying difficulty here is… the generality of integrated information. Integrated information is so pervasive and so necessary for almost all complex functions in the brain that the theory is essentially unfalsifiable. Whatever consciousness may be, it depends in some manner on integrated information and decreases as integration in the brain is compromised.</p></blockquote>
<p>In other words, IIT doesn&#8217;t do much to explain why some brain processes are conscious and others are not, since <em>all</em> of them involve integrated information. Indeed, as far as I can tell, IIT proponents think that a great many brain processes typically thought of as paradigm cases of unconscious cognitive processing are in fact conscious, but we are unaware of this.<a class="see-footnote" id="footnoteref256_5nmsma6" title="E.g. see the section on &quot;multiple complexes&quot; in Tononi et al. (2016).  " href="#footnote256_5nmsma6">256</a> In principle, I agree that a well-confirmed theory could make surprising predictions about things we can&#8217;t observe (yet, or possible ever), and that if the theory is well-enough supported then we should take those predictions quite seriously, but I don&#8217;t think IIT is so well-confirmed yet. In the meantime, IIT seems unsatisfying to the extent that it fails to predict some fairly important explananda of consciousness, for example that some highly “integrated” congnitive processing is, as far as we know, unconscious.</p>
<p>Moreover, Graziano says, IIT doesn&#8217;t do much to explain the reportability of consciousness (in any detail<a class="see-footnote" id="footnoteref257_f6pzely" title="Proponents of IIT do say some things about IIT and reportability (e.g. see Tononi et al. 2016), but if they've said anything about IIT specifically predicts the specific features of conscious self-report we observe, then I have been unable to understand what that account is, in what I've read about IIT so far.  " href="#footnote257_f6pzely">257</a>):</p>
<blockquote><p>The only objective, physically measurable truth we have about consciousness is that we can, at least sometimes, report that we have it. I can say, “The apple is green,” like a well-regulated wavelength detector, providing no evidence of consciousness; but I can also claim, “I am sentient; I have a conscious experience of green.”</p>
<p>…The integrated information [theory]… is silent on how we get from being conscious to being able to report, “I have a conscious experience.” Yet any serious theory of consciousness must explain the one objective fact that we have about consciousness: that we can, in principle, at least sometimes, report that we have it.</p>
<p>In discussion with colleagues, I have heard the following argument… The brain has highly integrated information. Highly integrated information is (so the theory goes) consciousness. Problem solved. Why do we need a special mechanism to inform the brain about something that it already has? The integrated information is already in there; therefore, the brain should be able to report that it has it.</p>
<p>…[But] the brain contains a lot of items that it can&#8217;t report. The brain contains synapses, but nobody can introspect and say, “Yup, those serotonin synapses are particularly itchy today.” The brain regulates the flow of blood through itself, but nobody has cognitive access to that process either. For a brain to be able to report on something, the relevant item can&#8217;t merely be present in the brain but must be encoded as information in the form of neural signals that can ultimately inform the speech circuitry.</p>
<p>The integrated information theory of consciousness does not explain how the brain, possessing integrated information (and, therefore, by hypothesis, consciousness), encodes the fact that it has consciousness, so that consciousness can be explicitly acknowledged and reported. One would be able to report, “The apple is green,” like a well-calibrated spectral analysis machine… One would be able to report a great range of information that is indeed integrated. The information is all of a type that a sophisticated visual processing computer, attached to a camera, could decode and report. But there is no proposed mechanism for the brain to arrive at the conclusion, “Hey, green is a conscious experience.” How does the presence of conscious experience get turned into a report?</p>
<p>To get around this difficulty and save the integrated information theory, we would have to postulate that the integrated information that makes up consciousness includes not just information that depicts the apple but also information that depicts what a conscious experience is, what awareness itself is, what it means to experience. The two chunks of information would need to be linked. Then the system would be able to report that it has a conscious experience of the apple…</p></blockquote>
<p>These examples illustrate (but don&#8217;t exhaust) the ways in which IIT doesn&#8217;t predict the explananda of consciousness in as much detail as I&#8217;d like.</p>
<p>What about global workspace theory?</p>
<p><span style="float: right;">[<a href="/node/858/edit/60">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="GlobalWorkspace">Global workspace theory</h4>
<p>One particularly well-articulated theory of consciousness is Bernard Baars’ <a href="https://en.wikipedia.org/wiki/Global_Workspace_Theory">Global Workspace Theory</a> (GWT), including variants such as Stanislas Dehaene&#8217;s Global Neuronal Workspace Theory (<a href="http://www.penguin.com/book/consciousness-and-the-brain-by-stanislas-dehaene/9780143126263">Dehaene 2014</a>), and GWT&#8217;s implementation in the LIDA cognitive architecture (<a href="http://www.sciencedirect.com/science/article/pii/S2212683X12000060">Franklin et al. 2012</a>).<a class="see-footnote" id="footnoteref258_9eqw3w5" title="For an introduction to LIDA, see Franklin et al. (2016).  " href="#footnote258_9eqw3w5">258</a> </p>
<p><a href="http://www.polity.co.uk/book.asp?ref=9780745653440">Weisberg (2014)</a>, ch. 6, explains the basics of GWT succinctly:<a class="see-footnote" id="footnoteref259_98jrkaj" title="Note that the Müller-Lyer illusion image included below does not appear in the quoted passage of Weisberg (2014); I added it for convenience.  " href="#footnote259_98jrkaj">259</a></p>
<blockquote><p>Perhaps the best developed empirical theory of consciousness is the global workspace view (Baars 1988; 1997). The basic idea is that conscious states are defined by their “promiscuous accessibility,” by being available to the mind in ways that nonconscious states are not. If a state is nonconscious, you just can&#8217;t do that much with it. It will operate automatically along relatively fixed lines. However, if the state is conscious, it connects with the rest of our mental lives, allowing for the generation of far more complex behavior. The global workspace (GWS) idea takes this initial insight and develops a psychological theory – one pitched at the level of cognitive science, involving a high-level decomposition of the mind into functional units. The view has also been connected to a range of data in neuroscience, bolstering its plausibility…</p>
<p>So, how does the theory go? First, the GWS view stresses the idea that much of our mental processing occurs modularly. Modules are relatively isolated, “encapsulated” mechanisms devoted to solving limited, “domain-specific” problems. Modules work largely independently from each other and they are not open to “cross talk” coming from outside their focus of operation. A prime example is how the early vision system works to create the 3-D array we consciously experience. Our early-vision modules automatically take cues from the environment and deliver rapid output concerning what&#8217;s in front of us. For example, some modules detect edges, some the intersection of lines or “vertices,” some subtle differences in binocular vision, and so on. To work most efficiently, these modules employ built-in assumptions about what we&#8217;re likely to see. In this way, they can quickly take an ambiguous cue and deliver a reliable output about what we&#8217;re seeing. But this increase in speed leads to the possibility of error when the situation is not as the visual system assumes. <img src="/files/Research/Moral_Patienthood/Muller-Lyer.png" style="width:200px;height:76px;padding:10px;" align="right" alt="Muller-Lyer.png" />In the Müller-Lyer illusion [see right], two lines of the same length look unequal because of either inward- or outward-facing “points” on the end of the lines. And even if we know they&#8217;re the same length, because we&#8217;ve seen these dang lines hundreds of times, we still consciously see them as unequal. This is because the process of detecting the lines takes the vertices where the points attach to the lines as cues about depth. In the real world, when we see such vertices, we can reliably use them to tell us what&#8217;s closer to what. But the Müller-Lyer illusion uses this fact to trick early vision into seeing things incorrectly. The process is modular because it works automatically and it&#8217;s immune to correction from our conscious beliefs about the lines.</p>
<p>Modularity is held to be a widespread phenomenon in the mind. Just how widespread is a matter of considerable debate, but most researchers would accept that at least some processes are modular, and early perceptual processes are the best candidates. The idea of the GWS is that the workspace allows us to connect and integrate knowledge from a number of modular systems. This gives us much more flexible control of what we do. And this cross-modular integration would be especially useful to a mind more and more overloaded with modular processes. Hence, we get an evolutionary rationale for the development of a GWS: when modular processing becomes too unwieldy and when the complexity of the tasks we must perform increases, there will be advantages to having a cross-modular GWS.</p>
<p>Items in the global workspace are like things posted on a message board or a public blog. All interested parties can access the information there and act accordingly. They can also alter the info by adding their own input to the workspace. The GWS is also closely connected to short-term working memory. Things held in the workspace can activate working memory, allowing us to keep conscious percepts in mind as we work on problems. Also, the GWS is deeply intertwined with attention. We can activate attention to focus on specific items in the network. But attention can also influence what gets into the workspace in the first place. Things in the network can exert a global “top-down” influence on the rest of the mind, allowing for coordination and control that couldn&#8217;t be achieved by modules in isolation. To return to a functionalist way of putting things, if a system does what the GWS does, then the items in that system are conscious. That&#8217;s what consciousness amounts to [according to GWT].</p>
<p>[To sum up:] Much mental activity is nonconscious, occurring in low-level modules. However, when modular information is “taken up” by the GWS, it becomes available to a wide range of mental systems, allowing for flexible top-down control. This is the functional mark of consciousness.</p></blockquote>
<p>I won&#8217;t explain GWT any further here; see other sources for more detail.<a class="see-footnote" id="footnoteref260_0725jem" title="Baars (1988); Baars et al. (2013); Shanahan (2010), ch. 4; Dehaene (2014); Dehaene et al. (2014); Shevlin (2016); Franklin et al. 2012. I have not read these sources in full.  " href="#footnote260_0725jem">260</a> Instead, I jump once again to the primary issue that runs throughout this section<a class="see-footnote" id="footnoteref261_xei3azq" title="As with IIT, I am hardly an expert on GWT, so my criticisms could be misguided, but even if they are, I hope they will help to illustrate how I think about theories of consciousness.  " href="#footnote261_xei3azq">261</a> this time applied to GWT.</p>
<p>To be concrete, I&#8217;ll address Dehaene&#8217;s neurobiological version of GWT. What, exactly, is a conscious state, according to Dehaene?<a class="see-footnote" id="footnoteref262_faedewz" title="Quoted text is from Dehaene (2014), ch. 5:  When we say that we are aware of a certain piece of information, what we mean is just this: the information has entered into a specific storage area that makes it available to the rest of the brain. Among the millions of mental representations that constantly criss-cross our brains in an unconscious manner, one is selected because of its relevance to our present goals. Consciousness makes it globally available to all our high-level decision systems. We possess a mental router, an evolved architecture for extracting relevant information and dispatching it. The psychologist Bernard Baars calls it a &quot;global workspace&quot;: an internal system, detached from the outside world, that allows us to freely entertain our private mental images and to spread them across the mind's vast array of specialized processors (figure 24).  Figure 24, borrowed from Dehaene et al. (1998), is:    Dehaene continues:  According to this theory, consciousness is just brain-wide information sharing. Whatever we become conscious of, we can hold it in our mind long after the corresponding stimulation has disappeared from the outside world. That's because our brain has brought it into the workspace, which maintains it independently of the time and place at which we first perceived it. As a result, we may use it in whatever way we please. In particular, we can dispatch it to our language processors and name it; this is why the capacity to report is a key feature of a conscious state. But we can also store it in long-term memory or use it for our future plans, whatever they are…  …  Like the psychologist Bernard Baars, I believe that consciousness reduces to what the workspace does: it makes relevant information globally accessible and flexibly broadcasts it to a variety of brain systems…  Flexible information sharing requires a specific neuronal architecture to link the many distant and specialized regions of the cortex into a coherent role. Can we identify such a structure inside our brains? …Unlike the dense mosaic of cells that make up our skin, the brain comprises enormously elongated cells: neurons. With their long axon, neurons possess the property, unique among cells, of measuring up to meters in size. A single neuron in the motor cortex may send its axon to extraordinarily distant regions of the spinal cord, in order to command specific muscles. Most interestingly, …long-distance projection cells are quite dense in the cortex… From their locations in the cortex, nerve cells shaped like pyramids often send their axons all the way to the back of the brain or to the other hemisphere…  Importantly, not all brain areas are equally well connected. Sensory regions, such as the primary visual area V1, tend to be choosy and to establish only a small set of connections, primarily with their neighbors. Early visual regions are arranged in a coarse hierarchy: area V1 speaks primarily to V2, which in turns speaks to V3 and V4, and so on. As a result, early visual operations are functionally encapsulated: visual neurons initially receive only a small fraction of the retinal input and process it in relative isolation, without any &quot;awareness&quot; of the overall picture.  In the higher association areas of the cortex, however, connectivity loses its local nearest-neighbor or point-to-point character, thus breaking the modularity of cognitive operations. Neurons with long-distance axons are most abundant in the prefrontal cortex… This region connects to many other sites in the inferior parietal lobe, the middle and anterior temporal lobe, and the anterior and posterior cingulate areas that lie on the brain's midline. These regions have been identified as major hubs — the brain's main interconnection centers. All are heavily connected by reciprocal projections: if area A projects to area B, then almost invariably B also sends a projection back to A… Furthermore, long-distance connections tend to form triangles: if area A projects jointly to areas B and C, then they, in turn, are very likely to be interconnected.  These cortical regions are strongly connected to additional players, such as the central lateral and intralaminar nuclei of the thalamus (involved in attention, vigilance, and synchronization), the basal ganglia (crucial for decision making and action), and the hippocampus (essential for memorizing the episodes of our lives and for recalling them). Pathways linking the cortex with the thalamus are especially important. The thalamus is a collection of nuclei, each of which enters into a tight loop with at least one region of the cortex and often many of them at once. Virtually all regions of the cortex that are directly interconnected also share information via a parallel information route through a deep thalamic relay. Inputs from the thalamus to the cortex also play a fundamental role in exciting the cortex and maintaining it in an &quot;up&quot; state of sustained activity…  The workspace thus rests on a dense network of interconnected brain regions — a decentralized organization without a single physical meeting site. At the top of the cortical hierarchy, an elitist board of executives, distributed in distant territories, stays in sync by exchanging a plethora of messages… We are now in a position to understand why these associative areas systematically ignite whenever a piece of information enters our awareness: those regions possess precisely the long-distance connectivity needed to broadcast messages across the long distances of the brain.  Later in the same chapter, Dehaene describes how his theory says a visual percept would be come conscious:  Suppose we could track all the connections that are activated as we consciously recognize a face… What kind of network would we see? Initially, very short connections, located inside our retinas, clean up the incoming image. The compressed image is then sent, via the massive cable of the optic nerve, to the visual thalamus, then on to the primary visual area in the occipital lobe. Via local U-shaped fibers, it gets progressively transmitted to several clusters of neurons in the right fusiform gyrus, where researchers have discovered… patches of neurons tuned to faces. All this activity remains unconscious. What happens next? Where do the fibers go? The Swiss anatomist Stéphanie Clarke found the surprising answer [Di Virgilio &amp; Clarke (1997)]: all of a sudden, long-distance axons allow the visual information to be dispatched to virtually any corner of the brain. From the right inferior temporal lobe, massive and direct connections project, in a single synaptic step, to distant areas of the associative cortex, including those in the opposite hemisphere. The projections concentrate in the inferior frontal cortex (Broca's area) and in the temporal association cortex (Wernicke's area). Both regions are key nodes of the human language network — and at this stage, therefore, words begin to be attached to the incoming visual information.  Because these regions themselves participate in a broader network of workspace areas, the information can now be further disseminated to the entire inner circle of higher-level executive systems; it can circulate in a reverberating assembly of active neurons. According to my theory, access to this dense network is all that is needed for the incoming information to become conscious.  With this (and more) in place, Dehaene finally describes what his theory says a particular conscious state is:  [My theory] proposes that a conscious state is encoded by the stable activation, for a few tenths of a second, of a subset of active workspace neurons. These neurons are distributed in many brain areas, and they all code for different facets of the same mental representation. Becoming aware of the Mona Lisa involves the joint activation of millions of neurons that care about objects, fragments of meaning, and memories.  During conscious access, thanks to the workspace neurons' long axons, all these neurons exchange reciprocal messages, in a massively parallel attempt to achieve a coherent and synchronous interpretation. Conscious perception is complete when they converge. The cell assembly that encodes this conscious content is spread throughout the brain: fragments of relevant information, each distilled by a distinct brain region, cohere because all the neurons are kept in sync, in a top-down manner, by neurons with long-distance axons.  Neuronal synchrony may be a key ingredient. There is growing evidence that distant neurons form giant assemblies by synchronizing their spikes with ongoing background electrical oscillations. If this picture is correct, the brain web that encodes each of our thoughts resembles a swarm of fireflies that harmonize their discharges according to the overall rhythm of the group's pattern. In the absence of consciousness, moderate-size cell assemblies may still synchronize locally — for instance, when we unconsciously encode a word's meaning inside the language networks of our left temporal lobe. However, because the prefrontal cortex does not gain access to the corresponding message, it cannot be broadly shared and therefore remains unconscious.  Let us conjure one more mental image of this neuronal code for consciousness. Picture the sixteen billion cortical neurons in your cortex. Each of them cares about a small range of stimuli. Their sheer diversity is flabbergasting: in the visual cortex alone, one finds neurons that care about faces, hands, objects, perspective, shape, lines, curves, colors, 3-D depth… Each cell conveys only a few bits of information about the perceived scene. Collectively, though, they are capable of representing an immense repertoire of thoughts. The global workspace model claims that, at any given moment, out of this enormous potential set, a single object of thought gets selected and becomes the focus of our consciousness. At this moment, all the relevant neurons activate in partial synchrony, under the aegis of a subset of prefrontal cortex neurons.  It is crucial to understand that, in this sort of coding scheme, the silent neurons, which do not fire, also encode information. Their muteness implicitly signals to others that their preferred feature is not present or is irrelevant to the current mental scene. A conscious content is defined just as much by its silent neurons as by its active ones.  (According to this page, the figure above from Dehaene et al. (1998) was copyrighted in 1998 by the National Academy of Sciences and does not require permission for noncommercial use.)  " href="#footnote262_faedewz">262</a></p>
<blockquote><p>…a conscious state is encoded by the stable activation, for a few tenths of a second, of a subset of active workspace neurons. These neurons are distributed in many brain areas, and they all code for different facets of the same mental representation. Becoming aware of the Mona Lisa involves the joint activation of millions of neurons that care about objects, fragments of meaning, and memories.</p>
<p>During conscious access, thanks to the workspace neurons’ long axons, all these neurons exchange reciprocal messages, in a massively parallel attempt to achieve a coherent and synchronous interpretation. Conscious perception is complete when they converge.</p></blockquote>
<p>Perhaps Dehaene is right that a conscious state results from the stable activation of workspace neurons that collectively code for all the different facets of that state, which occurs when the messages being passed by these neurons “converge.” But I still want to know: <em>how, exactly?</em> How does merely pooling information into a global workspace, allowing that information to be accessed by diverse cognitive modules, result in a phenomenal experience? Why should this make the brain insist that it is “conscious” of some things and not others? Why does this result in the intuition of an explanatory gap (the “hard problem”)? And so on.</p>
<p><span style="float: right;">[<a href="/node/858/edit/61">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="MoreSatisfying">What a more satisfying theory of consciousness could look like</h4>
<p>I could make similar comments about many other theories of consciousness, for example the theories which lean heavily on prediction error minimization (<a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00096/full">Hohwy 2012</a>; <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/whatever-next-predictive-brains-situated-agents-and-the-future-of-cognitive-science/33542C736E17E3D1D44E8D03BE5F4CD9">Clark 2013</a>), recurrent processing (<a href="http://www.tandfonline.com/doi/abs/10.1080/17588921003731586">Lamme 2010</a>), higher-order representations (<a href="http://plato.stanford.edu/entries/consciousness-higher/">Carruthers 2016</a>), and “multiple drafts” (<a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">Dennett 1991</a>). In all these cases, my concern is not so much that they are <em>wrong</em> (though they may be), but instead that they don&#8217;t “go far enough.”</p>
<p>In fact, I think it&#8217;s plausible that several of these theories say something important about how various brain functions work, including brain functions that are critical to conscious experience (in humans, at least). Indeed, on my view, it is quite plausible the case that consciousness depends on integrated information and higher-order representations.<a class="see-footnote" id="footnoteref263_7hs6e4m" title="However, I do not agree with those who argue that higher-order theories strongly imply that consciousness is rare. I suspect that even if consciousness is a fairly complicated, self-representational, higher-order phenomenon, it might still be implemented by small insect ganglia. The question is: is it?  " href="#footnote263_7hs6e4m">263</a> And it would not surprise me if human consciousness also depends on prediction error minimization, recurrent processing, “multiple drafts,” and a global workspace. The problem is just that none of these ideas, or even <em>all</em> of these ideas <em>combined</em>, seem sufficient to explain, with a decent amount of precision, most of the key features of consciousness we know about.</p>
<p>Graziano&#8217;s own attention schema theory has this problem, too, but (in my opinion) it “goes further” than most theories do (though, not by much).<a class="see-footnote" id="footnoteref264_p5xp1nj" title="See Graziano (2013).  Note that Graziano's theory is vulnerable to objections over &quot;vague psychological language&quot; like those I raise elsewhere. For example, Brian Tomasik suggests (here) that much of Graziano's broad theory seems to be satisfied by the Windows Task Manager, or a slightly modified version of it. I doubt that Graziano thinks the Windows Task Manager is conscious, but if not, the analogy to Windows Task Manager may allow Graziano to state his theory more precisely, in a way that excludes the Windows Task Manager from consciousness.  " href="#footnote264_p5xp1nj">264</a> In fact, it does so in part by assuming that integrated information, higher-order representations, a global workspace, and some features of Dennett&#8217;s “multiple drafts” account <em>do</em> play a role in consciousness, and then Graziano <em>adds</em> some details to that foundation, to construct a theory of consciousness which (in my opinion) explains the explananda of consciousness a bit more thoroughly, and with a bit more precision, than any of those earlier theories do on their own.</p>
<p>Note that I likely have this opinion of Graziano&#8217;s theory largely because it offers an (<a href="#AppendixF">illusionist</a>) explanation of our dualist intuitions, and our dualist intuitions constitute one explanandum of consciousness that, as far as I can tell, the theories I briefly surveyed above (temporal binding theory, IIT, GWT) don&#8217;t do much to explain. As such, non-illusionists may find Graziano&#8217;s theory less helpful.</p>
<p>Furthermore, I can think of ways to supplement Graziano&#8217;s theory with <em>additional</em> details that explain some additional consciousness explananda beyond what Graziano&#8217;s theory (as currently stated) can explain. For example, Graziano doesn&#8217;t say much about the ineffability of qualia, but I think a generalization of Gary Drescher&#8217;s “qualia as gensyms” account,<a class="see-footnote" id="footnoteref265_arxxx7p" title="See Drescher (2006), ch. 2, and notes from my conversation with Gary Drescher.  Many readers might wonder what a &quot;gensym&quot; is. It is a function for creating symbols in the Lisp programming language, though the term &quot;gensym&quot; is also sometimes used to refer to the generated symbol itself. I have not used Lisp before, but Daniel Dewey, a Program Officer for the Open Philanthropy Project, offers this brief explanation:  Symbols are a Lisp data type, like numbers or strings. For Drescher's purposes, the important feature of symbols is that the only acceptable operation on a symbol is checking whether it's identical with, or different from, another symbol. Instances of other data types, like numbers or strings, can be checked for equality (1 != 2, or &quot;Gary Drescher&quot; = &quot;Gary Drescher&quot;), but can also be operated on in other ways, exposing additional information they contain; for example, numbers can be added or multiplied, and strings can be broken up into their component characters. Symbols don't contain any other information, and can't be added, multiplied, or split apart; they can only be compared. A program can tell that the symbols 'italic' and 'italic' are identical, and that the symbols 'italic' and 'bold' are different, but it can't get any other information about 'italic' or 'bold'. &quot;Gensym&quot; is a function that generates a new symbol that's guaranteed not to be identical with any symbol that's been generated so far. (Because some versions of Lisp are designed to be useful instead of philosophically pure, some versions of Lisp don't totally follow these properties, and add more information to symbols that programs can access, but that's not relevant to Drescher's analogy.)  Note that the core idea of Drescher's &quot;qualia as gensyms&quot; account was described at least as early as Chalmers (1990):  Very briefly, here is what I believe to be the correct account of why we think we are conscious, and why it seems like a mystery. The basic notion is that of pattern processing. This is one of the things that the brain does best. It can take raw physical data, usually from the environment but even from the brain itself, and extract patterns from these. In particular, it can discriminate on the basis of patterns. The original patterns are in the environment, but they are transformed on their path through neural circuits, until they are represented as quite different patterns in the cerebral cortex. This process can also be represented as information flow (not surprisingly), from the environment into the brain. The key point is that once the information flow has reached the central processing portions for the brain, further brain function is not sensitive to the original raw data, but only to the pattern (to the information!) which is embodied in the neural structure.  Consider color perception, for instance. Originally, a spectral envelope of light-wavelengths impinges upon our eyes. Immediately, some distinctions are collapsed, and some pattern is processed. Three different kinds of cones abstract out information about how much light is present in various overlapping wavelength-ranges. This information travels down the optic nerve (as a physical pattern, of course), where it gets further transformed by neural processing into an abstraction about how much intensity is present on what we call the red-green, yellow-blue, and achromatic scales. What happens after this is poorly-understood, but there is no doubt that by the time the central processing region is reached, the pattern is very much transformed, and the information that remains is only an abstraction of certain aspects of the original data.  Anyway, here is why color perception seems strange. In terms of further processing, we are sensitive not to the original data, not even directly to the physical structure of the neural system, but only to the patterns which the system embodies, to the information it contains. It is a matter of access. When our linguistic system (to be homuncular about things) wants to make verbal reports, it cannot get access to the original data; it does not even have direct access to neural structure. It is sensitive only to pattern. Thus, we know that we can make distinctions between certain wavelength distributions, but we do not know how we do it. We've lost access to the original wavelengths - we certainly cannot say &quot;yes, that patch is saturated with 500-600 nm reflections&quot;. And we do not have access to our neural structure, so we cannot say &quot;yes, that's a 50 Hz spiking frequency&quot;. It is a distinction that we are able to make, but only on the basis of pattern. We can merely say &quot;Yes, that looks different from that.&quot; When asked &quot;How are they different?&quot;, all we can say is &quot;Well, that one's red, and that one's green&quot;. We have access to nothing more - we can simply make raw distinctions based on pattern - and it seems very strange.  So this is why conscious experience seems strange. We are able to make distinctions, but we have direct access neither to the sources of those distinctions, or to how we make the distinctions. The distinctions are based purely on the information that is processed. Incidentally, it seems that the more abstract the information-processing - that is, the more that distinctions are collapsed, and information recoded - the stranger the conscious experience seems. Shape- perception, for instance, strikes us as relatively non-strange; the visual system is extremely good at preserving shape information through its neural pathways. Color and taste are strange indeed, and the processing of both seems to involve a considerable amount of recoding.  The story for &quot;internal perception&quot; is exactly the same. When we reflect on our thoughts, information makes its way from one part of the brain to another, and perhaps eventually to our speech center. It is to only certain abstract features of brain structure that the process is sensitive. (One might imagine that if somehow reflection could be sensitive to every last detail of brain structure, it would seem very different.) Again, we can perceive only via pattern, via information. The brute, seemingly non-concrete distinctions thus entailed are extremely difficult for us to understand, and to articulate. That is why consciousness seems strange, and that is why the debate over the Mind-Body Problem has raged for thousands of years.  A related idea, cast in terms of neural networks, can be found in Loosemore (2012), which Tomasik (2014) summarizes like this:  Loosemore presents what I consider a biologically plausible sketch of connectionist concept networks in which a concept's meaning is assessed based on related concepts. For instance, &quot;chair&quot; activates &quot;legs&quot;, &quot;back&quot;, &quot;seat&quot;, &quot;sitting&quot;, &quot;furniture&quot;, etc. (p. 294). As we imagine lower-level concepts, the associations that get activated become more primitive. At the most primitive level, we could ask for the meaning of something like &quot;red&quot;. Since our &quot;red&quot; concept node connects directly to sensory inputs, we can't decompose &quot;red&quot; into further understandable concepts. Instead, we &quot;bottom out&quot; and declare &quot;red&quot; to be basic and ineffable. But our concept-analysis machinery still claims that &quot;red&quot; is something — namely, some additional property of experience. This leads us to believe in qualia as &quot;extra&quot; properties that aren't reducible.  " href="#footnote265_arxxx7p">265</a> plus the qualia-related suggestions of <a href="http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1350">Sloman &amp; Chrisley (2003)</a>,<a class="see-footnote" id="footnoteref266_xu73wb6" title="I quote the relevant passage below, though it is likely hard to follow without first reading the rest of Sloman &amp; Chrisley (2003):  …we explain qualia by providing an explanation of the phenomena that generate philosophical thinking of the sort found in discussions of qualia. Note that we are not talking merely about explaining behaviour, for we have repeatedly discussed explanations of how internal, possibly externally undetectable, states and processes can occur in certain virtual machine architectures.  The concept of ‘qualia' arose out of philosophical discussions of our ability to attend to aspects of internal information processing (internal self-awareness). That possibility is inherent in any system that has the H-CogAff architecture (see section V.5), though different varieties of the phenomenon will be present in different architectures, depending on the forms of representation and modes of monitoring available to meta-management. Some forms will provide the ability to attend not only to what is perceived in the environment, but to also features of the mode of perception that are closely related to properties of intermediate sensory data-structures…  Consider perceiving a table. Most adults (though not young children) can attend not only to the table and its fixed 3-D shape, but also to the 2-D appearance of the table in which angles and relative lengths of lines change as you change your viewpoint (or the table is rotated…). The appearance can also change as you squint, tap your eyeball, put on coloured spectacles, etc. This is exactly the sort of thing that led philosophers (and others) to think about qualia (previously called ‘sense data') as something internal, non-physical, knowable only from inside, etc. If meta-management processes have access to intermediate perceptual states, then this can produce self-monitoring of sensory contents, leading robot philosophers with this architecture to discover ‘the problem(s) of qualia'. And the same would go for anything which has that architecture: six robots with the H-CogAff architecture discussing various aspects of their experience of the same table seen from different viewpoints could get bogged down discussing consciousness, just like six blind philosophers.  What qualia are: qualia are what humans or future human-like robots refer to when referring to the objects of internal self observation.  …there is another way in which an information processing system can refer to its own states, which explain some aspects of notions of qualia.  Concept formation is a huge topic, but, for now, consider the likelihood that in many organisms there are processes of concept formation which emerge from interactions between a self-organising classification system and the information fed into it. A well known example of a mechanism that can achieve this is a Kohonen net (Kohonen, 1989). We describe as 'architecture-driven' sets of concepts created within an architecture as part of the individual history of the organism or machine. If individual A1 develops its own concepts used to describe internal states of another agent A2 on the basis of assumptions about the information processing architecture of A2, then the concepts are architecture-driven in relation to A1, and architecture-based in relation to A2, or any other system with the assumed architecture. It is possible for A1 to use architecture-driven architecture-based concepts to refer to itself. Architecture-driven concepts can refer to many different sorts of things, e.g. colours and shapes of objects in the environment, tastes, tactile qualities of objects, etc., if the concepts are developed by an organism on the basis of perceptual experience of those objects.  …Now suppose that an agent A with a meta-management system… uses a self-organising process to develop concepts for categorising its own internal virtual machine states as sensed by internal monitors. These will be architecture-driven concepts, but need not be architecture-based if the classification mechanism does not use an implicit or explicit theory of the architecture of the system it is monitoring, but merely develops a way of organising its 'sensory' input data. If such a concept C is applied by A to one of its internal states, then the only way C can have meaning for A is in relation to the set of concepts of which it is a member, which in turn derives only from the history of the self-organising process in A…  This means that if two agents A and B have each developed concepts in this way, then if A uses its concept Ca, to think the thought 'I am having experience Ca', and B uses its concept Cb, to think the thought 'I am having experience Cb', the two thoughts are intrinsically private and incommunicable, even if A and B actually have exactly the same architecture and have had identical histories leading to the formation of structurally identical sets of concepts. A can wonder: 'Does B have an experience described by a concept related to B as my concept Ca is related to me?'' But A cannot wonder 'Does B have experiences of type Ca', for it makes no sense for the concept Ca to be applied outside the context for which it was developed, namely one in which A's internal sensors classify internal states. They cannot classify states of B.  When different agents use architecture-driven concepts, produced by self organising classifiers, to classify internal states of a virtual machine, and are not even partly explicitly defined in relation to some underlying causes (e.g. external objects or a presumed architecture producing the sensed states), then there is nothing to give those concepts any user-independent content, in the way that our colour words have user-independent content because they refer to properties of physical objects in a common environment. Thus self-referential architecture-driven concepts used by different individuals are strictly non-comparable: not only can you not know whether your concepts are the same as mine, the question is incoherent. If we use the word 'qualia' to refer to the virtual machine states or entities to which these concepts are applied, then asking whether the qualia in two experiencers are the same would then would be analogous to asking whether two spatial locations in different frames of reference are the same, when the frames are moving relative to each other. But it is hard to convince some people that this makes no sense, because the question is grammatically well-formed. Sometimes real nonsense is not obvious nonsense.  We have now indicated how the process of coming to think about and ask questions about qualia is explained by the nature of the architecture of the thinker. The process arises when architecture-driven concepts produced by a self-monitoring sub-system refer internally. In talking about hese concepts we are using architecture-based concepts.  See also Sloman &amp; Chrisley (2016).  " href="#footnote266_xu73wb6">266</a> plus the usual points about how the fine-grained details of our percepts “overflow” the concepts we might use to describe those percepts,<a class="see-footnote" id="footnoteref267_2yux6es" title="As Carruthers (2016) puts it, conscious states will seem &quot;ineffable&quot; or &quot;indescribable&quot; because those states  …have fine-grained contents that can slip through the mesh of any conceptual net. We can always distinguish many more shades of red than we have concepts for, or could describe in language (other than indexically — e.g., 'That shade').  " href="#footnote267_2yux6es">267</a> explain that explanandum pretty well, and could be added to Graziano&#8217;s account. Graziano also doesn&#8217;t explain why we have the conviction that qualia cannot be “just” brain processes and nothing more, but intuitively it seems to me that an inference algorithm inspired by <a href="http://analysis.oxfordjournals.org/content/29/2/48.extract">Armstrong (1968)</a> might explain that conviction pretty well.<a class="see-footnote" id="footnoteref268_a97p3s7" title="Armstrong (1968):  To produce [the &quot;headless woman&quot;] illusion, a woman is placed on a suitably illuminated stage with a dark background and a black cloth is placed over her head. It looks to the spectators as if she has no head. The spectators cannot see the woman's head. But they gain the impression that they can see that the woman has not got a head. (Cf. 'I looked inside, and saw that he was not there.') Unsophisticated spectators might conclude that the woman did not in fact have a head.  What the example shows is that, in certain cases, it is very natural for human beings to pass from something that is true: 'I do not perceive that X is Y', to something that may be false: 'I perceive that X is not Y'. We have here one of those unselfconscious and immediate movements of the mind of which Hume spoke, and which he thought to be so important in our mental life.  It can now be suggested by the Materialist that we tend to pass from something that is true:  I am not introspectively aware that mental images are brain-processes  to something that is false:  I am introspectively aware that mental images are not brain-processes.  …Does ordinary experience, then, involve the illusion of the truth of anti-materialism? The Materialist can now admit that it does involve such an illusion, but urge that the illusion is no more than the illusion involved in the &quot;headless woman&quot;: the taking of an absence of awareness of X to be an awareness of the absence of X.  See also Smart (2006), a kind of follow-up to Armstrong's article.  " href="#footnote268_a97p3s7">268</a> But why do we find it so hard to even <em>make sense</em> of the hypothesis of <a href="illusionism">illusionism</a> about consciousness, even though we don&#8217;t have trouble understanding how other kinds of illusions could be illusions? Perhaps an algorithm inspired by <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00010">Kammerer (2016)</a> could instantiate this feature of human consciousness.<a class="see-footnote" id="footnoteref269_s7fso4g" title="Kammerer's explanation of his &quot;theoretical introspection hypothesis&quot; (TIH) is several pages long (and very much worth reading), but Frankish (2016c) provides a simplified but helpful summary:  Introspection is informed by an innate and modular theory of mind and epistemology, which states that (a) we acquire perceptual information via mental states — experiences — whose properties determine how the world appears to us, and (b) experiences can be fallacious, a fallacious experience of A being one in which we are mentally affected in the same way as when we have a veridical experience of A, except that A is not present.  Here, I'll interject to add that Kammerer does not require that our innate &quot;theories&quot; of mind and epistemology (which inform our introspection) represent or &quot;state&quot; (a) and (b). For example, one could give a dispositionalist account of these innate theories of mind and epistemology which nevertheless roughly capture statements (a) and (b). If anything like Kammerer's account is true, I would (personally) expect it to be a dispositionalist account. Anyway, back to Frankish's summary:  Given this theory, Kammerer notes, it is incoherent to suppose that we could have a fallacious experience [i.e. an illusory experience] of an experience, E. For that would involve being mentally affected in the same way as when we have a veridical experience of E, without E being present. But when we are having a veridical experience of E, we are having E (otherwise the experience wouldn't be veridical). So, if we are mentally affected in the same way as when we are having a veridical experience of E, then we are having E. So E is both present and not present, which is contradictory…  Kammerer proposes that this explains the peculiar hardness of the illusion problem. The illusionist thesis cannot be coherently articulated using our everyday concept of illusion, which is rooted in our naïve concept of fallacious experience. Moreover, if the naïve theory Kammerer sketches does inform our introspective activity, then we shall not be able to form any imaginative conception of what it would be like for illusionism to be true. Hence the common claim that, where consciousness is concerned, appearance is reality. As Kammerer stresses, this does not mean that illusionism actually is incoherent. It simply means that in order to state it we must employ a technical concept of illusion — as, say, a cognitively impenetrable, non-veridical mental representation that is systematically generated in certain circumstances.  Frankish notes that one might develop a similar illusionist account of our sense that introspective acquaintance is &quot;direct&quot;:  Of course, even if Kammerer is right about the source of our intuitive resistance to illusionism, this would not show that illusionism is true, though it would help to dispel one common objection to it. Realists will say that phenomenality is not an illusion even in a technical sense: our relation to our phenomenal properties is one of direct acquaintance, which does not depend on potentially fallible representational processes. Perhaps Kammerer could employ the strategy again here, arguing that our concept of introspective acquaintance is also a theoretical one.  Kammerer's account depends on a &quot;theory theory&quot; of introspection, according to which introspection interprets its (mental) objects through a theory or theories, e.g. a theory of mind. Kammerer's example of such a &quot;theory theory&quot; of introspection is that of Nichols &amp; Stich (2003), illustrated with &quot;boxological&quot; diagrams like this:    Personally, I expect introspection will turn out to be a big mess of competing processes, a la Schwitzgebel (2012):  My thesis is: introspection is not a single process but a plurality of processes. It's a plurality both within and between cases: most individual introspective judgments arise from a plurality of processes (that's the within-case claim), and the collection of processes issuing in introspective judgments differs from case to case (that's the between-case claim). Introspection is not the operation of a single cognitive mechanism or small collection of mechanisms. Introspective judgments arise from a shifting confluence of many processes, recruited opportunistically.  The following analogy might be helpful. Suppose you're at a psychology conference or a high school science fair and you're trying to quickly take in a poster. You are not equipped with a dedicated faculty of poster-taking-in. Rather, you opportunistically deploy a variety of processes with the aim of getting the gist of the poster: you look at the poster-or perhaps only listen to a recital of portions of it, if you're in the mood or visually impaired-you attend to what the poster's author is saying about it; you follow out implications, charitably rejecting some interpretations of the poster's content as too obviously foolish; you think about what it makes sense to claim given the social and scientific context and other work by the author or the author's advisor, if you know any; you pose questions and assess the author's responses both for overt content and for emotional flavor. Although the cognitive systems involved range widely and are not dedicated just to taking in posters, not just any activity counts as taking in a poster-one's judgments about the poster must aim to reflect a certain kind of sensitivity to its contents. Likewise for introspection, I will suggest: the cognitive activities range widely and vary between cases-that is the main claim I will defend-and yet, as I will suggest near the end of this essay, it wouldn't be natural to call a judgment introspective if it weren't formed with the aim or intention of reflecting a certain kind of sensitivity to the target mental state.  Schwitzgebel illustrates his own theory of introspection like this:    If one has a Schwitzgebel-like model of how introspection works, this poses a challenge to a Kammerer-like explanation of why it seems to us that consciousness, uniquely, cannot be an illusion. However, it could still be the case that something like Kammerer's account is an important piece of the &quot;big mess&quot; of introspection, and thereby goes a long way toward explaining what it aims to explain concerning phenomenal consciousness.  " href="#footnote269_s7fso4g">269</a></p>
<p>And so on. I take this to be the sort of work that <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00012">Marsinek &amp; Gazzaniga (2016)</a> call for in response to <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00002">Frankish (2016b)</a>’s defense of illusionism about consciousness:</p>
<blockquote><p>One major limitation of [illusionism as described by Frankish] is that it does not offer any mechanisms for how the illusion of phenomenal feelings works. As anyone who has seen a magic trick knows, it’s quite easy to say that the trick is an illusion and not the result of magical forces. It is much, much harder to explain how the illusion was created. Illusionism can be a useful theory if mechanisms are put forth that explain how the brain creates an illusion of phenomenal feelings…</p>
<p>…phenomenal consciousness may not be the product of one grand illusion. Instead, phenomenal consciousness may be the result of multiple ‘modular illusions’. That is, different phenomenal feelings may arise from the limitations or distortions of different cognitive modules or networks… Illusionism therefore may not have to account for one grand illusion, but for many ‘modular illusions’ that each have their own neural mechanisms.</p></blockquote>
<p>My suggestions also seem to be in line with the “<a href="https://www.researchgate.net/project/Computational-qualia">computational qualia</a>” project described by Ron Chrisley and Aaron Sloman:</p>
<blockquote><p>Making robot minds more like our own would be facilitated by finding designs that make robots susceptible to the same (mis-)conceptions concerning perception, experience and consciousness that humans have. Making a conscious-like robot will thus involve, beyond making robots that perceive, act, learn, remember, etc., making robots that find it natural to believe that their inner states have an intrinsic, immediate, ineffable and private character. The goal is to investigate what it would take for there to be a unified, virtual-machine-based explanation of a system&#8217;s tendencies to believe such things about itself.</p></blockquote>
<p>If I was a career consciousness theorist, I think this is how I would try to make progress toward a theory of consciousness, given my current intuitions about what is most likely to be successful:</p>
<ol><li>First, I&#8217;d write some “toy programs” that illustrate some of the key aspects of a Graziano / Drescher / Sloman / Armstrong / Kammerer (GDSAK) account to consciousness.<a class="see-footnote" id="footnoteref270_bzxzksq" title="By &quot;toy program,&quot; I have in mind something perhaps 5x-20x as large and complicated as Brian Tomasik's &quot;Simple Program to Illustrate the Hard Problem of Consciousness.&quot;  &#9;" href="#footnote270_bzxzksq">270</a></li>
<li>If step (1) seemed productive, I&#8217;d consider taking on the more ambitious project of working with a team of software engineers and machine learning experts to code a GDSAK-inspired cognitive architecture<a class="see-footnote" id="footnoteref271_7umxj8k" title="See Kotseruba et al. (2016) for an overview of cognitive architectures.  &#9;" href="#footnote271_7umxj8k">271</a> for controlling an agent in a simple virtual 3D world. We&#8217;d release the source code, and we&#8217;d write an explanation of how we think it explains, with some precision, many of the key explananda of consciousness.</li>
<li>We&#8217;d think about which features of our own everyday internal experiences, including our felt confusions about consciousness, don&#8217;t yet seem to be captured by the cognitive architecture we&#8217;ve coded, and we&#8217;d try to find ways to add those features to the cognitive architecture, and then explain how we think our additions to the cognitive architecture capture those additional features of consciousness.</li>
<li>We&#8217;d do the same thing for additional consciousness explananda drawn not from our own internal experiences, but from (reliable, validated) self-reports from others, e.g. from experimental studies and from brain lesion cases.<a class="see-footnote" id="footnoteref272_ec4fazw" title="If these first three steps of the project were described in a book, the structure of the exposition might be similar to that of Baars (1988), chs. 2-9 — but much longer, and with links to open-source code for every version of the program/model.  &#9;" href="#footnote272_ec4fazw">272</a></li>
<li>We&#8217;d invite others to explain why they don&#8217;t think this cognitive architecture captures the explananda we claim it captures, and which additional most-important explananda are still not captured by the architecture, and we&#8217;d try to modify and extend the cognitive architecture accordingly, and then explain why we think those modifications are successful.</li>
<li>We&#8217;d use the latest version of the cognitive architecture to make novel predictions about what human subjects will self-report under various experimental conditions if their consciousness is similar in the right ways to our cognitive architecture, and then test those predictions, and modify the cognitive architecture in response to the experimental results.</li>
</ol><p>One caveat to all this is that I&#8217;m not sure the cognitive architecture could ever be <em>run</em> in this case, as some parts of the code would have to be left as “black boxes” that we don&#8217;t know how to code. Coding a virtual agent that <em>really</em> acted like a conscious human, including in its generated speech about qualia, might be an <a href="https://en.wikipedia.org/wiki/AI-complete">AI-complete problem</a>. However, the hope would be that all the incomplete parts of the code wouldn&#8217;t be specific to <em>consciousness</em>, but would concern other features, such as general-purpose learning. As a result, the predictions generated from the cognitive architecture couldn&#8217;t be directly computed, but would instead need to be argued for, as in usual scientific practice.<a class="see-footnote" id="footnoteref273_4irlfy6" title="Another caveat about this project concerns the moral implications of trying to build potentially-conscious machines (Metzinger 2010, pp. 194-196):  Imagine you are a member of an ethics committee considering scientific grant applications. One says:  We want to use gene technology to breed mentally retarded human infants. For urgent scientific reasons, we need to generate human babies possessing certain cognitive, emotional, and perceptual deficits. This is an important and innovative research strategy, and it requires the controlled and reproducible investigation of the retarded babies' psychological development after birth. This is not only important for understanding how our own minds work but also has great potential for healing psychiatric diseases. Therefore, we urgently need comprehensive funding.  No doubt you will decide immediately that this idea is not only absurd and tasteless but also dangerous. One imagines that a proposal of this kind would not pass any ethics committee in the democratic world. The point of this thought experiment, however, is to make you aware that the unborn [conscious machines] of the future would have no champions on today's ethics committees. The first machines satisfying a minimally sufficient set of conditions for conscious experience and selfhood would find themselves in a situation similar to that of the genetically engineered retarded human infants. Like them, these machines would have all kinds of functional and representational deficits — various disabilities resulting from errors in human engineering. It is safe to assume that their perceptual systems — their artificial eyes, ears, and so on—would not work well in the early stages. They would likely be half-deaf, half-blind, and have all kinds of difficulties in perceiving the world and themselves in it — and if they were true [conscious machines], they would, ex hypothesi, also be able to suffer.  If they had a stable bodily self-model, they would be able to feel sensory pain as their own pain. If their postbiotic self-model was directly anchored in the low-level, self-regulatory mechanisms of their hardware — just as our own emotional self-model is anchored in the upper brainstem and the hypothalamus — they would be consciously feeling selves. They would experience a loss of homeostatic control as painful, because they had an inbuilt concern about their own existence. They would have interests of their own, and they would subjectively experience this fact. They might suffer emotionally in qualitative ways completely alien to us or in degrees of intensity that we, their creators, could not even imagine. In fact, the first generations of such machines would very likely have many negative emotions, reflecting their failures in successful self-regulation because of various hardware deficits and higher-level disturbances. These negative emotions would be conscious and intensely felt, but in many cases we might not be able to understand or even recognize them.  Take the thought experiment a step further. Imagine these postbiotic [conscious machines] as possessing a cognitive self-model — as being intelligent thinkers of thoughts. They could then not only conceptually grasp the bizarreness of their existence as mere objects of scientific interest but also could intellectually suffer from knowing that, as such, they lacked the innate &quot;dignity&quot; that seemed so important to their creators. They might well be able to consciously represent the fact of being only second-class sentient citizens, alienated postbiotic selves being used as interchangeable experimental tools. How would it feel to &quot;come to&quot; as an advanced artificial subject, only to discover that even though you possessed a robust sense of selfhood and experienced yourself as a genuine subject, you were only a commodity?  The story of the first artificial [conscious machines], those postbiotic phenomenal selves with no civil rights and no lobby in any ethics committee, nicely illustrates how the capacity for suffering emerges along with the phenomenal [self]… It also presents a principled argument against the creation of artificial consciousness as a goal of academic research…  This kind of worry might constitute another reason to leave some parts of the cognitive architecture as &quot;black boxes,&quot; and never actually run the cognitive architecture.  " href="#footnote273_4irlfy6">273</a></p>
<p>Perhaps this process sounds like a lot of work. Surely, it is. But it is not impossible. In fact, it is not too dissimilar from the process Bernard Baars, Stan Franklin, and others have used to implement global workspace theory in the LIDA cognitive architecture.</p>
<p>Moreover, the problem of consciousness is <em>worth</em> a lot of work, especially if you share my intuition that it may be the most important criterion for moral patienthood. A good theory of consciousness could help us understand which animals and computer programs we should morally care about, and what we can do to benefit them. Without such knowledge, it is difficult for altruists to target their limited resources efficiently.</p>
<p><span style="float: right;">[<a href="/node/858/edit/62">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="AppendixC">Appendix C. Evidence concerning unconscious vision</h3>
<p>In this appendix, I summarize much of the evidence cited in favor of the theory that human visual processing occurs in multiple streams, only one of which leads to conscious visual experience, as described briefly in an <a href="#UnconsciousVision">earlier section</a>. To simplify the exposition, I present here only the <em>positive</em> case for this theory, even though there is also substantial evidence that challenges the theory (see below), and thus I think we should only assign it (or something like it) moderate credence.</p>
<p>My primary source for most of what follows is <a href="https://global.oup.com/academic/product/sight-unseen-9780199596966">Goodale &amp; Milner (2013)</a>.<a class="see-footnote" id="footnoteref274_tqcwgzk" title="But see also Snowden et al. (2012), ch. 11; Goodale &amp; Ganel (2016); Kravitz et al. (2011). For wide-ranging discussions of this topic, see the essays in Gangopadhyay et al. (2010).  " href="#footnote274_tqcwgzk">274</a> (Hereafter, I refer to Goodale &amp; Milner as “G&amp;M,” and I refer to their 2013 book as “G&amp;M-13.”)</p>
<p><span style="float: right;">[<a href="/node/858/edit/63">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="MultipleVisionSystems">Multiple vision systems in simpler animals</h4>
<p>First, consider “vision for action” in organisms much simpler than humans:<a class="see-footnote" id="footnoteref275_qi2m5ea" title="G&amp;M-13, ch. 4. For more on the evolution of visual systems, see Milner &amp; Goodale (2006), ch. 1.  " href="#footnote275_qi2m5ea">275</a></p>
<blockquote><p>A single-cell organism like the <em>Euglena</em>, which lives in ponds and uses light as a source of energy, changes its pattern of swimming according to the different levels of illumination it encounters in its watery world. Such behavior keeps <em>Euglena</em> in regions of the pond where an important resource, sunlight, is available. But although this behavior is controlled by light, no one would seriously argue that the <em>Euglena</em> “sees” the light or that it has some sort of internal model of the outside world. The simplest and most obvious way to understand this behavior is that it works as a simple reflex, translating light levels into changes in the rate and direction of swimming. Of course, a mechanism of this sort, although activated by light, is far less complicated than the visual systems of multicellular organisms. But even in complex organisms like vertebrates, many aspects of vision can be understood entirely as systems for controlling movement, without reference to perceptual experience or to any general-purpose representation of the outside world.</p>
<p>Vertebrates have a broad range of different visually guided behaviors. What is surprising is that these different patterns of activity are governed by quite independent visual control systems. The neurobiologist, David Ingle, for example, showed during the 1970s that when frogs catch prey they use a quite separate visuomotor “module” from the one that guides them around visual obstacles blocking their path [<a href="http://science.sciencemag.org/content/181/4104/1053">Ingle (1973)</a>]. These modules run on parallel tracks from the eye right through the brain to the motor output systems that execute the behavior. Ingle demonstrated the existence of these modules by taking advantage of the fact that nerves… in the frog&#8217;s brain, unlike those in the mammalian brain, can regenerate new connections when damaged. In his experiments, he was able to “rewire” the visuomotor module for prey catching by first removing a structure called the optic tectum on one side. The optic nerves that brought information from the eye to the optic tectum on the damaged side of the brain were severed by this surgery. A few weeks later, however, the cut nerves re-grew, but finding their normal destination missing, crossed back over and connected with the remaining optic tectum on the other side of the brain. As a result, when these “rewired” frogs were later tested with artificial prey objects, they turned and snapped their tongue to catch the prey — but <em>in the opposite direction</em>… This “mirror-image” behavior reflected the fact that the prey-catching system in these frogs was now wired up the wrong way around.</p>
<p>But this did not mean that their entire visual world was reversed. When Ingle tested the same frogs’ ability to jump around a barrier blocking their route, their movements remained quite normal, even when the edge of the barrier was located in the same part of space where they made prey-catching errors… It was as though the frogs saw the world correctly when skirting around a barrier, but saw the world mirror-imaged when snapping at prey. In fact, Ingle discovered that the optic nerves were still hooked up normally to a separate “obstacle avoidance module” in a part of the brain quite separate from the optic tectum. This part of the brain, which sits just in front of optic tectum, is called the pretectum. Ingle was subsequently able to selectively rewire the pretectum itself in another group of frogs. These animals jumped right into an obstacle placed in front of them instead of avoiding it, yet still continued to show normal prey catching.</p>
<p>So what did these rewired frogs “see”? There is no sensible answer to this. The question only makes sense if you believe that the brain has a single visual representation of the outside world that governs all of an animal&#8217;s behavior. Ingle&#8217;s experiments reveal that this cannot possibly be true. Once you accept that there are separate visuomotor modules in the brain of the frog, the puzzle disappears. We now know that there are at least five separate visuomotor modules in the brains of frogs and toads, each looking after a different kind of visually guided behavior and each having distinct input and output pathways. Obviously the outputs of these different modules have to be coordinated, but in no sense are they all guided by a single visual representation of the world residing somewhere in the frog&#8217;s brain.</p>
<p>The same kind of visuomotor “modularity” exists in mammals as well. Evidence for this can be seen even in the anatomy of the visual system. …[The neurons] in the retina send information (via the optic nerve) directly to a number of different sites in the brain. Each of these brain structures in turn gives rise to a distinctive set of outgoing connections. The existence of these separate input–output lines in the mammalian brain suggests that they may each be responsible for controlling a different kind of behavior — in much the same way as they are in the frog. The mammalian brain is more complex than that of the frog, but the same principles of modularity still seem to apply. In rats and gerbils, for example, orientation movements of the head and eyes toward morsels of food are governed by brain circuits that are quite separate from those dealing with obstacles that need to be avoided while the animal is running around. In fact, each of these brain circuits in the mammal shares a common ancestor with the circuits we have already mentioned in frogs and toads. For example, the circuit controlling orientation movements of the head and eyes in rats and gerbils involves the optic tectum (or “superior colliculus” as it is called in mammals), the same structure in the frog that controls turning and snapping the tongue at flies.</p>
<p>The fact that each part of the animal&#8217;s behavioral repertoire has its own separate visual control system refutes the common assumption that all behavior is controlled by a single, general-purpose representation of the visual world. Instead, it seems, vision evolved, not as a single system that allowed organisms to “see” the world, but as an expanding collection of relatively independent visuomotor modules.</p></blockquote>
<p>According to G&amp;M, at least, “vision for action” systems seem to be primary in most animals, while “vision for perception” systems are either absent entirely or much less developed than what we observe in primates:<a class="see-footnote" id="footnoteref276_dckieen" title="This quote from Milner &amp; Goodale (2006), p. 65.  Another piece of evidence sometimes cited (e.g. by Wolpert 2011) in favor of the view that brains are primarily for controlling behavior is the fact that a tunicate (&quot;sea squirt&quot;), upon swimming to and attaching itself to a suitable rock (and thus no longer need to plan and control its movement), digests its own brain. (Dennett 1991, p. 177, humorously remarks: &quot;When it finds its spot and takes root, it doesn't need its brain anymore, so it eats it! It's rather like getting tenure.&quot;)  Wolpert and Dennett don't cite any sources, but see e.g. Mackie &amp; Burighel (2005), p. 169; Cloney (1982).   " href="#footnote276_dckieen">276</a></p>
<blockquote><p>…vision in vertebrates evolved in response to the demands of motor output, not for perceptual experience. Even with the evolution of the cerebral cortex this remained true, and in mammals such as rodents the major emphasis of cortical visual processing still appears to be on the control of navigation, prey catching, obstacle avoidance, and predator detection [<a href="http://psycnet.apa.org/psycinfo/1990-98262-006">Dean (1990)</a>]. It is probably not until the evolution of the primates, at a late stage of phylogenetic history, that we see the arrival on the scene of fully developed mechanisms for perceptual representation. The transformations of visual input required for perception would often be quite different from those required for the control of action. They evolved, we assume, as mediators between identifiable visual patterns and flexible responses to those patterns based on higher cognitive processing.</p></blockquote>
<p><span style="float: right;">[<a href="/node/858/edit/64">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="PrimateVisionSystems">Two vision systems in primates</h4>
<p>Given the evidence for multiple, largely independent vision systems in simpler animals, it should be no surprise that primates, too, have multiple, largely independent vision systems.</p>
<p>The direct evidence for two (mostly) functionally and anatomically distinct vision systems in the primate brain — one serving “vision for action” and the other serving “vision for perception” — comes from several sources, including:</p>
<ol><li>Lesion studies in humans and monkeys.</li>
<li>Dissociation studies in healthy humans and monkeys.</li>
<li>Single-neuron recordings, mostly in monkeys.</li>
<li>Brain imaging studies.</li>
<li>Studies which induce “temporary lesions” via transcranial magnetic stimulation (TMS).</li>
</ol><p>Below, I summarize some of this evidence.</p>
<p><span style="float: right;">[<a href="/node/858/edit/65">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="Dee">Visual form agnosia in Dee Fletcher</h4>
<p>Let&#8217;s start with G&amp;M&#8217;s most famous lesion patient, Dee Fletcher.<a class="see-footnote" id="footnoteref277_f509crk" title="Not her real name. The next several paragraphs draw from, and quote from, G&amp;M-13, ch. 1  " href="#footnote277_f509crk">277</a> In February 1988, Dee collapsed into a coma as a result of carbon monoxide poisoning caused by an improperly vented water heater in her home. Fortunately, her partner Carlo soon arrived home and rushed her to the hospital.</p>
<p>After a few days of recovery, it became clear that Dee&#8217;s vision was impaired. She could see colors and surface textures (e.g. the tiny hairs on someone&#8217;s hand), but she couldn&#8217;t recognize shapes, objects, or people unless (1) she could identify them via another sense (e.g. hearing someone&#8217;s voice, or touching a hand), or unless (2) she could guess the object or person&#8217;s identity with color and surface texture information alone, for example if a close friend visited her while wearing a distinctively blue sweater.</p>
<p><img src="/files/Research/Moral_Patienthood/Gabor_grating.png" style="width:200px;height:113px;" alt="Gabor grating" align="right" />This was confirmed in formal testing. For example, she performed just as well “as a normally-sighted person in detecting a circular ‘Gabor’ patch of closely spaced fine lines on a background that had the same average brightness” (see right<a class="see-footnote" id="footnoteref278_hff3xin" title="G&amp;M-13, Figure 1.3.  " href="#footnote278_hff3xin">278</a>), but she had no idea whether the lines were horizontal or vertical. Hence, it wasn&#8217;t that her vision was a <em>blur</em>. She could see detail. She just couldn&#8217;t see edges and outlines that would allow her to identify shapes, objects, and people.</p>
<p>When G&amp;M showed Dee a flashlight made of shiny metal and red plastic, she said: “It&#8217;s made of aluminium. It&#8217;s got red plastic on it. Is it some sort of kitchen utensil?” Given that she couldn&#8217;t see the object&#8217;s shape, and only its surface colors and texture, this was a sensible guess, since many kitchen tools are made of metal and plastic. As soon G&amp;M placed the flashlight in her hand, she immediately recognized it as a flashlight.<a class="see-footnote" id="footnoteref279_48ykgpy" title="Technically, G&amp;M-13 doesn't say whether it was G&amp;M, or some other experimenters, who showed Dee the flashlight, and I haven't bothered to find out for sure. For simplicity, I've simply guessed that it was G&amp;M who did this.  " href="#footnote279_48ykgpy">279</a> </p>
<p>Dee often had trouble separating an object from the background. According to her, objected seemed to “run into each other,” such that “two adjacent objects of similar color, such as a knife and fork, will often look to her like a single entity.”</p>
<p><img src="/files/Research/Moral_Patienthood/edges_of_objects.png" style="width:250px;height:287px;" alt="edges of objects" align="left" />G&amp;M showed Dee shapes whose edges were defined in four different ways: by color contrast, by differences in luminance, by differences in texture, and by way of some dots remaining still while others moved (see left<a class="see-footnote" id="footnoteref280_auydx0l" title="G&amp;M-13, Figure 1.4.  " href="#footnote280_auydx0l">280</a>). In none of these cases was she able to reliably detect objects or shapes, though she could report the colors accurately.</p>
<p>G&amp;M also tested Dee on “Efron shapes,” a series of rectangles that differ in shape but not in total surface area. For each round of the test, Dee was shown a pair of these shapes and asked to say whether they were the same or different. D&amp;M-13 reports:</p>
<blockquote><p>When we used any of the three rectangles that were most similar to the square, she performed at chance level. She sometimes even made mistakes when we used the most elongated rectangle, despite taking a long time to decide. Under each rectangle [in the image below<a class="see-footnote" id="footnoteref281_2e18916" title="G&amp;M-13, Figure 1.9.  " href="#footnote281_2e18916">281</a>] is the number of correct judgments (out of 20) that Dee made in a test run with that particular rectangle.</p></blockquote>
<p><img style="display: block; margin-left: auto; margin-right: auto; width:533px; height:293px;" src="/files/Research/Moral_Patienthood/Efron_shapes.png" alt="Efron shapes" align="middle" /></p>
<p>Dee&#8217;s problem is not that she struggles to verbally <em>name</em> shapes or objects, and nor is it a deficit in remembering what common objects look like. G&amp;M-13 reports:</p>
<blockquote><p>Dee has great difficulties in copying drawings of common objects or geometric shapes [see image below<a class="see-footnote" id="footnoteref282_ahginma" title="G&amp;M-13, Figure 1.5.  " href="#footnote282_ahginma">282</a>]. Some brain-damaged patients who are unable to identify pictures of objects can still slavishly copy what they see, line by line, and produce something recognizable. But Dee can&#8217;t even pick out the individual edges and contours that make up a picture in order to copy them. Presumably, unlike those other patients, Dee&#8217;s problem is not one of <em>interpreting</em> a picture that she sees clearly — her problem is that she can&#8217;t see the shapes in the picture to start with.</p></blockquote>
<p><img style="display: block; margin-left: auto; margin-right: auto; width:502px; height:480px;" src="/files/Research/Moral_Patienthood/Dee_model_copy_memory.png" alt="Dee model, copy, memory" align="middle" /></p>
<p>Dee couldn&#8217;t recognize any of the drawings in the left-most column above. When she tried to copy those objects (middle column), she could incorporate some elements of the drawing (such as the small dots representing text), but her overall copies are unrecognizable. However, when asked to draw objects from memories she formed before her accident (right-most column), she did just fine, except for the fact that when she lifted her pencil and put it back down, she sometimes put it back down in the wrong place (presumably due to her inability to see shapes and edges even as she was drawing them). When she was later shown the objects she had drawn from memory, she couldn&#8217;t identify them.</p>
<p>Dee&#8217;s ability to draw objects from memory suggests that she can see things “in her mind&#8217;s eye” just fine. So does her correct responses to queries like this: “Think of the capital letter D; now imagine that it has been rotated flat-side down; now put it on top of the capital letter V; what does it look like?” Most people say “an ice cream cone,” and so does Dee.</p>
<p>Dee also still dreams normally:</p>
<blockquote><p>[Dee] still sometimes reports experiencing a full visual world in her dreams, as rich in people, objects, and scenes as her dreams used to be before the accident. Waking up from dreams like this, especially in the early years, was a depressing experience for her. Remembering her dream as she gazed around [her now edgeless, shapeless, object-less] bedroom, she was cruelly reminded of the visual world she had lost.</p></blockquote>
<p>However, despite her severe deficits in identifying shapes, objects, and people, Dee displayed a nearly normal ability to walk around in her environment and use her hands to pick things up and interact with them. G&amp;M report the moment they realized just how striking the difference was between Dee&#8217;s ability to recognize objects and her ability to interact with them:<a class="see-footnote" id="footnoteref283_aun5sce" title="For the next several passages, I am now following the discussion in, and quoting from, G&amp;M-13, ch. 2.  " href="#footnote283_aun5sce">283</a> </p>
<blockquote><p>[In the summer of 1988] we were showing [Dee] various everyday objects to see whether she could recognize them, without allowing her to feel what they were. When we held up a pencil, we were not surprised that she couldn&#8217;t tell us what it was, even though she could tell us it was yellow. In fact, she had no idea whether we were holding it horizontally or vertically. But then something quite extraordinary happened. Before we knew it, Dee had reached out and taken the pencil, presumably to examine it more closely… After a few moments, it dawned on us what an amazing event we had just witnessed. By performing this simple everyday act she had revealed a side to her vision which, until that moment, we had never suspected was there. Dee&#8217;s movements had been quick and perfectly coordinated, showing none of the clumsiness or fumbling that one might have expected in someone whose vision was as poor as hers. To have grasped the pencil in this skillful way, she must have turned her wrist “in flight” so that her fingers and thumb were well positioned in readiness for grasping the pencil — just like a fully sighted person. Yet it was no fluke: when we took the pencil back and asked her to do it again, she always grabbed it perfectly, no matter whether we held the pencil horizontally, vertically, or obliquely.</p></blockquote>
<p>How could Dee do this? She had to be using vision; a blind person couldn&#8217;t have grabbed the pencil so effortlessly. But she couldn&#8217;t have been using her <em>conscious visual experience</em>, either, as her conscious visual experience didn&#8217;t include any information about the rotation of the pencil or its exact shape.</p>
<p><img src="/files/Research/Moral_Patienthood/matching_and_posting.png" style="width:424px; height:341px;" alt="matching and posting" align="right" />G&amp;M soon put this difference to a moral formal test. They built a simple mailbox-like slot that could be rotated to any angle (while Dee closed her eyes), and then they gave Dee a thin card to “post” into the slot. When asked to “post” the card, she had no difficulty. However, when she was asked to merely turn the card so that it matched the orientation of the slot, without reaching toward the slot, she performed no better than chance.<a class="see-footnote" id="footnoteref284_zs31dq4" title="This wasn't a deficit in Dee's ability to rotate the card. G&amp;M-13 report:  We were able to rule out that possibility by asking her to imagine a slot at different orientations. Once she had done this, she had no difficulty rotating the card to show us the orientation she'd been asked to imagine. It was only when she had to look at a real slot and match its orientation that her deficit appeared.  " href="#footnote284_zs31dq4">284</a> She couldn&#8217;t consciously see the orientation of the slot, but nevertheless when posting the card into the slot, she had no trouble rotating the card properly so that it went into the slot. The diagrams on the right<a class="see-footnote" id="footnoteref285_1qfru7p" title="G&amp;M-13, Figure 2.2.  " href="#footnote285_1qfru7p">285</a> show Dee&#8217;s performance relative to healthy control subjects, with the “correct” orientation always shown as vertical even though the slot was rotated to many different orientations. Video showed that when posting the card, Dee rotated it well before reaching the slot — clearly, a visually-guided behavior, even if it wasn&#8217;t guided by <em>conscious</em> vision.</p>
<p><img src="/files/Research/Moral_Patienthood/MGA.png" style="width:441px; height:224px;" alt="MGA" align="right" />G&amp;M also tested Dee&#8217;s grasping movements. When a normal patient is asked to reach out and grab an object on a table, they open their fingers and thumb as soon as their hand leaves the table. About 75% of the way to the object, the gulf between fingers and thumb is as wide as it gets — the “maximum grip aperture” (MGA). Thereafter, they begin to close their fingers and thumb so that a good grasp is achieved (see right<a class="see-footnote" id="footnoteref286_swlt5r3" title="G&amp;M-13, Figure 2.3.  " href="#footnote286_swlt5r3">286</a>). The MGA is always larger than the width of the target object, but the two are related: the bigger the object, the bigger the MGA.</p>
<p>G&amp;M tested Dee&#8217;s grasping behavior using some 3D wooden blocks they called “Efron blocks,” because they were modeled after the Efron shapes (again, with the same overall size but different dimensions). As expected, her grasping motions showed the same mid-flight grip scaling as those of healthy controls, and she grasped the Efron blocks just as smoothly as anyone else. Her performed just fine regardless of the orientation of Efron blocks, and she effortlessly rotated her wrist to grasp them width-wise rather than length-wise (just like healthy subjects).<a class="see-footnote" id="footnoteref287_h53ipgd" title="Of course for a square Efron block, width-wise and length-wise are the same.  " href="#footnote287_h53ipgd">287</a> She did this despite the fact that she performed very poorly when asked to distinguish the blocks when they were presented as pairs, and despite the fact that she could know show G&amp;M how wide each block was by looking at it and then using her fingers and thumb to indicate its width. When asked to estimate, with her thumb and forefinger, the width of a familiar object stored in her memory, such as a golf ball, she did fine.</p>
<p>G&amp;M also tested Dee on “Blake shapes,” a set of pebble-like objects that are smooth and rounded but irregular in shape, and thus are stably grasped at some points but not others. Again, Dee could reach out and grasp these objects just as well as healthy controls, even though she was unable to say whether pairs of the Blake shapes were the same or different.</p>
<p>G&amp;M also tested Dee&#8217;s ability to navigate obstacles. They visited a laboratory in which obstacles of various heights could be placed along a path, and sophisticated equipment could precisely measures the adjustments people made to their gait to step over the obstacles. Once again, Dee performed just like healthy subjects, stepping confidently over the obstacles without tripping, just barely clearing them (again, like healthy subjects). However, when asked to estimate the height of these obstacles, she performed terribly.</p>
<p>In short, as G&amp;M-13 puts it:</p>
<blockquote><p>The most amazing thing about Dee is that she is able to use visual properties of objects such as their orientation, size, and shape, to guide a range of skilled actions — despite having no conscious awareness of those same visual properties. This… indicates that some parts of the brain (which we have good reason to believe are badly damaged in Dee) play a critical role in giving us visual awareness of the world while other parts (relatively undamaged in her) are more concerned with the immediate visual control of skilled actions.</p></blockquote>
<p>Dee&#8217;s condition is now known as “visual form agnosia” (an inability to see “forms” or shapes), and a few other cases beside&#8217;s Dee have been reported.<a class="see-footnote" id="footnoteref288_9ihy2an" title="Heider (2000). For a detailed account of another patient, &quot;John,&quot; with an overlapping but non-identical set of symptoms, see Humphreys &amp; Riddoch (2013).  " href="#footnote288_9ihy2an">288</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/66">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="OpticAtaxia">Optic ataxia</h4>
<p>The case of Dee Fletcher raises the question: are their patients with the “opposite” condition, such that they can recognize shapes, objects, and people just fine, but have difficulty with visually-guided behavior, such as when grasping and manipulating objects?</p>
<p>Indeed there are:<a class="see-footnote" id="footnoteref289_6rgw9me" title="G&amp;M-13, ch. 3.  " href="#footnote289_6rgw9me">289</a></p>
<blockquote><p>The Hungarian neurologist Rudolph Bálint was the first to document a patient with this kind of problem, in 1909. The patient was a middle-aged man who suffered a massive stroke to both sides of the brain in a region called the parietal lobe… He could recognize objects and people, and could read a newspaper. He did tend to ignore objects on his left side and had some difficulty moving his eyes from one object to another. But his big problem was not a failure to recognize objects, but rather an inability to reach out and pick them up. Instead of reaching directly toward an object, he would grope in its general direction much like a blind man, often missing it by a few inches. Unlike a blind man, however, he could see the object perfectly well — he just couldn&#8217;t guide his hand toward it. Bálint coined the term “optic ataxia”… to refer to this problem in visually guided reaching.</p>
<p>Bálint&#8217;s first thought was that this difficulty in reaching toward objects might be due to a general failure in his patient to locate where the objects were in his field of vision. But it turned out that the patient showed the problem only when he used his right hand. When he used his left hand to reach for the same object, his reaches were pretty accurate. This means that there could not have been a generalized problem in <em>seeing</em> where something was. The patient&#8217;s visual processing of spatial location per se was not impaired. After further testing, Bálint discovered that the man&#8217;s reaching difficulty was not a purely motor problem either — some kind of generalized difficulty in moving his right arm correctly. He deduced this from asking the patient to point to different parts of his own body using his right hand with his eyes closed: there was no problem.</p>
<p>…It was not until the 1980s that research on patients with optic ataxia was kick-started again, mostly by Marc Jeannerod and his group in Lyon, France. In one landmark study, his colleagues Marie-Thérèse Perenin and Alain Vighetto made detailed video recordings of a sizeable group of patients with optic ataxia performing a number of different visuomotor tests… Like Bálint, they observed that although their patients couldn&#8217;t accurately point to the targets, they were able to give pretty accurate verbal reports of where those same objects were located. Also like Bálint, Perenin and Vighetto demonstrated that the patients had no difficulty in directing hand movements toward different parts of their own body. Subsequent work in their laboratory went on to show that the reaching and pointing errors made by many patients with optic ataxia are most severe when they are not looking directly at the target. But even when pointing at a target in the center of the visual field, the patients still make bigger errors than normal people do, albeit now on the order of millimeters rather than centimeters. In short, Perenin and Vighetto&#8217;s research confirms Bálint&#8217;s original conclusion: optic ataxia is a deficit in visually guided reaching, not a general deficit in spatial vision.</p></blockquote>
<p>Patients with optic ataxia also have difficulty avoiding collisions with obstacles as they reach for an object. For example, neuroscientist Robert McIntosh designed a test in which subjects are asked to reach from a fixed starting point to a strip 25cm away, between two vertical rubber cylinders. The location of the cylinders is varied, and healthy control subjects always vary their reach trajectory so as to stay well clear of the rubber cylinders. In contrast, optic ataxia patients do not vary their reach trajectory in response to where the rubber cylinders are located, and thus often come somewhat close to knocking over the rubber cylinders as they reach for the strip at the back of the table.</p>
<p>However, the failure of patients with optic ataxia to adjust their reach trajectory in response to the location of the cylinders is not due to a failure to (consciously) see where the cylinders are. When asked to point to the midpoint between the two cylinders, patients with optic ataxia are just as accurate as healthy controls.</p>
<p>G&amp;M were able to run this test on a patient with optic ataxia for only one hand. Morris Harvey has damage in his <em>left</em> parietal lobe, which means that his optic ataxia affects only his <em>right</em> hand, and only when reaching toward objects in his right visual field. How did Morris perform at the cylinders task? When reaching with his left hand, his reach trajectory was the same as healthy subjects, adjusted to maximally avoid the cylinders. But when reaching with his right hand, he studiously avoided the cylinder on the left, but took no account of the cylinder on the right.</p>
<p>(In contrast to those with optic ataxia, Dee Fletcher avoided the cylinders as normal when reaching out to the strip at the back of the table, but she performed poorly when asked to point to the midpoint between the two cylinders.)</p>
<p>Some optic ataxia patients also have trouble changing their reach trajectory mid-flight:</p>
<blockquote><p>Our French colleagues Laure Pisella and Yves Rossetti had [optic ataxia patient] Irène make a series of reaches to touch a small LED target. From time to time, however, the target would unpredictably shift leftwards or rightwards at the very instant Irène&#8217;s hand started to move toward it. Healthy volunteers doing this task had no problem in making the necessary in-flight corrections to their reach, and in fact they adjusted their reaches seamlessly as if their movements were on “automatic pilot,” particularly when under time pressure to move quickly. Yet Irène found these changes in target location frustratingly impossible to deal with. It was as if she no longer had that automatic pilot. To put it another way, Irène&#8217;s reaches seemed to be entirely predetermined at the outset of the movement, and remained impervious to unexpected changes in the position of the target, even though she could see them clearly enough and knew they might occur. On occasions when the target moved, she found herself reaching first to its original location, and only then shifting her finger to the new location.</p></blockquote>
<p>How do patients with optic ataxia perform on the mail slot task described in the previous section? Just as you&#8217;d expect:</p>
<blockquote><p>…[Perenin and Vighetto] examined the ability of their [optic ataxia] patients to reach out and pass their hand through an open slot cut in a disk, which could be positioned at different orientations at random… Remarkably, not only did the patients tend to make the expected spatial errors, in which their hand missed the slot altogether, but they also made orientation errors, in which the hand would approach the slot at the wrong angle. Yet most of these same patients could easily tell one orientation of the slot from another when asked to do so. So again we see a familiar story unfolding. The failure of the patients to rotate their hand as they reached out to pass it through a slot was not due to a difficulty in perceiving the orientation of the slot — the problem was visuomotor in nature, not perceptual. (Of course when their hand made contact with the disk they could correct themselves using touch, and then pass their hand through the slot. In other words the deficit was restricted to the modality of sight, and did not extend to touch.)</p></blockquote>
<p>What about the measures of grasping movements described in the previous section? Again, patients with optic ataxia perform just as you&#8217;d expect:</p>
<blockquote><p>Instead of first opening the hand during the early part of the reach, and then gradually closing it as it moved toward the target object, the optic ataxia patient would keep the hand widely opened throughout the movement, much as a person would do if reaching blindfolded toward the object… Jeannerod and his colleagues were the first to carry out systematic tests with Anne Thiérry, the optic ataxia patient we described earlier in this chapter. They used similar matching and grasping tasks to those we had used earlier with Dee…  Anne was found to show poor scaling of her grip when reaching for objects of different sizes, while remaining well able to demonstrate the sizes of the objects by use of her forefinger and thumb. Again, the pattern of deficits and spared abilities in Anne and the pattern in Dee complement each other perfectly.</p></blockquote>
<p>Next, what about Blake shapes? Again, the optic ataxia patient&#8217;s performance seems to be the mirror image of Dee Fletcher&#8217;s:</p>
<blockquote><p>Although [Ruth Vickers’] symptoms had cleared to some degree by the time we saw her, it was obvious that she still had severe optic ataxia. She could not reach with any degree of accuracy to objects that she could see but was not looking at directly. She could, however, reach reasonably accurately to objects directly in her line of sight.</p>
<p>Nevertheless, the reaches Ruth made to pick up an object she was looking at, although spatially accurate, were far from normal. Like Anne Thiérry, she would open her hand wide as she reached out, no matter how big or small the objects were, showing none of the grip scaling seen in healthy people… Yet despite this, when asked to show us how big she thought the object was using her finger and thumb, she performed quite creditably, again just like Anne. And she could describe most of the objects and pictures we showed her without any difficulty. In fact, although her strokes had left her unable to control a pencil or pen very well, she could draw quite recognizable copies of pictures she was shown… In other words, Ruth&#8217;s visual experience of the world seemed pretty intact, and she could readily convey to us what she saw — in complete contrast to Dee Fletcher.</p>
<p>Because Ruth could distinguish between many different shapes and patterns, we did not expect her to have much difficulty with the smooth pebble-like shapes we had tested Dee with earlier. We were right — when she was presented with a pair of “Blake shapes” she could generally tell us whether or not the two shapes were the same. Although she sometimes made mistakes, particularly when two identical shapes were presented in different orientations, her performance was much better than Dee&#8217;s. When it came to picking up the shapes, however, the opposite was the case. Ruth had real problems. Instead of gripping the Blake shapes at stable “grasp points,” she positioned her finger and thumb almost at random… This inevitably meant that after her fingers contacted the pebble she had to correct her grip by means of touch — if she did not, the pebble would often slip from her grasp. In other words, although some part of Ruth&#8217;s brain could code the shape of these objects to inform her visual experience, her hand was unable to use such shape information to guide its actions.</p></blockquote>
<p><span style="float: right;">[<a href="/node/858/edit/67">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="MonkeyLesions">Lesions in monkeys</h4>
<p>These lesion studies in humans provide suggestive evidence for two different streams of visual processing, one of which (the “vision for action” system) seems to be unconscious. Now we turn to the evidence from lesion studies in monkeys, which, I was surprised to learn, goes back to the <em>1860s</em>:<a class="see-footnote" id="footnoteref290_sitc56w" title="This section draws from, and quotes from, G&amp;M-13, ch. 4.  " href="#footnote290_sitc56w">290</a></p>
<blockquote><p>During the 1860s, [neurologist David Ferrier] removed what we now call the dorsal stream in a monkey, and discovered that the animal would misreach and fumble for food items set out in front of it. In a similar vein, recent work by Mitchell Glickstein in England has shown that small lesions in the dorsal stream can make a monkey unable to pry food morsels out of narrow slots set at different orientations. The monkey is far from blind, but it cannot use vision to insert its finger and thumb at the right angle to get the food. It eventually does it by touch, but its initial efforts, under visual guidance, fail. Yet the same monkey has no difficulty in telling apart different visual patterns, including lines of different orientation. These observations, and a host of others, have demonstrated that dorsal-stream damage in the monkey results in very similar patterns of disabilities and spared abilities to those we saw in [patients with optic ataxia]. In other words, monkeys with dorsal-stream lesions show major problems in vision for action but evidently not in vision for perception.</p>
<p>In direct contrast, Heinrich Klüver and Paul Bucy, working at the University of Chicago in the 1930s, found that monkeys with lesions of the temporal lobes, including most of what we now know as the ventral stream, did not have any visuomotor problems at all, but did have difficulties in recognizing familiar objects, and in learning to distinguish between new ones. Klüver and Bucy referred to these problems as symptoms of “visual agnosia,” and indeed they do look very like the problems that Dee Fletcher has. Moreover, like Dee, these monkeys with ventral-stream lesions had no problem using their vision to pick up small objects. The influential neuroscientist, Karl Pribram, once noted that monkeys with ventral-stream lesions that had been trained for months to no avail to distinguish between simple visual patterns, would sit in their cages snatching flies out of the air with great dexterity. Mitchell Glickstein recently confirmed that such monkeys do indeed retain excellent visuomotor skills. He found that monkeys with ventral-stream damage had no problem at all using their finger and thumb to retrieve food items embedded in narrow slots — quite unlike his monkeys with dorsal-stream lesions.</p></blockquote>
<p>Such studies in monkeys are widely thought to be informative for our understanding of human neuroscience, given the many similarities between human brains and monkey brains.</p>
<p><span style="float: right;">[<a href="/node/858/edit/68">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="HealthySubjects">Dissociation studies in healthy subjects</h4>
<p>G&amp;M hypothesize that the dorsal and ventral streams of visual processing use different frames of reference, in part due to computational constraints:<a class="see-footnote" id="footnoteref291_zmn7hgf" title="This section draws from G&amp;M-13, chs. 7-8.  " href="#footnote291_zmn7hgf">291</a></p>
<blockquote><p>When we perceive the size, location, orientation, and geometry of an object, we implicitly do so in relation to other objects in the scene we are looking at. In contrast, when we reach out to grab that same object, our brain needs to focus on the object itself and its relationship to us — most particularly, to our hand — without taking account of the… scene in which the object is embedded. To put it a different way, perception uses a scene-based frame of reference while the visual control of action uses egocentric frames of reference.</p>
<p>…The use of scene-based metrics means that the brain can construct this representation in great detail without having to compute the absolute size, distance, and geometry of each object in the scene. To register the absolute metrics of the entire scene would in fact be computationally impossible, given the rapidity with which the pattern of light changes on our retina. It is far more economical for perception to compute just the relational metrics of the scene, and even these computations do not generally need to be precise. It is this reliance on scene-based frames of reference that lets us watch the same scene unfold on a small television or on a gigantic movie screen without being confused by the differences in scale.</p>
<p>…But… scene-based metrics are the very opposite of what you need when you act upon the world. It is not enough to know that an object you wish to pick up is bigger or closer than a neighboring object. To program your reach and scale your grasp, your brain needs to compute the size and distance of the object in relation to your hand. It needs to use absolute metrics set within an egocentric frame of reference. It would be a nuisance, and potentially disastrous, if the illusions of size or distance that are a normal part of [scene and object] perception were to intrude into the visual control of your movements.</p></blockquote>
<p>If this account is right, it suggests a way to test for dorsal-ventral dissociation even in healthy subjects, since object and scene recognition should be subject to certain kinds of visual illusions that visually-guided <em>action</em> is not.</p>
<p><img src="/files/Research/Moral_Patienthood/VirtualWorkbench.png" alt="virtual workbench" align="right" />One way to test for this dissociation is to use virtual reality displays. In one study, <a href="http://www.mitpressjournals.org/doi/abs/10.1162/089892900562462#.WENCp5JWnnp">Hu &amp; Goodale (2000)</a> used a virtual reality display to show healthy subjects a series of 3D images of target blocks (marked with a red spot), each of which was displayed along with another “virtual” block that was either 10% wider or narrower than the target block. These blocks were shown for a half second or less, and then the subject was asked to either (1) reach out and grab the target block using their thumb and index finger, or to (2) indicate the size of the target block using their thumb and index finger, but not reach out toward it. To ensure a “real” (not pantomimed) grasping motion, a physical but unseen block was placed exactly where the virtual target block appeared to be (see right).<a class="see-footnote" id="footnoteref292_xmiuuqj" title="The &quot;virtual workbench&quot; image is used with permission from MIT Press. The full citation is:  Y. Hu and M. A. Goodale, &quot;Grasping after a Delay Shifts Size-Scaling from Absolute to Relative Metrics,&quot; Journal of Cognitive Neuroscience, 12:5 (September, 2000), pp. 8556-868. © 2000 by the Massachusetts Institute of Technology, published by the MIT Press. http://www.mitpressjournals.org/doi/abs/10.1162/089892900562462  " href="#footnote292_xmiuuqj">292</a></p>
<p>The point of having two (virtual) blocks of slightly different sizes was to induce a “size-contrast effect,” akin to the effect observed when a person you normally think of as tall stands next to a professional basketball player and suddenly seems shorter than they usually do. Hu &amp; Goodale&#8217;s expectation was that this size-contrast effect would would affect the subject&#8217;s (ventral) <em>perception</em> of the target block, and thus their attempt to indicate its size with their thumb and index finger (without reaching for it), but would <em>not</em> affect their (dorsally-guided) attempt to <em>grasp</em> the target block.</p>
<p>And this is just what happened. When the target block was paired with a <em>larger</em> companion block, subjects consistently judged it to be smaller than when the same target block was pared with a <em>smaller</em> companion block. But when subjects reached out to <em>grasp</em> the target block, they opened their thumb and index finger to an identical degree no matter which companion block appeared. In other words, the size-contrast effect affected the subjects’ perception of the target block, but <em>didn&#8217;t</em> affect their physical interaction with the target block.</p>
<p>Another proposed difference between the dorsal and ventral streams is that the dorsal system should operate only in real-time, whereas the ventral stream interacts with short- and long-term memory to help guide decision-making and action over a longer period of time. Consistent with this hypothesis, the subjects’ grip size calibration <em>was</em> affected by the size-contrast illusion when a delay was inserted between viewing the (virtual) blocks and reaching toward the target block:</p>
<blockquote><p>When the students [subjects] had to wait for five seconds before picking up the target object that they had just seen, the scaling of their grasp now fell prey to the influence of the companion block. Just as they did when they made perceptual judgments, they opened their hand wider when the target block was accompanied by a small block than when it was accompanied by a large block. This intrusion of the size-contrast effect into grip scaling after a delay is exactly what we had predicted. Since the dedicated visuomotor systems in the dorsal stream operate only in real time, the introduction of a delay disrupts their function. Therefore when a delay is introduced, the calibration of the grasp has to depend on a memory derived from perceptual processing in the ventral stream, and becomes subject to the same size-contrast illusions that perception is prone to.</p></blockquote>
<p><img src="/files/Research/Moral_Patienthood/Ebbinghaus.png" alt="Ebbinghaus illusion" align="right" />In another experiment, <a href="http://ieeexplore.ieee.org/document/6787990/">Haffendon &amp; Goodale (1998)</a> tested for a ventral-dorsal dissociation using the well-known Ebbinghaus illusion, which also makes use of size-contrast effects. In one version, two physically <em>identical</em> circles are perceived as being of different size (top right). In another version, two physically <em>different</em> circles are perceived as being of identical size (bottom right).<a class="see-footnote" id="footnoteref293_mrxi055" title="The image is G&amp;M-13, Figure 8.1.  " href="#footnote293_mrxi055">293</a></p>
<p>To test for an action-perception dissociation, Haffendon &amp; Goodale placed some flat disks atop the classic Ebbinhaus backgrounds, and then asked subjects to either “match” (indicate the size of) or “grasp” (reach out to grab) the target disk. As expected, subjects’ “match” attempts were affected by the visual illusion, but their “grasp” attempts were not.</p>
<p>Several experiments with other visual illusions have demonstrated similar results. Furtheremore, just as G&amp;M&#8217;s theory predicts, <em>both</em> perception and visuomotor control are affected if the illusion used is one that results from <em>early</em> visual processing (before the dorsal-ventral split).<a class="see-footnote" id="footnoteref294_8g9qprs" title="See G&amp;M-13, ch. 8; Goodale &amp; Ganel (2016).  " href="#footnote294_8g9qprs">294</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/69">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="SingleNeuron">Single-neuron recordings</h4>
<p>Further evidence for the “two streams” hypothesis comes from single-neuron recordings in monkeys:<a class="see-footnote" id="footnoteref295_kme01s6" title="Here again, I'm quoting from G&amp;M-13, ch. 4.  " href="#footnote295_kme01s6">295</a></p>
<blockquote><p>The 1980 Nobel laureates David Hubel and Torsten Wiesel… found that neurons in primary visual cortex [V1] would [fire] every time a visual edge or line was shown to the eye, so long as it was shown at the right orientation and in the right location within the field of view. The small area of the retina where a visual stimulus can activate a given neuron is called the neuron&#8217;s “receptive field.” Hubel and Wiesel discovered, in other words, that these neurons are “encoding” the orientation and position of particular edges that make up a visual scene out there in the world. Different neurons prefer (or are “tuned” to) different orientations of edges… Other neurons are tuned for the colors of objects, and still others code the direction in which an object is moving…</p>
<p>…The 1960s and early 1970s heralded great advances in single-cell recording as investigators pushed well beyond the early visual areas, out into the dorsal and ventral streams. It soon became apparent that neurons in the two streams coded the visual world very differently…</p></blockquote>
<p>To be more specific, but still oversimplify: neurons in the ventral stream tend to respond to fairly complex visual patterns (e.g. entire objects, or even specific faces), but many of them don&#8217;t “care” so much about details such as the angle of the object, its lighting conditions, or how far the object is from the eye: just the sort of behavior you&#8217;d expect from neurons in a pathway that specializing in perceiving objects. In contrast, neurons in the dorsal stream seem to typically code for more action-specific features, for example the <em>motion</em> of objects or small differences in their orientation, and often only respond when a monkey <em>responds</em> to a visual target, for example by reaching out to it or tracking the object&#8217;s motion with its eyes.<a class="see-footnote" id="footnoteref296_8zbtbjf" title="For a review, see Milner &amp; Goodale (2006), pp. 42-66, plus a few updates in ch. 8.  There is some neurophysiological evidence against G&amp;M's proposed functional dissociation, though (Cardoso-Leite &amp; Gorea 2010):  …accumulating neurophysiological evidence was also pointing to many instances where neurons and cortical sites in the ventral and dorsal streams behave contrary to predictions of the dissociation theory. For example, both neurophysiological and neuroimaging studies show evident dorsal stream responsiveness to stimulus features supposed to be processed in the ventral stream such as shape (e.g., Konen and Kastner, 2008; Lehky and Sereno, 2007) and color (e.g., Claeys et al., 2004; Toth and Assad, 2002). Equivalently some prototypical dorsal processing features such as motion are equally well processed in the ventral stream (e.g., Gur and Snodderly, 2007). Also, while the temporal processing characteristics of the two streams have also been cited in favor of their functional dissociation (with magnocellular neurons in dorsal areas responding earlier to visual stimulation than the parvocellular neurons in the ventral stream; e.g., Nowak and Bullier, 1997; Rossetti et al., 2003), the significance of such latency differences has been obscured by numerous reports that visual information processing is not strictly feedforward (as supposed in the classic view) so that frontal areas may respond to visual stimuli at about the same time as V1 (Lamme and Roelfsema, 2000; Schmolesky et al., 1998; Zanon et al., 2009). Hence, efferent signals from the frontal cortex may modulate processing in both the dorsal and ventral extrastriate areas (Moore and Armstrong, 2003; Moore and Fallah, 2001, 2004).  " href="#footnote296_8zbtbjf">296</a></p>
<p>What about single neuron recordings in humans? Such studies are still rare for ethical reasons,<a class="see-footnote" id="footnoteref297_x1pahci" title="Single-cell recordings currently require cutting a hole in the skull, and are thus only considered for humans in cases when a hole in the skull must be made for clinical reasons.  " href="#footnote297_x1pahci">297</a> but their findings are illuminating. For example, some cells just downstream of the inferotemporal cortex (in the ventral stream), in the medial temporal lobe (MTL), have been found to respond only to <em>specific faces</em>. For example, in one patient, a specific neuron responded most strongly to pictures (from any angle) of either Jennifer Aniston or Lisa Kudrow, both actresses on <em>Friends</em>. Another cell responded to any picture of actress Halle Berry, even when she was masked as Catwoman (a character she played), and also to the written words “HALLE BERRY,” but not to pictures of other people, or to other written names.<a class="see-footnote" id="footnoteref298_h7dedbw" title="For reviews, see Quian Quiroga (2012, 2014).  " href="#footnote298_h7dedbw">298</a> Unfortunately, I don&#8217;t know whether single-neuron recordings have been made in the human dorsal stream.</p>
<p><span style="float: right;">[<a href="/node/858/edit/70">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="VisionChallenges">Challenges</h4>
<p>There is additional evidence for this “two streams” account of visual processing, for example from fMRI studies,<a class="see-footnote" id="footnoteref299_sse47j3" title="See e.g. Milner &amp; Goodale (2006), ch. 8.  " href="#footnote299_sse47j3">299</a> but I won&#8217;t describe that evidence here (see G&amp;M-13). Instead, I&#8217;d like to briefly mention some challenges for the two streams theory:<a class="see-footnote" id="footnoteref300_x4ytrze" title="For example partial critiques of the two streams theory, see Briscoe &amp; Schwenkler (2015); Freud et al. (2016); Cardoso-Leite &amp; Gorea (2010); Schenk &amp; McIntosh (2009); Clark (2009); Gorea (2015); Shepherd (2015), sec. 4.1.  " href="#footnote300_x4ytrze">300</a></p>
<ul><li>Many of the relevant primary studies can be interpreted to support alternate hypotheses.<a class="see-footnote" id="footnoteref301_t0kh11h" title="Briscoe &amp; Schwenkler (2015); Cardoso-Leite &amp; Gorea (2010); Clark (2009).  " href="#footnote301_t0kh11h">301</a></li>
<li>Several studies suggest that the division of labor between the dorsal and ventral streams is not clear-cut: e.g. some neurons in the dorsal stream seem to subserve object recognition, and some neurons in the ventral stream seem to subserve visually-guided motor control.<a class="see-footnote" id="footnoteref302_83qfztk" title="Freud et al. (2016); Cardoso-Leite &amp; Gorea (2010).  " href="#footnote302_83qfztk">302</a></li>
<li>Personally, I would not be surprised if some of the neuroimaging studies used to argue in favor of G&amp;M&#8217;s view could be undermined by a careful examination of intepretive complications<a class="see-footnote" id="footnoteref303_3u7kebd" title="E.g. see Klein (2010).  " href="#footnote303_3u7kebd">303</a> and statistical errors<a class="see-footnote" id="footnoteref304_bk7yeti" title="E.g. see Eklund et al. (2016).  " href="#footnote304_bk7yeti">304</a> — though, this worry is not unique to imaging studies of conscious and unconscious vision (see <a href="#AppendixZ8">Appendix Z.8</a>).</li>
</ul><p>Considering all the evidence I&#8217;ve studied or skimmed, my impression is that something like G&amp;M&#8217;s “two streams” account of visual processing has a good chance of being true (with many complications), but also has a good chance of being quite mistaken.</p>
<p><em>If</em> something like the “two streams” account is right, then it could provide some evidence in favor of certain kinds of “cortex-required views” about consciousness, especially if we observe other kinds of cognitive processing having both conscious and unconscious components, with the conscious components being computed by the same broad regions of the brain that compute conscious vision.</p>
<p><span style="float: right;">[<a href="/node/858/edit/71">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="AppendixD">Appendix D. Some clarifications on nociception and pain</h3>
<p>In this appendix, I clarify how I use nociception-related and pain-related terms in this report, and provide my sources for the judgments I made in two rows of my <a href="#PCIFsTable">table of PCIFs</a>: those for “Has nociceptors” and “Has neural nociceptors.”</p>
<p>As I use the terms, <em>nociception</em> is the encoding and processing of noxious stimuli, where a <em>noxious stimulus</em> is an actually or potentially body-damaging event (either external or internal, e.g. cutaneous or visceral). A body-damaging event can be chemical (e.g. a strong acid), mechanical (e.g. pinching), or thermal (e.g. excessive heat). A sensory receptor that responds only or preferentially to noxious stimuli is a <em>nociceptor</em>. Not all noxious stimuli are successfully detected by nociceptors, e.g. when no nociceptor is located at the location of a noxious stimulus’ contact with the body. Those noxious stimuli that <em>are</em> detected are called <em>nociceptive stimuli</em>. </p>
<p>These definitions are identical to those of the International Association for the Study of Pain (IASP) — see <a href="http://journals.lww.com/pain/Citation/2008/07310/The_Kyoto_protocol_of_IASP_Basic_Pain_Terminology_.5.aspx">Loeser &amp; Treede (2008)</a> — except that I have dropped the word “neural,” and replaced the phrase “tissue-damaging” with “body-damaging.” I made both these modifications because I want to use nociception-related terms in the context of a wide variety of cognitive systems, including e.g. personal computers and robots, which in some cases have nociception-specific sensory receptors but not “neurons” in the usual sense of that word, and which are more typically said to have “bodies” than “tissues.” Also, in accordance with many other definitions (e.g. <a href="https://books.google.com/books?id=bNlvQJSqrB8C&amp;lpg=PP1&amp;pg=PA1#v=onepage&amp;q&amp;f=false">Ringkamp et al. 2013</a>), I have clarified that nociceptors can in some cases respond to both noxious and non-noxious stimuli, but must respond <em>preferentially</em> to noxious stimuli to be counted as “nociceptors.”</p>
<p>Some nociceptors respond to only one kind of noxious stimuli, while other (“polymodal”) nociceptors respond to multiple kinds of noxious stimuli. Some polymodal nociceptors are dedicated to noxious stimuli only, whereas other polymodal nociceptors — called wide dynamic range (WDR) neurons — respond to both noxious and non-noxious stimuli. (See e.g. <a href="http://www.scholarpedia.org/article/Painful_touch">Derbyshire 2014</a>; <a href="https://link.springer.com/referencework/10.1007%2F978-3-642-28753-4">Gebhart &amp; Schmidt 2013</a>, p. 4266; <a href="http://oxfordindex.oup.com/view/10.1093/acprof:oso/9780198523345.003.0004">Walters 1996</a>, p.97.)</p>
<p><em>Pain</em>, in contrast to mere nociception, is an <em>unpleasant conscious experience</em> associated with actual or potential body damage (or akin to unpleasant experiences associated with noxious stimuli). The IASP&#8217;s definition of pain (<a href="http://journals.lww.com/pain/Citation/2008/07310/The_Kyoto_protocol_of_IASP_Basic_Pain_Terminology_.5.aspx">Loeser &amp; Treede 2008</a>) is “an unpleasant sensory and emotional experience associated with actual or potential tissue damage or described in terms of such damage.” I have dropped the phrase about description because I want to talk about pain in cognitive systems that may or may not be able to describe their pain to others. Following <a href="https://books.google.com/books/about/Psychological_Mechanisms_of_Pain_and_Ana.html?id=MadpAAAAMAAJ">Price (1999)</a>, ch. 1, I have also added the parenthetical phrase “or akin to such experiences” so as to capture pain that “feels like” the unpleasant experiences normally associated (in humans, at least) with actual or potential body damage, even if those experiences are not in fact associated with such actual or potential damage, as with some cases of <a href="https://en.wikipedia.org/wiki/Neuropathic_pain">neuropathic pain</a>, and perhaps also as with some cases of psychologically-created experiences of pain, e.g. when a subject is hallucinating or dreaming a painful experience. But I keep my definition simpler than that of e.g. <a href="http://ilarjournal.oxfordjournals.org/content/50/4/338.short">Sneddon (2009)</a>, which adds that animals in pain should “quickly learn to avoid the noxious stimulus and demonstrate sustained changes in behaviour that have protective function to reduce further injury and pain, prevent the injury from recurring, and promote healing and recovery.” Whether such phenomena are indicative of pain or just nociception is an empirical question, and I do not wish to burden my definition of pain with such assumptions.</p>
<p>Nociception can occur without pain, and pain can occur without nociception. <a href="http://journals.lww.com/pain/Citation/2008/07310/The_Kyoto_protocol_of_IASP_Basic_Pain_Terminology_.5.aspx">Loeser &amp; Treede (2008)</a> provide examples: “after local anesthesia of the mandibular nerve for dental procedures, there is peripheral nociception without pain, whereas in a patient with thalamic pain [a kind of neuropathic pain resulting from stroke], there is pain without peripheral nociception.” <a href="http://onlinelibrary.wiley.com/doi/10.1111/faf.12010/full">Rose et al. (2014)</a> provide another example of nociception without pain: “carpal tunnel surgery is sometimes performed in awake patients following axillary local anesthetic injection, which blocks conduction in axons passing from receptors in the hand and arm to the spinal cord. Consequently, the patient can watch the surgery but feel nothing, in spite of intense nociceptor activation.”</p>
<p>The ability to detect and react to noxious stimuli is a basic adaptive capability, and thus nociceptors are found in humans (<a href="http://sites.sinauer.com/neuroscience5e/chapter10.html">Purves et al. 2011</a>) and many other species (<a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. 2014</a>; <a href="https://link.springer.com/article/10.1007/s00359-009-0482-z">Smith &amp; Lewin 2009</a>), including fruit flies (<a href="http://onlinelibrary.wiley.com/doi/10.1002/dvdy.22737/full">Im &amp; Galko 2012</a>), nematode worms (<a href="http://www.pnas.org/content/96/18/10477">Wittenberg &amp; Baumeister 1999</a>), and bacteria (<a href="https://books.google.com/books?hl=en&amp;lr=lang_en&amp;id=nWiURG5lmJEC&amp;oi=fnd&amp;pg=PA459&amp;dq=%22chemical+sensing+by+bacteria%22&amp;ots=fgy05t15x0&amp;sig=8HzwA8zPmCP4hzeShs5ZnxUKCZk#v=onepage&amp;q&amp;f=false">Paoni et al. 1981</a>). Not all the nociceptors are <em>neural</em> nociceptors, however.</p>
<p>Different species have evolved different nociceptors, presumably because they are exposed to different noxious stimuli, and because a stimulus that is noxious for one species might not be noxious for another species (<a href="https://link.springer.com/article/10.1007/s00359-009-0482-z">Smith &amp; Lewin 2009</a>). For example, the threshold for noxious heat in one species of trout is ~33 °C (<a href="http://www.sciencedirect.com/science/article/pii/S000689930701582X">Ashley et al. 2007</a>), while in chickens it is ~49 °C (<a href="http://www.sciencedirect.com/science/article/pii/S0306452201003189">Gentle et al. 2001</a>), and it may be higher still in the Pompeii worms that live near hydrothermal vents and regularly experience temperatures above 80 °C (<a href="http://www.nature.com/nature/journal/v391/n6667/abs/391545a0.html">Cary et al. 1998</a>). Another example is this: while most mammalian species have acid-detecting nociceptors — and so do many species of other phyla, including e.g. the leech <em>H. medicinalis</em> (<a href="http://jn.physiology.org/content/75/6/2268.short">Pastor et al. 1996</a>) — African naked mole-rats do not have acid-detecting nociceptors (<a href="http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0060013">Park et al. 2008</a>; <a href="http://science.sciencemag.org/content/334/6062/1557">Smith et al. 2011</a>).</p>
<p>Nociceptors are also built into many personal computers (PCs). For example, many PCs contain a sensor which detects whether the computer&#8217;s central processing unit (CPU) is becoming dangerously hot, such that if it is, a fan can be signalled to turn on, cooling the CPU (<a href="http://www.quepublishing.com/store/upgrading-and-repairing-pcs-9780789750006">Mueller 2013</a>, chapter 3, section: “Processor Cooling”). Some robots are equipped with sensors that detect both noxious and non-noxious stimuli (<a href="https://benjamins.com/#catalog/books/ais.2.17dah/details">Dahl et al. 2011</a>; <a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7422729">Kühn &amp; Haddadin 2017</a>), somewhat analogous to the previously-mentioned WDR neurons in humans.</p>
<p>Some readers may be skeptical of (non-neural) nociception in bacteria. For an overview of bacterial signal transduction and subsequent chemotaxis (movement in response to chemical stimuli), see <a href="http://www.nature.com/nrm/journal/v5/n12/abs/nrm1524.html">Wadhams &amp; Armitage (2004)</a>; <a href="http://stke.sciencemag.org/content/3/128/ra50">Wuichet &amp; Zhulin (2010)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0955067411001542">Sourjik &amp; Wingreen (2012)</a>. In the literature on bacterial chemotaxis, a noxious stimuli is typically called a “repellent,” and the behavioral response away noxious chemical stimuli is sometimes called “negative chemotaxis.” Example papers describing negative chemotaxis in bacteria include <a href="http://jb.asm.org/content/169/7/3118.short">Shioi et al. (1987)</a>; <a href="http://jb.asm.org/content/172/1/383.short">Yamamoto et al. (1990)</a>; <a href="http://link.springer.com/article/10.1007/BF02092120">Kaempf &amp; Greenberg (1990)</a>; <a href="http://www.sciencedirect.com/science/article/pii/037810979390099N">Ohga et al. (1993)</a>; <a href="http://www.pnas.org/content/92/21/9757.short">Khan et al. (1995)</a>; <a href="http://www.nrcresearchpress.com/doi/abs/10.1139/m96-069">Liu &amp; Fridovich (1996)</a>; <a href="http://aem.asm.org/content/early/2015/12/07/AEM.03397-15.abstract">Karmakar et al. (2015)</a>. For a more general account of how a single-celled organism can engage in fairly sophisticated computation, see <a href="http://yalebooks.com/book/9780300167849/wetware">Bray (2011)</a>.</p>
<p>As for nociceptors in the human enteric nervous system, <a href="http://www.morganclaypool.com/doi/abs/10.4199/C00039ED1V01Y201107ISP026">Wood (2011)</a> writes: “Presence in the gastrointestinal tract of pain receptors (nociceptors) equivalent to those connected with C-fibers and A-δ fibers elsewhere in the body is likely…”</p>
<p>My sources for the presence or absence of <em>neural</em> nociceptors in multiple taxa are <a href="http://sites.sinauer.com/neuroscience5e/chapter10.html">Purves et al. (2011)</a>, <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>, <a href="https://link.springer.com/article/10.1007/s00359-009-0482-z">Smith &amp; Lewin (2009)</a>, and <a href="http://www.morganclaypool.com/doi/abs/10.4199/C00039ED1V01Y201107ISP026">Wood (2011)</a>. My source for neural nociceptors in the common fruit fly is <a href="https://elifesciences.org/content/5/e12959">Terada et al. (2016)</a>.</p>
<p>Because my investigation at one point paid special attention to similarities between rainbow trout and chickens, I collected additional specific sources for those taxa, listed in a footnote.<a class="see-footnote" id="footnoteref305_wjmgiyo" title="Sneddon (2002), Sneddon et al. (2003), and Ashley et al. (2007) for rainbow trout, and Gentle (2011) and Egger et al. (2014) for chickens. On chicken behavior and cognition more generally, see Marino (2017) and Nicol (2015).  " href="#footnote305_wjmgiyo">305</a></p>
<p>Neural nociceptors in humans come in many types. The varieties of human nociceptors are described succinctly by <a href="http://www.sciencedirect.com/science/article/pii/S0092867409012434">Basbaum et al. (2009)</a>:</p>
<blockquote><p>The cell bodies of nociceptors are located in the dorsal root ganglia (DRG) for the body and the trigeminal ganglion for the face, and have both a peripheral and central axonal branch that innervates their target organ and the spinal cord, respectively… There are two major classes of nociceptors… The first includes medium diameter myelinated [insulated] (Aδ) afferents [“afferent” means “nerve fiber of a sensory neuron”] that mediate acute, well-localized “first” or fast pain. These myelinated afferents differ considerably from the larger diameter and rapidly conducting Aβ fibers that respond to innocuous mechanical stimulation (i.e., light touch). The second class of nociceptor includes small diameter unmyelinated “C” fibers that convey poorly localized, “second” or slow pain.</p>
<p>Electrophysiological studies have further subdivided Aδ nociceptors into two main classes. Type I (HTM: high-threshold mechanical nociceptors) respond to both mechanical and chemical stimuli but have relatively high heat thresholds (&gt;50 °C). If, however, the heat stimulus is maintained, these afferents will respond at lower temperatures. And most importantly, they will sensitize (i.e., the heat or mechanical threshold will drop) in the setting of tissue injury. Type II Aδ nociceptors have a much lower heat threshold, but a very high mechanical threshold. Activity of this afferent almost certainly mediates the “first” acute pain response to noxious heat… By contrast, the type I fiber likely mediates the first pain provoked by pinprick and other intense mechanical stimuli.</p>
<p>The unmyelinated C fibers are also heterogeneous. Like the myelinated afferents, most C fibers are polymodal, that is, they include a population that is both heat and mechanically sensitive (CMHs)… Of particular interest are the heat-responsive, but mechanically insensitive, unmyelinated afferents (so-called silent nociceptors) that develop mechanical sensitivity only in the setting of injury… These afferents are more responsive to chemical stimuli (capsaicin or histamine) compared to the CMHs and probably come into play when the chemical milieu of inflammation alters their properties. Subsets of these afferents are also responsive to a variety of itch-producing [stimuli]. It is worth noting that not all C fibers are nociceptors. Some respond to cooling, and [others]… appear to mediate pleasant touch…</p></blockquote>
<p>The variety of nociceptors across the entire animal kingdom is of course much broader.</p>
<p><span style="float: right;">[<a href="/node/858/edit/72">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="AppendixE">Appendix E. Some clarifications on “neuroanatomical similarity”</h3>
<p>In this appendix, I clarify how I estimated the values for “neuroanatomical similarity” when applying my “theory-agnostic estimation process” described <a href="#HighLevel">above</a>.</p>
<p>As far as I know, there is no widely used measure of neuroanatomical similarity. Moreover, hypotheses about structural homologies are often highly uncertain, and revised over time.</p>
<p>I am not remotely an expert in comparative neuroanatomy, and my very rough ratings for “neuroanatomical similarity with humans” are drawn from my cursory understanding of the field, and would likely be disputed by experts in comparative neuroanatomy. For a brief overview of the major “similarity” factors I&#8217;m considering, see e.g. <a href="http://onlinelibrary.wiley.com/doi/10.1002/9780470015902.a0000088.pub3/full">Powers (2014)</a>.<a class="see-footnote" id="footnoteref306_9nw7gdb" title="For convenience, I quote below some sections of the paper that describe key neuroanatomical differences in phylogeny, and explain (in brackets) a few especially important but perhaps unfamiliar terms:      One of the most important terms in understanding the evolution of the brain is 'homology'. Two structures are homologous if they can be traced to a common ancestor… When we seek to understand the evolution of the brain, we identify homologies and draw conclusions about what has changed or remained the same, based on those homologies. For example, the cerebellum of all fishes and tetrapods [amphibians, reptiles, birds, and mammals] is considered to be homologous, because there are similarities of origin, structure and function. In contrast, the cerebral cortex exists in its present form only in mammals…  &#9;Among the many phyla of invertebrates, two major groups can be distinguished: those with radial symmetry and those with bilateral symmetry. These groups differ fundamentally in the structure of their nervous systems. Radially symmetrical organisms evolved earlier and their nervous systems are net-like, with rings of neurons and typically no concentration of nerve cells in one place. In contrast, bilaterally symmetrical organisms tend to have concentrations of neurons in the head region that are often called ganglia but resemble the brains of vertebrates…  &#9;Bilaterally symmetrical invertebrates are very numerous, and their nervous systems vary from simple to complex…  &#9;More complex nervous systems are found in the [flatworms and segmented worms], which have a pair of nerves running the length of their bodies connected at each segment in a ladder-like arrangement and paired head ganglia at the anterior end. Among the most complex brains in invertebrates are the insects and crustaceans… and the octopi… Their nervous systems consist of paired ganglia at each segment and, in the head, fused paired ganglia forming the brain. The brain consists of multiple lobes, as many as 40 in octopi…  &#9;The earliest vertebrates still extant today are the jawless fishes, cyclostomes and hagfish. The brains of these animals possess all of the characters of vertebrates, including five divisions of the brain: medulla oblongata and pons (together called the hindbrain), midbrain, diencephalon and telencephalon. (Lampreys lack a cerebellum, and its presence is debated in hagfish, but it is present in all other groups.)…  &#9;In all vertebrates, the medulla oblongata is the caudalmost [toward the tail] region of the hindbrain…  &#9;Major sensory and motor tracts that connect the brain and spinal cord run through the medulla. These tracts are more extensive in mammals than in nonmammalian vertebrates: nonmammals do not have corticospinal tracts (because they have no neocortex) although some descending tracts run from the telencephalon to at least the hindbrain in birds…  &#9;Whether the cerebellum exists in hagfish is debated, and it is absent in lampreys…, but appears as a well-developed structure in chondrichthyes, the cartilaginous fishes, and in all gnathostomes (jawed vertebrates). It has been proposed that the cerebellum evolved by duplication of cerebellum-like structures in the dorsolateral wall of the hindbrain in cartilaginous fishes, which receive input from the lateral line system and the electrosensory system…   &#9;A cerebellum with similar cells and circuits and layers is present in other vertebrates, but has expanded independently in the taxa with elaborate cerebella. Sharks, fishes and birds, as well as mammals, possess an elaborate cerebellum with multiple lobes. In each group and also in the simpler cerebellums of cyclostomes, amphibians and nonavian reptiles, a cerebellar cortex is found, with the same three layers found in mammals…  &#9;The roof of the midbrain forms sensory centres in all vertebrates. In mammals, these are termed the superior (vision) and inferior (auditory) colliculi. The homologous structures in nonmammals are the optic tectum and the torus semicircularis, respectively. The optic tectum or superior colliculus is laminated, and it receives retinal input to its superficial layers and auditory, somatosensory, and where present, electrosensory input to its deep layers. In animals that rely heavily on vision, such as birds, however, the optic tectumis larger and more elaborate, suggesting that its most important functions are with vision…  &#9;The forebrain (diencephalon and cerebral hemispheres) is the most diverse portion of the vertebrate brain. Certain general principles apply to its organisation, but wide variation in structure is seen. In all vertebrates, the diencephalon consists of four divisions from dorsal to ventral: the epithalamus, dorsal thalamus, ventral thalamus and hypothalamus. The dorsal thalamus and, in some organisms, the ventral thalamus and/or hypothalamus relay sensory information to the telencephalon. The epithalamus and the hypothalamus function in the regulation of visceral functions, including reproduction, circadian rhythms, and sleep and waking…  &#9;The cerebral hemispheres, or telencephalon, in all vertebrates can be divided into dorsal [toward the animal's back] and ventral [toward the animal's front] parts, called pallial and subpallial. In mammals, pallial regions can be subdivided into the olfactory bulb and associated cortex (lateral pallium), neocortex (dorsal pallium), hippocampus (medial pallium), and claustrum and pallial amygdala (or ventral pallium). Subpallial regions become the basal ganglia and some limbic regions. In most nonmammals, the medial pallium is recognised as the equivalent of the hippocampus. The lateral pallium is also recognised as the olfactory cortex. But the identity of the dorsal pallium and the equivalent structures is controversial.  &#9;…  &#9;The basal ganglia can be recognised in all vertebrates. In lampreys, equivalences to all major components of the basal ganglia can be identified: the striatum, the globus pallidus, the subthalamic nucleus and the substantia nigra pars compacta, although identification of the globus pallidus is not yet conclusive… In cartilaginous fishes, a structure termed the area periventricularis ventralis is thought to be equivalent to the dorsal striatum, and a nucleus superficialis basalis may correspond to the ventral striatum. In actinopterygian fishes, it is also believed that there are structures corresponding to the dorsal and ventral striatum. In amphibians, a dorsal and ventral striatum can be recognised, and a nucleus that is believed to be pallidal, called the entopeduncular nucleus. In nonavian reptiles and birds, the dorsal striatum is called the lateral striatum, and the ventral striatum is the medial striatum. A major difference in basal ganglia among vertebrate groups is that a major output in mammals goes to the thalamus, which projects back to the cortex, whereas in nonmammals, and especially in anamniotes, the basal ganglia project downstream and modulate the downstream motor pathways. Mammals have descending pathways as well, but the loops involving the cortex predominate…  &#9;The cerebral hemispheres of mammals appear to be very different from those of nonmammalian vertebrates because all mammals have cerebral cortex, a layered structure on the surface of the brain. Nonmammalian amniotes have some cortical structures, equivalent to the olfactory cortex (lateral pallium) and hippocampus (medial pallium) of mammals, but these are made up of only three layers. The structure that is different in mammals is the neocortex (dorsal pallium), which is extensive and is made up of six interconected layers of cells…  &#9;There is no general agreement about the homologies of the cell populations of the pallium in mammals and nonmammals. Two major positions are represented at the present time, one called the neocortex hypothesis…, the other called the claustroamygdaloid hypothesis … The neocortex hypothesis is that the cell populations of the pallium in nonmammals, which are arranged in nuclei, are homologous with populations of cells in the neocortex of mammals, which are arranged in layers. The claustroamygdaloid hypothesis is that the cell populations of the pallium in nonmammals are homologous with cell populations in the amygdala and claustrum of mammals…  " href="#footnote306_9nw7gdb">306</a></p>
<p>To illustrate some of the similarities and differences I have in mind, I present below a table of the animal taxa I rated for “neuroanatomical similarity with humans” <a href="#HighLevel">above</a>, shown alongside my rating of neuroanatomical similarity, and some (but not all) of the factors influencing that rating judgment.<a class="see-footnote" id="footnoteref307_6xlyih9" title="My sources for the information in this table are Powers (2014) and T.M. Preuss' chapter &quot;Primate Brain Evolution&quot; in Kaas (2009). (Presumably the relevant chapters in Kaas 2016 are more up-to-date, but I haven't read them.)  " href="#footnote307_6xlyih9">307</a> Note the “processing power” measures (e.g. brain mass, neuron counts, or neuronal scaling rules) are excluded here, because they are captured by a separate column in <a href="#HighLevel">my previous table</a>.</p>
<p>Because the width of the table doesn&#8217;t fit within the horizontal space allotted for this page, the table must be scrolled horizontally to view all its contents.</p>
<table><tr><th></th>
<th>My rating</th>
<th>Bilateral or radial symmetry?</th>
<th>Ganglia or brain?</th>
<th>Midbrain?</th>
<th>Reticular formation?</th>
<th>Diencephalon?</th>
<th>Telencephalon?</th>
<th>Neocortex?</th>
<th>Dorsolateral prefrontal cortex?</th>
<th>Extreme hemispheric specialization?</th>
</tr><tr><td>Humans (for comparison)</td>
<td></td>
<td>Bilateral</td>
<td>Brain</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes, via evagination</td>
<td>Yes, disproportionately large</td>
<td>Yes</td>
<td>Yes<a class="see-footnote" id="footnoteref308_nicpt0d" title="See M.C. Corballis' chapter &quot;The Evolution of Hemispheric Specializations of the Human Brain&quot; in Kaas (2009).    &#9;" href="#footnote308_nicpt0d">308</a></td>
</tr><tr><td>Chimpanzees</td>
<td>High</td>
<td>Bilateral</td>
<td>Brain</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes, via evagination</td>
<td>Yes, disproportionately large</td>
<td>Yes</td>
<td>No</td>
</tr><tr><td>Cows</td>
<td>Moderate/high</td>
<td>Bilateral</td>
<td>Brain</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes, via evagination</td>
<td>Yes</td>
<td>Debated, but probably not<a class="see-footnote" id="footnoteref309_zsll3zr" title="T.M. Preuss, in his chapter &quot;Primate Brain Evolution&quot; (Kaas 2009, ch. 35), reviews the debate:    &#9;Among mammals, only primates have a region of cortex with a well-developed granular layer on the dorsolateral surface of the frontal lobe (Brodmann, 1909). The region is present in all primates that have been examined… Owing in part to the influence of Brodmann, the granular dorsolateral prefrontal cortex initially came to be regarded as a hallmark of the primate brain. The fact that some neurologists in the early part of the twentieth century regarded this region as the seat of higher-order cognitive functions reinforced this view. Modern experimental studies in nonhuman primates… reveal it to have strong connections with the higher-order parietal and temporal areas discussed above, and functional studies in humans and nonhuman primates indicate that different parts of the granular frontal cortex are involved in attention, working memory, and planning…  &#9;The idea that dorsolateral prefrontal cortex is special to primates has, nevertheless, been challenged (see the reviews of Preuss, 1995a, 2006). With the introduction of the first generation of techniques for studying cortical connectivity (lesion-degeneration techniques), it became clear that the cortical regions differed in their patterns of connectivity as well as their histology. Early research on the forebrain connections of the cortex focused on connections with the thalamus because cortical lesions produce degeneration in thalamic nuclei that project to them; most other connections could not be reliably resolved until improved methods became available in the 1970s. Rose and Woolsey (1949) championed the idea that regions of cortex could be defined by the thalamic nuclei that projected to them. As the dorsolateral prefrontal cortex, the largest prefrontal region in primates, receives its major thalamic inputs from the mediodorsal thalamic (MD) nucleus, prefrontal cortex came to be defined as MD-projection cortex (Rose and Woolsey, 1948). As it happens, all mammals that have been examined have a MD nucleus and a cortical territory to which it projects, so by this reasoning, all mammals possess a homologue of dorsolateral prefrontal cortex, even though the MD-projection cortex of nonprimates lacks the well-developed granular layer that marks this region in primates (Rose and Woolsey, 1948; Akert, 1964). It was also reported that dopamine-containing nuclei of the brainstem project very strongly to MD-projection cortex in both primates and nonprimates, and this has also been used to identify homologues in different mammals (Divac et al., 1978). Attempts have also been made to refine this analysis by identifying homologues of specific subdivisions of primate dorsolateral prefrontal cortex in nonprimates (Akert, 1964). A region of special interest has been the cortex that lines the principal sulcus of macaques (principalis cortex), because lesions of this region impair performance on spatial working memory tasks, a set of cognitive tasks that have been adapted for use in a wide range of mammals. Using the criteria ofMDprojections, dopamine projections, and involvement in spatial working memory tasks, homologues of macaque principalis cortex have been proposed in nonprimate species, and most importantly in rats, which are the most widely used model animals in mammalian neuroscience. In rats, the principalis homologue has usually been localized to the medial surface of the frontal lobe, and some workers have identified it specifically with area 32 (the prelimbic area)…  &#9;This might seem a satisfactory account of prefrontal homologies, but there are difficulties with both the evidence and the reasoning (Preuss, 1995a). For one thing, in primates, MD projects not only to the granular, dorsolateral prefrontal frontal cortex, but also to agranular regions, including orbital cortex, the classical anterior cingulate areas (areas 24 and 32 of Brodmann), and even to insular and premotor cortex. For another, while dorsolateral prefrontal cortex receives dopaminergic inputs, the strongest dopamine projections in primates are actually to the motor region and the orbital and medial cortex. Finally, in primates, lesions of the medial frontal cortex, involving the cingulate region and sparing the dorsolateral region, produce impairments on spatial working memory tasks. Thus, none of the features that have been used to identify homologues of granular prefrontal cortex in nonprimates are actually diagnostic of granular prefrontal cortex in primates. In fact, the medial frontal cortex of rodents very closely resembles the agranular parts of the medial frontal cortex of primates on a variety of structural and functional grounds – both are limbic regions, after all. It is true that the medial frontal cortex of rodents resembles primate granular frontal cortex in certain respects, but these are also the ways that the medial frontal cortex of primates resembles the dorsolateral prefrontal cortex of primates; the similarities are not diagnostic. Moreover, primate granular frontal cortex has additional features of areal organization and connectivity that do not match any known region of frontal cortex in any nonprimate mammal (Preuss, 1995a).  &#9;On present evidence, then, there are good grounds for concluding that dorsolateral prefrontal cortex is in fact one of the distinctive features of the primate brain.    &#9;" href="#footnote309_zsll3zr">309</a></td>
<td>No</td>
</tr><tr><td>Chickens</td>
<td>Low/moderate</td>
<td>Bilateral</td>
<td>Brain</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes, via evagination</td>
<td>Debated, but “not really”<a class="see-footnote" id="footnoteref310_am702s6" title="See e.g. Dugas-Ford et al. (2012) on possible neocortex homologs in avian brains. I say &quot;not really,&quot; because even if Dugas-Ford &amp; colleagues are right, it is still not the case that birds have the typical mammalian 6-layer neocortex.    &#9;" href="#footnote310_am702s6">310</a></td>
<td>No</td>
<td>No</td>
</tr><tr><td>Rainbow trout</td>
<td>Low/moderate</td>
<td>Bilateral</td>
<td>Brain</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes, via eversion<a class="see-footnote" id="footnoteref311_8w74jhr" title="The distinction between a telencephalon formed by eversion and a telencephalon formed by evagination is explained succinctly by Powers (2014):    &#9;In all vertebrates except actinopterygian fishes [i.e. ray-finned fishes, e.g. trout and nearly all other well-known fishes], the telencephalon developed by evagination, that is, it grew outward, away from the lateral ventricle in all directions. In actinopterygian fishes, however, the telencephalon developed by a different method, called eversion. The roof of the lateral ventricle thinned and stretched to form a membrane over the ventricle on the surface of the hemisphere, and the telencephalon forms a solid mass (with no ventricle). Because of this unique pattern of development, establishment of homologies between the actinopterygian and tetrapod brain is difficult…    &#9;" href="#footnote311_8w74jhr">311</a></td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr><tr><td>Gazami crabs</td>
<td>Very low</td>
<td>Bilateral</td>
<td>Ganglia</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr><tr><td>Common fruit flies</td>
<td>Very low</td>
<td>Bilateral</td>
<td>Ganglia</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr></table><p><span style="float: right;">[<a href="/node/858/edit/73">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="AppendixF">Appendix F. Illusionism and its implications</h3>
<p>In this appendix, I elaborate on my <a href="#illusionism">earlier</a> brief explanation of illusionism, and say more about its implications for my tentative conclusions in this report.</p>
<p><span style="float: right;">[<a href="/node/858/edit/74">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="IllusionismDefined">What I mean by “illusionism”</h4>
<p><a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00002">Frankish (2016b)</a> explains illusionism this way:</p>
<blockquote><p>Suppose we encounter something that seems anomalous, in the sense of being radically inexplicable within our established scientific worldview. Psychokinesis is an example. We would have, broadly speaking, three options. First, we could accept that the phenomenon is real and explore the implications of its existence, proposing major revisions or extensions to our science… In the case of psychokinesis, we might posit previously unknown psychic forces and embark on a major revision of physics to accommodate them. Second, we could argue that, although the phenomenon is real, it is not in fact anomalous and can be explained within current science. Thus, we would accept that people really can move things with their unaided minds but argue that this ability depends on known forces, such as electromagnetism. Third, we could argue that the phenomenon is illusory and set about investigating how the illusion is produced. Thus, we might argue that people who seem to have psychokinetic powers are employing some trick to make it seem as if they are mentally influencing objects.</p>
<p>The first two options are <em>realist</em> ones: we accept that there is a real phenomenon of the kind there appears to be and seek to explain it. Theorizing may involve some modest reconceptualization of the phenomenon, but the aim is to provide a theory that broadly vindicates our pre-theoretical conception of it. The third position is an <em>illusionist</em> one: we deny that the phenomenon is real and focus on explaining the appearance of it. The options also differ in explanatory strategy. The first is <em>radical</em>, involving major theoretical revision and innovation, whereas the second and third are <em>conservative</em>, involving only the application of existing theoretical resources.</p>
<p>Turn now to consciousness. Conscious experience has a subjective aspect; we say it is <em>like something</em> to see colours, hear sounds, smell odours, and so on. Such talk is widely construed to mean that conscious experiences have introspectable qualitative properties, or ‘feels’, which determine what it is like to undergo them. Various terms are used for these putative properties. I shall use ‘phenomenal properties’, and, for variation, ‘phenomenal feels’ and ‘phenomenal character’, and I shall say that experiences with such properties are phenomenally conscious… Now, phenomenal properties seem anomalous. They are sometimes characterized as simple, ineffable, intrinsic, private, and immediately apprehended, and many theorists argue that they are distinct from all physical properties, inaccessible to third-person science, and inexplicable in physical terms… Again, there are three broad options.</p>
<p>First, there is radical realism, which treats phenomenal consciousness as real and inexplicable without radical theoretical innovation. In this camp I group dualists, neutral monists, mysterians, and those who appeal to new physics… Second, there is conservative realism, which accepts the reality of phenomenal consciousness but seeks to explain it in physical terms, using the resources of contemporary cognitive science or modest extensions of it. Most physicalist theories fall within this camp, including the various forms of representational theory. Both radical and conservative realists accept that there is something real and genuinely qualitative picked out by talk of the phenomenal properties of experience, and they adopt this as their explanandum. That is, both address [Chalmers’] hard problem.</p>
<p>The third option is illusionism. This shares radical realism&#8217;s emphasis on the anomalousness of phenomenal consciousness and conservative realism&#8217;s rejection of radical theoretical innovation. It reconciles these commitments by treating phenomenal properties as illusory. Illusionists deny that experiences have phenomenal properties and focus on explaining why they seem to have them. They typically allow that we are introspectively aware of our sensory states but argue that this awareness is partial and distorted, leading us to misrepresent the states as having phenomenal properties… Whatever the details, they must explain the content of the relevant states in broadly functional terms, and the challenge is to provide an account that explains how real and vivid phenomenal consciousness seems. This is the illusion problem.</p></blockquote>
<p>Illusionism comes in many varieties. Here is an example of what Frankish calls “weak illusionism,” from <a href="http://www.openphilanthropy.org/some-initial-thoughts-moral-patients">Carruthers (2000)</a>, pp. 93-94:</p>
<blockquote><p>What would it take for [an explanatory theory] of phenomenal consciousness to succeed? What are the <em>desiderata</em> for a successful theory? I suggest that the theory would need to explain, or explain away, those aspects of phenomenal consciousness which seem most puzzling and distinctive, of which there are five:</p>
<ol><li>Phenomenally conscious states have a <em>subjective</em> dimension; they have <em>feel</em>; there is something which it is <em>like</em> to undergo them.</li>
<li>The properties involved in phenomenal consciousness seem to their subjects to be <em>intrinsic</em> and non-relationally individuated.</li>
<li>The properties distinctive of phenomenal consciousness can seem to their subjects to be <em>ineffable</em> or indescribable.</li>
<li>Those properties can seem in some way <em>private</em> to their possessors.</li>
<li>It can seem to subjects that we have <em>infallible</em> (as opposed to merely <em>privileged</em>) knowledge of phenomenally conscious properties.</li>
</ol><p>Note that only (1) is expressed categorically, as a claim about the actual nature of phenomenal consciousness. The other strands are expressed in terms of ‘seemings’, or what the possessors of phenomenally conscious mental states may be <em>inclined to think</em> about the nature of those states. This is because (1) is definitive of the very idea of phenomenal consciousness… whereas (2) to (5), when construed categorically, are the claims concerning phenomenal consciousness which raise particular problems for physicalist and functionalist conceptions of the mind…</p>
<p>Aspect (1) therefore needs to be <em>explained</em> in any successful account of phenomenal consciousness; whereas (2) to (5) – when transposed into categorical claims about the nature of phenomenal consciousness – should be <em>explained away</em>. If we can explain (2) to (5) in a way which involves no commitment to the truth of the things people are inclined to think about phenomenal consciousness, then we can be <em>qualia irrealists</em> (in the strong sense of ‘qualia’…). But if we can explain (1), then we can maintain that we are, nevertheless, naturalistic realists concerning phenomenal consciousness itself.</p></blockquote>
<p>Frankish calls Carruthers’ theory an example of “weak illusionism” because, while Carruthers suggests that <em>several</em> key features of consciousness are illusions, he seems to accept that perhaps the <em>most</em> central feature of consciousness — its “subjective,” “something it&#8217;s like” nature — is real, and needs to be “explained” rather than “explained away.”<a class="see-footnote" id="footnoteref312_spp7zsl" title="Though, see Frankish (2012a), where Frankish argues (in different terms) that Carruthers' account of consciousness may be an example of strong illusionism being &quot;mis-sold&quot; as weak illusionism.  Theories which use a &quot;phenomenal concepts strategy&quot; might in some cases qualify as examples of weak illusionism. For example, Tye (2000), p. 23:  I accept that experiences are fully, robustly physical but I maintain that there is no explanatory gap posed by their phenomenology. The gap, I claim, is unreal; it is a cognitive illusion to which we only too easily fall prey… There aren't two sorts of natural phenomena — the irreducibly subjective and the objective. The so-called &quot;explanatory gap&quot; derives largely from a failure to recognize the special features of phenomenal concepts. These concepts, I maintain, have a character that not only explains why we have the intuition that something important is left out by the physical (and/or functional) story but also explains why this intuition is not to be trusted.  Sensorimotor theory might also qualify as a weak (or perhaps even strong) illusionist theory. O'Regan &amp; Noe (2001):  In our view, the qualia debate rests on what Ryle (1949/1990) called a category mistake. Qualia are meant to be properties of experiential states or events. But experiences, we have argued, are not states. They are ways of acting. They are things we do. There is no introspectibly available property determining the character of one's experiential states, for there are no such states. Hence, there are, in this sense at least, no (visual) qualia. Qualia are an illusion, and the explanatory gap is no real gap at all.  It is important to stress that in saying this we are not denying that experience has a qualitative character. We have already said a good deal about the qualitative character of experience and how it is constituted by the character of the sensorimotor contingencies at play when we perceive… Our claim, rather, is that it is confused to think of the qualitative character of experience in terms of the occurrence of something (whether in the mind or brain). Experience is something we do and its qualitative features are aspects of this activity.  …Many philosophers, vision scientists, and lay people will say that seeing always involves the occurrence of raw feels or qualia. If this view is mistaken, as we believe, then how can we explain its apparent plausibility to so many? In order to make our case convincing, we must address this question.  In our view, there are two main sources of the illusion. The first pertains to the unity and complexity of experience. We tend to overlook the complexity and heterogeneity of experience, and this makes it seem as if in experience there are unified sensation-like occurrences. The second source of illusion has to do with the felt presence of perceptible qualities. Because, when we see, we have continuous access to features of a scene, it is as if we continuously represent those features in consciousness…  " href="#footnote312_spp7zsl">312</a> In contrast, Frankish stipulates, a “strong illusionist” would say that even the subjective, qualitative, “what it&#8217;s like”-ness of consciousness is an illusion.</p>
<p>My own view is probably best described as a variant of “strong illusionism,” and hereafter I will (like Frankish) use “illusionism” to mean “strong illusionism,” unless otherwise specified. (As Frankish argues, weak illusionism may collapse into strong illusionism anyway.)</p>
<p>However, unlike Frankish, I avoid saying things like “phenomenal consciousness is an illusion” or “phenomenal properties are illusory,” because whereas Frankish defines “phenomenal consciousness” and “phenomenal properties” in a particular philosophical way, I&#8217;m instead taking Schwitzgebel&#8217;s approach of definining these terms by example (see <a href="#Defined">above</a>).<a class="see-footnote" id="footnoteref313_jrp9b3j" title="Some of my reasons for wanting to avoid saying things like &quot;phenomenal consciousness is an illusion&quot; or &quot;phenomenal properties are illusory&quot; were expressed by Graziano (2016):  The attention schema theory [Graziano's theory] has much in common with illusionism. It clearly belongs to the same category of theory, and is especially close to the approach of Dennett (1991). But I confess that I baulk at the term ‘illusionism' because I think it miscommunicates. To call consciousness an illusion risks confusion and unwarranted backlash. To me, consciousness is not an illusion but a useful caricature of something real and mechanistic…  In my own discussions with colleagues, I invariably encounter the confusion and backlash. To most people, an illusion is something that does not exist. Calling consciousness an illusion suggests a theory in which there is nothing present that corresponds to consciousness. However, in the attention schema theory, and in the illusionism described by Frankish, something specific is present. In the attention schema theory, the real item that exists inside us is covert attention — the deep processing of selected information. Attention truly does exist. Our internal model of it lacks details and therefore provides us with a blurred, seemingly magicalist account of it.  Second, in normal English, to experience an illusion is to be fooled. To call consciousness an illusion suggests to most people that the brain has made an error. In the attention schema theory, and also in the illusionism approach described by Frankish, the relevant systems in the brain are not in error. They are well adapted. Internal models always, and strategically, leave out the unnecessary detail.  Third, most people understand illusions to be the result of a subjective experience. The claim that consciousness is an illusion therefore sounds inherently circular. Who is experiencing the illusion? It is difficult to explain to people that the experiencer is not itself conscious, and that what is important is the presence of the information and its impact on the system. The term illusion instantly aligns people's thoughts in the wrong direction.  All of the common objections I encounter have answers. They are based on a misunderstanding of illusionism. But the misunderstanding is my point. Why use a misleading word that requires one to backtrack and explain? For these reasons, in my own writing I have avoided calling consciousness an illusion except in specific circumstances, such as the consciousness we attribute to a ventriloquist puppet, in which the term seems to apply more exactly.  Perhaps I am too much of a visual physiologist at heart. To me, an illusion is a mistake in a sensory internal model. It introduces a consequential discrepancy between the internal model and the real world. That discrepancy can cause errors in behaviour. In contrast, an internal model, at all times, with or without an illusion, is an efficient, useful compression of data. It is never literally accurate. Even when it is operating correctly and guiding behaviour usefully, it is a caricature of reality. I am comfortable calling consciousness a caricature, but not an illusion. It is a cartoonish model of something real.  See also Sloman &amp; Chrisley (2016)'s comments on potential terminological distinctions between illusionism, eliminativism, &quot;revisionism,&quot; and &quot;hallucinationism.&quot;  " href="#footnote313_jrp9b3j">313</a> On this way of talking, phenomenal consciousness is real, and so are phenomenal properties, and there&#8217;s “something it&#8217;s like” to be me, and probably there&#8217;s “something it&#8217;s like” to be a chimpanzee, and probably there <em>isn&#8217;t</em> “something it&#8217;s like” to be a chess-playing computer, and these “phenomenal properties” and this “something it&#8217;s like”-ness aren&#8217;t what they <em>seem</em> to be when we introspect about them, and they don&#8217;t have the properties that many philosophers have assumed they must have, and <em>that</em> is the sense in which these features of consciousness are “illusory.”</p>
<p>Frankish sounds like he&#8217;d be fine with this way of talking, too, so long as we have some way to distinguish what the phenomenal realist means by phenomenality-related terms and what the illusionist means by them.<a class="see-footnote" id="footnoteref314_5r266y5" title="In Frankish (2016b), Frankish writes:  Is the illusionist claiming that we are mistaken in thinking we have conscious experiences? It depends on what we mean by ‘conscious experiences'. If we mean experiences with phenomenal properties, then illusionists do indeed deny that such things exist. But if we mean experiences of the kind that philosophers characterize as having phenomenal properties, then illusionists do not deny their existence. They simply offer a different account of their nature, characterizing them as having merely quasi-phenomenal properties. Similarly, illusionists deny the existence of phenomenal consciousness properly so-called, but do not deny the existence of a form of consciousness (perhaps distinct from other kinds, such as access consciousness) which consists in the possession of states with quasi-phenomenal properties and is commonly mischaracterized as phenomenal. Henceforth, I shall use ‘consciousness' and ‘conscious experience' without qualification in an inclusive sense to refer to states that might turn out to be either genuinely phenomenal or only quasi-phenomenal. In this sense realists and illusionists agree that consciousness exists.  Do illusionists then recommend eliminating talk of phenomenal properties and phenomenal consciousness? Not necessarily. We might reconceptualize phenomenal properties as quasi-phenomenal ones. Recall Pereboom's analogy with secondary qualities. The discovery that colours are mind-dependent did not lead scientists to deny that objects are coloured. Rather, they reconceptualized colours as the properties that cause our colour sensations. Similarly, we might respond to the discovery that experiences lack phenomenal properties by reconceptualizing phenomenal properties as the properties that cause our representations of phenomenal feels — that is, quasi-phenomenal properties…  In everyday life… we would surely continue to talk of the feel or quality of experience in the traditional, substantive sense. As subjects of experience, our interest is in how things seem to us introspectively — the illusion itself, not the mechanisms that cause it. Such talk may fail to pick out real properties, but it is not empty or pointless.  When I defined consciousness by example above, the paper I built on was Schwitzgebel (2016), which is a response to Frankish (2016b). In his response to Schwitzgebel's proposed definition, Frankish replied (Frankish 2016c):  [Schwitzgebel offers] a definition by example, describing a range of uncontentious positive and negative cases and identifying phenomenal consciousness as ‘the most folk-psychologically obvious thing or feature that the positive examples possess and that the negative examples lack'…   I think Schwitzgebel succeeds in identifying an important folkpsychological kind — indeed the very one that should be our focus in theorizing about consciousness…   …He has defined a neutral explanandum for theories of consciousness, which both realists and illusionists can adopt. (I have referred to this as consciousness in an inclusive sense. We might call it simply consciousness, or, if we need to distinguish it from other forms, putative phenomenal consciousness.)  So, my guess is that Frankish would not mind my way of talking, so long as we have some way of distinguishing what the phenomenal realist means by &quot;qualia&quot; and &quot;phenomenal properties&quot; and so on from what the illusionist means by such terms.  " href="#footnote314_5r266y5">314</a> Below and elsewhere in this report, I will use terms like “consciousness” and “qualia” and “phenomenal properties” to refer to the kinds of experiences defined by example <a href="#Defined">above</a>, which both “realists” and “illusionists” agree exist. To refer to the <em>special</em> kinds of qualia and phenomenal properties that “realists” think exist (and that illusionists deny), I&#8217;ll use phrases such as “the realist&#8217;s notion of qualia.”</p>
<p><a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00002">Frankish (2016b)</a> and the other papers in <a href="http://www.ingentaconnect.com/content/imp/jcs/2016/00000023/f0020011">the same journal issue</a> do a good job of explaining the arguments for and against illusionism, and I won&#8217;t repeat them here. I will, however, make some further effort to explain what illusionism <em>is</em>, since the idea can be difficult to wrap one&#8217;s head around, and also these issues are hard to talk about clearly.<a class="see-footnote" id="footnoteref315_whzt5un" title="Indeed, when reading the journal issue linked above, I often found myself wondering what the authors meant by terms like &quot;what it's like&quot; and &quot;phenomenal properties&quot; and &quot;quasi-phenomenal properties,&quot; and sometimes I couldn't tell which author I would agree with more if I was able to understand better what each of them was trying to say. Moreover, it seems likely to me that even assuming something like &quot;strong illusionism&quot; about consciousness is correct, our attempts to describe it using the terms and concepts and metaphors we've come up with so far will look quite naive and confused 50 years from now. In my view, detailed empirical work and computational modeling could be the most useful inputs, in the long run, to the clarification of illusionist and other hypotheses about consciousness (see my comments starting here). Nevertheless, I will attempt to clarify (what I see as) the illusionist view, using the concepts and metaphors available to me now.  " href="#footnote315_whzt5un">315</a> After that, I&#8217;ll make some brief comments about what implications illusionism seems to have for the distribution question and for my moral intuitions about moral patienthood.</p>
<p><span style="float: right;">[<a href="/node/858/edit/75">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="OtherIllusions">Other cognitive illusions
</h4><p>First, it&#8217;s worth calling to mind some cognitive illusions about other things, which can help to set the stage for understanding how some “more central” features of consciousness might <em>also</em> be illusory.</p>
<p><img src="/files/Research/Moral_Patienthood/Tabletops.png" alt="Tabletops illusion" align="right" />Consider the “tabletops” illusion on the right.<a class="see-footnote" id="footnoteref316_d46xilt" title="This is Figure 0.6 in Snowden et al. (2012) — i.e. Basic Vision: An Introduction to Visual Perception, Revised Edition by Robert Snowden et al (2012): Figure 0.6 (p.8) — reprinted here by permission of Oxford University Press. The image was adapted from the figure on p. 46 of Shepard (1990), which in turn was an elaboration of a figure on p. 298 of Shepard's chapter &quot;Psychophysical complementarity&quot; in Kubovy &amp; Pomerantz (1981).  " href="#footnote316_d46xilt">316</a> Would you believe that these two tabletops are exactly the same shape? When I first saw this illusion, I couldn&#8217;t make my brain believe they were the same shape no matter what I tried. <em>Clearly</em>, the table on the right is longer and narrower than the one on the left. To test this, I cut a piece of paper to be the same shape as the first tabletop, and then I moved and rotated it to cover the second tabletop. Sure enough, they were the same shape! After putting away the piece of paper, my brain <em>still</em> cannot perceive them as the same shape. But, with help from the piece of paper, I can convince myself they <em>are</em> the same shape, even though I may never be able to <em>perceive</em> them as the same shape.</p>
<p>This example illustrates a lesson that will be useful later: we can know something is an illusion even though our direct <em>perception</em> remains as fooled as ever, and even though we do not know how the illusion is produced.<a class="see-footnote" id="footnoteref317_856hmlf" title="In this case, we do know some things about why the visual illusion is produced, but reading the relevant studies isn't necessary for knowing that our perception has been tricked.  " href="#footnote317_856hmlf">317</a> </p>
<p>Of course, our brains don&#8217;t merely trick us about particular objects or stimuli. Other cognitive illusions affect us continuously, from birth to death, and some of these illusions have only been discovered quite recently. For example, the human eye&#8217;s natural blind spot — mentioned <a href="#illusionism">above</a> — wasn&#8217;t discovered until the 1660s.<a class="see-footnote" id="footnoteref318_oeoiwgd" title="Grzybowski &amp; Aydin (2007).  " href="#footnote318_oeoiwgd">318</a> Or, consider the fact that your entire visual field <em>seems</em> to be “in color,” but in fact you have greatly diminished color perception in the periphery your visual field, such that you cannot distinguish green and red objects at about 40° eccentricity (away from the center of your visual field), depending on the size of the objects.<a class="see-footnote" id="footnoteref319_q58dbxx" title="See e.g. Hansen et al. (2009).  " href="#footnote319_q58dbxx">319</a> As far as I know, this basic fact about our daily visual experience, which is very easy to test, wasn&#8217;t discovered until the 19th century.<a class="see-footnote" id="footnoteref320_7ia0b6x" title="Baird (1905), ch. 1.  " href="#footnote320_7ia0b6x">320</a></p>
<p>Next, consider a class of pathological illusions, in which patients are either convinced they have a disability they don&#8217;t have, or convinced they <em>don&#8217;t</em> have a disability they <em>do</em> have. For example, patients with Anton&#8217;s syndrome are blind, but they don&#8217;t <em>think</em> they are blind:<a class="see-footnote" id="footnoteref321_kggibcp" title="Quote from Heilman (1991).  " href="#footnote321_kggibcp">321</a></p>
<blockquote><p>Patients with Anton&#8217;s syndrome… cannot count fingers or discriminate objects, shapes, or colors… Some patients with Anton&#8217;s syndrome cannot even correctly tell if the room lights are on or off. Despite being profoundly… blind, patients typically deny having any visual difficulty. They confabulate responses such that they guess how many fingers the examiner is holding up or whether the lights are on or off. When confronted with their errors, they often make excused such as “The lights are too dim” or “I don&#8217;t have my glasses.”</p></blockquote>
<p>Or, consider a patient with <em>inverse</em> Anton&#8217;s syndrome:<a class="see-footnote" id="footnoteref322_onu4s0c" title="Hartmann et al. (1991).  " href="#footnote322_onu4s0c">322</a></p>
<blockquote><p>Although denying visual perception, [the patient] correctly named objects, colors, and famous faces, recognized facial emotions, and read various types of single words with greater than 50% accuracy when presented in the upper right visual field. Upon confrontation regarding his apparent visual abilities, the patient continued to deny visual perceptual awareness… [and] alternatively replied “I feel it,” “I feel like something is there,” “it clicks,” or “I feel it in my mind.”</p></blockquote>
<p>A patient can even be wrong about the most profound disability of all, death. From a case report on a patient called “JK”:<a class="see-footnote" id="footnoteref323_egfwadz" title="Young &amp; Leafhead (1996).  " href="#footnote323_egfwadz">323</a></p>
<blockquote><p>When she was most ill, JK claimed that she was dead. She also denied the existence of her mother, and believed that her (JK&#8217;s) body was going to explode. On one occasion JK described herself as consisting of mere fresh air and on another she said that she was “just a voice and if that goes I won&#8217;t be anything … if my voice goes I will be lost and I won&#8217;t know where I have gone”…</p>
<p>…[JK felt] guilty about having claimed social security benefits (to which she was fully entitled) on the grounds that she was dead while she was claiming…</p>
<p>…Her subjective experience of eating was similarly unreal; she felt as though she were “just placing food in the atmosphere”, rather than into her body…</p>
<p>We wanted to know whether the fact that JK had thoughts and feelings (however abnormal) struck her as being inconsistent with her belief that she was dead. We therefore asked her, during the period when she claimed to be dead, whether she could feel her heart beat, whether she could feel hot or cold, and whether she could feel when her bladder was full. She said she could. We suggested that such feelings surely represented evidence that she was not dead, but alive. JK said that since she had such feelings even though she was dead, they clearly did not represent evidence that she was alive. She said she recognised that this was a difficult concept for us to grasp and one which was equally difficult for her to explain, partly because the experience was unique to her and partly because she did not fully understand it herself.</p>
<p>We then asked JK whether she thought we would be able to feel our hearts beat, to feel hunger, and so on if we were dead. JK said that we wouldn&#8217;t, and repeated that this experience was unique to her; no one else had ever experienced what she was going through. However, she eventually agreed that it “might be possible”. Hence, JK recognised the logical inconsistency between someone&#8217;s being dead and yet remaining able to feel and think, but thought that she was none the less in this state.</p></blockquote>
<p>What is it like to be a patient with one of these illusions?<a class="see-footnote" id="footnoteref324_c1m99m9" title="Some of &quot;illusions&quot; of this sort might be more accurately called &quot;delusions,&quot; but I won't bother to make this distinction here. See e.g. Blackmore (2016).  " href="#footnote324_c1m99m9">324</a> In at least some such cases, a plausible interpretation seems to be that when the patient introspects about whether they are blind or dead, they seem to just <em>know</em>, “directly,” that they are dead, or blind, or not blind, and this feeling of “knowing” trumps the evidence presented to them by the examiner. (Perhaps if these patients had been trained as philosophers, they would claim they had “direct acquaintance”<a class="see-footnote" id="footnoteref325_aiukp2t" title="See e.g. Chalmers (1996), pp. 193-195.  " href="#footnote325_aiukp2t">325</a> with the fact that they were dead, or blind, or not blind.)</p>
<p>In any case, it&#8217;s clear that we can be subject to profound illusions about the external world, about ourselves, and about our own capacities. But can we be wrong about <em>our own subjective experience</em>? When perceiving the tabletops illusion above, we are wrong about the shape of the tabletops, but presumably we are right about <em>what our subjective experience of the tabletops is like</em> — right? Isn&#8217;t it the case that “where consciousness is concerned, the existence of the appearance <em>is</em> the reality”?<a class="see-footnote" id="footnoteref326_q407huz" title="Searle (1997), p. 112. Italics modified.  " href="#footnote326_q407huz">326</a></p>
<p>In fact, I think we are <em>very often</em> wrong about our own subjective experiences.<a class="see-footnote" id="footnoteref327_1u3628r" title="For additional clarifications on this point, see Schwitzgebel (2007a), the two papers it links to (Schwitzgebel 2007b; Dennett 2007), and Schwitzgebel (2008), especially this passage from section x:  I sometimes hear the following objection: When we make claims about our phenomenology, we're making claims about how things appear to us, not about how anything actually is. The claims, thus divorced from reality, can't be false; and if they're true, they're true in a peculiar way that shields them from error. In looking at an illusion, for example, I may well be wrong if I say the top line is longer; but if I say it appears or seems to me that the top line is longer, I can't in the same way be wrong. The sincerity of the latter claim seemingly guarantees its truth. It's tempting, perhaps, to say this: If something appears to appear a certain way, necessarily it appears that way. Therefore, we can't misjudge appearances, which is to say, phenomenology.  This reasoning rests on an equivocation between what we might call an epistemic and a phenomenal sense of &quot;appears&quot; (or, alternatively, &quot;seems&quot;). Sometimes, we use the phrase &quot;it appears to me that such-andsuch&quot; simply to express a judgment — a hedged judgment, of a sort — with no phenomenological implications whatsoever. If I say, &quot;It appears to me that the Democrats are headed for defeat,&quot; ordinarily I'm merely expressing my opinion about the Democrats' prospects. I'm not attributing to myself any particular phenomenology. I'm not claiming to have an image, say, of defeated Democrats, or to hear the word &quot;defeat&quot; ringing in my head. In contrast, if I'm looking at an illusion in a vision science textbook, and I say that the top line &quot;appears&quot; longer, I'm not expressing any sort of judgment about the line. I know perfectly well it's not longer. I'm making instead, it seems, a claim about my phenomenology, about my visual experience.  Epistemic uses of &quot;appears&quot; might under certain circumstances be infallible in the sense of the previous section. Maybe, if we assume that they're sincere and normally caused, their truth conditions will be a subset of their existence conditions — though a story needs to be told here. But phenomenal uses of &quot;appears&quot; are by no means similarly infallible. This is evident from the case of weak, nonobvious, or merely purported illusions. Confronted with a perfect cross and told there may be a &quot;horizontal-vertical illusion&quot; in the lengths of the lines, one can feel uncertainty, change one's mind, and make what at least plausibly seem to be errors about whether one line &quot;looks&quot; or &quot;appears&quot; or &quot;seems&quot; in one's visual phenomenology to be longer than another. You might, for example, fail to notice — or worry that you may be failing to notice — a real illusion in your experience of the relative lengths of the lines; or you might (perhaps under the influence of a theory) erroneously report a minor illusion that actually isn't part of your visual experience at all. Why not?  Block (2007b) argues against the idea that we can be wrong about our own subjective experiences, in his discussion of what he calls &quot;hyper-illusions.&quot;  " href="#footnote327_1u3628r">327</a> To get a sense for why I think so, try this experiment: close your eyes, picture in as much detail as you can the front of your house or apartment building from across the street, and ask a friend to read you these questions one at a time (pausing for several seconds between each question, so you have a chance to think about the answer):<a class="see-footnote" id="footnoteref328_2bg53s4" title="Quoted from Schwitzgebel (2011), ch. 3.  " href="#footnote328_2bg53s4">328</a></p>
<blockquote><p>How much of the scene can you vividly visualize at once? Can you keep the image of the chimney vividly in mind at the same time that you vividly imagine your front door, or how does the image of the chimney fade as you begin to think about the door? How much detail does your image have? How stable is it? If you can&#8217;t visually imagine the entire front of your house in rich detail all at once, what happens to the aspects of the image that are relatively less detailed? If the chimney is still experienced as part of the imagery when your imagemaking energies are focused on the front door, how exactly is it experienced? Does it have determinate shape, determinate color? In general, do the objects in your image have color before you think to assign color to them, or do some of the colors remain indeterminate, at least for a while…? If there is indeterminacy of color, how is that indeterminacy experienced? As gray? Does your visual image have depth in the same way that your sensory experienced does… or is your imagery somehow flatter…? …Do you experience the image as located somewhere in egocentric space - inside your head, or before your eyes, or in front of your forehead - or does it make no sense to attempt to assign it a position in this way?</p></blockquote>
<p>When questioned in this way, I suspect many people will quickly become quite uncertain about the subjective character of their own conscious experience of imagining the front of their house or apartment building.</p>
<p>Here is another exercise from <a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">Dennett (1991)</a>, ch. 4:</p>
<blockquote><p>…would you be prepared to bet on the following propositions? (I made up at least one of them.)</p>
<ol><li>You can experience a patch that is red and green all over at the same time — a patch that is both colors (not mixed) at once.</li>
<li>If you look at a yellow circle on a blue background (in good light), and the luminance or brightness of the yellow and blue are then adjusted to be equal, the boundary between the yellow and blue disappears.</li>
<li>There is a sound, sometimes called the auditory barber pole, which seems to keep on rising in pitch forever, without ever getting any higher.</li>
<li>There is an herb an overdose of which makes you incapable of understanding spoken sentences in your native language. Until the effect wears off, your hearing is unimpaired, with no fuzziness or added noise, but the words you hear sound to you like an entirely foreign language, even though you somehow know they aren&#8217;t.</li>
<li>if you are blindfolded, and a vibrator is applied to a point on your ann while you touch your nose, you will feel your nose growing like Pinocchio&#8217;s; if the vibrator is moved to another point, you will then have the eerie feeling of pushing your nose inside out, with your index finger coming to rest somewhere inside your skull.</li>
</ol></blockquote>
<p>Do you know which one Dennett fabricated? I reveal the answer in a footnote.<a class="see-footnote" id="footnoteref329_1722emc" title="Dennett fabricated #4.  Dennett lists sources for these phenomena in a footnote, which I've reformatted below to match the citation style of this report and to include links:  For the red and green patch, see Crane &amp; Piantanida (1983) and Hardin (1988); for the disappearing color boundary… see Spillmann and Werner (1990); for the auditory barber pole, see Shepard (1964); for the Pinocchio effect, see Lackner (1988)…  " href="#footnote329_1722emc">329</a></p>
<p>To try additional exercises of this sort, see <a href="https://mitpress.mit.edu/books/perplexities-consciousness">Schwitzgebel (2011)</a>.<a class="see-footnote" id="footnoteref330_tdwb8w8" title="For extensive debate on these issues, see also Hurlburt &amp; Schwitzgebel (2007) and volume 18, issue 1 of the Journal of Consciousness Studies.  " href="#footnote330_tdwb8w8">330</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/76">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="IllusionistRealistDisagree">Where do the illusionist and the realist disagree?</h4>
<p>In sum, the illusions to which we are susceptible are deep and pervasive.<a class="see-footnote" id="footnoteref331_fgx72p3" title="For discussion of additional visual illusions, see e.g. Shapiro &amp; Todorovic (2017) and Michael Bach's website on optical illusions and visual phenomena. On auditory illusions, see the website for Diana Deutsch's Audio Illusions.  " href="#footnote331_fgx72p3">331</a> But even given all this, what could it mean for the realist&#8217;s notion of the “what it&#8217;s like”-ness of conscious experience to be an illusion? What is it, exactly, that the realist and the weak illusionist both think exists, but which the strong illusionist does not?<a class="see-footnote" id="footnoteref332_rnjjoqe" title="Perhaps there are as many answers to this question as there are pairs of realists and illusionists.  " href="#footnote332_rnjjoqe">332</a></p>
<p>This question is difficult to answer clearly because, after all, the realist (and perhaps the weak illusionist) is claiming that (their notion of) “phenomenal property” is <em>sui generis</em>, and rather unlike anything else we understand.</p>
<p>More centrally, it is not clear what the realist and the weak illusionist think is left to explain once the strong illusionist has explained (or explained away) the apparent privacy, ineffability, subjectivity, and intrinsicness of qualia. <a href="http://www.sciencedirect.com/science/article/pii/S1053810011001176">Frankish (2012a)</a><a class="see-footnote" id="footnoteref333_ga887yz" title="See also Frankish (2012b).  " href="#footnote333_ga887yz">333</a> makes this point by distinguishing three (soda-inspired) notions of “qualia” that have been discussed by philosophers:</p>
<ul><li><em>Classic qualia</em>: “Introspective qualitative properties of experience that are intrinsic (see footnote<a class="see-footnote" id="footnoteref334_ba2wemy" title="Among the properties of &quot;classic&quot; qualia, the &quot;intrinsic character&quot; property is typically thought to be the most directly incompatible with physicalism. It might also be the most difficult to explain. Because of this, I'll elaborate here what is usually meant by the &quot;intrinsic character&quot; of (classic) qualia, even though I do not take the time to elaborate the meaning of other properties of classic qualia.  Harman (1990) puts it this way:  …when you attend to a pain in your leg or to your experience of the redness of an apple, you are aware of an intrinsic quality of your experience, where an intrinsic quality is a quality something has in itself, apart from its relations to other things. This quality of experience cannot be captured in a functional definition, since such a definition is concerned entirely with relations, relations between mental states and perceptual input, relations among mental states, and relations between mental states and behavioral output.  Or, here is Weisberg (2014), pp. 54-56:  Theories in physics deal in what we can call “relational” information only. Relational information tells us how things are lawfully connected, how changing one thing changes another according to the physical laws posited by the theory. And then, in turn, we can define physical things like electrons, protons, or quarks in terms of how they fit into this framework of causal connections… on this view to be an electron is just to be the kind of thing that fits into this pattern of causal relations – that plays this “causal role.” We learn with extreme precision from physical theory just what this special electron “role” is. But we don’t learn, so it seems, just what the thing playing the role is, on its own. It’s like learning all we can about what it is to be a goalie: a goalie is the person who defends the goal and can use his or her hands to stop the ball, who can’t touch the ball when it’s passed back by his or her own team, who usually kicks the goal kicks, and so on, without learning who is playing goalie for our team. We know all about the goalie role without learning that its Hope Solo or Tim Howard. We learn the role without learning about the role-filler, beyond that he or she plays that role. But there may be other things true of the role-filler beyond what he or she does in the role.  …  Another way to put this is that all physics tells us about is what physical stuff is disposed to do: what physical law dictates will occur, given certain antecedent conditions. In philosophers’ terms, physics only tells us about the dispositional properties of physical stuff, not the categorical base of these dispositions. A paradigm dispositional property is fragility. Something is fragile if it is disposed to break in certain situations. When we learn that a window is fragile, we learn that it will likely break if hit even softly. But there is a further question we can ask about the window or any fragile object. What is it about the makeup of the window that gives it this dispositional property? The answer is that the window has a certain molecular structure with bonds that are relatively weak in key areas. This molecular structure is what we call the categorical base for the window’s fragility. Returning to physics, at its most basic level, all we get is the dispositional properties, not the categorical base. We learn that electrons (or quarks or “hadrons”) are things disposed to do such-and-such, to be repelled by certain charges, to move in certain ways. We don’t learn what’s “underneath” making this happen.  For more on the idea of intrinsicality, see e.g. Marshall (2014, 2016).  For an accessible explanation of what it means for classic qualia to be &quot;subjective&quot; and &quot;ineffable,&quot; see e.g. ch. 1 of Frankish (2005).  " href="#footnote334_ba2wemy">334</a>), ineffable, and subjective.” (Classic qualia are widely thought to be incompatible with physicalism.)</li>
<li><em>Diet qualia</em>: “The phenomenal characters (subjective feels, what-it-is-likenesses, etc.) of experience.”</li>
<li><em>Zero qualia</em>: “The properties of experiences that dispose us to judge that experiences have introspectable qualitative properties that are intrinsic, ineffable, and subjective.”</li>
</ul><p>Which of these notions of qualia should be the core “explanandum” (thing to be explained) of consciousness? In my readings, the most popular option these days seems to be diet qualia, since assuming classic qualia as the explanandum seems rather presumptuous, and would seem to beg the question against the physicalist. Diet qualia, in contrast, appears to be a theory-neutral explanandum of consciousness. One can take diet qualia to be the core explanadum of consciounsess, and then argue that the best explanation of that explanandum is that classic qualia exist, or argue for a physicalist account of diet qualia, or argue something else.</p>
<p>Frankish, however, argues that the notion of diet qualia, when we look at it carefully, turns out to have no distinctive content beyond that of zero qualia. Frankish asks: if an experience could have zero qualia without having diet qualia,</p>
<blockquote><p>…what exactly would be missing? Well, a phenomenal character, a subjective feel, a what-it-is-likeness. But what is <em>that</em> supposed to be, if not some intrinsic, ineffable, and subjective qualitative property? This is the crux of the matter. I can see how the properties that dispose us to judge that our experiences have classic qualia might not be intrinsic, ineffable, and subjective, but I find it much harder to understand how a phenomenal character itself might not be. What could a phenomenal character be, if not a classic quale? How could a phenomenal residue remain when instrinsicality, ineffability, and subjectivity have been stripped away?</p>
<p>The worry can be put another way. There are competing pressures on the concept of diet qualia. On the one hand, it needs to be weak enough to distinguish it from that of classic qualia, so that functional or representational theories of consciousness are not ruled out a priori. On the other hand, it needs to be strong enough to distinguish it from the concept of zero qualia, so that belief in diet qualia counts as realism about phenomenal consciousness. My suggestion is that there is no coherent concept that fits this bill. In short, I understand what classic qualia are, and I understand what zero qualia are, but I do not understand what diet qualia are; I suspect the concept has no distinctive content.</p></blockquote>
<p>So what might the “something it&#8217;s like”-ness of diet qualia be, if it is more than zero qualia and less than classic qualia? Frankish surveys several possible answers, and finds all of them wanting. He concludes that, as far as he can tell, “there is no viable ‘diet’ notion of qualia which is stronger than that of zero qualia yet weaker than that of classic qualia and which picks out a theory-neutral explanandum [of consciousness].”</p>
<p>Frankish then complains about “the diet/zero shuffle”:</p>
<blockquote><p>I have argued that the notion of diet qualia has no distinctive content. If there are no classic qualia, then all that needs explaining (as far as ‘what-it-is-likeness’ goes) are zero qualia. This is not a popular view, but it is one that is tacitly reflected in the practice of philosophers who offer reductive accounts of consciousness. Typically, these accounts involve a three-stage process. First, diet qualia are introduced as a neutral explanandum. Second, diet qualia are identified with some natural, usually relational, property of experience, such as possession of a form of non-conceptual intentional content or availability to higher-order thinking. Third, this identification is defended by arguing that we would be disposed to judge that experiences with this property have intrinsic, ineffable, and subjective qualitative properties. In the end, diet qualia are not explained at all but simply identified with some other feature, and what actually get explained are zero qualia. I shall call this <em>the diet/zero shuffle</em>.</p></blockquote>
<p>If Frankish is right, the upshot is that there is no coherent “what it&#8217;s like”-ness that needs explaining above and beyond zero qualia, unless non-physicalists can argue convincingly for the existence of classic qualia. And if zero qualia are all that need to be explained, then the strong illusionist still has lots of work to do, but it looks like much less mysterious kind of work than one might have previously thought, and perhaps fairly similar to the kind of work that remains to explain human memory, attentional systems, and the types of cognitive illusions described above.</p>
<p>Now, supposing (strong) illusionism is right, what are the implications for the distribution question, and for my intuitions about which properties of consciousness are important for moral patienthood? I discussed the former question <a href="#IllusionistTheories">here</a>, and I discuss the latter question next.</p>
<p><span style="float: right;">[<a href="/node/858/edit/77">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="IllusionismPatienthood">Illusionism and moral patienthood</h4>
<p>Next, what are the implications of illusionism for my intuitions about moral patienthood? In one sense, there might not be any.<a class="see-footnote" id="footnoteref335_c9wieie" title="Though as with all the intuitions reported here, carrying out the &quot;extreme effort&quot; version of my process for making moral judgments could result in a different judgment.  " href="#footnote335_c9wieie">335</a> After all, my intuitions about (e.g.) the badness of conscious pain and the goodness of conscious pleasure were never dependent on the “reality” of specific features of consciousness that the illusionist thinks are illusory. Rather, my moral intuitions work more like the example I gave <a href="#Defined">earlier</a>: I sprain my ankle while playing soccer, don&#8217;t notice it for 5 seconds, and then feel a “rush of pain” suddenly “flood” my conscious experience, and I think “Gosh, well, <em>whatever this is</em>, I sure hope nothing like it happens to fish!” And then I reflect on what was happening <em>prior</em> to my conscious experience of the pain, and I think “But if <em>that</em> is all that happens when a fish is physically injured, then I&#8217;m not sure I care.” And so on. (For more on how my moral intuitions work, see <a href="#AppendixA">Appendix A</a>.)</p>
<p>But <em>if</em> we had a better-developed understanding of how consciousness works, this could of course have important implications for my intuitions about moral patienthood. Perhaps a satisfactory illusionist theory of consciousness developed 20 years from now will show that some of the core illusions of human consciousness are irrelevant to what I morally care about, such that animals without those particular illusions of <em>human</em> consciousness still have a morally relevant sort of consciousness. This is, indeed, the sort of reasoning that leads me to assign a higher probability that various animal taxa will have “consciousness of a sort I morally care about” than “consciousness as defined by example <a href="#Defined">above</a>.” But getting further clarity about this must await more clarity about how consciousness works.</p>
<p>Personally, my hunch is that the cognitive algorithms which produce (e.g.) the illusion of an explanatory gap, and other “core” illusions of (human) consciousness, are going to be pretty important to what I find morally important about consciousness. In order words, my hunch is that if we could remove those particular illusion-producing algorithms from how a human mind works, then all “pain” might be to that person like the “first 5 seconds of unnoticed nociception” from my sprained-ankle example.<a class="see-footnote" id="footnoteref336_7y8ocgy" title=" And, to be clear: that person would also have greatly impoverished behavior, and wouldn't talk the way we do about qualia.  " href="#footnote336_7y8ocgy">336</a> But this is just a hunch, and my hunch could easily being wrong, and I could imagine ending up with a different hunch if I spent ~20 hrs trying to extract where my intuitions on this are coming from, and I could also imagine myself having different moral intuitions about the sprained-ankle example and other issues if I took the time to do something closer to my “<a href="#ExtremeEffort">extreme effort</a>” process.</p>
<p>Also, it could even be that a brain similar to mine except that it lacked <em>particular</em> illusions might actually be a “moral <em>super</em>patient,” i.e. a moral patient with especially high <a href="#AppendixZ7">moral weight</a>. For example, suppose a brain like mine was modified such that its introspective processes had much greater access to, and understanding of, the many other cognitive processes going on within the brain, such that it&#8217;s experiences seemed much less “ineffable.” Arguably, such a being would have especially morally weighty experiences.</p>
<p><span style="float: right;">[<a href="/node/858/edit/78">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="AppendixG">Appendix G. Consciousness and fuzziness</h3>
<p>In this appendix, I elaborate on my <a href="#fuzzy">earlier comments</a> about the “fuzziness” of consciousness.</p>
<p><span style="float: right;">[<a href="/node/858/edit/79">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="FuzzinessMoral">Fuzziness and moral patienthood</h4>
<p>First, how does a “fuzzy” view of consciousness interact with the question of moral patienthood?</p>
<p>The question of which beings should be considered moral patients by virtue of their phenomenal consciousness requires the collaboration of two kinds of reasoning:</p>
<ol><li><em>Scientific explanation</em>: How does consciousness work? How can it vary? How can we measure it?</li>
<li><em>Moral judgment</em>: Which kinds of processes — in this case, related to consciousness — are sufficient for moral patienthood?</li>
</ol><p>In theory, a completed scientific explanation of consciousness <em>could</em> reveal a relatively clear dividing line between the conscious and the not-conscious. The discovery of a clear dividing line could allow for relatively straightforward moral judgments about which beings are moral patients (via their phenomenal consciousness). But if we discover (as I expect we will) that there is <em>no</em> clear dividing line between the conscious and the not-conscious — i.e. that consicousness is “fuzzy” — this could make our consciousness-derived moral judgments quite difficult.<a class="see-footnote" id="footnoteref337_qx3yxzw" title="I borrow the framing of this section from Eliezer Yudkowsky's &quot;Rescuing the utility function&quot; (2016). An alternate framing might be to say that we don't know how to translate our value for something in our &quot;manifest image&quot; of the world into an appropriate value for something in the &quot;scientific image&quot; of the world (see Dennett 2017, ch. 4; Sellars 1962).  For an example toy formalization of the &quot;cross-ontology value translation&quot; problem discussed in this section, see de Blanc (2011). See also section 5 of Soares (2015), section 3.2 of Soares (2016), Pärnpuu (2016), and Yudkowsky's &quot;Ontology Identification&quot; (2016).  For additional accounts of how scientific concepts evolve along with scientific progress, see e.g. Thagard (1992, 1999, 2008), Grisdale (2010), Laureys (2005), Chalmers (2009), and Chang (2004, 2012).  Block (2007b) cites some further examples:  …as theory, driven by experiment, advances, important distinctions come to light among what appeared at first to be unified phenomena (see Block &amp; Dworkin 1974, on temperature; Churchland 1986; 1994; 2002, on life and fire).  " href="#footnote337_qx3yxzw">337</a> To illustrate this point, I&#8217;ll tell a story about two fictional pre-scientific groups of people, each united by a common “sacred” value.</p>
<p>The first group, the <em>water-lovers</em>, have a sacred value for <em>water</em>. They don&#8217;t know what water is made of or how it works, but they have positive and negative (or <em>probably negative</em>) examples of it. The clear liquid in lakes is water; the clear liquid that falls from the sky sometimes is water; the discolored liquid in a pond is probably water with extra stuff in it, but they&#8217;re not absolutely certain; the red liquid that comes out of a stab wound probably <em>isn&#8217;t</em> water, but it <em>might</em> be water with some impurities, like the water in a pond. The water-lovers also have some observations and ideas about how water seems to work. It can quench thirst; it slips and slides over the body; it can&#8217;t be walked on; if it gets very cold it turns to ice; it seems to eventually float away into the air if placed in a pot with a fire beneath it; if snow falls on one&#8217;s hand it seems to transform into water; etc.</p>
<p>Then, some scientists invent modern chemistry and discover that water is H<sub>2</sub>O, that it can be separated into hydrogen and oxygen via electrolysis, that it has particular boiling and melting points, that the red liquid that flows from a stab wound is partly water but partly other things, and so on. In this case, the water-lovers have little trouble translating their sacred value for “water” — defined in a pre-scientific ontology — into a value for certain things in the ontology of the real world discovered by science. The resolution to <em>this</em> “cross-ontology value translation” challenge is fairly straightforward: they value H<sub>2</sub>O.<a class="see-footnote" id="footnoteref338_tnkwobh" title="In reality, things are bit more complex than water = H2O, both scientifically and historically (Chang 2012), but for the purposes of this illustration please assume the simple view that water = H2O.  " href="#footnote338_tnkwobh">338</a></p>
<p>A few of the water-lovers conclude that what scientists have really shown is that water <em>doesn&#8217;t exist</em> — only H<sub>2</sub>O exists. They suffer an existential crisis and become nihilists. But most water-lovers carry on as before, assigning sacred value to water in lakes, rain that falls from the sky, and so on. A few of the group&#8217;s intellectuals devote themselves to the task of making judgments about edge cases, such as whether “heavy water” (<sup>2</sup>H<sub>2</sub>O) should be thought to have sacred value.</p>
<p>Another group of people at the time are the <em>life-lovers</em>, who have a sacred value for <em>life</em>.<a class="see-footnote" id="footnoteref339_to6xmq6" title="I'm not aware of any real-world water-lovers, but there do seem to be some real-world life-lovers. For example Albert Schweitzer (see Warren 1997, ch. 2) and Mautner (2009).  " href="#footnote339_to6xmq6">339</a> They don&#8217;t know how life works, but they have many positive and probably-negative examples of it, and they have some observations and ideas about how it seems to work. Humans are alive, up to the point where they stop responding even upon being dunked in water. Animals are also alive, up to the point where they stop responding even when poked with a knife. Plants are alive, but they move much more slowly than animals do. The distinction between living and non-living things is thought to be that living things are possessed by a “life force,” or perhaps by some kind of supernatural soul, which allows living things — unlike non-living things — to grow and reproduce and respond to the environment.</p>
<p>Then, some scientists invent modern biology. They discover that the ill-defined set of processes previously gestured at with terms like “life” is fully explained by the mechanistic activity of atoms and molecules, with no role for a separate life force or soul. This set of processes <a href="https://en.wikipedia.org/wiki/Life#Biology">includes</a> the development and maintenance of a cellular structure, homeostasis, metabolism, reproduction, growth, a wide variety of mechanisms for responding to stimuli, and more. A beginner&#8217;s introduction to how these processes commonly work is not captured by a simple chemical formula, but by <a href="http://www.garlandscience.com/product/isbn/9780815345732;jsessionid=AyaC5YIPc-wmJpt1qMFIVg__">an 800-page textbook</a>.</p>
<p>Moreover, the exact set of processes at work, and the parameters of those processes, vary widely across systems. Virtually all “living systems” depend on photosynthesis-derived materials, but <a href="http://science.sciencemag.org/content/314/5798/479">some don&#8217;t</a>. Most survive in a relatively narrow range of temperatures, chemical environments, radiation levels, and gravity strengths, but many thrive in <a href="https://en.wikipedia.org/wiki/Extremophile">more extreme environments</a>. Most actively regulate internal variables (e.g. temperature) within a relatively narrow range and die if they fail to do so, but some can instead enter a potentially years-long state of non-activity, from which they can later be revived.<a class="see-footnote" id="footnoteref340_r1ano84" title="Clegg (2001); Kitano (2007).  " href="#footnote340_r1ano84">340</a> Most age, but <a href="https://en.wikipedia.org/wiki/Biological_immortality">some don&#8217;t</a>. They range from <a href="https://en.wikipedia.org/wiki/Pelagibacter_ubique">~0.5 µm</a> to <a href="http://www.bbc.com/earth/story/20141114-the-biggest-organism-in-the-world">2.8 km</a> in length, and from <a href="http://www.sciencedirect.com/science/article/pii/B9780126906479500089">mere weeks</a> to <a href="https://en.wikipedia.org/wiki/List_of_longest-living_organisms">thousands of years</a> in life span.<a class="see-footnote" id="footnoteref341_qeq93jw" title="Of course if you count the reproduction of single-celled organisms by cell division as defining a &quot;life span,&quot; then many single-celled organisms live for mere hours.  " href="#footnote341_qeq93jw">341</a> There are many edge cases, such as viruses, parasites, and bacterial spores.<a class="see-footnote" id="footnoteref342_p0h0nsw" title="Schulze-Makuch &amp; Irwin (2008), pp. 10-12:  By the traditional definition viruses are not considered living entities because they cannot reproduce and grow by themselves and do not metabolize. Nevertheless, they possess a genetic code that enables them to reproduce and direct a limited amount of metabolism inside another living cell. They thus fulfill the traditional criteria only part of the time and under special circumstances… Since viruses presumably evolved from bacteria that clearly are alive, do they represent a case in which a living entity has been transformed to a non-living state by natural selection? Or, alternatively, if viruses were indeed the precursors of the three domains of life (Archaea, Bacteria, and Eukarya) as recently suggested…, where would we draw the line between life and non-life? If we accept the proposition that viruses are not alive, how would we consider parasitic organisms or bacterial spores? Parasites cannot grow by themselves either and spores remain in dormant stages with no dynamic biological attributes until they become active under favorable environmental conditions. Thus, if we consider parasites or bacterial spores to be alive, the logical consequence would be to consider viruses alive as well.  Consider also the multi-part virus reported in Ladner et al. (2016).  " href="#footnote342_p0h0nsw">342</a></p>
<p>All this leaves the life-lovers with quite a quandary. How should they preserve their sacred value for “life” — defined in a pre-scientific ontology — as a value for some but not all of these innumerable varieties of life-related processes discovered by science? How should they resolve <em>their</em> cross-ontology value translation problem?</p>
<p>Some conclude they never should have cared about living things in the first place. Others conclude that any carbon-based thing which grows and reproduces should have sacred value — but one consequence of this is that they assign sacred value to certain kinds of crystals,<a class="see-footnote" id="footnoteref343_3t4m9z0" title="Schulze-Makuch &amp; Irwin (2008), pp. 8-9:  …just as cells grow in favorable environments with nutrients available, inorganic crystals can grow so long as ion sources and favorable surroundings are provided. Furthermore, just as the development of living organisms follows a regulated trajectory, so does the process of local surface reversibility regulate the course of silicate or metal oxide crystals that grow in aqueous solutions (Cairns-Smith 1982).  …The visible consequence of reproduction in living organisms is the multiplication of individuals into offspring of like form and function. Mineral crystals do not reproduce in a biological sense, but when they reach a certain size they break apart along their cleavage planes. This is clearly a form of multiplication. The consequence of biological reproduction is also the transmission of information. Biological information is stored in the one-dimensional form of a linear code (DNA, RNA), that, at the functional level, is translated into the 3-dimensional structure of proteins. Prior to multiplication, the one-dimensional genetic code is copied, and complete sets of the code are transmitted to each of the two daughter cells that originate from binary fission. An analogous process occurs in minerals, where information may be stored in the two-dimensional lattice of a crystal plane. If a mineral has a strong preference for cleaving across the direction of growth and in the plane in which the information is held (Cairns-Smith 1982), the information can be reproduced.  Note that I have not checked Schulze-Makuch &amp; Irwin's account for accuracy, as the exact details don't matter much to my story.  " href="#footnote343_3t4m9z0">343</a> and some life-lovers find this a bit nutty. Others assign sacred value only to systems which satisfy a longer list of life-related criteria that includes “autonomous reproduction,” but this excludes the <a href="https://en.wikipedia.org/wiki/Vespula_austriaca">cuckoo yellowjacket wasp</a> and <a href="https://en.wikipedia.org/wiki/Obligate_parasite">many other animals</a>, which again seems strange to many other life-lovers. The life-lovers experience a series of schisms over which life-related processes have sacred value.</p>
<p>Like many physicalists,<a class="see-footnote" id="footnoteref344_lg6486h" title="Antony (2008) argues in favor of a sharp dividing line (of a certain sort) between the conscious and the non-conscious, but he thinks most theorists disagree with him:  Not all theorists agree [with me] that there can be no borderline conscious states or creatures. In fact most think there can be, and are… In fact, most philosophers working on consciousness think borderline conscious states and creatures must be possible. That is because most philosophers working on consciousness reduce consciousness to complex physical (e.g., neurophysiological) or functional properties, and concepts for such properties are vague.  Papineau (1993) presents this &quot;fuzzy&quot; view of consciousness this way (pp. 123-126):  …any physicalist account of consciousness is likely to make consciousness a vague property. In the next section I shall argue that questions of consciousness may not only be vague, but quite arbitrary, in application to beings unlike ourselves…  The point about vagueness is suggested by the analogy with life. If life is simply a matter of a certain kind of physical complexity — the kind of complexity that fosters survival and reproduction, as I put it above — then it would seem to follow that there is no sharp line between life and non-life. For there is nothing in the idea of such physical complexity to give us a definite cut-off point beyond which you have enough complexity to qualify as alive. Rather as with baldness, or being a pile of sand, we should expect there to be some clear cases of life, and some clear cases of non-life, but a grey area in between where there there is no fact of the matter. And of course this is just what we do find. While there is no doubt that trees are alive and stones are not, there are borderline cases in between, like viruses, or certain kinds of simpler self-replicating molecules, where our physicalist account of life simply leaves it indeterminate whether these are living beings or not.  But now, if consciousness is like life, we should expect a similar point to apply to consciousness. For any physicalist account of consciousness is likely to make consciousness depend similarly on the possession of some kind of structural complexity — the kind of complexity which qualifies you as having self-representing states, say, or short-term memories. Yet any kind of such complexity is likely to come in degrees, with no clear cut-off point beyond which you definitely qualify as conscious, and before which you don't. So we should expect there to be borderline cases — such as the states of certain kinds of insects, say, or fishes, or cybernetic devices — where our physicalist account simply leaves it indeterminate whether these are conscious states or not.  …I think that the physicalist approach to consciousness is correct. So I reject the intuition that there is a sharp line between conscious and non-conscious states…  It would be a mistake to conclude from this, however, that consciousness is unimportant or unreal. Any number of genuine and important properties are vague. Consider the difference between being elastic or inelastic, or between being young or old, or, for that matter, between being alive and not being alive. All these distinctions will admit indeterminate borderline cases. But all of them involve perfectly serious properties, properties which enter into significant generalizations, are explanatorily important, and so on.  Dennett (1995) concurs:  The very idea of there being a dividing line between those creatures &quot;it is like something to be&quot; and those that are mere &quot;automata&quot; begins to look like an artifact of our traditional presumptions… Consciousness, I claim, even in the case we understand best — our own — is not an all-or-nothing, on-or-off phenomenon. If this is right, then consciousness is not the sort of phenomenon it is assumed to be by most of the participants in the debates over animal consciousness. Wondering whether it is &quot;probable&quot; that all mammals have it thus begins to look like wondering whether or not any birds are wise or reptiles have gumption: a case of overworking a term from folk psychology that has losts its utility along with its hard edges.  Also see Tye (2000), pp. 180-181:  Some philosophers will no doubt respond that the boundary between the creatures that are phenomenally conscious and those that are zombies cannot be blurry. Conscious experience or feeling is either present or it isn't… [But] it seems to me that we can make sense of the idea of a borderline experience. Suppose you are participating in a psychological experiment and you are listening to quieter and quieter sounds through headphones. As the process continues, a point may come at which you are unsure whether you hear anything at all. Now it could be that there is a still a fact of the matter here (as on the dimming light model); but, equally, it could be that whether you still hear anything is objectively indeterminate. So, it could be that there is no fact of the matter about whether there is anything it is like for you to be in the state you are in at that time. In short, it could be that you are undergoing a borderline experience.  See also Unger (1988), ch. 7 of Papineau (2002), Papineau (2003), the sources cited in footnote 6 of Antony (2008), Simon (2016), and Fakezas &amp; Overgaard (2016).  " href="#footnote344_lg6486h">344</a> I expect the scientific explanation of phenomenal consciousness to look less like the scientific explanation of water and more like the scientific explanation of life. That is, I don&#8217;t expect there to be a clear dividing line between the conscious and the non-conscious. And as scientists continue to decompose consciousness into its component processes and reveal their great variety, I expect people to come to radically different moral judgments about which kinds of consciousness-related processes are moral patients and which are not. I suspect this will be true even for people who started out with very similar values (as defined in a first-person ontology, or as defined in the inchoate scientific ontologies available to us in 2016).</p>
<p>To be clear: I&#8217;m not <em>just</em> saying that I expect there to be a wide variety of conscious experiences. That much is already obvious: consider the variety of subjective experiences and consciousness-related mechanisms revealed by psychedelic experiences, mystical experiences, hallucinations, sensory substitution, synesthesia, akinetic mutism, lucid dreams, out of body experiences, absence seizures, hypnosis, sensory agnosias, pain asymbolia, delirium, split-brain patients, fused-cranium conjoined twins who can experience their co-twin&#8217;s sensations, and other phenomena (see <a href="#AppendixZ2">Appendix Z.2</a>). These phenomena reveal dimensions of human consciousness that can vary from person to person or from moment to moment, and which we might morally value independently of each other. But such issues, taken as questions about potential <em>dimensions</em> of moral concern, are not the focus of this report. </p>
<p>Instead, what I&#8217;m saying is that I don&#8217;t expect there to be a clear dividing line between things that have <em>no conscious experience at all</em> and things that have <em>any conscious experience whatsoever</em>. This is what I mean when I say that consciousness is “fuzzy.”<a class="see-footnote" id="footnoteref345_m4a59r3" title="In this report, I rely most on my expectation that consciousness is fuzzy between individuals, in the sense the cognitive architectures of a set of individuals (say, across different species) might differ in a variety of ways, such that it is unclear which individuals we should consider to be &quot;conscious&quot; or &quot;not conscious&quot; (when they are reasonably developed, awake, etc.).  But I also suspect consciousness will turn out to be &quot;fuzzy&quot; within individuals, such that that if we considered a large set of time slices of an individual's cognitive architecture, we would again struggle (in some cases) to decide which states were &quot;conscious&quot; or &quot;not conscious,&quot; even assuming we had a completed theory of consciousness and perfect information about each time slice. This view is similar to Dennett's view that it is often indeterminate whether a given cognitive process was &quot;conscious&quot; or not (Dennett 1991, chs 5-6; Dennett 2005, ch. 4), and seems to imply a rejection of what Dennett calls the &quot;Cartesian theater.&quot;  Despite this, I do not avoid talking about cognitive processes being &quot;conscious&quot; or &quot;not conscious,&quot; or &quot;entering consciousness&quot; or &quot;exiting consciousness,&quot; just as I do not avoid talking about &quot;which beings are conscious&quot; or &quot;unconscious animals.&quot; I continue to use the common and arguably naive phrasings simply because it is very difficult to communicate about consciousness without them. Perhaps with practice, or when we understand consciousness better than we do now, I will find it easier to write about consciousness in a more precise way that explicitly acknowledges the fuzziness of the concept.  " href="#footnote345_m4a59r3">345</a></p>
<p>I don&#8217;t have a knock-down argument for the fuzzy view, but consider: does it seem likely there was a single gene mutation in phylogeny such that earlier creatures had no conscious experience at all, while carriers of the mutation <em>do</em> have some conscious experience? Does it seem likely there is some moment in the development of a human fetus or infant before which it has no conscious experience at all, and after which it <em>does</em> have some conscious experience? Is there a clear dividing line between what is and isn&#8217;t alive, or between software that does and doesn&#8217;t implement some form of “attention,” “memory,” or “self-modeling”? Is “consciousness” likely to be as simple as electrons, crystals, and water, or is it more likely to be a more complex set of interacting processes like “life” or “vision” or “language use” or “face-recognition,” which even in fairly “minimal” form can vary along many different dimensions such that there is no obvious answer as to whether some things should count as belonging to one of these classes or not, except by convention? (I continue this line of thinking <a href="FuzzinessDarwin">below</a>.) </p>
<p>In any case, one consequence of holding a fuzzy view of consciousness is that it can be hard to give a meaningful response to questions like “How likely do you think it is that chickens / fishes / fruit flies are conscious?” Or as <a href="http://www.jstor.org/stable/40971115">Dennett (1995)</a> puts it,</p>
<blockquote><p>Wondering whether it is “probable” that all mammals have [consciousness] thus begins to look like wondering whether or not any birds are <em>wise</em> or reptiles have <em>gumption</em>: a case of overworking a term from folk psychology that has [lost] its utility along with its hard edges.</p></blockquote>
<p>So, how can I proceed? I considered multiple options,<a class="see-footnote" id="footnoteref346_13of3xf" title="The other two major options I considered were:   Rather than investigating the likely distribution of &quot;consciousness,&quot; I could have investigated the likely distribution of better-characterized phenomena instead, such as neural nociception, goal-directed behavior, bodily self-awareness, long-term memory, or some collection of such attributes. But, since I don't whether any collection of these things is sufficient for phenomenal consciousness of any sort, I might not have learned much, from such an investigation, about which beings are conscious, or about which beings I would judge to be moral patients due to their consciousness. I could have characterized in some detail the varieties of &quot;consciousness&quot; I do and don't intuitively morally value, and then investigated the likely distribution of those roughly-sketched varieties of consciousness. But at that level of detail, my moral intuitions might have been fairly uncommon, and thus my findings might not have been of much value to others, including key decision-makers at the Open Philanthropy Project. Moreover, since the thing I (most confidently) intuitively value is still &quot;subjective experience&quot; of a certain sort, and since I don't know which physical processes are sufficient for subjective experience of that sort, it would have remained difficult to know what I was looking for in the animal kingdom and other taxa, even after using my moral intuitions to identify the types of &quot;consciousness&quot; I'd be looking for. Also, extracting evidence about the distribution of those types of consciousness from the relevant scientific literature would have required extra work, since there is no literature explicitly investigating &quot;evidence of types of consciousness Luke Muehlhauser intuitively morally values,&quot; but there are substantial literatures explicitly investigating the likely distribution of phenomenal consciousness (defined roughly as above).   " href="#footnote346_13of3xf">346</a> and in the end I decided to take the following approach for this report: I temporarily set my moral judgments aside, and investigate the likely distribution of “consciousness” (as defined <a href="#Defined">above</a>), while acknowledging the extreme fuzziness of the concept. Then, at the end, I bring my moral judgments back into play, and explore what my empirical findings might imply given my moral judgments.</p>
<p>If you&#8217;re interested, I explain a few of my moral intuitions in <a href="#AppendixA">Appendix A</a>. I suspect these intuitions affect this report substantially, because they probably affect my intuitions about which beings are “conscious,” even when I <em>try</em> to pursue that investigation with reference to consciousness as defined <a href="#Defined">above</a> rather than with reference to “types of consciousness I intuitively morally care about.”<a class="see-footnote" id="footnoteref347_ff29zuk" title="I suspect this is true for other writers on consciousness as well, and in many cases I think I would understand their reasoning better if I knew more about their moral intuitions, even if their reasoning doesn't explicitly appeal to their moral intuitions.  " href="#footnote347_ff29zuk">347</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/80">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="FuzzinessDarwin">Fuzziness and Darwin</h4>
<p>Here, I&#8217;d like to say a bit more about why I expect consciousness to be fuzzy.</p>
<p>In part, this is because I think we should expect the vast majority of biological concepts — including concepts defined in terms of biological cognition (as consciousness is defined <a href="#Defined">above</a>, even if it can also be applied to non-biological systems) — to be at least somewhat “fuzzy.”</p>
<p>One key reason for this, I think, is Darwin. <a href="https://www.cambridge.org/core/books/how-biology-shapes-philosophy/darwin-and-the-overdue-demise-of-essentialism/18DE8618B3C365F458C7A0D423255A39">Dennett (2016b)</a> explains:</p>
<blockquote><p>Ever since Socrates pioneered the demand to know what all <em>F</em>s have in common, in virtue of which they are <em>F</em>s, the ideal of clear, sharp boundaries has been one of the founding principles of philosophy. Plato&#8217;s <em>forms</em> begat Aristotle&#8217;s <em>essences</em>, which begat a host of ways of asking for <em>necessary and sufficient conditions</em>, which begat <em>natural kinds</em>, which begat <em>difference-makers</em> and other ways of tidying up the borders of all the sets of things in the world. When Darwin came along with the revolutionary discovery that the sets of <em>living</em> things were not eternal, hard-edged, in-or-out classes but historical populations with fuzzy boundaries… the main reactions of philosophers were to either ignore this hard-to-deny fact or treat it as a challenge: Now how should we impose our cookie-cutter set theory on this vague and meandering portion of reality?</p>
<p>“Define your terms!” is a frequent preamble to discussions in philosophy, and in some quarters it counts as Step One in all serious investigations. It is not hard to see why. The techniques of argumentation inaugurated by Socrates and Plato and first systematized by Aristotle are not just intuitively satisfying… but demonstrably powerful tools of discovery… Euclid&#8217;s plane geometry was the first parade case, with its crisp isolation of definitions and axioms, inference rules, and theorems. If only all topics could be tamed as thoroughly as Euclid had tamed geometry! The hope of distilling everything down to the purity of Euclid has motivated many philosophical enterprises over the years, different attempts to euclidify all the topics and thereby impose classical logic on the world. These attempts continue to this day and have often proceeded as if Darwin never existed…</p>
<p>…</p>
<p>An argument that exposes the impact of Darwinian thinking is David Sanford&#8217;s (1975) nice “proof” that there aren&#8217;t any mammals:</p>
<ol><li>Every mammal has a mammal for a mother.</li>
<li>If there have been any mammals at all, there have been only a finite number of mammals.</li>
<li>But if there has been even one mammal, then by (1), there have been an infinity of mammals, which contradicts (2), so there can&#8217;t have been any mammals. It&#8217;s a contradiction in terms.</li>
</ol><p>Because we know perfectly well that there are mammals, we take this argument seriously only as a challenge to discover what fallacy is lurking within it. And we know, in a general way, what has to give: if you go back far enough in the family tree of any mammal, you will eventually get to the therapsids, those strange, extinct bridge species between the reptiles and the mammals… A gradual transition occurred over millions of years from clear reptiles to clear mammals, with a lot of intermediaries filling in the gaps. What should we do about drawing the lines across this spectrum of gradual change? Can we identify a mammal, the Prime Mammal, that didn&#8217;t have a mammal for a mother, thus negating premise (1)? On what grounds? Whatever the grounds are, they will compete with the grounds we could use to support the verdict that that animal was not a mammal – after all, its mother was a therapsid. What could be a better test of therapsid-hood than that? Suppose that we list ten major differences used to distinguish therapsids from mammals and declare that having five or more of the mammal marks makes an animal a mammal. Aside from being arbitrary – why ten instead of six or twenty, and shouldn&#8217;t they be ordered in importance? – any such dividing line will generate lots of unwanted verdicts because during the long, long period of transition between obvious therapsids and obvious mammals there will be plenty of instances in which mammals (by our five + rule) mated with therapsids (fewer than five mammal marks) and had offspring that were therapsids born of mammals, mammals born of therapsids born of mammals, and so forth! …What should we do? We should quell our desire to draw lines. We can live with the quite unshocking and unmysterious fact that, you see, there were all these gradual changes that accumulated over many millions of years and eventually produced undeniable mammals.</p>
<p>The insistence that there must be a Prime Mammal, even if we can never know when and where it existed, is an example of <em>hysterical realism</em>. It invites us to reflect that if we just knew enough, we&#8217;d see – we&#8217;d <em>have</em> to see – that there is a special property of mammal-hood – the <em>essence</em> of mammal-hood – that defines mammals once and for all. To deny that there is such an essence, philosophers sometimes say, is to confuse metaphysics with epistemology: the study of what there (really) is with the study of what we can know about what there is. I reply that there may be occasions when thinkers do go off the rails by confusing a metaphysical question with a (merely) epistemological question, but this must be shown, not just asserted. In this instance, the charge of confusing metaphysics with epistemology is just a question-begging way of clinging to one&#8217;s crypto essentialism in the face of difficulties.</p>
<p>…</p>
<p>In particular, the demand for essences with sharp boundaries blinds thinkers to the prospect of gradualist theories of complex phenomena, such as life, intentions, natural selection itself, moral responsibility, and consciousness.</p>
<p>If you hold that there can be no borderline cases of being alive (such as, perhaps, viruses or even viroids or motor proteins), you are more than halfway to élan vital before you start thinking about it. If no proper part of a bacterium, say, is alive, what “truth maker” gets added that tips the balance in favor of the bacterium&#8217;s being alive? The three more or less standard candidates are having a metabolism, the capacity to reproduce, and a protective membrane, but since each of these phenomena, in turn, has apparent borderline cases, the need for an arbitrary cutoff doesn&#8217;t evaporate. And if single-celled “organisms” (if they deserve to be called that!) aren&#8217;t alive, how could two single-celled entities yoked together with no other ingredients be alive? And if not two, what would be special about a three-cell coalition? And so forth.</p></blockquote>
<p>Of course, “fuzziness” is not limited to biological and biology-descended concepts. Mathematical concepts and some concepts used in fundamental physics have sharp boundaries, but the further from these domains we travel, the less sharply defined our concepts tend to be.<a class="see-footnote" id="footnoteref348_ykk0qf3" title="See also Unger (1988).  " href="#footnote348_ykk0qf3">348</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/81">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="FuzzinessAAD">Fuzziness and auto-activation deficit</h4>
<p>Finally, here is one more “intuition pump” (<a href="http://books.wwnorton.com/books/detail.aspx?id=4294972441">Dennett 2013</a>) in favor of a “fuzzy” view about consciousness.</p>
<p>Consider a form of akinetic mutism known variously as “athymhormia,” “psychic akinesia,” or “auto-activation deficit.” <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=zdMgAwAAQBAJ&amp;oi=fnd&amp;pg=PA334#v=onepage&amp;q&amp;f=false">Leys &amp; Henon (2013)</a> explain:</p>
<blockquote><p>Patients with akinetic mutism appear alert or at least wakeful, because their eyes are open and they have active gaze movements. They are mute and immobile, but they are able to follow the observer or moving objects with their eyes, to whisper a few monosyllables, and to have slow feeble voluntary movements under repetitive stimuli. The patients can answer questions, but otherwise never voluntarily start speaking. In extreme circumstances such as noxious stimuli, they can become agitated and even say appropriate words. This neuropsychological syndrome occurs despite the lack of obvious alteration of sensory motor functions. This syndrome results in impaired abilities in communicating and initiating motor activities.</p>
<p>…Akinetic mutism is due to disruption of reticulo-thalamo-frontal and extrathalamic reticulo-frontal afferent pathways…</p>
<p>…A discrepancy may be present between hetero- and auto-activation, the patient having an almost normal behavior under external stimuli. This peculiar form of akinetic mutism has been reported as [athymhormia], “loss of psychic self-activation” or “pure psychic akinesia”…</p></blockquote>
<p>Below, I quote some case case reports of auto-activation deficit, and then explain why I think they (weakly) suggest a fuzzy view of consciousness.</p>
<p><strong>Case report #1: A 35-year-old woman</strong></p>
<p>From ch. 4 of <a href="https://smile.amazon.com/Neuropsychology-Emotion-Advances-Behavioral-Neurology/dp/089862200X">Heilman &amp; Satz (1983)</a>, by Damasio &amp; Van Hoesen, on pp. 96-99:</p>
<blockquote><p><em>J</em> is a 35-year-old woman…</p>
<p>	On the night of admission she was riding in a car driven by her husband and talking normally, when she suddenly slumped forward, interrupted her conversation, and developed weakness of the right leg and foot. On arrival at the hospital she was alert but speechless…</p>
<p>	…There was a complete absence of spontaneous speech. The patient lay in bed quietly with an alert expression and followed the examiner with the eyes. From the standpoint of affect her facial expression could be best described as neutral. She gave no reply to the questions posed to her, but seemed perplexed by this incapacity. However, the patient did not appear frustrated… She never attempted to mouth words… She made no attmept to supplement her verbal defect with the use of gesture language. In striking contrast to the lack of spontaneous speech, the patient was able, from the time of admission, to repeat words and sentences slowly, but without delay in initiation. The ease in repetition was not accompanied by echolalia [unsolicited repetition of vocalizations made by someone else], and the articulation and melody of repeated speech were normal. The patient also gave evidence of good aural comprehension of language by means of nodding behavior… Performance on the Token Test was intact and she performed normally on a test of reading comprehension…</p>
<p>	Spontaneous and syntactically organized utterances to nurses and relatives appeared in the second week postonset, in relation to immediate needs only. She was at this point barely able to carry a telephone conversation using mostly one- and two-word expressions. At 3 weeks she was able to talk in short simple but complete sentences, uttered slowly… Entirely normal articulation was observed at all times…</p>
<p>	On reevaluation, 1 month later, the patient was remarkably recovered. She had considerable insight into the acute period of the illness and was able to give precious testimony about her experiences then. Asked if she ever suffered anguish for being apparently unable to communicate she answered negatively. There was no anxiety, she reported. She didn&#8217;t talk beacuse she had “nothing to say.” Her mind was “empty.” Nothing “mattered.” She apparently was able to follow our conversations even during the early period of the illness, but felt no “will” to reply to our questions. In the period after discharge she continued to note a feeling of tranquility and relative lack of concern…</p></blockquote>
<p><strong>Case report #2: A 61-year-old clerk</strong></p>
<p><a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0404.1991.tb04708.x/abstract">Bogousslavsky et al. (1991)</a> explain:</p>
<blockquote><p>A 61-year old clerk with known [irregular heartbeat] was admitted because of “confusion” and [a drooping eyelid]…</p>
<p>According to his family, [following a stroke] he had become “passive” and had lost any emotional concern. He was [drowsy] and was orientated in time and place…, but remained apathetic and did not speak spontaneously. He moved very little unless asked to do so, only to go to the bathroom three or four times a day. He would sit down at the table to eat only when asked by the nurses or family and would stop eating after a few seconds unless repeatedly stimulated. During the day, he would stay in bed or in an armchair unless asked to go for a walk. He did not react to unusual situations in his room, such as Grand Mal seizures in another patient. He did not read the newspapers and did not watch the television. This behaviour contrasted with preserved motor and speech abilities when he was directly stimulated by another person: with constant activation, he was able to move and walk normally, he could play cards, answer questions, and read a test and comment on it thereafter; however, these activities would stop immediately if the external stimulation disappeared. He did not show imitation and utilization behaviour [imitation behavior occurs when patients imitate an examiner&#8217;s behavior without being instructed to do so; utilization behavior occurs when patients try to grab and use everyday objects presented to them, without being instructed to do so], and could inhibit socially inadequate acts, even when asked to perform them by an examiner (shouting in the room, undressing during daytime); however, he did not react emotionally to such orders. Also, he showed no emotional concern [about] his illness, though he did not deny it, and he remained indifferent when he had visitors or received gifts. He did not smile, laugh or cry. He never mentioned his previous activities and, when asked about his job, he answered he had no project to go back to work. When asked about his private thoughts, he just said “that&#8217;s all right”, “I think of nothing,” “I don&#8217;t want anything.” Because his motor and mental abilities seemed normal when stimulated by another person, his family and friends wondered whether he was really ill or was inactive on purpose, to annoy them. They complained he had become “a larva.”</p>
<p>Formal neuropsychological examination using a standard battery of tests was performed 3, 10, 25 and 60 days after stroke, including naming, repetition, comprehension, writing, reading, facial recognition, visuospatial recognition, topographic orientation on maps, drawing, copy of the Rey-Osterrieth figure, which were normal. No memory dysfunction was found: the patient could evoke remote and recent events of his past, visual… and verbal… learning, delayed reproduction of the Rey-Osterrieth figure showed normal results for age. Only minor disturbances were found on “frontal lobe tests”… His symbolic understanding of proverbs was preserved. The patient could cross out 20 lines distributed evenly on a sheet of paper; with no left- or right-side preference. [Editor&#8217;s note: see the paper for sources describing these tests.]</p>
<p>The patient was discharged unchanged two months after stroke to a chronic care institution, because his family could not cope with his behavioural disturbances, though they recognized that his intellect was spared.</p></blockquote>
<p><strong>Case report #3: A 64-year-old housewife</strong></p>
<p>This case is also from <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0404.1991.tb04708.x/abstract">Bogousslavsky et al. (1991)</a>:</p>
<blockquote><p>A 64-year-old housewife was in good health until she was admitted to hospital after she lost consciousness for 5 min and subsequently complained that she was unable to look downwards… According to her family, the patient&#8217;s mood had changed completely after her stroke. Previously, she was active, she liked jokes, and enjoyed being with her grandchildren, but she had become indifferent, did not manifest emotions and did not laugh anymore. Though she was not sleepy, she would stay in bed for the whole morning, unless the nurse would come and ask her to get up and wash herself. She spoke very little spontaneously and did not join conversations with other patients, except to answer questions. She did not seem to be interested in anything and did not enjoy or criticize the meals, though she used to be an expert cook. Unless stimulated by the staff or her family, she did not do anything spontaneously, except sitting in front of the television for hours, going to the bathroom, and browsing among magazines, but without apparent curiosity. She remembered perfectly what she had seen on television or in the magazines, but without expressing any particular interest in any item. She did not have any projects for the future and did not report personal thoughts. This passive behaviour contrasted with her ability to perform usual daily activities when stimulated and ordered by another person. She could sew, knit, play dominoes and go shopping when assisted by her daughter, who had to tell her what to do next. However, she did not show imitation and utilization behaviour, and even after a rather automatic activity (like knitting), she showed no perseverations.</p>
<p>Neuropsychological examination was done 2 and 17 days after stroke. The patient was not disorientated or distracted, but tended to stop doing the tests unless constantly stimulated. With such help, she collaborated well with the examiner. Naming, repetition, comprehension, reading, writing, Poppelreuter, and recognition of objects and faces were normal. [Verbal and visual] learning was unimpaired. She made no mistake but some self-corrected perseverations on Luria&#8217;s conflicting tasks and, on Stroop&#8217;s test, naming was slightly slowed… On Wisconsin card sorting test, she found 4 clues out of 6. [Editor&#8217;s note: see the paper for sources describing these tests.]</p>
<p>The patient was discharged 3 weeks after stroke to her daughter&#8217;s home. On follow-up phone calls 2 and 4 months later, the daughter reported that the apathy and indifference had slightly improved, but that the patient still needed external stimulations to accomplish her daily activities. Under this condition, the patient was able to help cleaning the house and to cook; she did not seem, however, to have feelings of happiness or sadness.</p></blockquote>
<p><strong>Case report #4: Mr. V</strong></p>
<p>From <a href="http://jnnp.bmj.com/content/47/4/377.short">Laplane et al. (1984)</a>:</p>
<blockquote><p>In 1968, Mr V, a 41-year-old healthy man was stung on the left arm by a wasp. He immediately sustained a convulsive coma for 24 hours, then adopted intensive choreic [brief, irregular, “dance-like”] movements (which were alleviated by thioproperazine), and impairment of gait. These… symptoms diminished over several months. Then, and during the twelve following years he appeared to be a mild dement. He was evaluated by us in 1980, and was at this time not receiving drugs. All his activities were dramatically reduced. He spent many days doing nothing, without initiative or motivation, but without getting bored. The patient described this state as “a blank in my mind.” His affect was disturbed. When talking about family problems, sad or pleasant, he had an appropriate behaviour and gave signs of normal interest, but this attitude did not last and he became rapidly indifferent again. His fantasy life was poor, but dreaming was preserved. But when stimulated by external events, or more specially by another person, he could perform quite correct complex tasks (for example, playing bridge). This fact was dramatically demonstrated by neuropsychological tests which showed intellectual capacities within the normal range…</p>
<p>Two years after the encephalopathy, he began to show stereotyped activities. The most frequent consisted in mental counting, for example, up to twelve or a multiple of twelve, but sometimes it was a more complex calculation. Such mental activities sometimes were accompanied by gestures, such as a finger pacing of the counts. To switch on and off a light for one hour or more was another of his most common compulsions. When asked about this behaviour he answered that he had to count… that he could not stop it… as that it was stronger than him… Once he was found on his knees pushing a stone with the hands; he gave the explanation that he must push the stone, and he used the hands because he experienced some difficulties in skilled movements with his legs. There was however no anxiety, and in his past history there was no suggestion of an obsessional neurosis. Personality evaluation was normal…</p>
<p>Neurological examination showed abnormal movements. At the time of examination in 1980 choreic movements were very mild but voluntary movements were often brisk. He had a permanent facial rictus [a curl of the upper lip, as in an expression of contempt] with some facial or mandibular movements somewhat resembling tics. With his finger movements it was difficult to distinguish between involuntary or “voluntary” activity associated with mental counting. Walking was a mixture of Parkinsonism and choreic disturbances.</p>
<p>Several drugs were systematically tried. Dopaminergic agents (agonist and antagonist), serotoninergic, cholinergic, noradrenergic and benzodiazepines were used. Most of this drugs had no or mild effects on the patient&#8217;s symptomatology. Then, clomipramine was given up to 250 mg/day (under cardiovascular supervision) and this drug induced a dramatic improvement. For the first time for twelve years the patient was able to take the initiative to drive a car, and to initiate talking. Speech fluency reduction and stereotyped activities, however, remained.</p>
<p>The patient died suddenly from massive inhalation of food.</p></blockquote>
<p><strong>Case report #5: Mr. D</strong></p>
<p>Also from <a href="http://jnnp.bmj.com/content/47/4/377.short">Laplane et al. (1984)</a>:</p>
<blockquote><p>Mr D was 23-years-old when he sustained in November 1979 carbon monoxide poisoning… He was examined for the first time by us in January 1980. Neurological examination was normal except for intellectual performance; memory and verbal fluency seemed deeply disturbed. The patient was examined again one year later. He had still severe disorders of memory and verbal fluency. However, intellectual performances were dramatically improved. He could perform complex tasks correctly and solve problems. But these faculties were largely underused. In the absence of external stimulation he lay for hours, eyes open, doing nothing. The contrast between his intellectual capacities and this inactivity was obvious in all aspects of his life. He talked only if asked, he took part in sport (he was a sports coach) only if stimulated by his wife, he went to visit friends only if invited by a phone call, and so on. His only spontaneous activity was of a routine nature like going out and getting the newspaper. His affect was impaired in the same way. If asked about the recent death of someone he cared about he cried sincerely, but if asked about recent events of his life he forgot the death and talked only about some political news. Stereotyped activities were not reported spontaneously by the patient, nor by his wife. But when questioned on this point, he admitted counting when he was alone with nothing to do; he counted from 1 to 20 again. To stop it he had to go out, or watch TV. This purely mental activity did not give him anxiety nor did its withdrawal. Besides this activity, there was no sign of an obsessional neurosis. Neurological examination was normal…</p></blockquote>
<p><strong>Case report #6: Mr. P</strong></p>
<p>Also from <a href="http://jnnp.bmj.com/content/47/4/377.short">Laplane et al. (1984)</a>:</p>
<blockquote><p>Mr P, born in Russia in 1911 and living in France since 1933, suffered in March 1970 carbon monoxide poisoning. There was a short coma followed by several days of headache and confusion. This man, whose profession was that of an artistic painter but who also did the job of messenger, was unable to re-start his work. He attended for neurologic consultation in April 1970, after being fired from his job because he was too slow. He had… a resting tremor…. [Impairment of voluntary movement] was obvious and verbal fluency was markedly reduced. Intellectual processes were slow and the whole picture tended to give the impression of mental deterioration. He was, however, able to perform complex tasks on request. He was institutionalised and his status improved during the first year… At first unable on admission to conceive and execute basic drawings, one year later he could produce elaborate and artistic drawings and paintings. The most striking feature lay in his dramatic passivity, his lack of initiative despite the fact that his motor and mental capacities were largely preserved. He stayed in a ward, spending most of the time doing nothing and he never attempted to leave hospital. If questioned on this point, he answered that he didn&#8217;t know, or that an authorisation to leave was required, that he had no family anyway, he didn&#8217;t feel bored. His lack of initiative was, however, not total since he had some spontaneous activities, such as helping the other patients in eating and shaving, and sometimes he watched TV, or read the newspaper. He went out to walk in the park only if he had been actively encouraged. He was able to perform artistic paintings; but for years he painted the same landscape of moors and fens, and this several dozens of times. His affect was poor in relation to his solitude — when questioned about his biography, he evoked spontaneously with sadness the death of his mother or brother.</p></blockquote>
<p><strong>Case report #7: A 58-year-old salesman</strong></p>
<p>From <a href="http://stroke.ahajournals.org/content/31/7/1762.short">Engelborghs et al. (2000)</a>:</p>
<blockquote><p>During a [test of how blood flows through one&#8217;s arteries], this 58-year-old right-handed patient suddenly… [entered a] coma… </p>
<p>…Consciousness was fully recovered 3 days after stroke.</p>
<p>The patient&#8217;s behavior had dramatically changed. Instead of the active man he used to be, he had become an apathetic, passive, and indifferent person who seemed to have lost all emotional concern and initiative. His affect was clearly blunted. The patient remained indifferent to visitors or when he received gifts. He did not show any concern for his relatives or his illness and manifested no desire, no complaint, and no concern about the future. The patient, who used to be a successful traveling salesman, had entirely lost concern about his business. There was a striking absence of thoughts and spontaneous mental activity. He rarely spoke spontaneously and took no verbal initiative. When asked about the content of his thoughts, the patient claimed he had none, suggesting a state of mental emptiness. Unless encouraged by the hospital staff or his relatives, he did not initiate any activity. When external stimulation disappeared, any induced activity was immediately interrupted. Every morning, the patient stayed in bed until he was encouraged to rise and get dressed. Once dressed, he returned to bed again or sat down in an armchair for the entire day. He moved very little unless urged to do so and reverted to his habitual state of athymia [lack of emotion] once he was left alone. There were no symptoms of depression. No stereotyped activities were observed.</p>
<p>…Two weeks after onset, a neurolinguistic examination revealed [language impairments]. Language functions progressively improved. Six months after onset, the neurolinguistic profile disclosed only a reduced verbal fluency.</p>
<p>In the acute phase, the Wechsler Adult Intelligence Scale (WAIS) reflected a generalized cognitive dysfunction (global IQ of 78). Concentration, sustained attention, and frontal problem solving were impaired. A general memory disorder was objectified by means of the Wechsler Memory Scale (revised). The remainder of the neuropsychological examination was within normal limits.</p>
<p>During the next 12 months, a systematic increase in the WAIS scores showed a complete recovery of intelligence levels (global IQ of 114). Although memory, concentration, and problem-solving capacity improved, the patient did not regain his premorbid levels. Despite intensive cognitive rehabilitation, it was impossible to re-engage the patient in his former professional activities because the athymic syndrome remained unchanged.</p></blockquote>
<p><a href="http://onlinelibrary.wiley.com/doi/10.1002/mds.1185/full">Laplane &amp; Dubois (2001)</a> summarize findings from several other cases that I have not read because they were published in French. For example, citing the case study by Damasio &amp; Van Hoesen plus three French papers containing additional case studies, they write:</p>
<blockquote><p>It is surprising that subjects who are cognitively unimpaired can remain inactive for hours without complaining of boredom. Their mind is “empty, a total blank,” they say. In the most typical cases, they have no thoughts and no projections in the future. Although purely subjective, this feeling of emptiness seems to be a reliable symptom, since it has been reported in almost the same terms by numerous patients.</p></blockquote>
<p>What can we learn from these case studies? One lesson, I think, is that phenomena we might have previously thought were inseparable are, in fact, separable, as <a href="http://onlinelibrary.wiley.com/doi/10.1002/0471468975.ch3/summary">Watt &amp; Pincus (2004)</a> argue. They suggest that “milder versions” of akinetic mutism, such as cases in which patients “respond to verbal inquiry” (as in the cases above), “appear to offer evidence of the independence of consciousness from an emotional bedrock, and that the former can exist without the latter.” They also offer a slightly different interpretation, according to which</p>
<blockquote><p>lesser versions of [akinetic mutism]… may allow some phenomenal content, while the more severe versions… may show a virtual “emptying out” of consciousness. In these cases, events may be virtually meaningless and simply don&#8217;t matter anymore… [In more severe cases] patients [might] live in a kind of strange, virtually unfathomable netherworld close to the border of a persistent vegetative state.</p></blockquote>
<p>They go on to say that their summary of disorders of consciousness “emphasizes their graded, progressive nature and eschews an all-or-nothing conceptualization. While intuitively appealing, an all-or-nothing picture of consciousness provides a limited basis for heuristic empirical study of the underpininings of consciousness from a neural systems point of view, as compared to a graded or hierarchical one that emphasizes the core functional envelopes of emotion, intention, and attention.”<br /><br />
For a similar illustration involving various pain phenomena rather than AAD, see <a href="http://link.springer.com/article/10.1007/s11098-013-0186-7">Corns (2014)</a>. </p>
<p><span style="float: right;">[<a href="/node/858/edit/82">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="AppendixH">Appendix H. First-order views, higher-order views, and hidden qualia</h3>
<p>In this appendix, I point to what I see as some of the most important arguments in the debate between first-order and higher-order theorists, which (in the present literature) is perhaps the most common way for theorists to argue about the complexity of consciousness and the distribution question (as mentioned <a href="#ComplexityOverview">above</a>). See <a href="#FirstOrder">here</a> for an explanation of what distinguishes first-order and higher-order views about consciousness.</p>
<p>Let&#8217;s start with the case for a relatively complex account of consciousness. In short, a great deal of cognitive processing that seems to satisfy (some) first-order accounts of consciousness — e.g. the processing which enables the blindsight subject to correctly guess the orientation of lines in her blind spot, or which occurs in the dorsal stream of the human visual system,<a class="see-footnote" id="footnoteref349_oba26bu" title="Arguably, Tye's PANIC theory doesn't imply that dorsal stream processing should be conscious, for dorsal stream processing arguably impacts behavior without impacting &quot;beliefs&quot; or &quot;desires,&quot; but one can easily imagine a very similar first-order theory which does imply that dorsal stream processing should be conscious.  " href="#footnote349_oba26bu">349</a> or which occurs in “smart” webcams or in Microsoft Windows — is, as far as we know, unconscious. This is the basic argument that consciousness must be more complex than first-order theorists suggest, whether or not “higher-order” theories, as narrowly defined in e.g. <a href="https://academic.oup.com/analysis/article-abstract/71/3/419/212987/The-higher-order-approach-to-consciousness-is">Block (2011)</a>, are correct. What can the first-order theorist say in reply?</p>
<p>Most responses I&#8217;ve seen seem unpersuasive to me.<a class="see-footnote" id="footnoteref350_4snsl3z" title="Two example responses that are unpersuasive to me are summarized by Carruthers (2016):  What options does a first-order theorist have to resist this conclusion? One is to deny that the data are as problematic as they appear (as does Dretske 1995). It can be said that the unconscious states in question lack the kind of fineness of grain and richness of content necessary to count as genuinely perceptual states. On this view, the contrast discussed above isn't really a difference between conscious and unconscious perceptions, but rather between conscious perceptions, on the one hand, and unconscious belief-like states, on the other. Another option is to accept the distinction between conscious and unconscious perceptions, and then to explain that distinction in first-order terms. It might be said, for example, that conscious perceptions are those that are available to belief and thought, whereas unconscious ones are those that are available to guide movement (Kirk 1994).  Another reply I found unpersuasive, this time responding specifically to the proposed counterexample of blindsight, is given by Tye (2000), p. 63:  If their reports are to be taken at face value, blindsight subjects… have no phenomenal consciousness in the blind region. What is missing, on the PANIC theory, is the presence of appropriately poised, nonconceptual, representational states. There are nonconceptual states, no doubt representationally impoverished, that make a cognitive difference in blindsight subjects. For some information from the blind field does reach the cognitive centers and controls their guessing behavior. But there is no complete, unified representation of the visual field, the content of which is poised to make a direct difference in beliefs. Blindsight subjects do not believe their guesses. The cognitive processes at play in these subjects are not belief-forming at all.  " href="#footnote350_4snsl3z">350</a> However, there is at least one reply that seems (to me) to have substantial merit, though it does not settle the issue.<a class="see-footnote" id="footnoteref351_do6itxh" title="One other reply that may have some merit, but which I don't discuss here, is given by Carruthers (2017):  So the questions posed to first-order theories were these: Why should nonconceptual contents have feel or be like something to undergo when available to central thought processes, while lacking such properties otherwise? How can these differences in functional role confer on one set of contents a distinctive subjective dimension that the other set lacks? In short: how does role (specifically, global broadcasting) create phenomenal character?  The seeming-unanswerability of these questions motivated Carruthers to propose his dual-content theory, according to which one effect of global broadcasting is to make first-order nonconceptual contents available to a higher-order mentalizing or &quot;mindreading&quot; faculty capable of entertaining higher-order thoughts about those experiences. This, when combined with the truth of some or other kind of consumer semantics, was said to add a dimension of higher-order nonconceptual content to the first-order experiences in question. Every globally broadcast experience is then both a nonconceptual representation of the world or body (red, say) and a nonconceptual representation of that experience of the world or body (seeming red, or experiencing red, say). Globally broadcast experiences are thus not just world-presenting but also self-presenting. They thereby acquire a subjective dimension and become like something to undergo. Moreover, only globally broadcast experiences have this sort of dual content, on the assumption that only such experiences are available to the mindreading faculty. Hence the conscious / unconscious distinction can genuinely be explained, it was claimed.  …a first-order theorist needs to say something about why nonconceptual contents should be phenomenally conscious if globally broadcast, but not otherwise. It is obviously true (almost by definition) that global broadcasting renders nonconceptual content access-conscious. But what is it about global broadcasting that renders nonconceptual content phenomenally conscious? Even if one seeks to deny that these concepts pick out distinct properties, something needs to be said to explain why what-it-is-likeness and other properties distinctive of phenomenal consciousness should only co-occur with global broadcasting.  The way forward for first-order theorists, I suggest, is to co-opt the operationalization of phenomenal consciousness first proposed by Carruthers &amp; Veillet (2011)… [namely] that phenomenal consciousness can be operationalized as whatever gives rise to the &quot;hard problems&quot; of consciousness… That is, a given type of content can qualify as phenomenally conscious if and only if it seems ineffable, one can seemingly imagine zombie characters who lack it, one can imagine what-Mary-didn't-know scenarios for it, and so on. For the very notion of phenomenal consciousness seems constitutively tied to these issues. If there is a kind of state or a kind of content for which none of these problems arise, then what would be the point of describing it as phenomenally conscious nonetheless? And conversely, if there is a novel type of content not previously considered in this context for which hard-problem thought-experiments can readily be generated, then that would surely be sufficient to qualify it as phenomenally conscious.  Once phenomenal consciousness is operationalized as whatever gives rise to hard-problem thought-experiments, however, it should be obvious that the initial challenge to first-order representationalism collapses. The reason why nonconceptual contents made available to central thought processes are phenomenally conscious, whereas those that are not so available are not, is simply that without thought one cannot have a thought-experiment. Only those nonconceptual contents available [via global broadcasting] to central thought are ones that will seem to slip through one's fingers when one attempts to describe them (that is, be ineffable), only they can give rise to inversion and zombie thought-experiments, and so on. This is because those thought-experiments depend on a distinctively first-personal way of thinking of the experiences in question. This is possible if the experiences thought about are themselves available to the systems that generate and entertain such thoughts, but not otherwise. Experiences that are used for online guidance of action, for example, cannot give rise to zombie thought-experiments for the simple reason that they are not available for us to think about in a first-person way, as this experience or something of the sort. They can only be thought about third-personally, as the experience that guides my hand when I grasp the cup, or whatever.  There is simply no need, then, to propose that dual higher-order / first-order nonconceptual contents are necessary in order for globally broadcast experiences to acquire a subjective dimension and be like something to undergo.  One reason I don't (yet) find this line of reasoning compelling is that I'm not sure it makes sense to operationalize phenomenal consciousness as &quot;whatever gives rise to the 'hard problems' of consciousness.&quot;  " href="#footnote351_do6itxh">351</a></p>
<p>This reply says that there may be “hidden qualia” (perhaps including even “hidden conscious subjects”), in the sense that there may be conscious experiences — in the human brain and perhaps in other minds — that are not accessible to introspection, verbal report, and so on. If so, then this would undermine the basic argument (outlined above) that consciousness must be more complex than first-order theorists propose. Perhaps (e.g.) the dorsal stream <em>is</em> conscious, but its conscious experiences simply are not accessible to “my” introspective processes and the memory and verbal reporting modules that are hooked up to “my” introspective processes.</p>
<p>This “hidden qualia” view certainly seems <em>coherent</em> to me.<a class="see-footnote" id="footnoteref352_jir8s47" title="Of course, I don't think phenomenology can be dissociated from function, as Cohen &amp; Dennett (2011) worry about. I'm a functionalist, after all. But I don't see why qualia couldn't be dissociated from the representations, verbal reports, etc. that I call &quot;me,&quot; and that &quot;I&quot; have introspective access to.  " href="#footnote352_jir8s47">352</a> The problem, of course, is that (by definition) we can&#8217;t get any introspective evidence of hidden qualia, and without a stronger model of how consciousness works, we can&#8217;t use third-person methods to detect hidden qualia, either. Nevertheless I see some reasons to think hidden qualia are at least <em>plausible</em>, given what little we know so far about consciousness. Nevertheless, there are some (inconclusive) reasons to think that hidden qualia may exist.<a class="see-footnote" id="footnoteref353_a9zoa1g" title="In addition to the sources discussed below, see also White (1991), ch. 6; Papineau (2002), section 7.14; Munevar (2012); Schechter (2014).  " href="#footnote353_a9zoa1g">353</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/83">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="overflowargument">Block&#8217;s overflow argument</h4>
<p>First, consider the arguments in <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/consciousness-accessibility-and-the-mesh-between-psychology-and-neuroscience/63BF0B7E5DB1654ED680E160D413F3D0">Block (2007b)</a>:</p>
<blockquote><p>No one would suppose that activation of the fusiform face area all by itself is sufficient for face-experience. I have never heard anyone advocate the view that if a fusiform face area were kept alive in a bottle, that activation of it would determine face-experience – or any experience at all… The <em>total</em> neural basis of a state with phenomenal character C is itself sufficient for the instantiation of C. The <em>core</em> neural basis of a state with phenomenal character C is the <em>part</em> of the total neural basis that distinguishes states with C from states with other phenomenal characters or phenomenal contents, for example the experience as of a face from the experience as of a house… So activation of the fusiform face area is a candidate for the core neural basis – not the total neural basis – for experience as of a face…</p>
<p>Here is the illustration I have been leading up to. There is a type of brain injury which causes a syndrome known as <em>visuo-spatial extinction</em>. If the patient sees a single object on either side, the patient can identify it, but if there are objects on both sides, the patient can identify only the one on the right and claims not to see the one on the left… With competition from the right, the subject cannot attend to the left. However… when [a patient named] G.K. claims not to see a face on the left, his fusiform face area (on the right, fed strongly by the left side of space) lights up almost as much as when he reports seeing the face… Should we conclude that [a] G.K. has face experience that – because of lack of attention – he does not know about? Or that [b] the fusiform face area is not the whole of the core neural basis for the experience, as of a face? Or that [c] activation of the fusiform face area is the core neural basis for the experience as of a face but that some other aspect of the total neural basis is missing? How are we to answer these questions, given that all these possibilities predict the same thing: no face report?</p></blockquote>
<p>Block argues that option [a] is often what&#8217;s happening inside our brains (whether or not it happens to be happening for G.K. and face experiences in particular). In other words, he thinks there are genuine phenomenal experiences going on inside our heads to which we simply don&#8217;t have cognitive access, because the capacity/bandwidth of the cognitive access mechanisms are more limited than the capacity/bandwidth of the phenomenality mechanisms — in other words, that “phenomenality overflows access.”</p>
<p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/div-classtitleexperience-and-agency-slipping-the-meshdiv/F16928E106E8E99824746D302CE0F35D">Clark &amp; Kiverstein (2007)</a> summarize Block&#8217;s “overflow argument” like this:</p>
<blockquote><p>The psychological data seem to show that subjects can see much more than working memory enables them to report. Thus, in the Landman et al. (2003) experiments, for instance, subjects show a capacity to identify the orientation of only four rectangles from a group of eight. Yet they typically report having seen the specific orientation of all eight rectangles. Working memory here seems to set a limit on the number of items available for conceptualization and hence report.</p>
<p>Work in neuroscience then suggests that unattended representations, forming parts of strong-but-still-losing clusters of activation in the back of the head, can be almost as strong as the clusters that win, are attended, and hence get to trigger the kinds of frontal activity involved in general broadcasting (broadcasting to the “global workspace”). But whereas Dehaene et al. (2006) treat the contents of such close-seconds as preconscious, because even in principle (given their de facto isolation from winning frontal coalitions) they are unreportable, Block urges us to treat them as phenomenally conscious, arguing that “the claim that they are not conscious <em>on the sole ground of unreportability simply assumes</em> metaphysical correlationism”… That is to say, it simply assumes what Block seeks to question – that is, that the kind of functional poise that grounds actual or potential report is part of what constitutes phenomenology. Contrary to this way of thinking, Block argues that by treating the just-losing coalitions as supporting phenomenally conscious (but in principle unreportable) experiences, we explain the psychological results in a way that meshes with the neuroscience.</p>
<p>The argument from mesh (which is a form of inference to the best explanation) thus takes as its starting point the assertion that the only grounds we have for treating the just-losing back-of-the-head coalitions as non-conscious is the unreportability of the putative experiences.</p></blockquote>
<p>Block&#8217;s arguments don&#8217;t settle the issue, of course. As the numerous replies to <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/consciousness-accessibility-and-the-mesh-between-psychology-and-neuroscience/63BF0B7E5DB1654ED680E160D413F3D0">Block (2007b)</a> in that same journal issue point out, there are a great many models which fit the data Block describes (plus other data reported by others). I haven&#8217;t evaluated these experimental data and these models in enough detail to have a strong opinion about the strength of Block&#8217;s argument, but at a glance it seems to deserve at least <em>some</em> weight, at least in our current state of ignorance about how consciousness works.<a class="see-footnote" id="footnoteref354_623ykfh" title="I should also mention that Block's arguments might even undermine the case for being as complex as first-order theorists suggest it is.  " href="#footnote354_623ykfh">354</a></p>
<p><span style="float: right;">[<a href="/node/858/edit/84">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="splitbrain">Split-brain patients</h4>
<p>Second, consider the famous studies of <a href="https://en.wikipedia.org/wiki/Split-brain">split-brain</a> patients, conducted by Michael Gazzaniga and others, which have often been argued to provide evidence of at least two separate conscious subjects in a single human brain, with each one (potentially) lacking introspective access to the other.</p>
<p>Gazzaniga himself originally interpreted his split-brain studies to indicate two separate streams of consciousness (<a href="https://books.google.com/books?id=dhhRBAAAQBAJ&amp;dq=gazzaniga+ledoux&amp;source=gbs_navlinks_s">Gazzaniga &amp; LeDoux 1978</a>), but later (<a href="https://books.google.com/books?id=jk2UmfntzYsC">Gazzaniga 1992</a>) rejected the “double-consciousness” view, and suggested instead that consciousness is computed by the left hemisphere. Later (<a href="http://www.nature.com/scientificamerican/journal/v12/n1s/pdf/scientificamerican0402-26sp.pdf">Gazzaniga 2002</a>), he “conceded that the right hemisphere might be conscious to some degree, but the left hemisphere has a qualitatively different kind of consciousness, which far exceeds what’s found in the right.”<a class="see-footnote" id="footnoteref355_w871m39" title="This quote from Prinz (2012).  " href="#footnote355_w871m39">355</a> Most recently, in 2016, Gazzaniga wrote that “there is ample evidence suggesting that the two hemispheres possess independent streams of consciousness following split-brain surgery.”<a class="see-footnote" id="footnoteref356_8ofgxye" title="Marsinek &amp; Gazzaniga (2016).  " href="#footnote356_8ofgxye">356</a></p>
<p>Glancing at the literature myself, the evidence seems unclear. For example, as far as I know, we do not have detailed verbal reports of conscious experience from both hemispheres, even though three different split-brain patients have been able to learn to engage in some limited verbal communication from the right (and not just the left) hemisphere.<a class="see-footnote" id="footnoteref357_gragp1w" title="See episode 117 of the Brain Science Podcast with Ginger Campbell.  " href="#footnote357_gragp1w">357</a> I don&#8217;t know whether we lack this evidence because those patients just weren&#8217;t asked the relevant questions (e.g. they weren&#8217;t asked to describe their experience), or because their right-hemisphere verbal communication is too deficient (as with the “verbal communication” of the tiny portion of those with hydranencephaly that can utter word-ish vocalizations at all). See also the arguments back and forth in <a hef="http://www.tandfonline.com/doi/abs/10.1080/09515089.2011.579417">Schechter (2012)</a> and <a href="https://academic.oup.com/brain/article-abstract/doi/10.1093/brain/aww358/2951052/Split-brain-divided-perception-but-undivided">Pinto et al. (2017)</a>.</p>
<p>Also note that if the evidence ends up supporting the view that there are (at least) two streams of consciousness in split-brain patients, this would seem to undermine the “basic argument” for relatively complex theories of consciousness outlined above than e.g. Block&#8217;s overflow argument does, since a “double consciousness” view might just show that each hemisphere has the resources to support (a still quite complex) stream of consciousness if the two hemispheres are disconnected, rather than suggesting that the human brain may support a multitude of (potentially quite simple) streams of conscious processing.</p>
<p><span style="float: right;">[<a href="/node/858/edit/85">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="hemispheredisconnection">Other cases of hemisphere disconnection</h4>
<p>As <a href="http://www.cognethic.org/jcn/jcnv3i4_Blackmon.pdf">Blackmon (2016)</a> points out, in addition to split-brain patients there are also patients who have undergone a variety of “hemisphere disconnection” procedures:</p>
<blockquote><p>surgical hemisphere disconnections are distinct from the more familiar “split-brain” phenomenon in which both hemispheres, despite having their connection via the corpus callosum severed, are connected with the rest of the brain as well as with the body via functioning sensory and motor pathways. Split-brain patients have two functioning hemispheres which receive sensory data and send motor commands; hemispherectomy patients do not.</p></blockquote>
<p>Hemisphere disconnection procedures include:</p>
<ul><li><em>Anatomical hemispherectomy</em>, in which an entire hemisphere is surgically removed from the cranium.</li>
<li><em>Functional hemispherectomy</em>, in which some of a hemisphere is removed while the rest is left inside the cranium (but still disconnected).</li>
<li><em>Hemispherotomy</em>, in which a hemisphere is disconnected entirely from the rest of the brain but left entirely in the cranium.</li>
<li>The <em>Wada test</em>, which (successively) anesthetizes each hemisphere while the other hemisphere remains awake, in order to test how well each hemisphere does with memory, language, etc. without the help of the other hemisphere. We can think of this as a “temporary” hemisphere disconnection.</li>
</ul><p>Like split-brain patients, patients undergoing these procedures seem to remain conscious, recover quickly, hold regular jobs, describe their experiences on online forums, and so on.</p>
<p>Studies of hemisphere disconnection provide data which complement the consciousness-relevant data we have from studies of split-brain patients. For example, hemisphere disconnection studies provide unambiguous data about the capabilities of the surviving hemisphere, whereas there is some ambiguity on this matter from split-brain studies, since interhemispheric cortical transfer of information remains possible in split-brain patients (via the superior colliculus), and the outputs of each hemisphere in split-brain patients might be integrated elsewhere in the brain (e.g. in the cerebellum).</p>
<p>I haven&#8217;t examined the hemisphere disconnection literature. But as with split-brain research, it seems as though it <em>might</em> (upon further examination) do some work to undermine the “basic argument” for relatively complex theories of consciousness outlined above.</p>
<p><span style="float: right;">[<a href="/node/858/edit/86">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="Shiller">Shiller&#8217;s arguments</h4>
<p><a href="http://link.springer.com/article/10.1007/s13164-015-0297-5">Shiller (2016)</a> makes two additional arguments for the plausibility of hidden qualia actually existing.<a class="see-footnote" id="footnoteref358_pp4khu0" title="See also notes from my conversation with Derek Shiller.  " href="#footnote358_pp4khu0">358</a></p>
<p>His <em>exceptionality</em> argument goes like this: just like our other senses didn&#8217;t evolve to detect and make use of all <em>potentially</em> available information (e.g. about very small things, or far away things, or very high-pitched sounds, or ultraviolet light), because doing so would be more costly than it&#8217;s worth (evolutionarily), probably our introspective powers also haven&#8217;t evolved to access all the qualia and other processes going on in our heads. Unless introspection is <em>exceptional</em> (in a certain sense) among our senses, we should expect there to be qualia (and other cognitive processes) that our introspection just doesn&#8217;t have access to.</p>
<p>Next, Shiller&#8217;s <em>argument from varieties</em> goes like this: </p>
<blockquote><p>The exceptionality argument centered on the likely limitations of introspection. The second argument that I will present focuses on the variety of kinds of hidden qualia that we might possibly have. Since it is independently plausible that we have many different varieties of qualia, it is more than merely plausible that we have at least one variety of hidden qualia. I have in mind a probabilistic argument: if we judge that there is a not too small probability that we have hidden qualia of each kind, then (given their independence) we are committed to thinking that it is fairly probable that we have at least one kind of hidden qualia. I will briefly describe five kinds of hidden qualia and present a few considerations for thinking that we might have hidden qualia of each kind…</p></blockquote>
<p>At a glance, these two arguments seem to have some force, especially the first one. But I haven&#8217;t spent much time evaluating them in detail.</p>
<p>Of course, even if these and other arguments related to the complexity of consciousness turned out to strongly favor either a “relatively simple” or a “relatively complex” account of consciousness <em>as defined by example <a href="#Defined">above</a></em>, one could still argue that “<em>morally-relevant</em> consciousness” can be more or less simple than “consciousness as defined by example above.”</p>
<p><span style="float: right;">[<a href="/node/858/edit/87">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h3 id="AppendixZ">Appendix Z. Miscellaneous elaborations and clarifications</h3>
<p>This appendix collects a variety of less-important sub-appendices.</p>
<p><span style="float: right;">[<a href="/node/858/edit/88">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="AppendixZ1">Appendix Z.1. Some theories of consciousness</h4>
<p>Below, I list some theories of consicousness that I either considered briefly or read about in some depth.</p>
<p>The items in the table below overlap each other, they are not all theories of the exact same bundle of phenomena, and several of them are families of theories rather than individual theories. In some cases, the author(s) of a theory might not explicitly endorse both physicalism and functionalism about consciousness, but it seems to me their theories could be adapted in some way to become physicalist functionalist theories.</p>
<p>Obviously, this list is not comprehensive.</p>
<p>In no particular order:</p>
<table><tr><th>Theory</th>
<th>Some expository references</th>
<th>Example critiques</th>
</tr><tr><td>Global workspace theories</td>
<td><a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00200/full">Baars et al. (2013)</a>; <a href="https://global.oup.com/academic/product/embodiment-and-the-inner-life-9780199226559?cc=us&amp;lang=en&amp;">Shanahan (2010)</a>, ch. 4; <a href="http://www.penguin.com/book/consciousness-and-the-brain-by-stanislas-dehaene/9780143126263">Dehaene (2014)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0959438813002298">Dehaene et al. (2014)</a>; <a href="http://academicworks.cuny.edu/gc_etds/1604/">Shevlin (2016)</a></td>
<td><a href="http://www.sciencedirect.com/science/article/pii/S1364661311001057">Lau &amp; Rosenthal (2011)</a>; <a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>, ch. 1; <a href="https://books.google.com/books?id=E3QYBAAAQBAJ&amp;lpg=PP1&amp;pg=PA161-IA4#v=onepage&amp;q&amp;f=false">Block (2014)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S1053811914005965">Pitts et al. (2014)</a>; <a href="http://nc.oxfordjournals.org/content/2015/1/niv006.abstract">Kemmerer (2015)</a><a class="see-footnote" id="footnoteref359_r1geyc5" title="See also notes from my conversation with Aaron Sloman. For a contrary follow-up on the experiments by Pitts et al., see Rutiku et al. (2015).      " href="#footnote359_r1geyc5">359</a></td>
</tr><tr><td>Temporal binding theory</td>
<td><a href="http://authors.library.caltech.edu/40352/">Crick &amp; Koch (1990)</a></td>
<td><a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>, ch. 1</td>
</tr><tr><td>Daniel Dennett&#8217;s multiple drafts / user-illusion theory</td>
<td>Dennett (<a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">1991</a>, <a href="https://mitpress.mit.edu/books/sweet-dreams">2005</a>, <a href="http://books.wwnorton.com/books/detail.aspx?ID=4294992671">2017</a>)<a class="see-footnote" id="footnoteref360_a648j0e" title="For a shorter explanation of Dennett (1991)'s positive theory of consciousness, see ch. 3 of Thompson (2009).      " href="#footnote360_a648j0e">360</a></td>
<td><a href="http://www.jstor.org/stable/2940970?seq=1">Block (1993)</a> and the critiques cited in Dennett (<a href="http://www.sciencedirect.com/science/article/pii/S1053810083710044">1993a</a>, <a href="http://www.tandfonline.com/doi/abs/10.1080/00201749308602315?journalCode=sinq20">1993b</a>, <a href="doi.org/10.5840/philtopics1994221/212">1994</a>)</td>
</tr><tr><td>First-order representational theories</td>
<td>Kirk (<a href="https://global.oup.com/academic/product/raw-feeling-9780198240815?lang=en&amp;cc=in">1994</a>, <a href="https://global.oup.com/academic/product/zombies-and-consciousness-9780199229802?cc=in&amp;lang=en&amp;">2007</a>, <a href="http://www.bloomsbury.com/us/robots-zombies-and-us-9781474286589/">2017</a>); <a href="https://mitpress.mit.edu/books/naturalizing-mind">Dretske (1995)</a>; <a href="https://mitpress.mit.edu/books/ten-problems-consciousness">Tye (1995)</a>; <a href="https://global.oup.com/academic/product/consciousness-9780199277360?cc=us&amp;lang=en&amp;">Carruthers (2005)</a>, ch. 3</td>
<td><a href="http://www.cambridge.org/nu/academic/subjects/philosophy/philosophy-mind-and-language/phenomenal-consciousness-naturalistic-theory?format=HB">Carruthers (2000)</a>; <a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>, ch. 1; <a href="https://mitpress.mit.edu/books/consciousness-paradox">Gennaro (2011)</a>, ch. 3</td>
</tr><tr><td>Higher-order theories</td>
<td><a href="https://mitpress.mit.edu/books/consciousness-and-experience">Lycan (1996)</a>; <a href="http://www.cambridge.org/nu/academic/subjects/philosophy/philosophy-mind-and-language/phenomenal-consciousness-naturalistic-theory?format=HB">Carruthers (2000)</a>; Rosenthal (<a href="https://global.oup.com/academic/product/consciousness-and-mind-9780198236979?lang=en&amp;cc=us">2006</a>, <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199262618.001.0001/oxfordhb-9780199262618-e-14">2009</a>); <a href="http://www.sciencedirect.com/science/article/pii/S1364661311001057">Lau &amp; Rosenthal (2011)</a>; <a href="http://plato.stanford.edu/entries/consciousness-higher/">Carruthers (2016)</a></td>
<td><a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>, ch. 1; <a href="http://analysis.oxfordjournals.org/content/71/3/419.short">Block (2011)</a>; <a href="http://faculty.philosophy.umd.edu/pcarruthers/Carruthers%20recants.pdf">Carruthers (2017)</a></td>
</tr><tr><td>AIR theory</td>
<td><a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a><a class="see-footnote" id="footnoteref361_8yidfik" title="Here is what Prinz (2012), pp. 342-343, says about the distribution question:      …there are ways in which the AIR theory can be applied to answer perennial questions about which creatures are conscious. Are human infants conscious? Are nonhuman animals conscious? It is difficult to answer these questions on the basis of behavior alone. Attention is difficult to distinguish from orienting, and working memory is difficult to distinguish from what ethologists call reference memory, a longer-term storage of objects and their locations (Green and Stanton, 1989). To search for consciousness in infants and animals, it would help to look for the neural signatures of consciousness. In infant brains, connections between areas are underdeveloped (Homae et al., 2010), and this may limit the capacity for allocating attention. The mammalian brain is similar across species, and we do know that gamma activity can be found in rodents (e.g., Vianney-Rodrigues, Iancu, and Welsh, 2011). But there are also differences across mammalian brains. At a cellular level, human visual streams are even subtly different from those of great apes and monkeys (Preuss and Coleman, 2002). We don’t yet know whether these differences have functional implications or implications for consciousness, but given the large numbers of similarities, it is likely that primates and perhaps all mammals are conscious. Many of the experiments cited in developing the AIR theory were performed on mammals, and the extraordinary lessons we have learned from that research may ultimately bear on the ethics of its continuation. What about birds? Their brains are built out of structures that are related to the mammalian subcortex, but these structures have evolved to function like the mammalian neocortex, resulting in working-memory and attention capacities that look surprisingly similar to our own (Güntürkün, 2005; Kirsch et al., 2008; Milmine, Rose, and Colombo, 2008). Cephalopods engage in strategic hunting behavior, and that may require attending to and briefly storing information about the spatial locations of their prey. The brain mechanisms of octopus working memory have been explored (Shomrat et al., 2008). There has also been work on the neural mechanisms of short-term memory in crabs (Tomsic, Berón de Astrada, and Sztarker, 2003). Pushing things even farther, there are studies of attention in fruit flies, and their capacity to attend is linked to genes that allow for short-term memory (van Swinderen, 2007). Obviously, there is also great variation between us and these other creatures. Given the doubts raised about multiple realizability in chapter 9, it wouldn’t be surprising to discover that only mammals have what it takes to be conscious on the AIR theory. But there is also astonishing continuity across animal phyla, so it is important to investigate the extent of similarity with respect to the mechanisms of consciousness. Then we can decide whether we can eat octopuses with impunity.      " href="#footnote361_8yidfik">361</a></td>
<td><a href="http://philreview.dukejournals.org/content/124/1/163.short">Lee (2015)</a>; <a href="http://ndpr.nd.edu/news/36606-the-conscious-brain-how-attention-engenders-experience/">Mole (2013)</a>; <a href="http://www.ingentaconnect.com/search/article?option1=tka&amp;value1=Consciousness%2c+Attention%2c+and+Working+Memory%3a&amp;pageSize=10&amp;index=1">Barrett (2014)</a></td>
</tr><tr><td>Thomas Metzinger&#8217;s theory</td>
<td>Metzinger (<a href="https://mitpress.mit.edu/books/being-no-one">2003</a>, <a href="http://www.basicbooks.com/full-details?isbn=9780465020690">2010</a>)</td>
<td><a href="http://mind.oxfordjournals.org/content/113/450/369.full.pdf+html">Graham &amp; Kennedy (2004)</a></td>
</tr><tr><td>Integrated information theory</td>
<td><a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003588">Oizumi et al. (2014)</a>; <a href="http://www.scholarpedia.org/article/Integrated_information_theory">Tononi (2015)</a></td>
<td><a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004286">Cerullo (2015)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S1053810014001007">Thagard &amp; Stewart (2014)</a>; <a href="https://global.oup.com/academic/product/consciousness-and-the-social-brain-9780199928644?cc=us&amp;lang=en&amp;">Graziano (2013)</a>, ch. 11</td>
</tr><tr><td>Antonio Damasio&#8217;s theory</td>
<td><a href="http://knopfdoubleday.com/book/196548/self-comes-to-mind/">Damasio (2010)</a></td>
<td><a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>, ch. 1</td>
</tr></table><p>Other theories I looked at briefly include prediction error minimization (<a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00096/full">Hohwy 2012</a>), Nicholas Humphrey&#8217;s theory (<a href="http://press.princeton.edu/titles/9398.html">Humphrey 2011</a>), sensorimotor theory (<a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/div-classtitlea-sensorimotor-account-of-vision-and-visual-consciousnessdiv/BA1638CB7389102A12B336CE687EC270">O&#8217;Regan &amp; Noe 2001</a>; O&#8217;Regan <a href="https://global.oup.com/academic/product/why-red-doesnt-sound-like-a-bell-9780199775224?cc=us&amp;lang=en&amp;">2011</a>, <a href="http://link.springer.com/article/10.1007/s11023-012-9279-x">2012</a>), the geometric theory (<a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01041/full">Fekete et al. 2016</a>), semantic pointer competition (<a href="http://www.sciencedirect.com/science/article/pii/S1053810014001007">Thagard &amp; Stewart 2014</a>), the radical plasticity thesis (<a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2011.00086/full">Cleeremans 2011</a>), Björn Merker&#8217;s theory (Merker <a href="http://www.sciencedirect.com/science/article/pii/S1053810003000023">2005</a>, <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=1007572&amp;fileId=s0140525x07000891">2007</a>), Markkula&#8217;s narrative behavior theory (<a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2015.00803/full">Markkula 2015</a>), Derek Denton&#8217;s theory (<a href="https://global.oup.com/academic/product/the-primordial-emotions-9780199203147?cc=us&amp;lang=en&amp;">Denton 2006</a>), attention schema theory (<a href="https://global.oup.com/academic/product/consciousness-and-the-social-brain-9780199928644?cc=us&amp;lang=en&amp;">Graziano 2013</a>; <a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2015.00500/full">Webb &amp; Graziano 2015</a>), Julian Jaynes’ theory (<a href="http://www.hmhco.com/shop/books/The-Origin-of-Consciousness-in-the-Breakdown-of-the-Bicameral-Mind/9780618057078">Jaynes 1976</a>; <a href="http://www.julianjaynes.org/book/">Kuijsten 2008</a>), Gary Drescher&#8217;s theory (<a href="https://mitpress.mit.edu/books/good-and-real">Drescher 2006</a>, ch. 2; <a href="/sites/default/files/Gary_Drescher_07-18-16_%28public%29.pdf">notes from my conversation with Gary Drescher</a>), and Orch-OR theory (<a href="http://www.sciencedirect.com/science/article/pii/S1571064513001188">Hameroff &amp; Penrose 2014</a>).</p>
<p>What, exactly, must a theory of phenomenal consciousness explain? Example lists of desiderata include <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=wEjqp9kTXnUC&amp;oi=fnd&amp;pg=PA61&amp;ots=Wxpg0B33OY&amp;sig=0g7Xtgw_BSzNv9_O-zMhS_27IdY#v=onepage&amp;q&amp;f=false">Van Gulick (1995)</a>, pp. 93-95 of <a href="http://www.cambridge.org/nu/academic/subjects/philosophy/philosophy-mind-and-language/phenomenal-consciousness-naturalistic-theory?format=HB">Carruthers (2000)</a>, chapter 3 of <a href="https://mitpress.mit.edu/books/being-no-one">Metzinger (2003)</a>, table 1 of <a href="http://www.sciencedirect.com/science/article/pii/S1053810004000923">Seth &amp; Baars (2005)</a>, <a href="http://www.ingentaconnect.com/content/imp/jcs/2007/00000014/00000007/art00001">Aleksander (2007)</a>, chapter 1 of <a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>, much of <a href="https://books.google.com/books?id=7w6IYeJRqyoC&amp;printsec=frontcover&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjLkMuXsYvQAhWHllQKHXPEAlkQ6AEIHTAA#v=onepage&amp;q&amp;f=falseÎ">Baars (1988)</a>, and section 4.3 of <a href="http://academicworks.cuny.edu/gc_etds/1604/">Shevlin (2016)</a>.</p>
<p>For some other thoughts on theories of consciousness, see <a href="#AppendixB">Appendix B</a>.</p>
<p><span style="float: right;">[<a href="/node/858/edit/89">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="AppendixZ2">Appendix Z.2. Some varieties of conscious experience</h4>
<p>In the table below, I list some varieties of conscious experience, illustrating the diversity of conscious states for which we have some evidence about the subjective qualities of the phenomena from human verbal self-report.</p>
<table><tr><th>Phenomenon</th>
<th>Example sources</th>
</tr><tr><td>Akinetic mutism, especially a form called “athymhormia,” “psychic akinesia,” or “auto-activation deficit”</td>
<td><a href="http://onlinelibrary.wiley.com/doi/10.1002/mds.1185/full">Laplane &amp; Dubois (2001)</a>; <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=zdMgAwAAQBAJ&amp;oi=fnd&amp;pg=PA334#v=onepage&amp;q&amp;f=false">Leys &amp; Henon (2013)</a>; <a href="http://brain.oxfordjournals.org/content/136/10/3076.short">Leu-Semenescu et al. (2013)</a>; <a href="http://bjps.oxfordjournals.org/content/early/2015/04/14/bjps.axv012.short">Klein (2015)</a>; <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=QXUgCwAAQBAJ&amp;oi=fnd&amp;pg=PA122#v=onepage&amp;q&amp;f=false">Davies &amp; Levy (2016)</a></td>
</tr><tr><td>Memory disorders, e.g. anterograde amnesia and persistent déjà vu</td>
<td><a href="https://books.google.com/books?id=anl5AgAAQBAJ&amp;lpg=PP1&amp;pg=PA327#v=onepage&amp;q&amp;f=false">O&#8217;Connor et al. (2007)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0072975207880079">Markowitsch (2008)</a>; <a href="http://www.penguinrandomhouse.com/books/212292/patient-hm-by-luke-dittirch/">Dittrich (2016)</a></td>
</tr><tr><td>Sensory agnosias and similar, including e.g. face blindness and simultanagnosia</td>
<td><a href="https://www.routledge.com/Case-Studies-in-the-Neuropsychology-of-Vision/Humphreys/p/book/9780863778964">Humphreys (1999)</a>; <a href="https://mitpress.mit.edu/books/visual-agnosia">Farah (2004)</a>; <a href="http://www.scholarpedia.org/article/Hemineglect">Husain (2008)</a>; <a href="http://www.mitpressjournals.org/doi/10.1162/jocn.2008.20002">Coslett &amp; Lie (2008)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1002/wcs.42/full">Behrmann &amp; Nishimura (2010)</a>; <a href="https://books.google.com/books?hl=en&amp;lr=lang_en&amp;id=jF_MBQAAQBAJ&amp;oi=fnd&amp;pg=PA209#v=onepage&amp;q&amp;f=false">Coslett (2011)</a>; <a href="http://www.sciencedirect.com/science/article/pii/B9780444529039000157">Barton (2011)</a>; <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199988693.001.0001/oxfordhb-9780199988693-e-018">Zihl (2013)</a>; <a href="http://journal.frontiersin.org/article/10.3389/fnhum.2014.00123/full">Chechlacz &amp; Humphreys (2014)</a>; <a href="http://journals.lww.com/co-ophthalmology/Abstract/2015/09000/Achromatopsia___a_review.2.aspx">Remmer (2015)</a></td>
</tr><tr><td>Synesthesia</td>
<td><a href="http://www.annualreviews.org/doi/abs/10.1146/annurev-psych-113011-143840">Ward (2013)</a>; <a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2014.01414/full">Banissy et al. (2014)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0028393215301214">Deroy &amp; Spence (2016)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118661666.ch21/summary">Brogaard (2016)</a></td>
</tr><tr><td>Hypnosis</td>
<td><a href="https://books.google.com/books?id=gmNqAAAAMAAJ">Shor &amp; Orne (1965)</a>; <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=aNotkrp2om0C&amp;oi=fnd&amp;pg=PA309#v=onepage&amp;q&amp;f=false">Pekala &amp; Kumar (2000)</a>; <a href="https://global.oup.com/academic/product/hypnosis-and-conscious-states-9780198569800?cc=us&amp;lang=en&amp;">Jamieson (2007)</a>; <a href="https://global.oup.com/academic/product/the-oxford-handbook-of-hypnosis-9780198570097?lang=en&amp;cc=us">Nash &amp; Barnier (2008)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0010945212001074">Cardeña et al. (2013)</a></td>
</tr><tr><td>Phantom limbs and other phantom experiences</td>
<td><a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=aNotkrp2om0C&amp;oi=fnd&amp;pg=PA45#v=onepage&amp;q&amp;f=false">Katz (2000)</a>; <a href="http://www.tandfonline.com/doi/abs/10.1080/13546800244000111">Halligan (2002)</a>; <a href="http://www.scholarpedia.org/article/Phantom_touch">Ramachandran &amp; Brang (2009)</a>; <a href="http://www.hindawi.com/journals/tswj/2014/686493/">Andreotti et al. (2014)</a></td>
</tr><tr><td>Dreaming, especially e.g. <a href="https://en.wikipedia.org/wiki/Rapid_eye_movement_sleep_behavior_disorder">RBD</a>, <a href="https://en.wikipedia.org/wiki/False_awakening">dreams within dreams</a>, sleepwalking, and other unusual dreaming phenomena</td>
<td><a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=84263&amp;fileId=S0140525X00003976">Hobson et al. (2000)</a>; <a href="https://web.archive.org/web/20060301220434/http://news.ufl.edu/2001/02/13/dog-sleep/">Carey (2001)</a>; sec. 4.2.5 of <a href="https://mitpress.mit.edu/books/being-no-one">Metzinger (2003)</a>; <a href="http://psycnet.apa.org/index.cfm?fa=search.displayRecord&amp;UID=2007-09896-001">Domhoff (2007)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.2009.05115.x/abstract">Boeve (2010)</a>; <a href="http://www.ingentaconnect.com/content/ben/ctmc/2011/00000011/00000019/art00002">Mahowald (2011)</a>; <a href="http://journals.ub.uni-heidelberg.de/index.php/IJoDR/article/view/9085">Buzzi (2011)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0959438813000676">Chen et al. (2013)</a>; <a href="http://link.springer.com/chapter/10.1007/978-1-4939-2089-1_45">Schenck (2015)</a></td>
</tr><tr><td>Daydreaming</td>
<td><a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=aNotkrp2om0C&amp;oi=fnd&amp;pg=PA147#v=onepage&amp;q&amp;f=false">Giambra (2000)</a>; <a href="https://books.google.com/books?id=z8XlCgAAQBAJ&amp;lpg=PA233&amp;ots=4uobMz1t5y&amp;lr&amp;pg=PA233#v=onepage&amp;q&amp;f=false">Smallwood (2015)</a>; <a href="http://link.springer.com/article/10.1007/s13164-014-0221-4">Dorsch (2015)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1002/wcs.1392/full">Konishi &amp; Smallwood (2016)</a></td>
</tr><tr><td>Lucid dreaming</td>
<td><a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=TqOMfSJdnQoC&amp;oi=fnd&amp;pg=PA269#v=onepage&amp;q&amp;f=false">LaBerge &amp; DeGracia (2000)</a>; <a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00746/full">Metzinger (2013)</a>; <a href="http://www.jstor.org/stable/10.5406/amerjpsyc.127.2.0191?seq=1#page_scan_tab_contents">Stumbrys et al. (2014)</a>; <a href="http://open-mind.net/papers/what-is-the-state-of-the-art-on-lucid-dreaming-recent-advances-and-questions-for-future-research/at_download/paperPDF">Voss &amp; Hobson (2015)</a></td>
</tr><tr><td>Sensory substitution</td>
<td><a href="http://www.sciencedirect.com/science/article/pii/S1364661303002900">Bach-y-Rita &amp; Kercel (2003)</a>; <a href="https://books.google.com/books?id=lCH7RfejyWQC&amp;lpg=PA275&amp;ots=Es6zTclOau&amp;lr=lang_en&amp;pg=PA275#v=onepage&amp;q&amp;f=false">Lenay et al. (2003)</a>; <a href="https://books.google.com/books?id=x4fAYBTpacoC&amp;lpg=PA211&amp;ots=cGUI4j9_IB&amp;lr=lang_en&amp;pg=PA211#v=onepage&amp;q&amp;f=false">Collignon et al. (2011)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0149763413002765">Maidenbaum et al. (2014)</a>; <a href="https://books.google.com/books?id=unQsCgAAQBAJ&amp;lpg=PA86&amp;pg=PA655#v=onepage&amp;q&amp;f=false">Stiles &amp; Shimojo (2015)</a></td>
</tr><tr><td>Body ownership illusions, e.g. the rubber hand illusion and out-of-body experiences</td>
<td><a href="http://link.springer.com/article/10.1007%2Fs00221-009-1810-9">Banissy et al. (2009)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S1364661308002507">Blanke &amp; Metzinger (2009)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S002839320900390X">Tsakiris (2010)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0896627315008181">Blanke et al. (2015)</a>; <a href="http://journal.frontiersin.org/article/10.3389/fnhum.2015.00141/full">Kilteni et al. (2015)</a></td>
</tr><tr><td>Psychedelic experiences</td>
<td><a href="http://www.drugtext.org/pdf/Psychedelics/psychonautics-a-model-and-method-for-exploring-the-subjective-effects-of-psychoactive-drugs.pdf">Newcombe &amp; Johnson (1999)</a>; <a href="https://global.oup.com/academic/product/the-antipodes-of-the-mind-9780199252930?cc=us&amp;lang=en&amp;">Shanon (2002)</a>; chs. 5-7 in volume 2 of <a href="http://www.abc-clio.com/ABC-CLIOCorporate/product.aspx?pc=A2707C">Cardeña &amp; Winkelman (2011)</a>; <a href="https://psychonautwiki.org/wiki/Psychedelic">PsychonautWiki (2016)</a></td>
</tr><tr><td>Mystical experience</td>
<td><a href="https://books.google.com/books?id=DP7jpzngHJ0C&amp;lpg=PA215&amp;ots=yh7Hat5NR7&amp;lr&amp;pg=PA215#v=onepage&amp;q&amp;f=false">McNamara &amp; Butler (2013)</a>; <a href="http://psycnet.apa.org/books/14258/013">Wulff (2014)</a></td>
</tr><tr><td>Absence seizures</td>
<td><a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1528-1167.2008.01777.x/full">Panayiotopoulos (2008)</a>; <a href="https://books.google.com/books?id=mxE2FYWoY0wC&amp;lpg=PA192&amp;pg=PA192#v=onepage&amp;q&amp;f=false">Arzimanoglou &amp; Ostrowsky-Coste (2010)</a>; <a href="http://epilepsycurrents.org/doi/abs/10.5698/1535-7511-13.3.135">Tenney &amp; Glauser (2013)</a>; <a href="http://link.springer.com/chapter/10.1007/978-3-642-37580-4_5">Gotman &amp; Kostopoulos (2013)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0969996114002459">Luijtelaar et al. (2014)</a></td>
</tr><tr><td>Pain asymbolia and (maybe) other sensory/affective dissociations</td>
<td><a href="https://mitpress.mit.edu/books/feeling-pain-and-being-pain">Grahek (2007)</a>; <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=9173620&amp;fileId=S0963180113000686">Shriver (2014)</a>; <a href="http://www.tandfonline.com/doi/abs/10.1080/00048402.2013.822399">Bain (2014)</a>; <a href="http://mind.oxfordjournals.org/content/124/494/493.full?keytype=ref&amp;ijkey=OuT671iVQczLT06">Klein (2015)</a></td>
</tr><tr><td>Delirium</td>
<td><a href="http://jnnp.bmj.com/content/78/11/1167.short">Bhat &amp; Rockwood (2007)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0306987713000157">Eeles et al. (2013)</a></td>
</tr><tr><td>Fluent aphasia, <em>a.k.a.</em> “Wernicke&#8217;s aphasia”</td>
<td><a href="http://www.cambridge.org/us/academic/subjects/languages-linguistics/psycholinguistics-and-neurolinguistics/fluent-aphasia?format=HB">Edwards (2005)</a>; <a href="http://www.tandfonline.com/doi/abs/10.1080/02687030802553688?journalCode=paph20">Marshall (2010)</a>; <a href="https://www.youtube.com/watch?v=3oef68YabD0">video example from Tactus Therapy</a></td>
</tr><tr><td>Hemisphere disconnection, including split-brain procedures and (congenital) agenesis of the corpus callosum</td>
<td><a href="https://books.google.com/books?id=dhhRBAAAQBAJ&amp;dq=gazzaniga+ledoux&amp;source=gbs_navlinks_s">Gazzaniga &amp; LeDoux (1978)</a>; <a href="http://brain.oxfordjournals.org/content/123/7/1293?links=false">Gazzaniga (2000)</a>; ch. 5 of <a href="https://www.elsevier.com/books/handbook-of-neuropsychology-2nd-edition/boller/978-0-444-50358-9">Boller &amp; Grafman (2000)</a>;<a class="see-footnote" id="footnoteref362_u08bp1m" title="Chapter 5 is &quot;Hemispheric interactions and specializations: insights from the split brain&quot; (pp. 103-120) by Margaret Funnell, Paul Corballis, and Michael Gazzaniga.      " href="#footnote362_u08bp1m">362</a> <a href="http://www.nature.com/nrn/journal/v8/n4/abs/nrn2107.html">Paul et al. (2007)</a>; <a href="http://thejns.org/doi/full/10.3171/FOC/2008/25/9/E14">de Ribaupierre and Delalande (2008)</a>; <a href="http://brain.oxfordjournals.org/content/123/7/1293?links=false">Bayne (2008)</a>; <a href="http://www.tandfonline.com/doi/abs/10.1080/09515089.2011.579417">Schechter (2012)</a>; <a href="http://www.cognethic.org/jcn/jcnv3i4_Blackmon.pdf">Blackmon (2016)</a></td>
</tr><tr><td>Dissociative identity disorder</td>
<td><a href="http://anp.sagepub.com/content/48/5/402.short">Dorahy et al. (2014)</a>; <a href="https://books.google.com/books?id=Uh0nCgAAQBAJ&amp;lpg=PR1&amp;pg=PA407#v=onepage&amp;q&amp;f=false">Lynn et al. (2014)</a></td>
</tr><tr><td>Hallucinations</td>
<td><a href="http://www.apa.org/pubs/books/4318044.aspx">Aleman &amp; Larøi (2008)</a>; <a href="http://www.springer.com/us/book/9781441912220">Blom (2010)</a>; <a href="http://www.annualreviews.org/doi/abs/10.1146/annurev-anthro-081309-145819">Luhrmann (2011)</a>; <a href="http://www.cambridge.org/us/academic/subjects/psychology/health-and-clinical-psychology/hearing-voices-histories-causes-and-meanings-auditory-verbal-hallucinations?format=HB">McCarthy-Jones (2012)</a>; <a href="http://www.springer.com/us/book/9781461409588">Blom &amp; Sommer (2012)</a>; <a href="http://www.springer.com/us/book/9781461441205">Jardri et al. (2013)</a>; <a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118731700,subjectCd-HEG0.html">Collerton et al. (2015)</a></td>
</tr><tr><td>Craniopagus twins, especially Krista and Tatiana Hogan</td>
<td><a href="http://www.nytimes.com/2011/05/29/magazine/could-conjoined-twins-share-a-mind.html">Dominus (2011)</a>; <a href="http://ojs.library.ubc.ca/index.php/ubcujp/article/view/2521/0">Squair (2012)</a>; <a href="http://www.cbc.ca/doczone/episodes/twin-life-sharing-mind-and-body">Pyke (2014)</a>; <a href="http://link.springer.com/article/10.1007/s11098-014-0393-x">Langland-Hassan (2015)</a>; <a href="http://www.macleans.ca/news/canada/conjoined-twins-share-each-others-senses/">MacQueen (2015)</a><a class="see-footnote" id="footnoteref363_xklfbsj" title="On the potential implications of such &quot;mindmelding&quot; for arguments about the privacy of consciousness, see Hirstein (2012).      " href="#footnote363_xklfbsj">363</a></td>
</tr><tr><td>Blindsight and related deficits, e.g. “numb-sense” and “deaf-hearing”</td>
<td><a href="https://global.oup.com/academic/product/consciousness-lost-and-found-9780198524588?cc=us&amp;lang=en&amp;">Weiskrantz (1997)</a>; <a href="https://broadviewpress.com/product/blindsight-and-the-nature-of-consciousness/">Holt (2003)</a>; <a href="http://www.tandfonline.com/doi/abs/10.1080/02724980343000882">Cowey (2004)</a>; <a href="http://www.scholarpedia.org/article/Blindsight">Weiskrantz (2007)</a>; <a href="http://link.springer.com/article/10.1007/s00221-009-1914-2">Cowey (2010)</a>; <a href="http://link.springer.com/article/10.1007/s00221-011-2578-2">Overgaard (2011)</a></td>
</tr><tr><td>Delusional misidentification syndromes, e.g. Cotard delusion</td>
<td><a href="http://www.sciencedirect.com/science/article/pii/S1053810008000068">Young (2008)</a>; <a href="http://link.springer.com/article/10.1007%2Fs11920-009-0031-z">Debruyne (2009)</a>; <a href="http://www.tandfonline.com/doi/abs/10.1080/13546800903414891">Hirstein (2010)</a>; <a href="http://www.karger.com/Article/Fulltext/337748">Politis &amp; Loane (2012)</a>; <a href="http://hpy.sagepub.com/content/25/1/87.short">Blom (2014)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/tops.12108/full">Langdon et al. (2014)</a>; <a href="http://www.jaapl.org/content/42/3/369.short">Klein &amp; Hirachan (2014)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0010945214003037">Walsh et al. (2015)</a></td>
</tr><tr><td>Aphantasia</td>
<td><a href="http://www.sciencedirect.com/science/article/pii/S0010945215001781">Zeman et al. (2015)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0010945215002166">Vito &amp; Bartolomeo (2016)</a></td>
</tr><tr><td>Anesthesia awareness</td>
<td><a href="http://www.sciencedirect.com/science/article/pii/S1053810008000913">Mashour &amp; LaRock (2008)</a>; <a href="http://link.springer.com/chapter/10.1007/978-1-60761-462-3_8">Mashour (2009)</a></td>
</tr><tr><td>Anosognosia</td>
<td><a href="https://books.google.com/books?id=xze89PCLaWMC&amp;lpg=PP1&amp;pg=PA53#v=onepage&amp;q&amp;f=false">Heilman (1991)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0010945208709183">Vuilleumier (2004)</a>; <a href="http://content.iospress.com/articles/restorative-neurology-and-neuroscience/rnn00355">Vallar &amp; Ronchi (2006)</a>; <a href="https://global.oup.com/academic/product/the-study-of-anosognosia-9780195379099?cc=us&amp;lang=en&amp;">Prigatano (2010)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0166432811005250">Moro et al. (2011)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/psyg.12064/full">Chen et al. (2014)</a></td>
</tr><tr><td>Locked-in syndrome</td>
<td>Wikipedia&#8217;s <a href="https://en.wikipedia.org/wiki/List_of_people_with_locked-in_syndrome">List of people with locked-in syndrome</a>; <a href="http://link.springer.com/article/10.1007%2FBF00313105">Bauer et al. (1979)</a>; <a href="http://www.thomasnelson.com/ghost-boy">Pistorius (2013)</a>; <a href="http://link.springer.com/article/10.1007/s11097-013-9344-9">Kyselo &amp; Paolo (2015)</a><a class="see-footnote" id="footnoteref364_zwd1q8x" title="Locked-in syndrome is included in this table because even though people cannot give verbal report while they are locked in, there are cases in which people have provided verbal report of their experiences during locked-in syndrome after recovering out of a locked-in state.      " href="#footnote364_zwd1q8x">364</a></td>
</tr><tr><td>Terminal lucidity<a class="see-footnote" id="footnoteref365_173yta2" title="If terminal lucidity is genuine phenomenon, it might not be particularly interesting as a variation in phenomenal experience, but it might provide a (fleeting) opportunity to ask the patient what their subjective experience of less-lucid states of consciousness were like for them. Of course, less-fleeting opportunities are also available in cases of normal recovery from disorders of consciousness and other varieties of conscious experience, and these cases are far more numerous.      " href="#footnote365_173yta2">365</a></td>
<td><a href="http://www.sciencedirect.com/science/article/pii/S0167494311001865">Nahm et al. (2012)</a></td>
</tr><tr><td>Sight restoration following long-term congenital blindness</td>
<td><a href="http://journals.sagepub.com/doi/abs/10.1111/j.1467-9280.2006.01827.x">Ostrovsky et al. (2006)</a>; <a href="http://www.nature.com/neuro/journal/v14/n5/abs/nn.2795.html">Held et al. (2011)</a>; <a href="https://plato.stanford.edu/entries/molyneux-problem/">Degenaar &amp; Lokhorst (2014)</a></td>
</tr></table><p>These and other phenomena which reveal the variety of human conscious experience are collected in <a href="https://benjamins.com/#catalog/books/aicr.20/main">Kunzendorf &amp; Wallace (2000)</a>, <a href="http://psycnet.apa.org/journals/bul/131/1/98/">Vaitl et al. (2005)</a>, <a href="https://global.oup.com/academic/product/oxford-companion-to-consciousness-9780198569510?cc=us&amp;lang=en&amp;">Bayne et al. (2009)</a>, <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=iAknLrmZe2MC&amp;oi=fnd&amp;pg=PA229#v=onepage&amp;q&amp;f=false">Windt (2011)</a> pp. 238-244, <a href="http://www.abc-clio.com/ABC-CLIOCorporate/product.aspx?pc=A2707C">Cardeña &amp; Winkelman (2011)</a>, <a href="http://psycnet.apa.org/index.cfm?fa=browsePB.chapters&amp;pbid=14258">Cardeña et al. (2014)</a>, <a href="http://www.nature.com/nrneurol/journal/v10/n2/abs/nrneurol.2013.279.html">Giacino et al. (2014)</a>, <a href="https://books.google.com/books?id=ozUICwAAQBAJ&amp;lpg=PA57&amp;ots=35EvrIZ7DT&amp;lr&amp;pg=PA57#v=onepage&amp;q&amp;f=false">Bayne &amp; Hohwy (2016)</a>, <a href="http://www.sciencedirect.com/science/book/9780128009482">Laureys et al. (2015)</a>, <a href="https://global.oup.com/academic/product/the-varieties-of-consciousness-9780199846122?cc=us&amp;lang=en&amp;">Kriegel (2015)</a>; ch. 4 of <a href="https://www.routledge.com/Consciousness/Gennaro/p/book/9781138827714">Gennaro (2016)</a>, chs. 3-19 of <a href="https://benjamins.com/#catalog/books/aicr.36/main">Perry et al. (2002)</a>, and other sources.</p>
<p>Phenomena <em>not</em> included in the table above, because we don&#8217;t (to my knowledge) have human verbal self-report of what it is like to subjectively experience them, include:</p>
<ul><li><em>Possible consciousness detected via neuroimaging of vegetative state patients</em>: see e.g. <a href="http://www.annualreviews.org/doi/abs/10.1146/annurev-psych-113011-143729">Owen et al. (2013)</a>; <a href="http://bjps.oxfordjournals.org/content/early/2015/04/14/bjps.axv012.short">Klein (2015)</a>; <a href="http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002593">Chaudhary et al. (2017)</a>.</li>
<li><em>Hydranencephaly</em>: According to <a href="http://onlinelibrary.wiley.com/doi/10.1111/apa.12718/full">Aleman &amp; Merker (2014)</a>, a small number of persons with hydranencephaly seem to use words meaningfully, and a small (perhaps overlapping) number also survive into their teenage or later years. However, I doubt there are any cases in which hydranencephalics can verbally report some details of their subjective experiences (if they have any).<a class="see-footnote" id="footnoteref366_zjcki0m" title="I asked Merker via email, in August 2016, whether he thought there are likely to be any cases of hydranencephalics who are able to describe their subjective experiences. He said that such cases &quot;are not likely to exist as far as I am able to tell. These children – of any age – are utterly without anything that may be called human language. When a minority of [caregivers] answer [a survey question about word use] in the affirmative, they are referring to things like one or a few sounds that the child uses less than randomly, say ‘ba' when the mother is present. These may be associatively learned vocalizations, reinforced by delighted caregiver reactions, but do not amount to anything even close to propositional language. The cognitive state of these children is one of profound dementia…&quot;  " href="#footnote366_zjcki0m">366</a></li>
</ul><p><span style="float: right;">[<a href="/node/858/edit/90">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="AppendixZ3">Appendix Z.3. Challenging dualist intuitions</h4>
<p>I suspect that our dualist intuitions are the biggest barrier to embracing a physicalist, functionalist, illusionist theory of consciousness, which might otherwise be, as <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00005">Dennett (2016a)</a> puts it, “the obvious default theory of consciousness.”</p>
<p>Some sources that might be especially useful here include Dennett (<a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">1991</a>, <a href="http://books.wwnorton.com/books/detail.aspx?ID=4294992671">2017</a>), chapter 2 of <a href="https://mitpress.mit.edu/books/good-and-real">Drescher (2006)</a>, <a href="http://www.basicbooks.com/full-details?isbn=9780465020690">Metzinger (2010)</a>, and <a href="https://global.oup.com/academic/product/consciousness-and-the-social-brain-9780199928644?cc=us&amp;lang=en&amp;">Graziano (2013)</a>.</p>
<p>I think it can also be helpful to think about what kind of scientific progress might be needed for one to grok, at a “gut level,” how phenomenal consciousness could be “just” a set of physical processes and nothing more — even though the <em>feeling</em> of understanding is not necessarily strong evidence of <em>accurate</em> understanding (Trout <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1747-9991.2007.00081.x/full">2007</a>, <a href="https://global.oup.com/academic/product/wondrous-truths-9780199385072?cc=us&amp;lang=en&amp;">2016</a>). </p>
<p>In my experience, the subjective feeling of understanding often comes when I can “see” (visualize) how a system of processes I <em>already</em> understand could “add up to” the system I&#8217;m trying to understand. Eliezer Yudkowsky <a href="https://arbital.com/p/rescue_utility/">illustrates</a> this phenomenon with two examples, heat and socks:</p>
<blockquote><p>On a high level, we can see heat melting ice and flowing from hotter objects to cooler objects. We can, by imagination, see how vibrating particles could actually constitute heat rather than causing a mysterious extra ‘heat’ property to be present. Vibrations might flow from fast-vibrating objects to slow-vibrating objects via the particles bumping into each other and transmitting their speed. Water molecules vibrating quickly enough in an ice cube might break whatever bonds were holding them together in a solid object.</p>
<p>…</p>
<p>For an even more transparent reductionist identity, consider, “You&#8217;re not really wearing socks, there are no socks, there&#8217;s only a bunch of threads woven together that looks like a sock.” Your visual cortex can represent this identity directly, so it feels immediately [obvious] that the sock just is the collection of threads; when you imagine sock-shaped woven threads, you automatically feel your visual model recognizing a sock.</p>
<p>…</p>
<p>The gap between mind and brain is larger than the gap between heat and vibration, which is why humanity understood heat as disordered kinetic energy long before anyone had any idea how ‘playing chess’ could be decomposed into non-mental simpler parts [as when chess-playing computers were invented].</p></blockquote>
<p>Similarly, <a href="http://psycnet.apa.org/journals/cap/27/2/149/">Dennett (1986)</a> relates the following:</p>
<blockquote><p>Sherry Turkle… talks about the reactions small children have to computer toys when they open them up and look inside. What they see is just an absurd little chip and a battery and that&#8217;s all. They are baffled at how that could possibly do what they have just seen the toy do. Interestingly, she says they look at the situation, scratch their heads for a while, and then they typically say very knowingly: “It&#8217;s the battery!” (A grown-up version of the same fallacy is committed by the philosopher John Searle, 1980, when he, arriving at a similar predicament, says: “It&#8217;s the mysterious causal powers of the brain that explain consciousness.”) Suddenly facing the absurdly large gap between what we know from the inside about consciousness and what we see if we take off the top of somebody&#8217;s skull and look in can provoke such desperate reactions. When we look at a human brain and try to think of it as the seat of all that mental activity, we see something that is just as incomprehensible as the microchip is to the child when she considers it to be the seat of all the fascinating activity that she knows so well as the behavior of the simple toy.</p></blockquote>
<p>Unfortunately, current theories of consciousness aren&#8217;t yet detailed enough for most or all of us to visualize how the processes they posit could “add up to” subjective experience. (See also <a href="#AppendixB">Appendix B</a>.)</p>
<p>When I feel as though no amount of functional cognitive processing could ever “add up to” the phenomenality of phenomenal consciousness, I try to remind myself that some ancient biologists probably felt the same way when they tried to imagine how the interaction of inanimate, non-living parts could ever “add up to” living systems. I also remind myself of Edgar Allen Poe, in 1836, failing to see how mechanical parts could ever “add up to” the intelligent play of chess, <em>despite</em> his awareness of Charles Babbage&#8217;s <a href="https://en.wikipedia.org/wiki/Analytical_Engine">Analytical Engine</a>:</p>
<blockquote><p>Arithmetical or algebraical calculations are, from their very nature, fixed and determinate. Certain data being given, certain results necessarily and inevitably follow… But the case is widely different with the Chess-Player [i.e. von Kempelen&#8217;s “Mechanical Turk”]. With him there is no determinate progression. No one move in chess necessarily follows upon any one other. From no particular disposition of the [chess pieces] at one period of a game can we predicate their disposition at a different period… Now even granting that the movements of the Automaton Chess-Player were in themselves determinate, they would be necessarily interrupted and disarranged by the indeterminate will of his antagonist. There is then no analogy whatever between the operations of the Chess-Player, and those of the calculating machine of Mr. Babbage… It is quite certain that the operations of the Automaton are regulated by <em>mind</em>, and by nothing else. Indeed this matter is susceptible of a mathematical demonstration, <em>a priori</em>.<a class="see-footnote" id="footnoteref367_yzzkabp" title="Quoted from Poe (2014), pp. 369-370. I borrow the Poe example from Eliezer Yudkowsky.  " href="#footnote367_yzzkabp">367</a></p></blockquote>
<p>So, if you cannot now intuitively grok how phenomenal consciousness could be “just” a set of physical processes and nothing more, perhaps you can nevertheless take it to be a lesson of history that, once the mechanisms of consciousness are much more thoroughly understood and described than they are now, you may <em>then</em> be able to “see” how they add up to phenomenal consciousness, just as you can now see how some computer hardware and software can add up to intelligent chess play. (If you&#8217;re not familiar with how computers play chess, see e.g. <a href="https://books.google.com/books?id=61_lPgAACAAJ">Levy &amp; Newborn 1991</a>.)</p>
<p>Of course, nothing I&#8217;ve said in this section engages the arguments that have been put forward for why a physicalist, functionalist explanation of consciousness may not be forthcoming (e.g. Chalmers <a href="https://global.oup.com/academic/product/the-conscious-mind-9780195117899?cc=us&amp;lang=en&amp;">1997</a>, <a href="https://global.oup.com/academic/product/the-character-of-consciousness-9780195311105?cc=us&amp;lang=en&amp;">Chalmers 2010</a>). I don&#8217;t discuss those arguments in this report (see <a href="#Nature">above</a>);<a class="see-footnote" id="footnoteref368_ct13f5r" title="My own replies to these arguments are similar to those made by e.g. Dennett (1991); Carruthers &amp; Schier (2014).  " href="#footnote368_ct13f5r">368</a> the purpose of this Appendix Z.6s merely to give the reader a <em>sense</em> of why I distrust my dualistic intuitions about consciousness, and point to some recommended readings (see the first paragraph of this appendix).</p>
<p><span style="float: right;">[<a href="/node/858/edit/91">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="AppendixZ4">Appendix Z.4. Brief comments on unconscious emotions</h4>
<p><a href="#ProCRVsSummary">Previously</a>, I mentioned that research on unconscious emotions might lend support to some “cortex-required views” about consciousness. I did not investigate this literature thoroughly, but I make some brief comments on unconscious emotions below.</p>
<p>In common parlance, typical emotion words such as “fear” and “desire” and “excitement” refer to a particular kind of conscious experience in addition to a set of physiological and behavioral responses. However, most scientific studies of “emotion” do not measure (self-reported) conscious experience, but instead measure only physiological or behavioral responses. This is obviously true for studies of <em>animal</em> emotion, since animals cannot report conscious experiences. But it is also true of many studies of human emotion.<a class="see-footnote" id="footnoteref369_w2fmx8a" title="LeDoux (2015), ch. 2, provides one example:  In June 2014, a psychology website's headline read: &quot;Fear Center in Brain Larger Among Anxious Kids.&quot; The story that followed described a study that measured the level of anxiety in a large group of children based on a questionnaire answered by their parents. The brains of these children were then imaged and the findings related to the parents' assessments. The results showed that the larger the amygdala of the child, the higher the level of anxiety rated by the parents. Let's consider what this actually means. In this study the parents did what animal researchers often do: They based a conclusion about anxiety, an inner feeling, on observations of behavior — their child seemed nervous, edgy, or had trouble concentrating or sleeping. Thus, although the size of the amygdala might well correlate with certain behaviors, whether it was related to feelings of anxiety was not tested. The website's headline was inaccurate in three respects: (1) What was being measured was behavioral activity, not the feeling of anxiety; (2) the kids were not anxious in the clinical sense, in spite of some being described as &quot;anxious&quot; in the story; and (3) the amygdala is not the fear center (and certainly not the anxiety center) if by fear or anxiety we mean a conscious feeling.  " href="#footnote369_w2fmx8a">369</a> Hence, most scientific studies of “emotion,” especially in animals, don&#8217;t necessarily assume that “emotions” must involve conscious experiences.</p>
<p>Thus, in this report, I use “emotion” to refer to certain kinds of cognitive processing, physiological responses, and behavioral responses, which might or might not <em>also</em> involve conscious experience. But, in keeping with everyday usage, I will reserve the word “feelings,” and terms for specific emotions such as “fear,” for emotional responses that <em>do</em> involve certain kinds of conscious experiences. In contrast, when referring <em>only</em> to cognitive processing, physiological responess, and behavioral responses — e.g. as examined in animal studies — I will avoid consciousness-implying terms like “fear” in favor of more neutral terms like “threat response.”<a class="see-footnote" id="footnoteref370_h838g5h" title="My usage is inspired by the proposal in LeDoux (2015), ch. 2:  When scientifically discussing fear and anxiety, we should let the words &quot;fear&quot; and &quot;anxiety&quot; have their everyday meaning — namely, as descriptions of conscious experiences that people have when threatened by present or anticipated events. The scientific meaning will obviously go deeper and be more complex than the lay meaning, but both will refer to the same fundamental concept. In addition, we should avoid using these words that refer to conscious feeling when discussing systems that nonconsciously detect threats and control defense responses to them.  Thus, rather than saying that fear stimuli activate a fear system to produce fear responses, we should state that threat stimuli elicit defense responses via activation of a defensive system. Because &quot;threat&quot; and &quot;defense&quot; are not terms derived specifically from human subjective experiences, using them would go a long way toward making it easier to distinguish brain mechanisms underlying the conscious feeling of being afraid or anxious from mechanisms that detect and respond to actual or perceived danger. Similarly, what we now call fear conditioning can simply be called what it is: threat conditioning. So, in place of &quot;fear CSs&quot; and &quot;fear CRs,&quot; we can refer instead to &quot;threat CSs&quot; and &quot;defensive CRs&quot;…  " href="#footnote370_h838g5h">370</a></p>
<p>Under this terminology, an “unconscious emotion” is an emotional response — involving certain kinds of cognitive processing, physiological response, and/or behavioral response — <em>without</em> an accompanying conscious experience of that emotion. This is analogous to the above discussion of “unconscious vision,” which involves certain kinds of cognitive processing and behavioral response without any conscious experience of that visual processing.</p>
<p>If humans exhibit genuinely unconscious emotions, and the conscious experience of emotion seems to depend on neural circuits in certain cortical structures, this could lend some further support to some “cortex-required” views.</p>
<p>So, what is the current state of the evidence? As far as I can tell, the matter of unconscious emotions remains under considerable debate.<a class="see-footnote" id="footnoteref371_p4tdsjo" title="For evidence and argument on the topic of unconscious emotions, see LeDoux (2015); Keltner et al. (2013), ch. 7; Amting et al. (2010); Dawkins (2015); Rose et al. (2014); Barrett et al. (2005); Hofree &amp; Winkielman (2012); Lewis (2013); Rolls (2013), ch. 10; Berridge &amp; Winkielman (2003); Winkielman &amp; Berridge (2004); Berridge &amp; Kringelbach (2016); Sato &amp; Aoki (2006); Tamietto &amp; de Gelder (2010); Hatzimoysis (2007); Anselme &amp; Robinson (2016); Dawkins (2017).  " href="#footnote371_p4tdsjo">371</a> Morever, my sense is that the neuroscience of emotions is less well-developed than the neuroscience of vision (likely, because neuroscience of vision is easier to study).</p>
<p>Below, I make some brief remarks related to just one example of “unconscious emotion”: namely, unconscious “pleasure.”</p>
<p>Perhaps the leading theory in the neuroscience of pleasure is the liking/wanting theory developed by <a href="http://www-personal.umich.edu/~berridge/">Kent Berridge</a> and others. In short, this theory claims that (<a href="https://books.google.com/books?id=cbKhDAAAQBAJ&amp;lpg=PP1&amp;ots=5CTpwx-vng&amp;lr&amp;pg=PA133#v=onepage&amp;q&amp;f=false">Berridge &amp; Kringelbach 2016</a>):</p>
<blockquote><p>Affective neuroscience studies have further indicated that even the simplest pleasant experience, such as a mere sensory reward, is actually a more complex set of processes containing several psychological components, each with distinguishable neurobiological mechanisms… These include, in particular, distinct components of reward <em>wanting</em> versus reward <em>liking</em> (as well as reward <em>learning</em>), and each psychological component has both conscious and nonconscious subcomponents. Liking is the actual pleasure component or hedonic impact of a reward; wanting is the motivation for reward; and learning includes the associations, representations, and predictions about future rewards based on past experiences.</p>
<p>We distinguish between the conscious and nonconscious aspects of these subcomponents because both exist in people… At the potentially nonconscious level, we use quotation marks to indicate that we are describing objective, behavioral, or neural measures of these underlying brain processes. As such, “liking” reactions result from activity in identifiable brain systems that paint hedonic value on a sensation such as sweetness, and produce observable affective reactions in the brain and in behavior such as facial expressions. Similarly, “wanting” includes incentive salience or motivational processes within reward that mirror hedonic “liking” and make stimuli into motivationally attractive incentives. “Wanting” helps spur and guide motivated behavior, when incentive salience is attributed to stimulus representations by the mesolimbic brain systems. Finally, “learning” includes a wide range of processes linked to implicit knowledge as well as associative conditioning, such as basic Pavlovian and instrumental associations.</p>
<p>…By themselves, core “liking” and “wanting” processes can occur nonconsciously, even in normal people.</p></blockquote>
<p><a href="http://www.sciencedirect.com/science/article/pii/S0896627315001336">Berridge &amp; Kringelbach (2015)</a> summarizes the evidence on unconscious vs. conscious “liking” and “wanting”:</p>
<blockquote><p>…in humans, the [unconscious and conscious] forms of hedonic reaction can be independently measured. For example, objective hedonic “liking” reactions can sometimes occur alone and unconsciously in ordinary people without any subjective pleasure feeling at all, at least in particular situations (e.g., evoked by subliminally brief or mild affective stimuli)… Unconscious “liking” reactions still effectively change goal-directed human behavior, though those changes may remain undetected or be misinterpreted even by the person who has them… More commonly, “liking” reactions occur together with conscious feelings of liking and provide a hedonic signal input to cognitive ratings and subjective feelings. However, dissociations between [unconscious and conscious] hedonic reaction[s] can still sometimes occur in normal people due to the susceptibility of subjective ratings of liking to cognitive distortions by framing effects, or as a consequence of theories concocted by people to explain how they think they should feel… For example, framing effects can cause two people exposed to the same stimulus to report different subjective ratings, if one of them had a wider range of previously experienced hedonic intensities (e.g., pains of childbirth or severe injury)… In short, there is a difference between how people feel and report subjectively versus how they objectively respond with neural or behavioral affective reactions. Subjective ratings are not always more accurate about hedonic impact than objective hedonic reactions and the latter can be measured independently of the former.</p></blockquote>
<p>I have not read the primary studies cited in these review articles, but my sense is that there is much less evidence concerning unconscious pleasure than there is concerning unconscious vision.</p>
<p><span style="float: right;">[<a href="/node/858/edit/92">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="AppendixZ5">Appendix Z.5. The lack of consensus in consciousness studies</h4>
<p>Here I use a small set of examples to illustrate the lack of consensus about several different aspects of consciousness.</p>
<p>Note that while I have made <em>some</em> effort to ensure that the sources cited below are “talking about the same thing” — phenomenal consciousness, as opposed to e.g. self-consciousness or the capacity for distinct waking/sleeping states — there is (perhaps unavoidably) much ambiguous language in the consciousness literature, and thus no doubt <em>some</em> of the apparent diversity of opinion results from experts talking about different things rather than having different views about “the same thing.” For example, some of the discussions below may have been intended as accounts of a limited class of phenomenal consciousness (e.g. <em>human</em> phenomenal consciousness), rather than as accounts account of all types of phenomenal consciousness.</p>
<p>Example disagreements in consciousness studies:</p>
<p><em>Is consciousness physical?</em> The largest survey of professional philosophers I&#8217;ve seen — the PhilPapers Survey, conducted in late 2009 (<a href="http://philpapers.org/surveys/results.pl">results</a>; <a href="http://link.springer.com/article/10.1007/s11098-013-0259-7">paper</a>) — found that among “Target Faculty” (i.e. “all regular faculty members in 99 leading departments of philosophy”), 56.5% of respondents accepted or leaned toward physicalism about the mind, 27.1% of respondents accepted or leaned toward non-physicalism about the mind, and 16.4% of respondents gave an “Other” response.</p>
<p><em>What is the extent of consciousness in the natural world / when did it evolve?</em> For overviews, see <a href="http://www.tandfonline.com/doi/abs/10.1080/21582041.2012.692099">Velmans (2012)</a> and <a href="http://www.springer.com/us/book/9789400754188">Swan (2013)</a>. Some specific possibilities include:</p>
<ul><li>Consciousness precedes the emergence of living systems (<a href="https://books.google.com/books?id=FgPWBgAAQBAJ&amp;lpg=PP1&amp;ots=u4GvpahnbAlr&amp;pg=PA246#v=onepage&amp;q&amp;f=false">Chalmers 2015</a>; <a href="http://reducing-suffering.org/is-there-suffering-in-fundamental-physics/">Tomasik 2016b</a>; <a href="https://mospace.umsystem.edu/xmlui/handle/10355/49763">Howe 2015</a>).</li>
<li>Consciousness evolved at least as early as some single-celled organisms (<a href="http://www.tandfonline.com/doi/abs/10.1080/03081079.2014.920999">Baluškaa &amp; Mancuso 2014</a>; <a href="http://psycnet.apa.org/journals/cns/2/4/377/">Braun 2015</a>; and for context see <a href="http://journal.frontiersin.org/article/10.3389/fmicb.2015.00264/full">Lyon 2015</a> and <a href="http://yalebooks.com/book/9780300167849/wetware">Bray 2011</a>).</li>
<li>Consciousness evolved at least as early as some plants (<a href="http://www.ingentaconnect.com/content/imp/jcs/1997/00000004/00000003/760">Nagel 1997</a>; <a href="http://link.springer.com/chapter/10.1057/9781137554895_2">Smith 2016</a>).</li>
<li>Consciousness evolved ~520 million years ago, during or just before the Cambrian explosion (<a href="https://mitpress.mit.edu/books/ancient-origins-consciousness">Feinberg &amp; Mallatt 2016</a>; <a href="http://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_00623">Graziano 2014</a>; <a href="http://link.springer.com/article/10.1162/biot.2007.2.3.231">Ginsburg &amp; Jablonka 2007</a>).</li>
<li>Consciousness evolved at least as early as some insects (<a href="http://www.pnas.org/content/113/18/4900.short">Barron &amp; Klein 2016</a>; <a href="https://sentience-politics.org/research/policy-papers/invertebrate-suffering/">Knutsson 2016</a>; maybe also <a href="http://www.sciencedirect.com/science/article/pii/S1053810003000023">Merker 2005</a>).</li>
<li>Consciousness evolved after fishes but before birds and mammals (<a href="http://www.sciencedirect.com/science/article/pii/S0166432808006542">Cabanac et al. 2009</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/faf.12010/full">Rose et al. 2014</a>; <a href="http://animalstudiesrepository.org/animsent/vol1/iss3/1/">Key 2016</a>).</li>
<li>Consciousness evolved very late, in humans and perhaps in some apes (<a href="https://books.google.com/books?id=7D40mLDJco8C&amp;lpg=PP1&amp;pg=PA253#v=onepage&amp;q&amp;f=false">Macphail 2000</a>; <a href="http://www.cambridge.org/nu/academic/subjects/philosophy/philosophy-mind-and-language/phenomenal-consciousness-naturalistic-theory?format=HB">Carruthers 2000</a>; <a href="http://www.jstor.org/stable/40971115">Dennett 1995</a>).</li>
</ul><p><em>Was consciousness selected for, or is it a spandrel?</em> For an overview of the debate, see <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=9789883&amp;fileId=S2053447714000104">Robinson et al. (2015)</a>.</p>
<p><em>What is the functional role of consciousness?</em> <a href="http://www.basicbooks.com/full-details?isbn=9780465020690">Metzinger (2010)</a>, p. 55, summarizes some of the proposed options:</p>
<blockquote><p>Today, we have a long list of potential candidate functions of consciousness: Among them are the emergence of intrinsically motivating states, the enhancement of social coordination, a strategy for improving the internal selection and resource allocation in brains that got too complex to regulate themselves, the modification and interrogation of goal hierarchies and long-term plans, retrieval of episodes from long-term memory, construction of storable representations, flexibility and sophistication of behavioral control, mind reading and behavior prediction in social interaction, conflict resolution and troubleshooting, creating a densely integrated representation of reality as a whole, setting a context, learning in a single step, and so on.</p></blockquote>
<p><em>Which theory of consciousness is most likely correct?</em> Lists, taxonomies, and collections of theories of consciousness include chapter 3 of <a href="https://global.oup.com/academic/product/consciousness-9780199277360?cc=us&amp;lang=en&amp;">Carruthers (2005)</a>, <a href="http://www.ingentaconnect.com/content/imp/jcs/2013/00000020/F0020005/art00003">Katz (2013)</a>, <a href="http://www.springer.com/us/book/9783662440872">Cavanna &amp; Nani (2014)</a>, <a href="http://ebooks.cambridge.org/chapter.jsf?bid=CBO9780511816789&amp;cid=CBO9780511816789A018">Sun &amp; Franklin (2007)</a>, <a href="http://ebooks.cambridge.org/chapter.jsf?bid=CBO9780511816789&amp;cid=CBO9780511816789A020&amp;tabName=Chapter">McGovern &amp; Baars (2007)</a>, <a href="http://plato.stanford.edu/entries/consciousness/">Van Gulick (2014)</a>, chs. 10-11 of <a href="https://www.routledge.com/Consciousness-The-Science-of-Subjectivity/Revonsuo/p/book/9781841697260">Revonsuo (2009)</a>, and Part III of <a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-1405120193.html">Velmans &amp; Schneider (2007)</a>. Some introductory texts on consciousness also capably survey the leading theories, e.g. <a href="http://www.polity.co.uk/book.asp?ref=9780745653440">Weisberg (2014)</a>.</p>
<p>Finally, to illustrate the methodological diversity of consciousness studies, compare:</p>
<ul><li>David Chalmers’ metaphysical thought experiments (Chalmers <a href="https://global.oup.com/academic/product/the-conscious-mind-9780195117899?cc=us&amp;lang=en&amp;">1997</a>, <a href="https://global.oup.com/academic/product/the-character-of-consciousness-9780195311105?cc=us&amp;lang=en&amp;">2010</a>).</li>
<li>The largely science-driven arguments of <a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">Dennett (1991)</a>, <a href="https://mitpress.mit.edu/books/being-no-one">Metzinger (2003)</a>, <a href="http://www.sciencedirect.com/science/article/pii/S1053810003000023">Merker (2005)</a>, <a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>, <a href="http://www.penguin.com/book/consciousness-and-the-brain-by-stanislas-dehaene/9780143126263">Dehaene (2014)</a>, and <a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye (2016)</a>, chs. 5-9.</li>
<li>The paradigms for probing human consciousness discussed in sources such as <a href="https://benjamins.com/#catalog/books/aicr.92/main">Miller (2015)</a>, <a href="https://global.oup.com/academic/product/behavioural-methods-in-consciousness-research-9780199688890?cc=us&amp;lang=en&amp;">Overgaard (2015)</a>, <a href="https://global.oup.com/academic/product/sight-unseen-9780199596966">Goodale &amp; Milner (2013)</a>, <a href="https://www.elsevier.com/books/the-neurology-of-consciousness/laureys/978-0-12-801175-1">Laureys et al. (2015)</a>, and <a href="https://global.oup.com/academic/product/visual-masking-9780198530671?cc=us&amp;lang=en&amp;">Breitmeyer &amp; Ogmen (2006)</a>.</li>
<li>Consciousness-relevant investigations in comparative ethology and comparative neuroanatomy, such as can be found in <a href="https://global.oup.com/academic/product/cognition-evolution-and-behavior-9780195319842?cc=us&amp;lang=en&amp;">Shettleworth (2009)</a>, <a href="https://global.oup.com/academic/product/the-oxford-handbook-of-comparative-evolutionary-psychology-9780199738182?cc=us&amp;lang=en&amp;">Vonk &amp; Shackelford (2012)</a>, <a href="https://www.routledge.com/Animal-Learning-and-Cognition-3rd-Edition-An-Introduction-3rd-Edition/Pearce/p/book/9781841696560">Pearce (2008)</a>, <a href="https://he.palgrave.com/page/detail/animal-cognition-clive-d-l.wynne/?sf1=barcode&amp;st1=9781137367297">Wynne &amp; Udell (2013)</a>, and <a href="https://mitpress.mit.edu/books/human-advantage">Herculano-Houzel (2016)</a>.</li>
<li><a href="http://arxiv.org/abs/0802.4121">Standish (2013)</a>’s anthropic argument against ant consciousness.</li>
<li>Brian Tomasik&#8217;s arguments for (what I call) “panpsychism about consciousness via pan-everythingism about everything.”<a class="see-footnote" id="footnoteref372_tk6w6un" title="[TODO: add link to conversation notes when they are published]  " href="#footnote372_tk6w6un">372</a></li>
</ul><p>Obviously, this list of examples if far from exhaustive.</p>
<p><span style="float: right;">[<a href="/node/858/edit/93">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="AppendixZ6">Appendix Z.6. Against hasty eliminativism</h4>
<p>Earlier, I <a href="#eliminativism">argued</a> against “hasty” eliminativism. Here, I develop this line of thinking a bit further.</p>
<p>I agree with a great deal of <a href="http://www.springer.com/us/book/9789400751729">Irvine (2013)</a>’s survey of the scientific and measurement difficulties currently obstructing progress in consciousness studies, but whereas Irvine is ready to embrace eliminativism about consciousness, I prefer to wait to see how our concept of consciousness evolves in response to empirical discoveries, and decide later whether we want to modify the concept or toss it in the trash next to “phlogiston.”</p>
<p>One reason to resist quick elimination of “consciousness” is articulated nicely by <a href="https://mitpress.mit.edu/books/consciousness-reconsidered">Flanagan (1992)</a>, pp. 23-24:</p>
<blockquote><p>Two formidable arguments for [eliminating] consciousness involve attempts to secure an analogy between the concept of consciousness and some other concept in intellectual ruin. It has been suggested that the concept of consciousness is like the concept of phlogiston or the concept of karma. One shouldn&#8217;t think in terms of such concepts as phlogiston or karma. And it would be a philosophical embarrassment to try to develop a positive theory of karma or phlogiston. There simply are no such phenomena for there to be theories about. Let me explain why the analogies with karma and phlogiston do not work to cast doubt on the existence of consciousness or on the usefulness of the concept.</p>
<p>…There is no single orthodox concept of consciousness. Currently afloat in intellectual space are several different conceptions of consciousness, many of them largely inchoate. The <em>Oxford English Dictionary</em> lists 11 senses for ‘conscious’ and 6 for ‘consciousness’. The meanings cover self-intimation, being awake, phenomenal feel, awareness of one&#8217;s acts, intentions, awareness of external objects, and knowing something with others. The picture of consciousness as a unified faculty has no special linguistic privilege, and none of the meanings of either ‘conscious’ or ‘consciousness’ wear any metaphysical commitment to immaterialism on its sleeve. The concept of consciousness is neither unitary nor well regimented, at least not yet.</p>
<p>This makes the situation of the concept of consciousness in the late twentieth century very different from that of the concept of phlogiston in the 1770s. I don&#8217;t know if ordinary folk had any views whatsoever about phlogiston at that time. But the concept was completely controlled by the community of scientists who proposed it in the late 1600s and who characterized phlogiston as a colorless, odorless, and weightless “spirit” that is released rapidly in burning and slowly in rusting. Once the spirit is fully released, we are left with the “true material,” a pile of ash or rust particles.</p>
<p>…if I am right that the concept of consciousness is simply not owned by any authoritative meaning-determining group in the way the concept of phlogiston was owned by the phlogiston theorists, then it will be harder to isolate any single canonical concept of consciousness that has recently come undone or is in the process of coming undone, and thus that deserves the same tough treatment that the concept of phlogiston received.</p></blockquote>
<p>For additional elaboration of (something like) my reasons for resisting quick eliminativism about consciousness (among other concepts), see e.g. <a href="http://link.springer.com/article/10.1007/s10670-011-9340-9">Chang (2011)</a>, <a href="http://link.springer.com/article/10.1007/s10670-011-9344-5">Arabatzis (2011)</a>, <a href="http://www.sciencedirect.com/science/article/pii/S1369848613001325">Ludwig (2014)</a>, <a href="http://link.springer.com/article/10.1007/s10670-015-9793-3">Ludwig (2015)</a>, and <a href="http://link.springer.com/article/10.1007/s13194-016-0136-2">Taylor &amp; Vickers (2016)</a>. In particular, I want to highlight my agreement with <a href="http://www.sciencedirect.com/science/article/pii/S1369848613001325">Ludwig (2014)</a> that</p>
<blockquote><p>Scientific ontologies are constantly changing through the introduction of new entities and the elimination of old entities that have become obsolete… The ubiquity of elimination controversies in the human sciences raises the general but rarely discussed… question [of when] scientists should eliminate an entity from their ontology. Typically, elimination controversies focus on one specific entity and consider other cases of ontological elimination only briefly through analogies to obsolete entities in the history of science such as the élan vital, ether, phlogiston, phrenological organs, or even witchcraft… I want to argue that this situation is unfortunate as it often leads to the implicit use of an oversimplified “phlogiston model” of ontological elimination… that proves inadequate for many debates in the human sciences… [I propose] a more complex model that interprets ontological elimination as typically located on gradual scale between criticism of empirical assumptions and conceptual choices…</p></blockquote>
<p>Another way to express my attitude on the matter is to express sympathy for the view that <a href="http://www.sciencedirect.com/science/article/pii/S1053810083710020">Baars &amp; McGovern (1993)</a> attribute to “average cognitive psychologists”:</p>
<blockquote><p>…if we were to compel the average cognitive psychologists to give an opinion on the matter, we would no doubt hear something like… “Yes, of course, we will ultimately be able to explain human behavior and experience in neural terms. In the meantime, it is useful, and perfectly good science, to work at a higher level of analysis, in which we postulate mental events such as thoughts, percepts, goals, and the like.”</p></blockquote>
<p><span style="float: right;">[<a href="/node/858/edit/94">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="AppendixZ7">Appendix Z.7. Some candidate dimensions of moral concern</h4>
<p>In this appendix I briefly remark on some candidate dimensions of moral concern that could be combined to estimate the relative “moral weight” of a species or other taxon of cognitive system, as mentioned briefly <a href="#WhyWeCare">above</a>.</p>
<p>Many commonly-discussed candidate dimensions of moral concern are captured by theories of well-being. <a href="http://plato.stanford.edu/entries/well-being/">Crisp (2013)</a> organizes philosophical theories of well-being into three categories: <em>hedonistic theories</em> according to which well-being is the presence of pleasure and the absence of pain, <em>desire theories</em> according to which well-being is getting what one wants, and <em>objective list theories</em> according to which well-being is the presence or absence of certain objective characteristics (potentially including hedonistic and desire-related characteristics). <a href="https://www.routledge.com/products/9780415714532">Fletcher (2015)</a> offers a different categorization, including chapters on hedonistic theories, perfectionistic theories, desire-fulfillment theories, objective list theories, hybrid theories, subject-sensitive theories, and eudaimonistic theories. In the social sciences, human objective well-being is often measured using variables such as education, health status, personal security, income, and political freedom (see e.g. <a href="http://www.oecd-ilibrary.org/economics/how-was-life_9789264214262-en">OECD 2014</a>), while human subjective well-being is typically conceived of in terms of life satisfaction, hedonic affect, and eudaimonia (psychological “flourishing”): see e.g. <a href="http://www.oecd-ilibrary.org/economics/oecd-guidelines-on-measuring-subjective-well-being_9789264191655-en">OECD (2013)</a>. For another overview of several approaches to well-being, see the chapters in Part II of <a href="https://global.oup.com/academic/product/the-oxford-handbook-of-well-being-and-public-policy-9780199325818?cc=us&amp;lang=en&amp;">Adler &amp; Fleurbaey (2016)</a>.<a class="see-footnote" id="footnoteref373_bo7xgba" title="For a &quot;network theory of well-being&quot; that integrates both subjective and objective factors, see Bishop (2015). Lin (2015) proposes a subjective list theory of well-being. See Daswani &amp; Leike (2015) for a suggested formal definition of well-being for reinforcement learning agents, and see Oesterheld (2016) for an example preliminary formalization of preference satisfaction.  " href="#footnote373_bo7xgba">373</a></p>
<p>Some candidate dimensions of moral concern captured by these theories of well being — e.g. pain and pleasure — are widely thought to vary in their moral importance depending on other parameters such as “intensity” and duration.<a class="see-footnote" id="footnoteref374_59ab739" title="See e.g. the discussion of intensity and other factors in Fakezas &amp; Overgaard (2016).  " href="#footnote374_59ab739">374</a> With respect to duration, there is also a question about objective vs. subjective duration.<a class="see-footnote" id="footnoteref375_dwgsp2x" title="See e.g. Tomasik (2016a), Lee (2014), Phillips (2014), and the following points from Hanson (2016), ch. 6, about processing speed and body size:  The natural oscillation periods of most consciously controllable human body parts are greater than a tenth of a second. Because of this, the human brain has been designed with a matching reaction time of roughly a tenth of a second. As it costs more to have faster reaction times, there is little point in paying to react much faster than body parts can change position.  …the first resonant period of a bending cantilever, that is, a stick fixed at one end, is proportional to its length, at least if the stick's thickness scales with its length. For example, sticks twice as long take twice as much time to complete each oscillation. Body size and reaction time are predictably related for animals today… [Healy et al. (2013)]  " href="#footnote375_dwgsp2x">375</a></p>
<p>Another set of candidate dimensions of moral concern involve various ways in which consciousness can be “unified” or “disunified.” Following <a href="https://global.oup.com/academic/product/the-unity-of-consciousness-9780199215386">Bayne (2010)</a>’s taxonomy (ch. 1), we might consider the moral relevance of “subject unity,” “representational unity,” and “phenomenal unity” — each of which has a “synchronic” (momentary) and “diachronic” (across time) aspect (see footnote<a class="see-footnote" id="footnoteref376_9absl8e" title="Here is Bayne on subject unity:  My conscious states possess a certain kind of unity insofar as they are all mine; likewise, your conscious states possess that same kind of unity insofar as they are all yours. We can describe conscious states that are had by or belong to the same subject of experience as subject unified. Within subject unity we need to distinguish the unity provided by the subject of experience across time (diachronic unity) from that provided by the subject at a time (synchronic unity).  On representational unity, Bayne says:  Let us say that conscious states are representationally unified to the degree that their contents are integrated with each other. Representational unity comes in a variety of forms. A particularly important form of representational unity concerns the integration of the contents of consciousness around perceptual objects—what we might call ‘object unity'. Perceptual features are not normally represented by isolated states of consciousness but are bound together in the form of integrated perceptual objects. This process is known as feature-binding. Feature-binding occurs not only within modalities but also between them, for we enjoy multimodal representations of perceptual objects.  And on phenomenal unity, he says:  Subject unity and representational unity capture important aspects of the unity of consciousness, but they don't get to the heart of the matter. Consider again what it's like to hear a rumba playing on the stereo whilst seeing a bartender mix a mojito. These two experiences might be subject unified insofar as they are both yours. They might also be representationally unified, for one might hear the rumba as coming from behind the bartender. But over and above these unities is a deeper and more primitive unity: the fact that these two experiences possess a conjoint experiential character. There is something it is like to hear the rumba, there is something it is like to see the bartender work, and there is something it is like to hear the rumba while seeing the bartender work. Any description of one's overall state of consciousness that omitted the fact that these experiences are had together as components, parts, or elements of a single conscious state would be incomplete. Let us call this kind of unity — sometimes dubbed ‘co-consciousness' — phenomenal unity.  Phenomenal unity is often in the background in discussions of the ‘stream' or ‘field' of consciousness. The stream metaphor is perhaps most naturally associated with the flow of consciousness — its unity through time — whereas the field metaphor more accurately captures the structure of consciousness at a time. We can say that what it is for a pair of experiences to occur within a single phenomenal field just is for them to enjoy a conjoint phenomenality — for there to be something it is like for the subject in question not only to have both experiences but to have them together. By contrast, simultaneous experiences that occur within distinct phenomenal fields do not share a conjoint phenomenal character.  " href="#footnote376_9absl8e">376</a>. For example, Daniel Dennett seems to appeal to some kinds of disunity when explaining why he doesn&#8217;t worry much about the conscious experiences of most animals (if they have any).<a class="see-footnote" id="footnoteref377_grsi8o6" title="In a 2004 interview conducted by Keith Frankish (audio; transcript), Dennett said:  Animals are of course awake, they can feel pain, and they can experience pleasure, but they can't, I think, …dwell on things the way we can. They can't shift their attention the way we can. They can't reflect on things the way we can. That sort of recursive, reflective mulling over and letting one thing remind you of something else and so forth, and being able to control that to some degree — that's what animals (I think) can't do: not chimpanzees, not dolphins, not dogs… their consciousness is so disunified, so fragmented, so impoverished compared to ours, that to call them conscious is almost certainly to misimagine their circumstances.   …I like to ask people, &quot;What is it like to be a brace of oxen, a pair of oxen yoked together?&quot; And they say &quot;Well, it's not like anything, of course. I mean it's like something to be one ox, and it's like something to be the other ox — the left and the one on the right — but it's not like anything to be a brace of oxen, because they aren't unified in the right way.&quot; Well, but you'd be amazed [at] the extent of which many animals are like a brace of oxen, [at] how much disunity is possible in a mammalian nervous system. It's this further unification which is the fruits of the Joycean machine [Dennett's term for the &quot;virtual machine&quot; that he thinks enables human consciousness; see Dennett (1991)] that gets installed on us.  What is it like to be an ant colony? Well, it's not like anything to be an ant colony, even if it's like something to be an individual ant, so people think. Well, stop and think. A brain is composed of billions of neurons, each one of those is a lot stupider than an ant. They happen to be enclosed in a skull and their inter-communications are rich but of the same sort that is possible between one ant and another. Now if we opened up somebody's head and we found inside, not neurons but millions of little ants, maybe we would say, &quot;Oh gosh, maybe it's not like anything to be this person.&quot; Well, an ant colony can exhibit a lot of the same unified behaviour, a lot of the same protracted projects… that an organism inside a skin can exhibit.   Now if you think it's pretty obvious that an ant colony is not something that is itself conscious… then you should be at least willing to entertain the hypothesis that a bird is just as unconscious as an ant colony is. Now I'm deliberately setting the bar high, forcing the burden of proof onto those who say, it's just obvious that, say, other mammals (at least) are conscious the way we are. I say, &quot;No, it's not obvious, prove it.&quot; And the more we learn about specific organisms — that's why you have to do the science — the more we find out that a lot of things that are obvious to philosophers in the armchair are just false.   What is it like to be a rabbit? Well you may think that it's obvious that rabbits have an inner life that's something like ours. Well it turns out that if you put a patch over a rabbit's left eye and train it in a particular circumstance to be (say) afraid of something, and then you move the patch to the right eye, so that the very same signal, the very same circumstance that it has been trained to be afraid of, now is coming in the other eye, you have a naive rabbit, because in the rabbit brain the connections that are standard in our brains just aren't there, there isn't that unification. What is it like to be which rabbit? The rabbit on the left, or the rabbit on the right? The disunity in a rabbit's brain is stunning when you think about it, and you just haven't tested many species to see just how disunified they can be. The answer is they can be quite disunified.  See also his earlier Dennett (1995).  I asked Dennett which rabbit study he was referring to in the quote above, and he pointed me to a research abstract that I have not read: Ian Steele-Russell's &quot;The absence of interhemispheric communication in the rabbit&quot; in the Society for Neuroscience's Abstracts, Volume 20 (1994): 414.11. (This abstract is in Part 2 of Volume 20; Part 1 runs through 383.20).  This type of study is known as an &quot;interocular transfer&quot; study. For reviews of such studies — and other studies of interhemispheric transfer — in a variety of species, see e.g. Steele-Russell et al. (1979).  Here is but one bizarre-seeming result from this literature, reported in Remy &amp; Watanabe (1993):  …an ubiquitous and particularly striking result in many [interocular transfer] studies [in birds] is the &quot;mirror image reversal&quot; effect (MIR). Thus, when pigeons were trained monocularly on a mirror-image discrimination (e.g., 45° vs. 135° oblique lines) and then exposed to the stimuli with the untrained eye, they preferred the previously negative… stimulus [Mello (1965)].  " href="#footnote377_grsi8o6">377</a> </p>
<p>In a cost-benefit framework, one&#8217;s estimates concerning the moral weight of various taxa are likely more important than one&#8217;s estimated probabilities of the moral patienthood of those taxa. This is because, for the range of possible moral patients of most interest to us, it seems very hard to justify probabilities of moral patienthood much lower than 1% or much higher than 99%. In contrast, it seems quite plausible that the moral weights of different sorts of beings could differ by several others of magnitude. Unfortunately, estimates of moral weight are trickier to make, and in many senses depend upon, one&#8217;s estimates concerning moral patienthood.</p>
<p><span style="float: right;">[<a href="/node/858/edit/95">Edit this section</a>]</span><br /></p><div class='toc-back-to-top'><a href='#toc'>Back to Top</a></div><h4 id="AppendixZ8">Appendix Z.8. Some reasons for my default skepticism of published studies</h4>
<p>In several places throughout this report, I&#8217;ve mentioned my relatively high degree of skepticism about the expected reliability of consciousness-related scientific studies (e.g. on animal behavior, or self-reported human consciousness) that I have cited but not personally examined beyond a cursory look. Here, I provide some examples of published findings or personal research experiences that have led me to become unusually skeptical about the likely robustness of most published scientific studies (that I haven&#8217;t examined personally):</p>
<ol><li><a href="http://science.sciencemag.org/content/349/6251/aac4716">Nosek et al. (2015)</a> conducted high-powered replications of 100 studies from three top-ranked psychology journals, and found that “the mean effect size (r) of the replication effects… was half the magnitude of the mean effect size of the original effects,” 97% of the original studies had significant results (P &lt; .05) whereas only 36% of replications had significnt results, and only “39% of effects were subjectively rated to have replicated the original result.” Attempts to downplay the significance of this result have <a href="http://andrewgelman.com/2016/03/09/bruised-and-battered-i-couldnt-tell-what-i-felt-i-was-ungeneralizable-to-myself/">not been persuasive</a> (to me).</li>
<li>This “replication crisis” is not limited to psychology. Similarly discouraging results have been observed across many studies in cancer biology (<a href="http://www.nature.com/nrd/journal/v10/n9/full/nrd3439-c1.html">Prinz et al. 2011</a>; <a href="http://www.nature.com/nature/journal/v483/n7391/full/483531a.html">Begley et al. 2012</a>; <a href="http://www.sciencemag.org/news/2017/01/rigorous-replication-effort-succeeds-just-two-five-cancer-papers">Kaiser 2017</a>), economics (<a href="http://science.sciencemag.org/content/351/6280/1433">Camerer et al. 2016</a>; <a href="https://econjwatch.org/articles/replications-in-economics-a-progress-report?ref=articles">Duvendack et al. 2015</a>; <a href="http://journals.sagepub.com/doi/abs/10.1177/056943459203600106">Hubbard &amp; Vetter 1992</a>), genetics (<a href="http://www.nature.com/gim/journal/v4/n2/abs/gim200210a.html">Hirschhorn et al. 2002</a>; <a href="http://www.nature.com/ejhg/journal/v18/n7/full/ejhg201026a.html">Siontis et al. 2010</a>; <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3592970/">Benjamin et al. 2012</a>; <a href="http://www.nature.com/mp/journal/v20/n5/full/mp201516a.html">Farrell et al. 2015</a>), marketing (<a href="http://www.sciencedirect.com/science/article/pii/0167811694900035">Hubbard &amp; Armstrong 1994</a>), forecasting (<a href="http://www.sciencedirect.com/science/article/pii/S0169207009001393">Evanschitzky &amp; Armstrong 2010</a>), and other fields. For comments on how the replication crisis and some of the specific issues listed below may affect neuroimaging studies in particular, see e.g. <a href="http://www.nature.com/nrn/journal/v18/n2/full/nrn.2016.167.html">Poldrack et al. (2017)</a> and <a href="https://books.google.com/books?id=VMbXDQAAQBAJ&amp;lpg=PA196&amp;ots=js2Q-GfBZY&amp;lr=lang_en&amp;pg=PA196#v=onepage&amp;q&amp;f=false">Vul &amp; Pashler (2017)</a>.</li>
<li>Similar results seem to often hold beyond the domain of strict replications. For example, a systematic review (<a href="http://jamanetwork.com/journals/jama/fullarticle/201218">Ioannidis 2005a</a>) of highly-cited, top-journal clinical research studies published from 1990-2003 found that, among those 34 studies which could be compared against a later study (or meta-analysis) of the same questions and comparable or larger sample size <em>or</em> a better-controlled design, 41% were either contradicted by subsequent studies or found noticeably smaller effects than subsequent studies did. (The other 59% were supported by the later studies.)</li>
<li>Or, consider the case of “medical reversal,” in which a current “standard of care” medical therapy, which has already undergone enough testing to be widely provided — sometimes to millions of patients and at the cost of billions of dollars — is found (usually via a large randomized controlled trial) to actually be <em>inferior</em> to a lesser or prior standard of care (e.g. not effective at all, and sometimes even harmful). In a systematic review of published articles testing a current standard of care, 40.2% of these studies resulted in medical reversal (<a href="http://www.sciencedirect.com/science/article/pii/S0025619613004059">Prasad et al. 2013</a>; <a href="https://jhupbooks.press.jhu.edu/content/ending-medical-reversal">Prasad &amp; Cifu 2015</a>).</li>
<li>Many (most?) published studies suffer from low power. It is commonly thought (and taught) that low power is a problem primarily because it can lead to a failure to detect true effects, resulting in a waste of research funding. But low power also has two arguably more insidious consequences: it increases the probability of false positives, and it exaggerates the size of measured true effects (<a href="http://www.nature.com/nrn/journal/v14/n5/full/nrn3475.html">Button et al. 2013</a>). A systematic review of power issues in 10,000 papers in psychology and cognitive neuroscience suggested that the rate of false positives for this literature likely exceeds 50%. The problem seems to be especially bad in cognitive neuroscience (<a href="http://biorxiv.org/content/early/2016/08/25/071530">Szucs &amp; Ioannidis 2016</a>), with obvious implications for the scientific study of consciousness.</li>
<li>Even merely re-analyzing the data from a study can often produce a different result. For example, in a systematic review of reanalyses of data from randomized clinical trials, 35% of the re-analyses “led to interpretations different from that of the original article” (<a href="http://jamanetwork.com/journals/jama/fullarticle/1902230">Ebrahim et al. 2014</a>).</li>
<li>In my experience, when I read review articles in the life and social sciences, and then examine the primary evidence myself, I almost always come away with a lower opinion of the strength of the reported evidence than is suggested in the relevant review articles. For example, compare the review articles cited in my report on <a href="/behavioral-treatments-insomnia">behavioral treatments for insomnia</a> to my own conclusions in that report.</li>
<li>Given the strong incentives to publish positive results, and the many available methods for doing so even in the absence of “true” positive results (via “researcher degrees of freedom”<a class="see-footnote" id="footnoteref378_dp78o80" title="On &quot;researcher degrees of freedom,&quot; see Simmons et al. (2011), Gelman &amp; Loken 2013, Simonsohn et al. (2015), Wicherts et al. (2016), and FiveThirtyEight's p-hacking tool.  " href="#footnote378_dp78o80">378</a>), some simulations<a class="see-footnote" id="footnoteref379_lz9ole9" title="E.g. Ioannidis 2005b; Simmons et al. 2011; Nissen et al. 2016; Jørgensen et al. 2016; Smaldino &amp; McElreath 2016.  " href="#footnote379_lz9ole9">379</a> suggest that we should expect many (perhaps most) published research findings to be false. Moreover, researchers do seem to make extensive use of these researcher degrees of freedom: for example, in a systematic review of 241 fMRI studies, there were “nearly as many unique analysis pipelines as there were studies in the sample” (<a href="http://www.sciencedirect.com/science/article/pii/S1053811912007057">Carp 2012</a>).</li>
<li>In my experience, most top-journal primary studies and meta-analyses in the life and social sciences (that I read closely) turn out to rely heavily on statistical techniques that are inappropriate given the design of the study and/or the nature of the underlying data. Here is one example: in the life sciences and social sciences, the most common algorithm for quantitative synthesis of effect sizes from multiple primary studies is probably the DerSimonian-Laird (DL) algorithm (<a href="http://www.sciencedirect.com/science/article/pii/0197245686900462">DerSimonian &amp; Laird 1986</a>), even though (a) it was never shown (in simulations) to be appropriate for use in most of the situations for which it is used<a class="see-footnote" id="footnoteref380_36u26xj" title="My claim about &quot;most of the situations for which it is used&quot; is just a guess, based on my own experience reading or skimming hundreds of meta-analyses which use the DL algorithm, across many different fields in the life and social sciences.  " href="#footnote380_36u26xj">380</a> (e.g. when primary studies vary greatly in sample size, or when primary studies are few and small and heterogeneous), (b) better-performing algorithms are available (e.g. <a href="http://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-25">IntHout et al. 2014</a>), and (c) even the authors of the algorithm seem to concede it is not appropriate for establishing the statistical significance of summary effects (<a href="http://www.sciencedirect.com/science/article/pii/S1551714415300781">DerSimonian &amp; Laird 2015</a>, p. 142). Here is a second example: when studying the literature on subjective well-being, I learned (from <a href="http://journals.sagepub.com/doi/abs/10.1177/0146167206287721">Cranford et al. 2006</a>) that studies using ecological momentary assessment (EMA) had, for decades, typically used statistical tests appropriate for studying between-person differences, whereas EMA focuses on within-person changes. A long-time leader in the field confirmed this in conversation.<a class="see-footnote" id="footnoteref381_htnob7x" title="See notes from my conversation with Joel Hektner.  " href="#footnote381_htnob7x">381</a></li>
<li>In line with my personal experience, systematic reviews of the appropriateness of statistical tests used in published papers find high rates of straightforward statistical errors, for example inappropriate use of parametric tests, failure to account for multiple comparisons,<a class="see-footnote" id="footnoteref382_0lsc2oa" title="This is likely a problem for many meta-analyses, not just primary studies (Bender et al. 2008; Tendall et al. 2011).  " href="#footnote382_0lsc2oa">382</a> the use of “fail-safe N” to protect against publication bias, and other problems (<a href="http://www.sciencedirect.com/science/article/pii/S0140673600020390">Assman et al. 2000</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0022534701686464">Scales et al. 2005</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2006.01141.x/full">Whittingham et al. 2006</a>; <a href="https://arxiv.org/abs/1010.2326">Heene 2010</a>; <a href="http://www.nature.com/nrn/journal/v14/n5/full/nrn3475.html">Button et al. 2013</a>; <a href="http://link.springer.com/article/10.3758/s13428-015-0664-2">Nuijten et al. 2016</a>; <a href="http://www.pnas.org/content/113/28/7900.abstract">Eklund et al. 2016</a>; <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152719">Westfall &amp; Yarkoni 2016</a>).<a class="see-footnote" id="footnoteref383_gl0rb98" title="For a readable guide to many common statistical errors, see Reinhart et al. (2015).  " href="#footnote383_gl0rb98">383</a></li>
<li>In surveys, a substantial fraction of researchers from a variety of fields admit to engaging in “questionable research practices” (QRPs) that may undermine the validity of their published results (<a href="http://www.nature.com/nature/journal/v435/n7043/full/435737a.html">Martinson et al. 2005</a>; <a href="http://journals.sagepub.com/doi/abs/10.1177/0956797611430953">John et al. 2012</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0048733314000900">Necker 2014</a>).<a class="see-footnote" id="footnoteref384_nc7f9q7" title="But also see Fiedler &amp; Schwarz (2015).  " href="#footnote384_nc7f9q7">384</a> This general finding is bolstered by a variety of analytic attempts to estimate the rate of QRPs in various fields, for example by examining how reported findings change from the dissertation version of a study to the published article version (<a href="https://www.cambridge.org/core/journals/industrial-and-organizational-psychology/article/forgetting-what-we-learned-as-graduate-students-harking-and-selective-outcome-reporting-in-io-journal-articles/00C345869D74A96255C1E9F30ECD82DB">Mazzola &amp; Deuling 2013</a>), or by directly inspecting published correlation matrices (<a href="http://onlinelibrary.wiley.com/doi/10.1111/peps.12111/abstract">Bosco et al. 2015</a>). For an overall review, see <a href="http://link.springer.com/article/10.1007/s10869-016-9456-7">Banks et al. (2016)</a>.</li>
<li>Huge swaths of literature in the social sciences depend on self-report measures that have not been validated using the standards typically required (<a href="https://www.routledge.com/Handbook-of-Item-Response-Theory-Modeling-Applications-to-Typical-Performance/Reise-Revicki/p/book/9781138787858">Reise &amp; Revicki 2015</a>; <a href="https://www.routledge.com/Statistical-Approaches-to-Measurement-Invariance/Millsap/p/book/9781848728196">Millsap 2011</a>) for measures used in (e.g.) high-stakes testing or <a href="http://www.nihpromis.com/faqs">patient-reported outcomes</a> in health care. Also, systematic comparisons of self-reported data against data collected using “gold standard” objective measures (e.g. administrative data) suggest that self-report measures across a variety of fields result in substantial measurement error (see the sources in <a href="#self-report">this footnote</a>).</li>
<li>Failure to share study data is widespread,<a class="see-footnote" id="footnoteref385_4dfbxtp" title="See e.g. Vanpaemel et al. (2015) for psychology, and Chang &amp; Li (2015) for economics.  " href="#footnote385_4dfbxtp">385</a> and (in psychology, at least) predicts less-robust results (<a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026828">Wicherts et al. 2011</a>). I would guess the same is true in many other fields.</li>
</ol><p>Of course, failed replications and flawed methods don&#8217;t prove the original reported results are <em>false</em>, merely that they are not adequately supported. But that is just what I mean when I say that, prior to examining them myself, I expect most published studies not to “hold up” under attempted replication or close scrutiny.</p>
<p>Unfortunately, these points <em>also</em> undermine my ability to fully trust <em>the studies I&#8217;ve cited above about the trustworthiness of published studies</em>, only a few of which I&#8217;ve personally examined “somewhat closely” (i.e. for more than 30 minutes). Obviously, such “meta-research” is not immune from replication failures and flawed methodology.</p>
<p>To illustrate: when I first began to study the rate at which non-randomized studies are confirmed or contradicted by later (and presumably more trustworthy) randomized controlled trials, one of the first studies I came across was <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2011.00506.x/full">Young &amp; Karr (2011)</a>. After reading the study, I excitedly <a href="https://twitter.com/lukeprog/status/639477808381620225">tweeted</a> it, along with the following comment: “Reminder: very few correlations from epidemiological studies hold up in RCTs — in this study, an impressive 0/52.” However, Young &amp; Karr (2011) itself suffers multiple flaws. For example, the selection of primary studies was not systematic, and the authors provide no detail on how the studies they examined were chosen. Moreover, as I sought out additional reviews of this issue, I found that every large-scale review of this question that <em>was</em> conducted in a systematic, transparently-reported way found very different results than Young &amp; Karr did (e.g. <a href="https://www.journalslibrary.nihr.ac.uk/hta/hta7270">Deeks et al. 2003</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1002/14651858.MR000012.pub3/abstract">Odgaard-Jensen et al. 2011</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1002/14651858.MR000034.pub2/full">Anglemyer et al. 2014</a><a class="see-footnote" id="footnoteref386_zqcz3w2" title="On why Odgaard-Jensen et al. (2011) and Anglemyer et al. (2014) may have gotten somewhat different results, see Howick &amp; Mebius (2015).  " href="#footnote386_zqcz3w2">386</a>). Today, I do not trust Young &amp; Karr&#8217;s result.</p>
<ul class="footnotes"><li class="footnote" id="footnote1_5a070m1"><a class="footnote-label" href="#footnoteref1_5a070m1">1.</a> Related terms include “moral status,” “moral standing,” “moral considerability,” “personhood,” “moral subject” (this is fairly rare, but see e.g. <a href="http://www.jstor.org/stable/30301713">Wetlesen 1999</a>), and “member of the moral community.” Sometimes these terms are used more-or-less interchangeably, and sometimes they are not.<br /><br />
My preference for the terms “moral patient” and “moral patienthood” (see e.g. <a href="http://psycnet.apa.org/journals/psp/96/3/505/">Gray &amp; Wegner 2009</a>; <a href="https://global.oup.com/academic/product/on-moral-considerability-9780195123913?cc=us&amp;lang=en&amp;">Bernstein 1998</a>) is a pragmatic one. “Moral patient” is more succinct than “being with moral status,” “being with moral standing,” “being worthy of moral consideration,” and “member of the moral community.” It is <em>less</em> succinct and common than “person,” but “person” comes with fairly strong connotations of properties such as (1) having a temporally extended narrative about one&#8217;s life, (2) having a fairly sophisticated kind of agency in the world, (3) being human or human-like, (4) being able to participate in a community of other moral agents, and so on. Moreover, it is often used to mean some or all of those things <em>denotatively</em>, though in some other cases it is operationally defined as “being with moral status.”<br /><br />
One strike against “moral patient” is that it comes with a problematic connotation of helplessness, but not a terribly strong one, I don&#8217;t think. I presume “moral patient” gets its connotations by association with the concept of a medical patient, and while medical patients are primarily in a role of <em>receiving</em> help (or harm) from medical professionals (the intended connotation), they are not in most cases <em>helpless</em> (an unintended connotation).<br /><br />
For reviews of these related terms and concepts, see <a href="http://onlinelibrary.wiley.com/doi/10.1002/9780470510544.ch37/summary">Newson (2007)</a>, <a href="http://www.inderscienceonline.com/doi/abs/10.1504/IER.2002.053890">Hancock (2002)</a>, <a href="http://plato.stanford.edu/entries/grounds-moral-status/">Jaworska &amp; Tannenbaum (2013)</a>, <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195371963.001.0001/oxfordhb-9780195371963-e-10">Morris (2011)</a>, ch. 3 of <a href="https://global.oup.com/academic/product/principles-of-biomedical-ethics-9780199924585?cc=us&amp;lang=en&amp;">Beauchamp &amp; Childress (2012)</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1111/japp.12164/full">Kagan (2016)</a>, and the entry on “Moral Status” by James W. Walters on pp. 1855-1864 of <a href="https://books.google.com/books/about/Encyclopedia_of_Bioethics.html?id=if0mjwEACAAJ">Post (2004)</a>.<br /><br />
Some authors define “moral patient” such that a moral patient cannot also be a moral agent (e.g. <a href="https://books.google.com/books?id=4iIoAQAAMAAJ">Rodd 1990</a>, p. 241), but that is not how I use the term.<br /><br />
Some authors use the term “moral patiency” instead of “moral patienthood.”<br /><br />
One more clarification: in this report I refer to moral patients as distinctly identifiable individuals, but this is largely for convenience of communication. In the limit of scientific understanding, I suspect group minds (see e.g. <a href="http://link.springer.com/article/10.1007/s11098-014-0387-8">Schwitzgebel 2015</a>; <a href="http://link.springer.com/article/10.1007/s11098-016-0658-7">Roelofs 2016</a>; <a href="http://link.springer.com/article/10.1007/s11098-014-0393-x">Langland-Hassan 2015</a>, sec. 4; <a href="http://link.springer.com/chapter/10.1057/9781137286734_15">Theiner 2014</a>) and other structures for morally relevant cognitive processing (e.g. perhaps the “utilitronium” of <a href="http://link.springer.com/chapter/10.1007/978-3-642-32560-1_11">Pearce 2013</a>) will challenge our notions of morally relevant individual identity. (See also <a href="https://philpapers.org/rec/CHAVH">Chappell 2010</a> on valuing individuals vs. parts of individuals.)<br /><br />
For some historical background on the term “moral patient,” see <a href="https://books.google.com/books/about/The_Boundaries_of_Moral_Discourse.html?id=UgkyAAAAMAAJ">Hajdin (1994)</a>, p. 180.
</li>
<li class="footnote" id="footnote2_p3jbfjm"><a class="footnote-label" href="#footnoteref2_p3jbfjm">2.</a> The relevant studies, naturally enough, are those testing the effectiveness of behavioral treatments on sleep quality! And, assuming the perspective of <a href="https://en.wikipedia.org/wiki/Evidence-based_medicine">evidence-based medicine</a>, it&#8217;s fairly clear <em>which</em> of those studies are <em>most</em> informative: it&#8217;s the randomized controlled trials, especially those with large numbers of subjects, well-validated outcome measures, long-term follow-up, and other properties which improve the internal and external validity of a study.
</li>
<li class="footnote" id="footnote3_bw1ma1z"><a class="footnote-label" href="#footnoteref3_bw1ma1z">3.</a> Thus I have no doubt gotten some things wrong, and said some things that are silly, and I hope readers will call my attention to whatever errors I have made.</li>
<li class="footnote" id="footnote4_4xetcyt"><a class="footnote-label" href="#footnoteref4_4xetcyt">4.</a> If you want to read a series of <em>arguments</em> about the likely distribution of conscious experience, see e.g. <a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye (2016)</a>. Also see <a href="/sites/default/files/Michael_Tye_08-24-16_%28public%29.pdf">notes from my conversation with Michael Tye</a>.
</li>
<li class="footnote" id="footnote5_c1l81l5"><a class="footnote-label" href="#footnoteref5_c1l81l5">5.</a> Hence, each sub-investigation reported below was cut short long before I “completed” it, to save time (following something like the <a href="https://en.wikipedia.org/wiki/Pareto_principle">80/20 rule</a>). Thus, I can only share initial tentative conclusions based on a variety of partially-completed inquiries.
</li>
<li class="footnote" id="footnote6_1ytgq4h"><a class="footnote-label" href="#footnoteref6_1ytgq4h">6.</a> For more details on what I mean by “naturalistic” vs. “rationalistic,” see the introductory chapter in <a href="https://www.routledge.com/Experimental-Philosophy-Rationalism-and-Naturalism-Rethinking-Philosophical/Fischer-Collins/p/book/9781138887282">Fischer &amp; Collins (2015)</a>.<br /><br />
Examples might be more helpful, though. Example works in the “rationalistic” tradition are <a href="https://global.oup.com/academic/product/the-conscious-mind-9780195117899?cc=us&amp;lang=en&amp;">Chalmers (1996)</a> and <a href="https://global.oup.com/academic/product/from-metaphysics-to-ethics-9780198250616?cc=us&amp;lang=en&amp;">Jackson (1998)</a>. Example works in the “naturalistic” tradition are <a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674015456">Wimsatt (2007)</a> and <a href="https://www.cambridge.org/core/books/how-biology-shapes-philosophy/EF781FECCEB61E6F482348E78199A108">Smith (2016)</a>.<br /><br />
Another way to indicate the tradition of my thinking is to identify myself with (what is often called) “Quinean naturalism,” after Willard van Orman Quine (<a href="https://plato.stanford.edu/entries/quine/">Hylton 2014</a>; <a href="http://onlinelibrary.wiley.com/book/10.1002/9781118607992">Harman &amp; Lepore 2014</a>). Even better would be to coin the term “Dennettian naturalism” and identify myself with it, due especially (but not exclusively) to the way that Daniel Dennett updated and transformed Quinean naturalism with his more thorough appreciation for the impacts of both Darwin and the computer on philosophy, two lines of thinking have greatly impacted my own philosophical thinking. (This doesn&#8217;t mean I agree with Dennett on everything about consciousness, of course.)
</li>
<li class="footnote" id="footnote7_wdtyzkb"><a class="footnote-label" href="#footnoteref7_wdtyzkb">7.</a> Below is an incomplete list of alternative approaches for how to think about what should count as “good.” These alternative approaches have their own merits, and in some cases might lead us to make different grantmaking choices if we adopted them in favor of our current framing (about moral patients, and dimensions of moral concern). I have not considered these alternatives in detail yet, but I have tried to at least <em>expose</em> myself to a wide range of viewpoints.<br /><br />
Below are some alternate approaches to the question of what should count as “good,” which I have considered at least briefly.<br /><br /><a href="https://books.google.com/books?id=e7FME0btkH0C&amp;lpg=PP1&amp;pg=PA162#v=onepage&amp;q&amp;f=false">Rachels (2004)</a> writes:
<blockquote><p>There is no characteristic, or reasonably small set of characteristics, that sets some creatures apart from others as meriting respectful treatment. That is the wrong way to think about the relation between an individual&#8217;s characteristics and how he or she may be treated. Instead we have an array of characteristics and an array of treatments, with each characteristic relevant to justifying some types of treatment but not others. If an individual possesses a particular characteristic (such as the ability to feel pain), then we may have a duty to treat it in a certain way (not to torture it), even if that same individual does not possess other characteristics (such as autonomy) that would mandate other sorts of treatment (refraining from coercion).</p>
<p>We could spin these observations into a theory of moral standing that would compete with the other theories. Our theory would start like this: There is no such thing as moral standing <em>simpliciter</em>. Rather, moral standing is always moral standing with respect to some particular mode of treatment. A sentient being has moral standing with respect to not being tortured. A self-conscious being has moral standing with respect to not being humiliated. An autonomous being has moral standing with respect to not being coerced. And so on…</p>
<p>It would do no harm, however, and it might be helpful for clarity&#8217;s sake, to drop the notion of [moral] “standing” altogether and replace it with a simpler conception. We could just say that the fact that doing so-and-so would cause pain to someone (to any individual) is a reason not to do it. The fact that doing so-and-so would humiliate someone (any individual) is a reason not to do it. And so on. Sentience and self-consciousness fit into the picture like this: Someone&#8217;s sentience and someone&#8217;s self-consciousness are facts about them that explain why they are susceptible to the evils of pain and humiliation.</p>
<p>We would then see our subject as part of the theory of reasons for action. We would distinguish three elements: what is done to the individual; the reason for doing it or not doing it, which connects the action to some benefit or harm to the individual; and the pertinent facts about the individual that help to explain why he or she is susceptible to that particular benefit or harm…</p>
<p>So, part of our theory of reasons for action would go like this: We always have reason not to do harm. If treating an individual in a certain way harms him or her, that is a reason not to do it. The fact that he or she is autonomous, or self-conscious, or sentient simply helps to explain why he or she is susceptible to particular kinds of harms.</p></blockquote>
<p>A related but not-identical point is made by <a href="https://books.google.com/books?id=RYOYAwAAQBAJ&amp;lpg=PA316&amp;ots=A-Z6ui8Hvw&amp;lr=lang_en&amp;pg=PA316#v=onepage&amp;q&amp;f=false">Bostrom &amp; Yudkowsky (2014)</a>:</p>
<blockquote><p>Alternatively, one might deny that moral status comes in degrees. Instead, one might hold that certain beings have more significant interests than other beings. Thus, for instance, one could claim that it is better to save a human than to save a bird, not because the human has higher moral status, but because the human has a more significant interest in having her life saved than does the bird in having its life saved.</p></blockquote>
<p>For additional related arguments against “moral status talk,” see <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0114.2010.01387.x/abstract">Sachs (2011)</a>.<br /><br />
Another approach is to think about moral status in the context of the concept of <em>personhood</em>, which might or might not be sufficient for a being to have moral status. For an overview of such approaches, see <a href="http://onlinelibrary.wiley.com/doi/10.1002/9780470510544.ch37/summary">Newson (2007)</a>.<br /><br />
Substantially different approaches to thinking about what is “good” include deontological approaches, virtue ethics approaches, capabilities approaches, contractualist approaches, and more. For example essays on how these approaches can be applied to concerns about animals, see e.g. <a href="https://academic.oup.com/analysis/article/75/4/638/132210/Non-Consequentialist-Theories-of-Animal-Ethics">Sachs (2015)</a> and the chapters in Part II of <a href="https://global.oup.com/academic/product/the-oxford-handbook-of-animal-ethics-9780195371963?cc=us&amp;lang=en&amp;">Beauchamp &amp; Frey (2011)</a>.<br /><br />
See also <a href="http://theronpummer.com/wp-content/uploads/2016/10/EJ-October-2016.pdf">Crisp &amp; Pummer (2016)</a> on “effective justice” and “effective altruism,” which is relevant due to our substantial ties to the effective altruism community (see e.g. <a href="http://www.openphilanthropy.org/blog/three-key-issues-ive-changed-my-mind-about">this blog post</a>).</p>
</li>
<li class="footnote" id="footnote8_1a1sx5d"><a class="footnote-label" href="#footnoteref8_1a1sx5d">8.</a> That is, we must act under “moral uncertainty.” See e.g. <a href="http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.627884">MacAskill (2014)</a>; <a href="http://nebula.wsimg.com/1cc278bf0e7470c060032c9624508149?AccessKeyId=07941C4BD630A320288F&amp;disposition=0&amp;alloworigin=1">Bogosian (2016)</a>; <a href="https://global.oup.com/academic/product/moral-uncertainty-and-its-consequences-9780195126105">Lockhart (2000)</a>; <a href="http://link.springer.com/chapter/10.1007/978-3-319-30549-3_5">Möller (2016)</a>; <a href="http://users.ox.ac.uk/~mert2255/papers/mu-about-pe.pdf">Greaves &amp; Ord (2016)</a>.<br /><br />
See also our <a href="/blog/worldview-diversification">blog post</a> on worldview diversification.
</li>
<li class="footnote" id="footnote9_zry3ia1"><a class="footnote-label" href="#footnoteref9_zry3ia1">9.</a> For example, if you think fishes are moral patients but have very low “intensity of valenced subjective experience” compared to chickens, and you consider “intensity of valenced subjective experience” to be a very important (i.e., heavily-weighted) dimension of moral concern, then you might still prioritize chicken welfare interventions over fish welfare investigations, even if you think that fish and chickens have roughly the same probability of being moral patients at all.<br /><br />
Of course, even after we&#8217;ve come to tentative conclusions about a taxon&#8217;s “moral weight,” there remain various second-order effects of welfare interventions to evaluate, among other considerations. Example sources on second-order effects and other considerations, with respect to animal welfare in particular, include <a href="http://link.springer.com/article/10.1007/s10806-005-1805-x">Matheny &amp; Chan (2005)</a>, <a href="https://global.oup.com/academic/product/compassion-by-the-pound-9780199551163?cc=us&amp;lang=en&amp;">Norwood &amp; Lusk (2011)</a>, <a href="http://www.ingentaconnect.com/contentone/ufaw/aw/2012/00000021/A00101s1/art00001">Christensen et al. (2012)</a>, <a href="http://www.palgrave.com/br/book/9781137286260">Višak (2013)</a>, and <a href="http://reflectivedisequilibrium.blogspot.com/2015/11/some-considerations-for-prioritization.html">Shulman (2015)</a>.<br /><br />
There are also those who argue that the numbers of moral patients helped or harmed shouldn&#8217;t matter, e.g. <a href="http://www.jstor.org/stable/2264945">Taurek (1977)</a>, but I won&#8217;t discuss that issue here.
</li>
<li class="footnote" id="footnote10_0ltzae6"><a class="footnote-label" href="#footnoteref10_0ltzae6">10.</a> Though, there may not be a sharp dividing line between beings which are moral patients and beings which are not; see my <a href="#fuzzy">later comments</a> on the likely-fuzzy line between conscious and non-conscious beings.
</li>
<li class="footnote" id="footnote11_q8ddby2"><a class="footnote-label" href="#footnoteref11_q8ddby2">11.</a> Introductions to metaethics include <a href="http://plato.stanford.edu/entries/metaethics/">Sayre-McCord (2012)</a>, <a href="https://www.polity.co.uk/book.asp?ref=9780745646596">Miller (2013)</a>, and <a href="http://open-mind.net/papers/naturalizing-metaethics">Prinz (2015)</a>.<br /><br />
Personally, I think it&#8217;s pretty clear that different people use moral language in different ways, and often a single person uses moral language in different ways at different times. This view is sometimes called “meta-ethical pluralism” (<a href="http://www.tandfonline.com/doi/abs/10.1080/09515089.2011.633751">Wright et al. 2012</a>).
</li>
<li class="footnote" id="footnote12_294q9sa"><a class="footnote-label" href="#footnoteref12_294q9sa">12.</a> I don&#8217;t claim this is necessarily how <em>other</em> people use moral language, though.
</li>
<li class="footnote" id="footnote13_wndl62d"><a class="footnote-label" href="#footnoteref13_wndl62d">13.</a> For more on conceptual analysis, see <a href="http://plato.stanford.edu/entries/analysis/">Beaney (2014)</a>; <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199668779.001.0001/oxfordhb-9780199668779-e-7">King (2016)</a>.
</li>
<li class="footnote" id="footnote14_d25ddn3"><a class="footnote-label" href="#footnoteref14_d25ddn3">14.</a> Though, some panpsychists would say a rock is conscious, and thus a moral patient if one also assumes consciousness is sufficient for moral patienthood.
</li>
<li class="footnote" id="footnote15_drytp2y"><a class="footnote-label" href="#footnoteref15_drytp2y">15.</a> See e.g. <a href="http://www.springer.com/gp/book/9783319448435">Lagercrantz (2016)</a>.
</li>
<li class="footnote" id="footnote16_qpphniz"><a class="footnote-label" href="#footnoteref16_qpphniz">16.</a> See e.g. <a href="http://link.springer.com/article/10.1007/s13347-013-0122-y">Basl (2014); </a><a href="http://onlinelibrary.wiley.com/doi/10.1111/misp.12032/full">Schwitzgebel &amp; Garza (2015)</a>; <a href="https://books.google.com/books?id=RYOYAwAAQBAJ&amp;lpg=PA316&amp;ots=A-Z6ui8Hvw&amp;lr=lang_en&amp;pg=PA316#v=onepage&amp;q&amp;f=false">Bostrom &amp; Yudkowsky (2014)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118736302.ch20/summary">Sandberg (2014)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0893608013000968">Reggia (2013)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S1053810007000347">Gamez (2008)</a>.
</li>
<li class="footnote" id="footnote17_m3z9f4j"><a class="footnote-label" href="#footnoteref17_m3z9f4j">17.</a> See e.g. <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=7163249">Rinner et al. (2015)</a>.
</li>
<li class="footnote" id="footnote18_aunemhh"><a class="footnote-label" href="#footnoteref18_aunemhh">18.</a> See e.g. <a href="http://reducing-suffering.org/do-video-game-characters-matter-morally/">Tomasik (2015)</a>.
</li>
<li class="footnote" id="footnote19_puasdyd"><a class="footnote-label" href="#footnoteref19_puasdyd">19.</a> See <a href="https://www.newscientist.com/article/mg21628951.900-gut-instincts-the-secrets-of-your-second-brain/">Young (2012)</a>.<br /><br />
For a detailed discussion of whether the autonomic nervous system (of which the enteric nervous system is one part) satisfies various theories and criteria of consciousness, see <a href="http://prism.ucalgary.ca/handle/1880/29130">Ryder (1996)</a>. For example, Ryder examines Daniel Dennett&#8217;s theory about which processes are sufficient for various kinds of consciousness, and argues that those processes occur in the autonomic nervous system (ANS). Ryder reports:
<blockquote><p>In conversation, after I pointed out some of the complexities of ANS operation, [Dennett] suggested to me that the ANS would have approximately the same degree of consciousness as someone blind and deaf since birth.</p></blockquote>
<p><em>If</em> Dennett is right about that, then my moral intuitions suggest that I should consider the ANS a moral patient, for the same reasons I morally care about the subjective experiences of a human born blind and deaf. Others’ moral intuitions may vary.</p>
</li>
<li class="footnote" id="footnote20_sp207hh"><a class="footnote-label" href="#footnoteref20_sp207hh">20.</a> See e.g. <a href="https://books.google.com/books?id=dhhRBAAAQBAJ&amp;dq=gazzaniga+ledoux&amp;source=gbs_navlinks_s">Gazzaniga &amp; LeDoux (1978)</a>, ch. 7; <a href="https://books.google.com/books?id=jk2UmfntzYsC">Gazzaniga (1992)</a>, pp. 121- 137; <a href="http://www.tandfonline.com/doi/abs/10.1080/09515089.2011.579417">Schechter (2012)</a>; <a href="http://www.cognethic.org/jcn/jcnv3i4_Blackmon.pdf">Blackmon (2016)</a>; <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00012">Marinsek &amp; Gazzaniga (2016)</a>; <a href="https://academic.oup.com/brain/article-abstract/doi/10.1093/brain/aww358/2951052/Split-brain-divided-perception-but-undivided">Pinto et al. (2017)</a>.
</li>
<li class="footnote" id="footnote21_kp7xhww"><a class="footnote-label" href="#footnoteref21_kp7xhww">21.</a> On ecosystems as moral patients, see e.g. <a href="http://plato.stanford.edu/entries/ethics-environmental/">Brennan &amp; Lo (2015)</a> and <a href="http://www.cambridge.org/us/academic/subjects/philosophy/ethics/morally-deep-world-essay-moral-significance-and-environmental-ethics?format=PB">Johnson (1993)</a>. On companies as moral patients, see e.g. <a href="http://www.tandfonline.com/doi/abs/10.1080/002017401300075974">Graham (2001)</a>. On nations as potentially conscious, and thus moral patients under some views, see e.g. <a href="http://link.springer.com/article/10.1007/s11098-014-0387-8">Schwitzgebel (2015)</a>.
</li>
<li class="footnote" id="footnote22_mp7najs"><a class="footnote-label" href="#footnoteref22_mp7najs">22.</a> On personhood, see e.g. <a href="http://onlinelibrary.wiley.com/doi/10.1002/9780470510544.ch37/summary">Newson (2007)</a>. On interests, see e.g. <a href="http://plato.stanford.edu/entries/grounds-moral-status/">Jaworska &amp; Tannenbaum (2013)</a>.
</li>
<li class="footnote" id="footnote23_mq24c2m"><a class="footnote-label" href="#footnoteref23_mq24c2m">23.</a> For discussions of the relevance of phenomenal consciousness to moral patienthood, see e.g. <a href="http://philpapers.org/rec/SHECAM-3">Shepherd &amp; Levy (forthcoming)</a>.
</li>
<li class="footnote" id="footnote24_fppy7hy"><a class="footnote-label" href="#footnoteref24_fppy7hy">24.</a> See section 4.1 of <a href="http://plato.stanford.edu/entries/grounds-moral-status/">Jaworska &amp; Tannenbaum (2013)</a>. Also see <a href="http://plato.stanford.edu/entries/cognitive-disability/">Wasserman et al. (2012)</a>.
</li>
<li class="footnote" id="footnote25_4o7d642"><a class="footnote-label" href="#footnoteref25_4o7d642">25.</a> See section 4.2 of <a href="http://plato.stanford.edu/entries/grounds-moral-status/">Jaworska &amp; Tannenbaum (2013)</a>.
</li>
<li class="footnote" id="footnote26_comp5i5"><a class="footnote-label" href="#footnoteref26_comp5i5">26.</a> See section 4.3 of <a href="http://plato.stanford.edu/entries/grounds-moral-status/">Jaworska &amp; Tannenbaum (2013)</a>.
</li>
<li class="footnote" id="footnote27_6smhaeb"><a class="footnote-label" href="#footnoteref27_6smhaeb">27.</a> See references in sections 4.4 and 4.5 of <a href="http://plato.stanford.edu/entries/grounds-moral-status/">Jaworska &amp; Tannenbaum (2013)</a> and the discussion of deep ecology in <a href="http://plato.stanford.edu/entries/ethics-environmental/">Brennan &amp; Lo (2015)</a>.
</li>
<li class="footnote" id="footnote28_c7nil48"><a class="footnote-label" href="#footnoteref28_c7nil48">28.</a> <a href="http://plato.stanford.edu/entries/grounds-moral-status/">Jaworska &amp; Tannenbaum (2013)</a>:<br /><blockquote><p>Accounts differ on what it is about the individual that grounds or confers moral status and to what degree, with implications for which beings do or do not have moral status and for their comparative status… For each account discussed, one could hold either a threshold or scalar conception of moral status, though the former is more commonly found in the literature… According to the threshold conception, as it is usually discussed, if capacity C grounds FMS [full moral status], then any being that has C, regardless of how well it can exercise this capacity, has as much moral status as any other being that has C and this status is full. If C is not only sufficient but necessary for FMS, then all beings lacking C would not have FMS, though the threshold conception would nevertheless leave it open whether having some other feature (e.g., parts of C or something lesser but akin to C) might ground lesser degrees of moral status. In contrast, a scalar conception of moral status would hold that if capacity C grounds moral status, then any being who has C has some status; the better it can exercise this capacity, the higher its degree of moral status… [Or] instead of focusing on how well capacity C is exercised, the views could instead focus on the number of relevant capacities a being has. A threshold view might specify some number n of the relevant capacities as both necessary and sufficient for FMS. A scalar conception would hold, on the other hand, that a being with n+1 capacities would have a higher moral status than one with merely n capacities.</p></blockquote>
</li>
<li class="footnote" id="footnote29_ibubqrf"><a class="footnote-label" href="#footnoteref29_ibubqrf">29.</a> Some authors distinguish “unconscious” from “non-conscious,” but I use these terms interchangeably.
</li>
<li class="footnote" id="footnote30_mk7pzi6"><a class="footnote-label" href="#footnoteref30_mk7pzi6">30.</a> The findings in this literature are relatively new and under substantial debate. For reviews, see <a href="http://onlinelibrary.wiley.com/doi/10.1002/wcs.1320/full">Sytsma (2014)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/phc3.12266/full">Goodwin (2015)</a>; <a href="http://link.springer.com/article/10.1007/s13164-012-0104-5">Jack &amp; Robbins (2012)</a>.
</li>
<li class="footnote" id="footnote31_tr1oadx"><a class="footnote-label" href="#footnoteref31_tr1oadx">31.</a> For other examples of self-amputation behaviors, see Wikipedia&#8217;s article on <a href="https://en.wikipedia.org/wiki/Autotomy">autootomy</a>; <a href="http://beheco.oxfordjournals.org/content/17/5/857.short">Maginnis (2006)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1469-185X.2007.00020.x/full">Fleming et al. (2007)</a>.
</li>
<li class="footnote" id="footnote32_ahyddbm"><a class="footnote-label" href="#footnoteref32_ahyddbm">32.</a> In this report I use “subjective experience” as a synonym for “phenomenal consciousness.” Some authors use the term “experience” more broadly, though. For example, <a href="http://www.cambridge.org/us/academic/subjects/philosophy/political-philosophy/animals-issue-moral-theory-practice">Carruthers (1992)</a>, pp. 170-171 writes:<br /><blockquote><p>Suppose that Abbie is driving her car over a route she knows well, her conscious attention wholly abstracted from her surroundings. Perhaps she is thinking deeply about some aspect of her work, or fantasising about her next summer holiday, to the extent of being unaware of what she is doing on the road. Suddenly she ‘comes to,’ returning her attention to the task in hand with a startled realisation that she has not the faintest idea what she has been doing or seeing for some minutes past. Yet there is a clear sense in which she must have been seeing, or she would have crashed the car. Her passenger sitting next to her may correctly report that she had seen a vehicle double-parked by the side of the road, for example, since she deftly steered the car around it. But she was not aware of seeing that obstacle, either at the time or later in memory.</p>
<p>Another example: when washing up dishes I generally put on music to help pass the time. If it is a piece that I love particularly well I may become totally absorbed, ceasing to be conscious of what I am doing at the sink. Yet someone observing me position a glass neatly on the rack to dry between two coffee mugs would correctly say that I must have seen that those mugs were already there, or I should not have placed the glass where I did. Yet I was not aware of seeing those mugs, or of placing the glass between them. At the time I was swept up in the Finale of Schubert&#8217;s <em>Arpeggione Sonata</em>, and if asked even a moment later I should not have been able to recall what I had been looking at.</p>
<p>Let us call such experiences <em>non-conscious</em> ones. What does it feel like to be the subject of a non-conscious experience? It feels like nothing. It does not feel like anything to have a non-conscious visual experience as of a vehicle parked at the side of the road, or as of two coffee mugs placed on a draining rack — precisely because to have such an experience is not to be conscious of it. Only conscious experiences have a distinctive phenomenology, a distinctive feel. Non-conscious experiences are ones that may help to control behavior without being felt by the conscious subject.</p>
<p>[These points] are already sufficient to show that it is wrong to identify the question [of] whether a creature has experiences with the question [of] whether there is something it feels like to be that thing. For there is a class — perhaps a large class — of non-conscious experiences that have no phenomenology.</p></blockquote>
<p>In contrast to Carruthers’ usage of “experience,” I shall in this report only use the phrase “subjective experience” to refer to what Carruthers calls “conscious experiences.”</p>
</li>
<li class="footnote" id="footnote33_p6jy861"><a class="footnote-label" href="#footnoteref33_p6jy861">33.</a> I do not, however, assume (like Block) that “phenomenal consciousness” must be “distinct from any cognitive, intentional, or functional property.” In <a href="http://analysis.oxfordjournals.org/content/71/3/438.short">Weisberg (2011)</a>’s terms, I intend a “moderate” rather than “zealous” reading of the phrase “phenomenal consciousness.”
</li>
<li class="footnote" id="footnote34_om62x2g"><a class="footnote-label" href="#footnoteref34_om62x2g">34.</a> Schwitzgebel is hardly the first to propose such a definition for “consciousness.” As Schwitzgebel notes:<br /><blockquote><p>Definition by example is a common approach among recent phenomenal realists. I interpret Searle (1992, p. 83), Block (1995/2007, p. 166–8), and Chalmers (1996, p. 4) as aiming to define phenomenal consciousness by a mix of synonymy and appeal to example…</p></blockquote>
<p>See also e.g. <a href="http://link.springer.com/article/10.1007/s13164-015-0297-5">Shiller (2016)</a>:</p>
<blockquote><p>I intend for the content of the concept [of “qualia”] to be fixed by its prototypical examples. Qualia are whatever kind of mental qualities we associate with experiences of redness, pain, satisfaction, and déjà vu. This approach leaves the veracity of our intuitive assumptions open to investigation.</p></blockquote>
<p>Note that Schwitzgebel&#8217;s additional “wonderfulness” criterion also seems useful in a definition of consciousness. I skip discussing it here merely for brevity.</p>
</li>
<li class="footnote" id="footnote35_mab7s3l"><a class="footnote-label" href="#footnoteref35_mab7s3l">35.</a> For example, see Chalmers’ “catalog of conscious experiences” on pp. 4-9 of <a href="https://global.oup.com/academic/product/the-conscious-mind-9780195117899?cc=us&amp;lang=en&amp;">Chalmers (1996)</a>, and also <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199262618.001.0001/oxfordhb-9780199262618-e-13">Perry (2009)</a>.
</li>
<li class="footnote" id="footnote36_t60ucxy"><a class="footnote-label" href="#footnoteref36_t60ucxy">36.</a> Another contested negative example is this: one might be tempted to say that in a binocular rivalry task (see e.g. <a href="https://benjamins.com/#catalog/books/aicr.90.08ste/details">Sterzer 2013</a>), the image the subject self-reports as having experienced (at a given time) provides a positive example of consciousness, and the other image that was <em>not</em> consciously experienced despite being processed to some extent by the brain provides a negative example.<br /><br />
Yet another contested negative example is illustrated by an online exchange between Scott Aaronson and Guilio Tononi, discussed in <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004286">Cerullo (2015)</a>. Tononi has developed the Integrated Information Theory (IIT) of consciousness, according to which consciousness is equal to a measure of integrated information denoted Φ (“phi”). <a href="http://www.scottaaronson.com/blog/?p=1799">Aaronson (2014a)</a>, in reply to Tononi&#8217;s theory, argued that IIT “unavoidably predicts vast amounts of consciousness in physical systems that no sane person would regard as particularly ‘conscious’ at all.” To illustrate this, Aaronson defined a particular kind of expander graph that, according to IIT, has enormous amounts of consciousness, despite not doing anything remotely intelligent, nor exhibiting any features we typically think of as “conscious.” Tononi agreed that certain kinds of simple systems could generate arbitrarily large values of Φ, but disagreed with Aaronson that we should credit our intuitions that such mathematical objects cannot be enormously more conscious than humans (<a href="http://www.scottaaronson.com/tononi.docx">Tononi 2014</a>).<br /><br />
Another contested negative example is dreamless sleep, which is often given as a paradigm case of a state during which one has no phenomenal experience. However, this example has recently been contested (<a href="http://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(16)30152-8">Windt et al. 2016</a>).<br /><br />
The “distracted driver” case seems to be another contested example. For me, it is most natural to say the stimuli to which the distracted driver is responding (e.g. to keep the car in the correct lane), but which she has no memory of consciously experiencing and which she cannot report (because her conscious attention was focused on her cell phone call), were not consciously experienced (as far as we know). But <a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye (2016)</a> seems instead to count the distracted driver&#8217;s processing of stimuli (that she can&#8217;t remember or report experiencing) as an example of conscious experience (pp. 14-15):
<blockquote><p>Take, for example, the visual experiences of the distracted driver as she drives her car down the road. She is concentrating hard on other matters (the phone call she is answering about the overdue rent; the coffee in her right hand, etc.), so her visual experiences are unconscious. But her experiences exist alright. How else does she keep the car on the road?…</p>
<p>…Sometimes when we say that a mental state is conscious, we mean that it is, in itself, an inherently conscious state. At other times, when we say that a mental state is conscious, we have in mind the subject&#8217;s attitude toward the state. We mean that the subject of the mental state is conscious <em>of</em> it or conscious <em>that</em> it is occurring. The latter consciousness is a species of what is sometimes called “creature consciousness.” The [view I&#8217;ve articulated] has it that, in the first sense, a mental state is conscious (conscious<sub>1</sub>) if and only if it is an experience. In the latter sense, a mental state is conscious (conscious<sub>2</sub>) if and only if another conscious<sub>1</sub> state, for example, a conscious thought, is directed upon it. [My view] holds that this higher-order conscious state (in being a conscious<sub>1</sub> state) is itself an experience, for example, the experience of thinking of the first-order state or thinking that the first-order state is occurring.</p>
<p>The visual experiences of the distracted driver are unconscious in that she is not conscious <em>of</em> them. Nor is she conscious <em>that</em> they are occurring. So she lacks creature consciousness with respect to certain mental states that are themselves conscious<sub>1</sub>. Being distracted, she is not conscious of what those experiences are like. There is no inconsistency here. The [distracted driver] objection conflates high-order consciousness with first-order consciousness. Experiences are first-order conscious states on which second-order conscious states may or may not be directed.</p></blockquote>
<p>A similar example is provided by <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=6740052&amp;fulltextType=RA&amp;fileId=S0140525X00038188">Block (1995)</a>:</p>
<blockquote><p>…suppose you are engaged in intense conversation when suddenly at noon you realize that right outside your window there is — and has been for some time — a deafening pneumatic drill digging up the street. You were aware of the noise all along, but only at noon are you consciously aware of it. That is, you were [phenomenally conscious] of the noise all along…</p></blockquote>
<p>But here, I am instead inclined to say that, in this hypothetical scenario, I was <em>not</em> phenomenally conscious of the pneumatic drill. Or, perhaps somewhere in my brain there was a phenomenally conscious experience of the pneumatic drill, but I don&#8217;t yet have any (introspective) evidence of that.</p>
</li>
<li class="footnote" id="footnote37_fzawukj"><a class="footnote-label" href="#footnoteref37_fzawukj">37.</a> Or, if we want to make things more complicated, we could construct a spectrum from clear positive examples to clear negative examples, similar to <a href="https://books.google.com/books?id=7w6IYeJRqyoC&amp;printsec=frontcover&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjLkMuXsYvQAhWHllQKHXPEAlkQ6AEIHTAA#v=onepage&amp;q&amp;f=falseÎ">Baars (1988)</a>, Figure 1.1 (p. 12).
</li>
<li class="footnote" id="footnote38_rfa7r7y"><a class="footnote-label" href="#footnoteref38_rfa7r7y">38.</a> In particular, as I have already hinted, my Schwitzgebel-inspired definition of consciousness may have some trouble distinguishing “phenomenal consciousness” from “access consciousness” (in roughly the sense of <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=6740052&amp;fulltextType=RA&amp;fileId=S0140525X00038188">Block 1995</a>), if indeed the two can be distinguished. In any case, I expect our consciousness-related definitions to evolve and become more useful as we learn more.
</li>
<li class="footnote" id="footnote39_fy5ry7o"><a class="footnote-label" href="#footnoteref39_fy5ry7o">39.</a> For examples, see the sources relating to the evolution of scientific concepts listed in <a href="#AppendixZ6">Appendix Z.6</a> and <a href="#concepts">this footnote</a>. See also e.g. <a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674015456">Wimsatt (2007)</a>, especially ch. 6.
</li>
<li class="footnote" id="footnote40_hy12szm"><a class="footnote-label" href="#footnoteref40_hy12szm">40.</a> I borrow this example from <a href="https://mitpress.mit.edu/books/mind-and-mechanism">McDermott (2001)</a>, pp. 25-26:<br /><blockquote><p>Suppose one had demanded of Van Loewenhook and his contemporaries that they provide a similar sort of definition for the concept of life and its subconcepts, such as respiration and reproduction. It would have been a complete waste of time, because what Van Loewenhook wanted to know, and what we are now figuring out, is <em>how life works</em>. We know there are borderline cases, such as viruses, but we don&#8217;t care exactly where the border lies, because our understanding encompasses both sides. The only progress we have made in defining “life” is to realize that it doesn&#8217;t need to be defined. Similarly, what we want to know about minds is <em>how they work</em>.</p></blockquote>
</li>
<li class="footnote" id="footnote41_wma3bbn"><a class="footnote-label" href="#footnoteref41_wma3bbn">41.</a> In the context of consciousness, some philosophers use “physicalism” to refer to “identity theory” (<a href="http://plato.stanford.edu/entries/mind-identity/">Smart 2007</a>), but that isn&#8217;t how I use the term.<br /><br />
In brief, I assume physicalism about consciousness for two major reasons.<br /><br />
First, it seems to me that physicalism is supported by much stronger evidence than can be assembled by the human intuitions used to argue against it. To illustrate: which of these do you think has greater evidential justification?
<ol><li>“The universe is a mathematically simple low-level unified causal process with no non-natural elements or attachments.” (This phrasing is from Eliezer Yudkowsky&#8217;s “<a href="https://arbital.com/p/executable_philosophy/">Executable philosophy</a>.”)</li>
<li>My intuitive judgments about anti-physicalist thought experiments involving <a href="http://plato.stanford.edu/entries/zombies/">zombies</a>, “<a href="https://en.wikipedia.org/wiki/Knowledge_argument">Mary the super-scientist</a>,” etc.</li>
</ol><p>I think the evidence for (1) is overwhelming at this point, and the trust we should put in (2) is pretty weak. For that reason, I&#8217;m comfortable betting that the mystery of consciousness, like every mystery ever solved before it, will eventually be resolved (<em>if</em> it is ever resolved) by understanding some set of physical processes better than we do today — and that the poorly-understood stuff we&#8217;re trying to point at with words like “consciousness” will turn out to be <em>constituted by</em> some set of physical processes. (Here, I mean “physical processes” in a broad sense that includes, e.g. water conceived of as H<sub>2</sub>O.)<br /><br />
Second, the assumption of physicalism has been enormously productive in the past, and has generally seemed ever more reasonable as evidence has accumulated about any given phenomenon. On this, see the Appendix of <a href="https://global.oup.com/academic/product/thinking-about-consciousness-9780199243822?cc=us&amp;lang=en&amp;">Papineau (2002)</a>, <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199262618.001.0001/oxfordhb-9780199262618-e-3">Papineau (2009)</a>, and just about any history of any science. (For arguments against this fairly standard view, see e.g. section 1.1 of <a href="https://global.oup.com/academic/product/consciousness-and-fundamental-reality-9780190677015?cc=us&amp;lang=en&amp;">Goff 2017</a>.)</p>
</li>
<li class="footnote" id="footnote42_xmyeryb"><a class="footnote-label" href="#footnoteref42_xmyeryb">42.</a> See <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=NEGK_ZStddkC&amp;oi=fnd&amp;pg=PA102&amp;ots=QpGMj9nnzy&amp;sig=QWNn2BvtdUwn8yHMs_8_hLtNFa8#v=onepage&amp;q&amp;f=false">Chalmers (2003)</a>’s explanation of different types of physicalism/materialism. Note that most physicalists in philosophy appear to be “type B” materialists (see <a href="https://dl.dropboxusercontent.com/u/8809125/DissolveTypeB.pdf">Yetter-Chappell 2015</a>, footnote 2).<br /><br />
Technically, I can see a case for classifying my view as either “type Q materialism” or “type C materialism” (i.e. “physicalism, fingers crossed” as <a href="https://mitpress.mit.edu/books/consciousness-color-and-content">Tye 2000</a>, p. 22 puts it), but if so, then I&#8217;m the sort of type Q or type C materialist (about consciousness) for whom the usual pros and cons of type A materialism arise in more-or-less the same form (rather than, say, the usual pros and cons of type B materialism).<br /><br />
Either way, I should clarify that unlike some type A materialists, I don&#8217;t think that merely explaining verbal reports and beliefs is all that needs to be explained. Even if external-to-me scientists could explain my beliefs and verbal reports, I would still want to additionally explain <em>why it feels like something to be me</em>. (See also <a href="https://global.oup.com/academic/product/the-character-of-consciousness-9780195311105?cc=us&amp;lang=en&amp;">Chalmers 2010</a>, pp. 52-58.) I just think that in the end, explaining certain functions will explain everything there is to explain, and hence I am probably best described as a type A materialist.
</li>
<li class="footnote" id="footnote43_atzfhty"><a class="footnote-label" href="#footnoteref43_atzfhty">43.</a> Here is a fuller explanation of functionalism, from <a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470674504.html">Mandik (2013)</a>, p. 110:<br /><blockquote><p>Two key ideas that functionalists have appealed to in developing their position are the ideas of a <em>functional kind</em> and of a <em>multiply realizable kind</em>. A kind is a grouping of things or entities, usually grouped in terms of one or more features common to members of the group. Examples of kinds include cats, diamonds, planets, and mousetraps. To illustrate the idea of a multiply realizable kind, let us draw a contrast between diamonds, which are not multiply realizable, and mousetraps, which are. What makes something a diamond? First off, a diamond has to be made out of carbon. Anything superficially resembling a diamond that is not made out of carbon is not a genuine diamond. Crystals of zirconium dioxide superficially resemble diamonds, but are composed of the chemical elements zirconium and oxygen. Further, the carbon atoms that compose diamonds need to be arranged in a certain way (tetrahedral lattices). Carbon atoms not so arranged make up coal and graphite, not diamonds.</p>
<p>Diamonds may be physically realized in only one way — with tetrahedral lattices of carbon atoms. Thus they are <em>not</em> multiply realizable. Contrast this with mousetraps, which are multiply realizable. There are many ways to make a mousetrap. Some involve metal spring-loaded killing bars mounted on wooden platforms. Others involve a strong sticky glue applied to a flat surface on which the mouse gets stuck. There is no particular chemical element that is necessary for making a mousetrap.</p>
<p>Mousetraps help to illustrate not just the idea of multiply realizable kinds, but also the idea of functional kinds. Functional kinds are defined by what they <em>do</em>, and are so named because they are defined by the function they perform. Mousetraps perform the function of restraining or killing mice… As long as a system is able to achieve its defining function, it is largely irrelevant which physical stuff it happens to be realized by.</p></blockquote>
<p>Mandik continues (pp. 110-111) with another important point about functionalism and consciousness:</p>
<blockquote><p>Much of the contemporary enthusiasm for functionalism stems from enthusiasm about analogies drawn between minds and computers… Computers are clearly both functional kinds and multiply realizable kinds. What makes something a computer is what it does — it computes…</p>
<p>All sorts of materials can be deployed to construct computers. Computers have been built from transistors and other electronic components. Others have been built from mechanical components such as cams and gears. A computer that plays tic-tac-toe has even been constructed out of Tinkertoys!</p>
<p>…a [computer] program is not identical to the activity of a <em>particular</em> computer. If brains made out of brainy stuff can just as well give rise to a mind as an electronic computer made out of non-brainy stuff, then perhaps [many functionalists suggest] the solution to the mind–body problem is to think of the mind as the software that is running on the hardware of the brain.</p></blockquote>
<p>I should clarify, however, that even those functionalists who speak of mind (including consciousness) as a type of computation don&#8217;t necessarily think a human mind is a “traditional” computer program (e.g. a <a href="https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence">GOFAI</a> program) running on a brain-implemented <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">Von Neumann architecture</a>, nor do they necessarily think that all relevant information processing occurs at the scale of neurons or larger — see e.g. <a href="https://global.oup.com/academic/product/computing-the-mind-9780195320671?cc=us&amp;lang=en&amp;">Edelman (2008)</a>, chs. 1-4.<br /><br />
I use computational language regularly in this report because I think it helps to clarify what I mean, but I have not studied the philosophy of computation (<a href="http://plato.stanford.edu/entries/computer-science/">Turner 2013</a>) much, and I don&#8217;t mean to assume any particular narrow conception of what does and doesn&#8217;t count as “computation” or a “computer program.” My assumption for this report is just <em>functionalism</em>, broadly defined.<br /><br />
There are different types of functionalism (about consciousness), of course (<a href="http://mcps.umn.edu/philosophy/9_12Block.pdf">Block 1978</a>; <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199262618.001.0001/oxfordhb-9780199262618-e-8">Van Gulick (2009)</a>; ch. 9 of <a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz 2012</a>; <a href="http://philosophiascientiae.revues.org/861?lang=en">Maley &amp; Piccinini 2013</a>). Intuitively, the sort of functionalism (about consciousness) to which I subscribe is perhaps most similar to what Aaron Sloman calls “virtual machine functionalism” (<a href="http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1350">Sloman &amp; Chrisley 2003</a>; <a href="http://www.cs.bham.ac.uk/research/projects/cogaff/misc/vm-functionalism.html">Sloman 2016</a>).</p>
</li>
<li class="footnote" id="footnote44_7pced46"><a class="footnote-label" href="#footnoteref44_7pced46">44.</a> For a recent review of the many parallels between contemporary neuroscience and machine learning research, see <a href="https://arxiv.org/abs/1606.03813">Marblestone et al. (2016)</a>.
</li>
<li class="footnote" id="footnote45_wshh6i0"><a class="footnote-label" href="#footnoteref45_wshh6i0">45.</a> And indeed, careful experiments have taught us much about how this illusion is produced. See e.g. <a href="https://global.oup.com/academic/product/filling-in-9780195140132?cc=us&amp;lang=en&amp;">Pessoa &amp; Weerd (2003)</a>.
</li>
<li class="footnote" id="footnote46_8kntxbz"><a class="footnote-label" href="#footnoteref46_8kntxbz">46.</a> Unlike physicalism, functionalism, and illusionism, “fuzziness” is not a standard term.
</li>
<li class="footnote" id="footnote47_fz2gjqm"><a class="footnote-label" href="#footnoteref47_fz2gjqm">47.</a> <a href="http://www.jstor.org/stable/40971115">Dennett (1995)</a>.
</li>
<li class="footnote" id="footnote48_raanpdr"><a class="footnote-label" href="#footnoteref48_raanpdr">48.</a> As also mentioned in <a href="#AppendixZ5">Appendix Z.5</a>, the 2009 <a href="http://philpapers.org/surveys/results.pl">PhilPapers Survey</a> found that among “Target Faculty,” 56.5% of respondents accepted or leaned toward physicalism about the mind, 27.1% of respondents accepted or leaned toward non-physicalism about the mind, and 16.4% of respondents gave an “Other” response.<br /><br />
The <a href="http://philpapers.org/surveys/results.pl">PhilPapers Survey</a> did not ask about functionalism directly. But, it seems to be a widely held understanding that the vast majority of philosophers of mind, but not all of them, are functionalists. Some example quotes are given below, in chronological order.<br /><br /><a href="http://mcps.umn.edu/philosophy/9_12Block.pdf">Block (1978)</a>:
<blockquote><p>The functionalist approach to the philosophy of mind is increasingly popular; indeed, it may now be dominant (Armstrong, 1968; Block &amp; Fodor, 1972; Field, 1975; Fodor, 1965, 1968a; Grice, 1975; Harman, 1973; Lewis, 1971, 1972; Locke, 1968; Lycan, 1974; Nelson, 1969, 1975; Putnam, 1966, 1967, 1970, 1975a; Pitcher, 1971; Sellars, 1968; Shoemaker, 1975; Smart, 1971; Wiggins, 1975).</p></blockquote>
<p><a href="https://mitpress.mit.edu/books/matter-and-consciousness">Churchland (1988)</a>, ch. 2:</p>
<blockquote><p>As this book is written, functionalism is probably the most widely held theory of mind among philosophers, cognitive psychologists, and artificial intelligence researchers.</p></blockquote>
<p><a href="http://prism.ucalgary.ca/handle/1880/29130">Ryder (1996)</a>: </p>
<blockquote><p>Most theories of consciousness are functional theories, as functionalism is something of a “received view” among materialists.</p></blockquote>
<p><a href="https://global.oup.com/academic/product/the-evolution-of-consciousness-9780198503248?cc=us&amp;lang=en&amp;">Macphail (1998)</a>, p. 213:</p>
<blockquote><p>The idea that consciousness is a product of functional organization lies at the heart of what is now the most widely held materialist account of the mind-body problem — so widely held that it has been claimed [by <a href="https://mitpress.mit.edu/books/rediscovery-mind">Searle (1992)</a>, p. 7] that functionalism now constitutes a virtual orthodoxy among psychologists and philosophers.</p></blockquote>
<p><a href="https://global.oup.com/academic/product/consciousness-9780198520917?cc=us&amp;lang=en&amp;">Gray (2004)</a>, p. vii, says “Today&#8217;s dominant view is functionalism,” though interestingly Gray defines functionalism as “the doctrine that states of consciousness can be identified with sets of functional (input-output) relationships that hold between a behaving organism and the environment in which it behaves, which is closer to how I might define <em>behaviorism</em>. In my sense of the term, functionalism need not be defined with respect to input-output relationships that hold between a behaving organism and its environment — see e.g. my comments on <a href="#Inessentialism">consciousness inessentialism</a>.</p>
<p><a href="https://westviewpress.com/books/philosophy-of-mind/">Kim (2010)</a>, ch. 5:</p>
<blockquote><p>In 1967 Hilary Putnam published a paper… [that] ushered in functionalism, which has since been a highly influential — arguably the dominant — position on the nature of mind.</p></blockquote>
<p><a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470674504.html">Mandik (2013)</a>, p. 122:</p>
<blockquote><p>Functionalism is the most popular current position on the mind–body problem…</p></blockquote>
<p><a href="http://www.sciencedirect.com/science/article/pii/S0893608013000968">Reggia (2013)</a>:</p>
<blockquote><p>It is probably the case that the vast majority of individuals investigating the philosophical and scientific basis of consciousness today, including those developing computer models of consciousness, are functionalists…</p></blockquote>
<p><a href="https://www.routledge.com/Philosophy-of-Mind-A-Contemporary-Introduction-3rd-Edition/Heil/p/book/9780415891752">Heil (2013)</a>, p. 87:</p>
<blockquote><p>These days functionalism dominates the landscape in the philosophy of mind, in cognitive science, and in psychology… When basic tenets of functionalism are put to non-philosophers, the response is, often enough, “Well, that&#8217;s obvious, isn&#8217;t it?”</p></blockquote>
<p>Nevertheless, functionalism is debated heavily within philosophy. Those arguments are well-covered elsewhere: see e.g. chapter 6 of <a href="http://www.polity.co.uk/book.asp?ref=9780745653440">Weisberg (2014)</a>; <a href="https://mitpress.mit.edu/books/consciousness-function-and-representation">Block (2007a)</a>; <a href="http://plato.stanford.edu/entries/functionalism/">Levin (2013)</a>; <a href="http://plato.stanford.edu/entries/qualia/">Tye (2015)</a>; <a href="https://global.oup.com/academic/product/the-multiple-realization-book-9780198732891?cc=us&amp;lang=en&amp;">Polger &amp; Shapiro (2016)</a>.</p>
<p>Note that some of the theories I consider “functionalist” are sometimes called “eliminativist.” See my comments on eliminativism in <a href="#AppendixZ6">Appendix Z.6</a>.</p>
<p>I&#8217;m not aware of surveys indicating how common illusionist approaches are, though <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00001">Frankish (2016a)</a> remarks that:</p>
<blockquote><p>The topic of this special issue is the view that phenomenal consciousness (in the philosophers’ sense) is an illusion — a view I call <em>illusionism</em>. This view is not a new one: the first wave of identity theorists favoured it, and it currently has powerful and eloquent defenders, including Daniel Dennett, Nicholas Humphrey, Derk Pereboom, and Georges Rey. However, it is widely regarded as a marginal position, and there is no sustained interdisciplinary research programme devoted to developing, testing, and applying illusionist ideas. I think the time is ripe for such a programme. For a quarter of a century at least, the dominant physicalist approach to consciousness has been a realist one. Phenomenal properties, it is said, are physical, or physically realized, but their physical nature is not revealed to us by the concepts we apply to them in introspection. This strategy is looking tired, however. Its weaknesses are becoming evident…, and some of its leading advocates have now abandoned it. It is doubtful that phenomenal realism can be bought so cheaply, and physicalists may have to accept that it is out of their price range. Perhaps phenomenal concepts don&#8217;t simply fail to represent their objects as physical but <em>mis</em>represent them as phenomenal, and phenomenality is an introspective illusion…</p></blockquote>
<p>I don&#8217;t know how widespread my “fuzziness” assumption is.</p>
</li>
<li class="footnote" id="footnote49_s8nfpwn"><a class="footnote-label" href="#footnoteref49_s8nfpwn">49.</a> <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00005">Dennett (2016a)</a>.
</li>
<li class="footnote" id="footnote50_k6nig23"><a class="footnote-label" href="#footnoteref50_k6nig23">50.</a> Classic sources and contemporary overviews include <a href="https://global.oup.com/academic/product/the-conscious-mind-9780195117899?cc=us&amp;lang=en&amp;">Chalmers (1996)</a>, <a href="http://www.cambridge.org/nu/academic/subjects/philosophy/philosophy-mind-and-language/phenomenal-consciousness-naturalistic-theory?format=HB">Carruthers (2000)</a>, <a href="https://books.google.com/books?id=dRpzhcWWbxIC">Frankish (2005)</a>, <a href="http://www.polity.co.uk/book.asp?ref=9780745653440">Weisberg (2014)</a>, <a href="http://link.springer.com/article/10.1007/s11245-014-9266-3">Carruthers &amp; Schier (2014)</a>, several chapters of <a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-1405120193.html">Velmans &amp; Schneider (2007)</a>, and several chapters of <a href="https://global.oup.com/academic/product/the-oxford-handbook-of-philosophy-of-mind-9780199262618?cc=us&amp;lang=en&amp;">McLaughlin et al. (2009)</a>. On illusionism, see Volume 23, <a href="http://www.ingentaconnect.com/content/imp/jcs/2016/00000023/f0020011">Numbers 11-12</a> of the <em>Journal of Consciousness Studies</em>.
</li>
<li class="footnote" id="footnote51_43r0rif"><a class="footnote-label" href="#footnoteref51_43r0rif">51.</a> It might also be helpful to consider historical cases of conceptual revision in which it was commonly thought that some view could be ruled out <em>a priori</em>, but in fact that supposedly ruled-out view is now the mainstream scientific view on the topic, e.g. perhaps “space” and “time” in the face of special relativity, or the idea of <a href="http://lesswrong.com/lw/pl/no_individual_particles/">individually identifiable particles in the face of quantum mechanics</a>.
</li>
<li class="footnote" id="footnote52_2zri1sx"><a class="footnote-label" href="#footnoteref52_2zri1sx">52.</a> Many sources employ a mix of both strategies. Example sources that seem to <em>primarily</em> use an “apply a theory” strategy include <a href="http://www.jstor.org/stable/2027110">Carruthers (1989)</a>, <a href="http://www.jstor.org/stable/40971115">Dennett (1995)</a>, ch. 8 of <a href="https://mitpress.mit.edu/books/consciousness-color-and-content">Tye (2000)</a>, <a href="http://www.sciencedirect.com/science/article/pii/S1053810003000023">Merker (2005)</a>, and <a href="http://www.pnas.org/content/113/18/4900.short">Barron &amp; Klein (2016)</a>. Example sources that seem to primarily use a “potentially consciousness-indicating features” strategy include ch. 4 of <a href="https://books.google.com/books?id=wzhrAAAAMAAJ">Smith &amp; Boyd (1991)</a>, <a href="http://www.sciencedirect.com/science/article/pii/S0003347205801277">Bateson (1991)</a>, <a href="http://www.ingentaconnect.com/content/imp/jcs/2008/00000015/00000003/art00001">Beshkar (2008)</a>, <a href="https://books.google.com/books?id=aMvonPqzu_cC&amp;printsec=frontcover">Braithwaite (2010)</a>, <a href="https://global.oup.com/academic/product/personhood-ethics-and-animal-cognition-9780199758784?cc=us&amp;lang=en&amp;">Varner (2012)</a>, <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>, <a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye (2016)</a>, and perhaps <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2010/00000017/F0020003/art00008">Arrables (2010)</a>. Note that most of my examples of the second kind aim to assess the likelihood of a taxon&#8217;s capacity for conscious <em>pain</em> in particular. But of course a capacity for conscious pain presumes a capacity for consciousness.
</li>
<li class="footnote" id="footnote53_fxfztt5"><a class="footnote-label" href="#footnoteref53_fxfztt5">53.</a> I&#8217;m not aware of a poll that asked this question of consciousness researchers, but I provide a few supporting sources below. Obviously this is not sufficient to prove that my impression is true. Instead my aim in this footnote is to point to a few example sources that left me with my current impressions.<br /><br />
Naturally, mysterians agree that no currently proposed theory of consciousness is clearly promising. See e.g. <a href="http://content.time.com/time/printout/0,8816,1580394,00.html">Pinker (2007); </a><a href="https://global.oup.com/academic/product/consciousness-and-its-objects-9780199267606?cc=us&amp;lang=en&amp;">McGinn (2004)</a>; <a href="http://www.cambridge.org/ir/academic/subjects/philosophy/philosophy-mind-and-language/nature-consciousness?format=HB">Rowlands (2001)</a>.<br /><br />
Many of those who write about the methodological difficulties of consciousness science also seem to share my general impression, e.g. <a href="http://www.springer.com/us/book/9789400751729">Irvine (2013)</a>, the authors of several chapters in <a href="https://benjamins.com/#catalog/books/aicr.92/main">Miller (2015)</a>, and the authors of several chapters in <a href="https://global.oup.com/academic/product/behavioural-methods-in-consciousness-research-9780199688890?cc=us&amp;lang=en&amp;">Overgaard (2015)</a>.<br /><br />
As another example, here is a passage from a review article on recent progress in consciousness science, written by several leaders in the field (<a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00625/full">Boly et al. 2013</a>):
<blockquote><p>In order to consolidate the results of many relevant experiments that have been conducted within a single conceptual framework, theories of consciousness must become more precise and generate experimentally testable predictions. Accomplishing this requires both additional conceptual work from theorists and greater knowledge of brain architecture and neural computations relevant to consciousness, in order to guide and constraint theory development… Overall, theoretical developments will help move from simple correlation between neural events and conscious level and content, toward causal and explanatory accounts that show how specific neural mechanisms give rise to specific aspects or dimensions of conscious phenomenology…</p></blockquote>
<p>Valerie Hardcastle, in chapter 12 of <a href="https://global.oup.com/academic/product/finding-consciousness-9780190280307?cc=us&amp;lang=en&amp;">Sinnott-Armstrong (2016)</a>, is especially blunt:</p>
<blockquote><p>I shall begin by stating what I believe to be obvious: We do not know what consciousness is…</p>
<p>…we do not have a good definition for consciousness, we do not know what the relevant psychological attributes of consciousness are, and we have no idea what the neural correlates for consciousness are either. We are not clear on what is sufficient for consciousness, and we at best have an incomplete list of what is necessary. We do not understand the relationship between alertness and awareness, if there is one, nor do we understand the connection between cognitive processing and consciousness, if there is one. At best, we can point to some things that some people believe index some aspects of consciousness. But by the standards of contemporary science and medicine, that is not pointing to very much at all.</p></blockquote>
<p>See also <a href="http://www.ingentaconnect.com/content/imp/jcs/2013/00000020/F0020005/art00003">Katz (2013)</a> and <a href="https://www.theguardian.com/science/2015/jan/21/-sp-why-cant-worlds-greatest-minds-solve-mystery-consciousness">Burkeman (2015)</a>.</p>
</li>
<li class="footnote" id="footnote54_0ghkbws"><a class="footnote-label" href="#footnoteref54_0ghkbws">54.</a> To test this hypothesis, one could enumerate theories of consciousness matching some criteria, and then check the year of “first peer-reviewed defense of the theory” (or similar) for each theory. I have not done this, but my impression is that most consciousness researchers would agree that theories of consciousness have proliferated greatly over the last couple decades.<br /><br />
I&#8217;ll quote just one example, from <a href="http://academicworks.cuny.edu/gc_etds/1604/">Shevlin (2016)</a>, pp. 191: “The last two decades have witnessed an explosion in the variety of theories of consciousness…”
</li>
<li class="footnote" id="footnote55_hmlbe4b"><a class="footnote-label" href="#footnoteref55_hmlbe4b">55.</a> <a href="https://en.wikipedia.org/wiki/Bj%C3%B6rn_Merker">Björn Merker</a> expressed this point to me, in an August 2016 email, this way (quoted with permission):<br /><blockquote><p>Consciousness theory currently labors in what is obviously a pre-paradigmatic stage of development. In this typically protracted prehistory of a science, competing schools in a nascent field find themselves in disagreement over fundamentals.</p>
<p>As described [by Thomas Kuhn], at this stage in the history of a science each competing school builds its system from its own first principles, occasionally metaphysical, in reliance on a rich array of observations and arguments but without criteria for assessing their relative significance either within or across schools. None of the schools is therefore able to take its fundamentals for granted, and each is forced to constantly reiterate a complex system of facts and interpretations, essentially “from scratch.” Argument tends to be interminable when even first principles are in dispute.</p>
<p>Kuhn&#8217;s description of the pre-paradigmatic stage of a nascent science applies rather literally to the current state of consciousness theory. It features a disparate array of competing proposals regarding the nature of consciousness, its scope, and genesis, with no agreement on first principles underlying analysis and interpretation. Thus, at one extreme consciousness is seriously proposed to be an intrinsic property of this universe itself on a par with mass, charge and space-time [David Chalmers], and at the other it is construed as a function or product of human language [Euan Macphail]. A field in which such diversity of fundamental commitments regarding its very subject matter can be taken seriously obviously has not yet arrived at the shared paradigm within which normal science, in Kuhn&#8217;s sense, proceeds to solve puzzles.</p></blockquote>
<p>For Kuhn&#8217;s account of the “pre-paradigmatic stage of development,” see chapter 2 of <a href="http://press.uchicago.edu/ucp/books/book/chicago/S/bo13179781.html">Kuhn (2012)</a>. (The first edition of Kuhn&#8217;s book was published in 1962.)</p>
<p>Or, here is a contemporary summary, from <a href="http://plato.stanford.edu/entries/thomas-kuhn/">Bird (2011)</a>:</p>
<blockquote><p>Kuhn describes an immature science, in what he sometimes calls its ‘pre-paradigm’ period, as lacking consensus. Competing schools of thought possess differing procedures, theories, even metaphysical presuppositions. Consequently there is little opportunity for collective progress. Even localized progress by a particular school is made difficult, since much intellectual energy is put into arguing over the fundamentals with other schools instead of developing a research tradition. However, progress is not impossible, and one school may make a breakthrough whereby the shared problems of the competing schools are solved in a particularly impressive fashion. This success draws away adherents from the other schools, and a widespread consensus is formed around the new puzzle-solutions.</p></blockquote>
<p>On the state of consciousness studies, see also e.g. <a href="https://mitpress.mit.edu/books/being-no-one">Metzinger (2003)</a>, p. 116:</p>
<blockquote><p>…there is yet no single, unified and paradigmatic theory of consciousness in existence which could serve as an object for constructive criticism and as a backdrop against which new attempts could be formulated. Consciousness research is still in a preparadigmatic stage.</p></blockquote>
<p>But this assessment is not universal. Bill Faw, in his entry “Consciousness, modern scientific study of” on pp. 182-188 of <a href="https://global.oup.com/academic/product/oxford-companion-to-consciousness-9780198569510?cc=us&amp;lang=en&amp;">Bayne et al. (2009)</a>, writes:</p>
<blockquote><p>To use Kuhn&#8217;s term, we might think of the period 1980–94 as representing the transition from the <em>pre-paradigm</em> stage of consciousness science to a <em>normal science</em> stage… The second half of the period — from 1994 to 2008 — constitutes what we might think of as an early phase of normal consciousness science.</p></blockquote>
</li>
<li class="footnote" id="footnote56_wcgn2zx"><a class="footnote-label" href="#footnoteref56_wcgn2zx">56.</a> I also found that it was often difficult for me to understand what, exactly, the authors of these theories are claiming, what evidence they think would falsify their theories, which aspects of their theories were intended as claims about consciousness in humans (or primates) rather than as claims about consciousness <em>in general</em>, and which aspects of their theories were intended as claims about scientific explanation as opposed to expressions about which types of processes they intuitively morally value.
</li>
<li class="footnote" id="footnote57_rikx6ql"><a class="footnote-label" href="#footnoteref57_rikx6ql">57.</a> See also <a href="https://books.google.com/books?id=ZEYvkeRq2ZEC&amp;lpg=PT115&amp;ots=DSmedW6rPC&amp;pg=PT115#v=onepage&amp;q&amp;f=false">Mitchell (2005)</a>.
</li>
<li class="footnote" id="footnote58_oagnhoz"><a class="footnote-label" href="#footnoteref58_oagnhoz">58.</a> For a more detailed discussion of the argument by analogy from my own consciousness to that of other humans, see <a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye (2016)</a>, ch. 4.
</li>
<li class="footnote" id="footnote59_iahqmss"><a class="footnote-label" href="#footnoteref59_iahqmss">59.</a> Pages 114-115.
</li>
<li class="footnote" id="footnote60_qmtdo2r"><a class="footnote-label" href="#footnoteref60_qmtdo2r">60.</a> For example, if I wanted to argue in favor of fish consciousness, I could present a table like this:<br /><table><tr><th>Potentially consciousness-indicating feature</th>
<th>True of a human?</th>
<th>True of a fish?</th>
</tr><tr><td>1. Forms and uses mental representations</td>
<td>Yes</td>
<td>Yes</td>
</tr><tr><td>2. Associates a current mental state with a memory</td>
<td>Yes</td>
<td>Yes</td>
</tr><tr><td>3. Can process emotions with a certain part of the brain</td>
<td>Yes</td>
<td>Yes</td>
</tr><tr><td>4. Can alter its view of an aversive situation depending on context</td>
<td>Yes</td>
<td>Yes</td>
</tr><tr><td>5. Can consider possible actions and ponder their consequences</td>
<td>Yes</td>
<td>Yes</td>
</tr></table><p>
But if I wanted to nudge you toward thinking that higher primates are conscious and fishes are <em>not</em>, and I knew that you were already inclined to think laptops aren&#8217;t conscious, I could instead present the following table:<br /></p>
<table><tr><th>Potentially consciousness-indicating feature</th>
<th>True of a human?</th>
<th>True of a chimpanzee?</th>
<th>True of a fish?</th>
<th>True of a laptop?</th>
</tr><tr><td>1. Forms and uses mental representations</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr><tr><td>2. Associates a current mental state with a memory</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr><tr><td>3. Can process emotions with a certain part of the brain</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr><tr><td>4. Can alter its view of an aversive situation depending on context</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr><tr><td>5. Can consider possible actions and ponder their consequences</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr><tr><td>6. Has a neocortex</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr><tr><td>7. Passes the mirror self-recognition test</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr><tr><td>8. Engages in complex social politics</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr></table><p>
Note that my first example table is adapted from <a href="https://books.google.com/books?id=aMvonPqzu_cC&amp;printsec=frontcover">Braithwaite (2010)</a>’s summary (at the end of chapter 4) of her case for fish consciousness, but it should <em>not</em> be attributed to her, since Braithwaite&#8217;s argument is more nuanced than what I&#8217;ve put in my example PCIFs table.<br /><br />
Braithwaite summarizes her case for fish consciousness in the paragraph below. I&#8217;ve added the numbered PCIFs from my example table to illustrate the similarities:</p>
<blockquote><p>So pulling the different threads together, fish really do appear to possess key traits associated with consciousness. Their ability to form and use mental representations indicates fish have some degree of access consciousness [PCIF #1]. They can consider a current mental state and associate it with a memory [PCIF #2]. Having an area of the brain specifically associated with processing emotion [PCIF #3] and evidence that they alter their view of an aversive situation depending on context [PCIF #4] suggests that fish have some form of phenomenal consciousness: they are sentient. This leaves monitoring and self consciousness, which I argue is in part what the eel and the grouper are doing: considering their actions and pondering the consequences [PCIF #5]. The grouper is clearly deciding it has no chance to get the prey itself and so swims off to get the eel. The eel is deciding that an easy meal is on offer. On balance then, fish have a capacity for some forms of consciousness, and so I conclude that they therefore have the mental capacity to feel pain. I suspect that what they experience will be different and simpler than the experiences we associate with pain and suffering, but I see no evidence to deny them these abilities, and quite a bit which argues that they will suffer from noxious stimuli.</p></blockquote>
<p>In the second example table, I compare fishes and laptops according to these PCIFs (plus a few others), but note that Braithwaite explicitly denies that laptops have consciousness (<a href="http://link.springer.com/chapter/10.1007/7854_2014_278">Droege &amp; Braithwaite 2015</a>).</p>
</li>
<li class="footnote" id="footnote61_gpfyzpk"><a class="footnote-label" href="#footnoteref61_gpfyzpk">61.</a> Each row in my PCIFs table is meant to report on the status of that row&#8217;s PCIF in normally-functioning, adult members of each taxon, except where that designation has no meaning.<br /><br />
Details on why I chose each taxon:<br /><table><tr><th>Taxon</th>
<th>Why include this taxon?</th>
</tr><tr><td><a href="https://en.wikipedia.org/wiki/Anatomically_modern_human">Human</a> (<em>Homo sapiens sapiens</em>)</td>
<td>For comparison.</td>
</tr><tr><td><a href="https://en.wikipedia.org/wiki/Chimpanzee">Chimpanzee</a> (<em>Pan troglodytes</em>)</td>
<td>For comparison: it&#8217;s the non-human species I&#8217;m most confident is conscious, given how closely related it is to humans, and how sophisticated its behavior and cognition appear to be.</td>
</tr><tr><td><a href="https://en.wikipedia.org/wiki/Cattle">Cow</a> (<em>Bos taurus</em>)</td>
<td>I wanted to include a heavily-consumed mammal that seems much less cognitively sophisticated than a chimpanzee.</td>
</tr><tr><td><a href="https://en.wikipedia.org/wiki/Chicken">Chicken</a> (<em>Gallus gallus domesticus</em>)</td>
<td>I wanted to include a bird species, and the chicken is the most heavily-consumed.</td>
</tr><tr><td><a href="https://en.wikipedia.org/wiki/Rainbow_trout">Rainbow trout</a> (<em>Oncorhynchus mykiss</em>)</td>
<td>I wanted to include a fish species. Among those fish species that are heavily-consumed, the rainbow trout is one of the most well-studied with respect to PCIFs, especially pain-related PCIFs. I could have chosen a subspecies, but unfortunately many studies of <em>Oncorhynchus mykiss</em> do not specify which subspecies was examined.</td>
</tr><tr><td><a href="https://en.wikipedia.org/wiki/Portunus_trituberculatus">Gazami crab</a> (<em>Portunus trituberculatus</em>)</td>
<td>I wanted to include a decapod species. Among decapods, some shrimp and crayfish species might be harvested in greater numbers than any crab species, but crabs have thus far been more thoroughly studied with respect to their likelihood of consciousness, largely due to the work of Robert W. Elwood. The Gazami crab is one of the most heavily-consumed species of crab.</td>
</tr><tr><td><a href="https://en.wikipedia.org/wiki/Drosophila_melanogaster">Common fruit fly</a> (<em>Drosophila melanogaster</em>)</td>
<td>I wanted to include an insect species, and the common fruit fly is perhaps the most-studied insect.</td>
</tr><tr><td><a href="https://en.wikipedia.org/wiki/Escherichia_coli"><em>E. coli</em></a></td>
<td>I wanted to include a single-celled species of bacteria for comparison purposes, and <em>E. coli</em> is among the most-studied species of bacteria.</td>
</tr><tr><td>Function sometimes executed non-consciously in humans?</td>
<td>For comparison; see explanation <a href="#nonconsciously">here</a>.</td>
</tr><tr><td>Adult human <a href="https://en.wikipedia.org/wiki/Enteric_nervous_system">enteric nervous system</a> (ENS)</td>
<td>For comparison; I wanted to include a biological sub-system that most people think of as non-conscious.</td>
</tr></table><p>My explanation for choosing each PCIF in the table is given in the footnote which appears in the first cell of each row of the table, except in cases where the reason for a PCIFs inclusion seems sufficiently obvious to me (e.g. brain mass), or in some cases where I did not take the time to say anything at all about a PCIF beyond listing it.</p>
</li>
<li class="footnote" id="footnote62_t8x094m"><a class="footnote-label" href="#footnoteref62_t8x094m">62.</a> <a href="http://www.hmhco.com/shop/books/The-Feeling-of-What-Happens/9780156010757">Damasio (1999)</a>, p. 6. Absence seizures involving automatisms are called “complex” absence seizures, and are more common than “simple” absence seizures (i.e. without automatisms). For more on absence automatisms, see e.g. <a href="http://jamanetwork.com/journals/jamaneurology/article-abstract/569081">Penry &amp; Dreifuss (1969)</a>; <a href="https://books.google.com/books?id=mxE2FYWoY0wC&amp;lpg=PA192&amp;pg=PA192#v=onepage&amp;q&amp;f=false">Arzimanoglou &amp; Ostrowsky-Coste (2010)</a>.
</li>
<li class="footnote" id="footnote63_fhcnhj6"><a class="footnote-label" href="#footnoteref63_fhcnhj6">63.</a> <a href="http://www.penguinrandomhouse.com/books/313935/anxious-by-joseph-ledoux/9780143109044/">LeDoux (2015)</a>, ch. 6, makes the point this way:<br /><blockquote><p>One strategy used to explore consciousness in animals assumes that if an organism can solve complex problems behaviorally, it has complex mental capacities and therefore mental state consciousness. But this approach conflates cognitive capacities with consciousness, which we&#8217;ve seen are not the same. Animals are not, as Descartes characterized them, simple beast machines that only react reflexively to the world. They use internal (cognitive) processing of external events to help them pursue goals, make decisions, and solve problems. But because the human brain can often carry out these same tasks nonconsciously, the mere existence of such cognitive capacities in animals can&#8217;t be used as evidence that consciousness was involved.</p></blockquote>
<p>Similarly, here is <a href="http://rstb.royalsocietypublishing.org/content/370/1668/20140167.abstract">Tononi &amp; Koch (2015)</a>:</p>
<blockquote><p>…the lessons learnt from studying the behavioural… and neuronal correlates of consciousness in people must make us cautious about inferring its presence in creatures very different from us, no matter how sophisticated their behaviour and how complicated their brain. Humans can perform complex behaviours—recognizing whether a scene is congruous or incongruous, controlling the size, orientation and strength of how one&#8217;s finger should grip an object, doing simple arithmetic, detecting the meaning of words or rapid keyboard typing—in a seemingly non-conscious manner [61–66]. When a bee navigates a maze, does it do so like when we consciously deliberate whether to turn right or left, or rather like when we type on a keyboard?</p></blockquote>
<p><a href="http://www.sciencedirect.com/science/article/pii/S0065345414000023">Dawkins (2015)</a> puts it this way:</p>
<blockquote><p>There are several reasons [to be cautious about inferences from behavior or physiology to consciousness]. First, we know from our own experience that the three components of human emotion (autonomic/behavioral/cognitive) do not necessarily correlate with each other (Oatley &amp; Jenkins, 1996). Sometimes, for example, strong subjective emotions occur with no obvious autonomic changes, as when someone experiences a rapid switch from excitement to fear on a roller coaster. This does not mean that the change in emotional experience has no physiological basis. It just means that it is probably due to a subtle change in brain state rather than the obvious autonomic changes that are what are usually referred to as physiological (autonomic) measures of emotion…</p>
<p>Second, there is increasing evidence that much more human behavior than we had realized takes place without consciousness at all. Many complex tasks in humans, such as driving a car, playing a musical instrument, or even breathing can be carried out either consciously or unconsciously (Blackmore, 2012; Paul, Harding, &amp; Mendl, 2005; Rolls, 2014; Weiskrantz, 2003). Some human patients with certain sorts of brain damage can successfully reach out and touch objects in front of them but then say they are not conscious of having seen them at all (Weiskrantz, 2003). They are simultaneously blind (as far as their verbal reports go) but also sighted (unconsciously guided reaching). For much of what we humans do there appears to be multiple routes to the same behavior, only some of which reach consciousness (Rolls, 2014). But if the same action (e.g., breathing or touching an object) can occur in humans through either an unconscious or conscious pathway, the argument that if the behavior of another animal is similar to that of a human, that animal must be conscious (der Waal, 2005) is seriously weakened. An animal could be doing the same behavior as a human using his or her unconscious circuits (McPhail, 1998). Unconscious mechanisms explain much more of human behavior than previously thought and may also underlie much animal behavior (Shettleworth, 2010b). Many of the more complex aspects of animal behavior, such as corvid re-caching, that had previously thought to involve awareness can be mimicked by relatively simple computer programs without a theory of mind (van der Vaart, Verbrugge, &amp; Hemelrijk, 2012). In fact, a recent trend in comparative psychology has been away from emphasizing the complexity of animal behavior and toward emphasizing the simplicity of human behavior (Shettleworth, 2010b).</p>
<p>Humans can even have unconscious emotions and changes of emotional state that they are completely unaware of (Morris, Ohman, &amp; Dolan, 1998; Berridge &amp; Winkielman, 2003; Sato &amp; Aoki, 2006). This has important implications for our interpretation of animal emotions, because if we can have unconscious emotions, then the fact that animals behave ‘like us’ says much less about their consciousness or otherwise than we might think (Dawkins, 2001b, 2012).</p></blockquote>
</li>
<li class="footnote" id="footnote64_u4ncf9g"><a class="footnote-label" href="#footnoteref64_u4ncf9g">64.</a> For this column, I distinguish “conscious” and “non-conscious” processes in the normal way they are discussed in the psychological and neuroscientific literature, and thus I temporarily set aside the possibility of “hidden qualia” (see <a href="#AppendixH">Appendix H</a>). However, in the full analysis, this possibility must be considered: it is possible that many of the cognitive processes normally described by psychologists and neuroscientists as “unconscious” actually instantiate phenomenally conscious experience, but not for the “self” who can report experiences to an external observer.
</li>
<li class="footnote" id="footnote65_tzy2f5o"><a class="footnote-label" href="#footnoteref65_tzy2f5o">65.</a> Moreover, I wouldn&#8217;t be surprised if many of the studies cited in this section turn out to not “hold up” very well upon closer scrutiny; see <a href="#AppendixZ8">Appendix Z.8</a>.
</li>
<li class="footnote" id="footnote66_5k1ohmn"><a class="footnote-label" href="#footnoteref66_5k1ohmn">66.</a> I included “years since last common ancestor with humans” as a PCIF because it is a relatively theory-agnostic measure of “similarity to humans.”<br /><br />
My source for “years since last common ancestor with humans” was the website <a href="http://www.timetree.org/">TimeTree</a>, which compiles estimates from a variety of published sources. Here are the specific pages from which I drew my numbers (in August 2016): <a href="http://www.timetree.org/search/pairwise/Homo%20sapiens/Pan%20troglodytes">chimpanzees</a>, <a href="http://www.timetree.org/search/pairwise/Homo%20sapiens/Bos%20taurus">cows</a>, <a href="http://www.timetree.org/search/pairwise/Homo%20sapiens/Gallus%20gallus">chickens</a>, <a href="http://www.timetree.org/search/pairwise/Homo%20sapiens/Oncorhynchus%20mykiss">rainbow trout</a>, <a href="http://www.timetree.org/search/pairwise/Homo%20sapiens/Portunus%20trituberculatus">gazami crab</a>, <a href="http://www.timetree.org/search/pairwise/Homo%20sapiens/Drosophila%20melanogaster">common fruit fly</a>, <a href="http://www.timetree.org/search/pairwise/Homo%20sapiens/Escherichia%20coli"><em>E. coli</em></a>.
</li>
<li class="footnote" id="footnote67_fg2ko4z"><a class="footnote-label" href="#footnoteref67_fg2ko4z">67.</a> My sources of brain mass estimates are: <a href="http://www.pnas.org/content/113/26/7255.abstract">Olkowicz et al. (2016)</a> for humans, <a href="http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9861(19990712)409:4%3C567::AID-CNE4%3E3.0.CO;2-J/abstract">Herndon et al. (1999)</a> for chimpanzees, <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0154580">Ballarin et al. (2016)</a> for cows, and <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.0022-1112.2004.00394.x/full">Sangiao-Alvarellos et al. (2004)</a> for rainbow trout. For chickens I computed the average brain mass across the 80 domestic chickens (from 8 breeds) summarized in table 1 of <a href="http://www.karger.com/Article/Abstract/69352">Rehkämper et al. (2003)</a>.
</li>
<li class="footnote" id="footnote68_xdth79j"><a class="footnote-label" href="#footnoteref68_xdth79j">68.</a> For humans, see <a href="http://www.pnas.org/content/113/26/7255.abstract">Olkowicz et al. (2016)</a>. For chickens I just used <a href="http://www.pnas.org/content/113/26/7255.abstract">Olkowicz et al. (2016)</a>’s estimate for the red junglefowl, which is the same species but a different subspecies from the domestic chicken. For the human ENS, I used the midpoint of <a href="http://link.springer.com/chapter/10.1007%2F978-1-4939-0897-4_3">Furness et al. (2014)</a>’s estimate of “200-600 million neurons.”<br /><br />
As far as I know, no one has yet counted the number of neurons in the brains of chimpanzees, cows, rainbow trout, or gazami crabs. My source for an estimate of neurons in the brain of the common fruit fly is <a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674046337">Strausfeld (2012)</a>, p. 80.
</li>
<li class="footnote" id="footnote69_b7pwudm"><a class="footnote-label" href="#footnoteref69_b7pwudm">69.</a> For humans, see <a href="http://www.pnas.org/content/113/26/7255.abstract">Olkowicz et al. (2016)</a>. For chickens I just used <a href="http://www.pnas.org/content/113/26/7255.abstract">Olkowicz et al. (2016)</a>’s estimate for the red junglefowl, which is the same species but a different subspecies from the domestic chicken.<br /><br />
As far as I know, no one has yet counted the number of pallial neurons in chimpanzees, cows, and rainbow trout.
</li>
<li class="footnote" id="footnote70_293uira"><a class="footnote-label" href="#footnoteref70_293uira">70.</a> It&#8217;s my impression that encephalization quotient is quickly falling out of favor as an important predictor of higher cognitive capacities — see e.g. Herculano-Houzel (<a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.2011.05976.x/abstract">2011</a>, <a href="https://mitpress.mit.edu/books/human-advantage">2016</a>), <a href="http://www.karger.com/Article/Abstract/102973">Deaner et al. (2007)</a>, and <a href="http://www.pnas.org/content/111/20/E2140">MacLean et al. (2014)</a>. Nevertheless, I include here the numbers collected in table 1 of <a href="http://www.sciencedirect.com/science/article/pii/S1364661305000823">Roth &amp; Dickie (2005)</a>. Where that table lists a range, I used the midpoint of that range.
</li>
<li class="footnote" id="footnote71_p6nuce7"><a class="footnote-label" href="#footnoteref71_p6nuce7">71.</a> Typically, all mammals are considered to have a neocortex (<a href="http://www.cell.com/cell/abstract/S0092-8674(11)00705-7">Liu et al. 2011</a>), but there is some terminological debate about whether any non-mammals should be considered to have a “neocortex” (e.g. see <a href="http://www.nature.com/nrn/journal/v6/n2/full/nrn1606.html">Jarvis et al. 2005</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1002/cne.20118/abstract">Reiner et al. 2004</a>).<br /><br />
For example arguments that a neocortex (or some structure performing similar functions) might be required for consciousness, see <a href="#Cortex">my later section on that debate</a>.
</li>
<li class="footnote" id="footnote72_q8ueafl"><a class="footnote-label" href="#footnoteref72_q8ueafl">72.</a> See <a href="#AppendixD">Appendix D</a>.
</li>
<li class="footnote" id="footnote73_8du00au"><a class="footnote-label" href="#footnoteref73_8du00au">73.</a> See <a href="#AppendixD">Appendix D</a>.
</li>
<li class="footnote" id="footnote74_ue9lc28"><a class="footnote-label" href="#footnoteref74_ue9lc28">74.</a> By <em>nociceptive reflexes</em> I mean “movement away from noxious stimuli.” In general, see <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>. On rainbow trout in particular, see e.g. <a href="http://www.lapshin.iitp.ru/cherv_94e.pdf">Chervova et al. (1994)</a>.<br /><br />
On nociceptive reflexes without conscious experience, see e.g. <a href="http://ilarjournal.oxfordjournals.org/content/52/2/185.short">Crook &amp; Walters (2011)</a>:
<blockquote><p>Nociceptive reflexes and nociceptive plasticity can occur without conscious, emotional experience because these responses are expressed not only in the simplest animals but also in reduced preparations, such as spinalized animals [<a href="http://onlinelibrary.wiley.com/doi/10.1053/eujp.2001.0230/full">Clarke and Harris (2001)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1978.sp012337/full">Egger (1978)</a>] and snail ganglia [<a href="http://jn.physiology.org/content/50/6/1543.short">Walters et al. (1983)</a>]. Similarly, in human patients nociceptive reflexes can occur without conscious awareness below a level of complete spinal transection [<a href="http://onlinelibrary.wiley.com/doi/10.1046/j.1351-5101.2003.00725.x/full">Finnerup and Jensen (2004)</a>].</p></blockquote>
</li>
<li class="footnote" id="footnote75_lj82flm"><a class="footnote-label" href="#footnoteref75_lj82flm">75.</a> By “physiological responses” I have in mind <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>’s “one or a combination of the following: change in respiration, heart rate or hormonal levels (e.g. cortisol in some vertebrates).” That paper and <a href="http://jeb.biologists.org/content/218/7/967.short">Sneddon (2015)</a> are my central sources for the values I put in the cells of this row. For debate about the interpretation of some physiological responses to nociception in fishes, see <a href="http://onlinelibrary.wiley.com/doi/10.1111/faf.12010/full">Rose et al. (2014)</a>.
</li>
<li class="footnote" id="footnote76_hpizn44"><a class="footnote-label" href="#footnoteref76_hpizn44">76.</a> This PCIF is ill-defined and I did not investigate it, but see e.g. the discussions in <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>.
</li>
<li class="footnote" id="footnote77_o9flb1k"><a class="footnote-label" href="#footnoteref77_o9flb1k">77.</a> I have not investigated this PCIF, but some potentially relevant sources include <a href="http://link.springer.com/article/10.3758/BF03195979">Parker (2003)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S009130570400019X">Riley &amp; Freeman (2004)</a>; <a href="https://global.oup.com/academic/product/conditioned-taste-aversion-9780195326581?cc=us&amp;lang=en&amp;">Reilly &amp; Schachtman (2008)</a>.
</li>
<li class="footnote" id="footnote78_b27x760"><a class="footnote-label" href="#footnoteref78_b27x760">78.</a> My central source for this PCIF is <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>.
</li>
<li class="footnote" id="footnote79_0x8k75c"><a class="footnote-label" href="#footnoteref79_0x8k75c">79.</a> My central source for this PCIF is <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>.
</li>
<li class="footnote" id="footnote80_bfojwx0"><a class="footnote-label" href="#footnoteref80_bfojwx0">80.</a> My central source for this PCIF is <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>.
</li>
<li class="footnote" id="footnote81_ap3oo92"><a class="footnote-label" href="#footnoteref81_ap3oo92">81.</a> My central source for this PCIF is <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>.
</li>
<li class="footnote" id="footnote82_qybsp9f"><a class="footnote-label" href="#footnoteref82_qybsp9f">82.</a> I did not investigate this PCIF. For fishes, see <a href="http://www.sciencedirect.com/science/article/pii/S0003347214003431">Sneddon et al. (2014)</a>.
</li>
<li class="footnote" id="footnote83_m5flqcx"><a class="footnote-label" href="#footnoteref83_m5flqcx">83.</a> My central source for this PCIF is <a href="http://learnmem.cshlp.org/content/21/4/232">Gerber et al. (2014)</a>.
</li>
<li class="footnote" id="footnote84_m8zodey"><a class="footnote-label" href="#footnoteref84_m8zodey">84.</a> This is usually taken to be the most important PCIF, but it is typically thought to be prone to many false negatives: i.e. there are likely systems that are conscious but simply do not have the faculties needed to describe their conscious experiences to human scientists.<br /><br />
I should note that one <em>might</em> argue that some monkeys “report” some “detail” about their conscious experience in binocular rivalry studies, as <a href="https://global.oup.com/academic/product/the-unity-of-consciousness-9780199215386">Bayne (2010)</a>, p. 97, mentions:
<blockquote><p>…the science of consciousness draws on data from creatures whose ability to produce any kind of reports is questionable.</p>
<p>In an influential set of experiments designed to identify the neural correlates of visual consciousness, Logothetis and colleagues examined the neural responses of rhesus monkeys to binocular rivalry… The monkeys were first trained to press bars in response to various images — horizontal and vertical gratings, for example — and then presented with rivalrous stimuli. As expected, their responses closely modelled those of human observers to the same stimuli. The question that concerns us here is not what this research tells us about the neural correlates of visual experience, but what we should say about the monkeys’ button-presses. Logothetis and colleagues describe the monkeys as <em>reporting</em> their mental states, but I would want to resist this interpretation. It seems to me that there is little reason to suppose that the monkeys were producing reports of any kind let alone introspective reports. Arguably, to report that such-and-such is the case one has to conceive of… one&#8217;s behaviour as likely to bring about a particular belief in the mind of one&#8217;s audience — indeed, as likely to bring this belief about in virtue of the fact that one&#8217;s audience appreciates that one&#8217;s behavior carries the relevant informational content — and I know of no good reason to believe that the monkeys conceived of their button-presses in these terms.</p>
<p>Does it follow that we have no grounds for thinking that the monkeys were experiencing binocular rivalry? Not at all; in fact, I think the monkeys’ button-presses qualify as very good evidence for the claim that they had rivalrous experiences. However, their button-presses constitute evidence of consciousness not because they were reports of any kind but because they were <em>intentional actions</em>…</p></blockquote>
</li>
<li class="footnote" id="footnote85_12x1hpe"><a class="footnote-label" href="#footnoteref85_12x1hpe">85.</a> My sense is that these are not yet well-developed enough to serve as a quantitative PCIF, but they may become useful for that purpose within a decade or two. For reviews, see <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=10429157&amp;fulltextType=RA&amp;fileId=S0140525X16000959">Burkart et al. (forthcoming)</a>; <a href="http://www.cambridge.org/us/academic/subjects/computer-science/artificial-intelligence-and-natural-language-processing/measure-all-minds-evaluating-natural-and-artificial-intelligence?format=HB">Hernandez-Orallo (2017)</a>; <a href="http://rsos.royalsocietypublishing.org/content/3/4/160104">Kabadayi et al. (2016)</a>.
</li>
<li class="footnote" id="footnote86_wn6dc8w"><a class="footnote-label" href="#footnoteref86_wn6dc8w">86.</a> <a href="http://link.springer.com/article/10.1007/BF00852469">Ng (1995)</a> makes the case for plastic behavior as a strong indicator of consciousness, and points to <a href="https://books.google.com/books?id=AnKoBQAAQBAJ">Bunge (1980)</a>, p. 45 for a definition of “plasticity”:<br /><blockquote><p>The ability of the [central nervous system] to change either its composition or its organization (structure), and consequently some of its functions (activities), even in the presence of a (roughly) constant environment, is called <em>plasticity</em> (cf. Paillard, 1976). Plasticity seems to be characteristic of the associative cerebral cortex from birth to senility, to the point that this system has been characterized as “the organ capable of forming new functional organs”… In psychological terms, plasticity is the ability to learn and unlearn. From a monistic perspective learning is activating neural systems not previously engaged in the task in question, presumably by establishing or reinforcing certain synaptic connections.</p></blockquote>
</li>
<li class="footnote" id="footnote87_s73619h"><a class="footnote-label" href="#footnoteref87_s73619h">87.</a> <a href="https://books.google.com/books?id=OQGJz1DVQNMC&amp;lpg=PA45&amp;ots=DMrTWLYB4f&amp;lr=lang_en&amp;pg=PA45#v=onepage&amp;q&amp;f=false">Rial et al. (2008)</a> propose detour behaviors as a PCIF:<br /><blockquote><p>The detour behaviour represents the ability of an animal to reach a goal by moving round an interposed obstacle with temporal loss of sensorial contact… The acquisition of “object constancy” in the human child, i.e., the ability to understand that an object temporally hidden is the same after being retrieved, has received considerable attention… Similarly, the detour behaviour requires the maintenance of a memory of the location of a disappeared object, that is, an internal representation of the environment and the production of a “mental” experiment as the animal should construct a complex motor trajectory in advance to the final behavioural performance… Looking at comparative and phylogenetic studies on the detour behaviour, numerous examples have been described in mammals. In birds, it has been convincingly demonstrated in chickens, quails and in herring gulls, but not in canaries [<a href="http://www.sciencedirect.com/science/article/pii/S0093934X00923034">Vallortigara 2000</a>]…</p></blockquote>
<p>I did not check whether the detour behavior has been observed in other animals besides humans and chickens.</p>
</li>
<li class="footnote" id="footnote88_ha6u9u0"><a class="footnote-label" href="#footnoteref88_ha6u9u0">88.</a> <a href="https://books.google.com/books?id=OQGJz1DVQNMC&amp;lpg=PA45&amp;ots=DMrTWLYB4f&amp;lr=lang_en&amp;pg=PA45#v=onepage&amp;q&amp;f=false">Rial et al. (2008)</a> propose play behaviors as a PCIF:<br /><blockquote><p>…play shows several traits indicative of consciousness. Besides of being an onerous activity, play seems to be always pleasant. The only explanation for the play paradox lies in considering that the expenditure of energy must have a wide variation in hedonic value, from rather unpleasant to extremely pleasurable, that is, it shows a wide range of alliesthesia. An animal confronted with the possibility of playing should rank the costs and the benefits of each alternative and its final decision will aim at maximizing pleasure. Therefore, the presence of play should be a sign of consciousness.</p></blockquote>
<p>Overview sources on animal play include <a href="https://mitpress.mit.edu/books/genesis-animal-play">Burghardt (2005)</a>; <a href="http://us.macmillan.com/books/9781403986023">Balcombe (2006)</a>, ch. 4; <a href="http://www.jstor.org/stable/10.1086/656903">Graham &amp; Burghardt (2010)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S000334721100008X">Held &amp; Špinka (2011)</a>.<br /><br />
According to <a href="http://www.jstor.org/stable/10.1086/656903">Graham &amp; Burghardt (2010)</a>, “play is well-developed in primates, rodents, carnivorans, ungulates, elephants, and cetaceans,” and according to figure 1 has been observed in several other taxa as well, including birds and ray-finned fishes. I have filled in the cells of this row accordingly.</p>
</li>
<li class="footnote" id="footnote89_bzy2plq"><a class="footnote-label" href="#footnoteref89_bzy2plq">89.</a> I have not investigated this PCIF, but some potentially relevant sources include <a href="http://press.uchicago.edu/ucp/books/book/chicago/H/bo12233936.html">King (2013)</a>; <a href="http://animalstudiesrepository.org/animsent/vol1/iss4/1/">King (2016)</a> and the replies in that issue of <em>Animal Sentience</em>; Preti (<a href="http://prx.sagepub.com/content/101/3/831.short">2007</a>, <a href="http://psycnet.apa.org/journals/cri/32/1/1/">2011</a>).
</li>
<li class="footnote" id="footnote90_x2wjfqn"><a class="footnote-label" href="#footnoteref90_x2wjfqn">90.</a> For overviews, see e.g. <a href="http://rstb.royalsocietypublishing.org/content/369/1655/20130483">Verschure et al. (2014)</a>; <a href="http://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262016636.001.0001/upso-9780262016636-chapter-006">Dickinson (2011)</a>; <a href="http://link.springer.com/article/10.1007/s10670-012-9379-2">Trestman (2012)</a>.<br /><br />
Also, it is perhaps worth combating a pervasive anecdote used to suggest that insect behavior is rigid rather than adaptive and (at least sometimes) goal-directed. I refer to what <a href="http://www.tandfonline.com/doi/abs/10.1080/09515089.2012.690177">Keijzer (2012)</a> calls “the sphex story”:
<blockquote><p>The Sphex story is an anecdote about a female digger wasp that at first sight seems to act quite intelligently, but subsequently is shown to be a mere automaton that can be made to repeat herself endlessly. Dennett and Hofstadter made this story well known and widely influential within the cognitive sciences, where it is regularly used as evidence that insect behavior is highly rigid…</p>
<p>Here is the version [of the anecdote] that became a classic of cognitive science…: “When the time comes for egg laying, the wasp Sphex builds a burrow for the purpose and seeks a cricket which she stings in such a way as to paralyze but not kill it. She drags the cricket into the burrow, lays her eggs alongside, closes the burrow, then flies away, never to return. In due course, the eggs hatch and the wasp grubs feed off the paralyzed cricket, which has not decayed, having been kept in the wasp equivalent of a deep freeze. To the human mind, such an elaborately organized and seemingly purposeful routine conveys a convincing flavor of logic and thoughtfulness—until more details are examined. For example, the wasp&#8217;s routine is to bring the paralyzed cricket to the burrow, leave it on the threshold, go inside to see that all is well, emerge, and then drag the cricket in. If, while the wasp is inside making her preliminary inspection, the cricket is moved a few inches away, the wasp, on emerging from the burrow, will bring the cricket back to the threshold, but not inside, and will then repeat the preparatory procedure of entering the burrow to see that everything is all right. If again the cricket is removed a few inches while the wasp is inside, once again the wasp will move the cricket up to the threshold and re-enter the burrow for a final check. The wasp never thinks of pulling the cricket straight in. On one occasion this procedure was repeated forty times, always with the same result.” [<a href="https://books.google.com/books?id=mZgKAAAAMAAJ">Wooldridge (1963)</a>, pp. 82–83.]</p>
<p>The message is clear and simple. Behavior that seems to be strikingly intelligent is actually the result of a straightforward mechanical setup that involves a strict and rigid sequencing of environmental triggers to regulate the several steps involved. The insect is not at all aware of what it is doing and its internal processes are in this sense very different from the characteristics of human cognition. Hofstadter even coined the term ‘sphexish’ to refer to such an unknowing and mechanical form of “seeming intelligence,” and set it as “totally opposite to what we feel we are all about, particularly when we talk about our own consciousness” (1985, p. 529). Dennett (1984) used this notion to refer to the possibility that we might be sphexish ourselves, only less obviously so, and investigated possible implications for free will. The general idea here is that, if this rigidity of behavior is true for insects as a fundamental property that can be uncovered under the right circumstances, then the same should apply to the more complex but not intrinsically different case of human beings.</p>
<p>…[But] looking at this history, there are several striking features. First and foremost, digger wasps <em>very often do not repeat themselves endlessly</em> when the cricket test is done. After a few trials many wasps take the cricket into their burrow without the visit. Second, in certain cases there are ecological and practical reasons for repeating the visit. Third, the cricket test focuses on an extremely minor component of digger wasp behavior, which has since its discovery been completely swamped by many other findings that provide a very different general picture of the mind of the digger wasp.</p>
<p>…</p>
<p>[One example study is] a wonderfully sophisticated and extensive report on the cricket test derives from a five year study done by Jane Brockmann (1985). The cricket test was only one aspect of this study, discussed under the name of “prey-retrieval behavior.” First she discusses six natural reasons why the prey of <em>Sphex ichneumoneus</em> may be missing when the wasp reappears from the nest. Subsequently she describes the results of the cricket test performed systematically on 31 wasps. For each wasp, she used 15 different places for repositioning the prey, positioned at four different distances (2, 4, 6, and 8 cm) from the entrance, spread in four right-angled directions, the 16th position being the place where the wasp left her prey herself. Brockmann placed the prey at each of the 15 non-standard positions in random order, and then finished by placing it in the normal position, from which the wasp always drew it in. Twelve wasps came to the end of the full procedure, repeating the visit fifteen times. Ten wasps drew the prey in from another position, breaking the loop. Of the remainder, five gave up searching for their missing prey, while four did not finish for other reasons. In a retest with fourteen wasps, four wasps remained stuck in their loop, while five broke out if it (Brockmann, 1985, pp. 639–641). In her discussion, where she also takes into account many other findings concerning the provisioning behavior of the great golden digger, Brockmann says: “Although the behavior generally follows one scheme, there are many situations that arise and the wasps behave in an adaptive manner towards each&#8230; . The fixity of repeatedly repositioning and re-entering the nest is almost certainly an adaptive response to prey that can easily become lodged in the nest if pulled in backwards.” (1985, p. 651)</p>
<p>And as a final concluding remark: “The adaptable provisioning behavior of Sphex ichneumoneus would be surprising to anyone who viewed insect behavior as stereotyped and fixed. The versatility of individuals extended to all phases of their behavior, from the habitats in which they hunted, to the types of prey captured, to the behavior used in getting the prey into the brood cell. Where responses show stereotypy, such as in repeated prey retrievals, there is an obvious, adaptive explanation. I suspect that long-term studies of known individuals in other species of insects would similarly reveal the same kind of adaptive behavioral versatility.” (Brockmann, 1985, p. 652)</p></blockquote>
<p>See also e.g. <a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674046337">Strausfeld (2012)</a>, pp. 307-308, and <a href="http://www.dailymail.co.uk/news/article-3563146/Bee-pulls-NAIL-brick-wall-feat-amazing-strength-buzzing-after.html">Mallinson (2016)</a>.</p>
</li>
<li class="footnote" id="footnote91_ioa2k3x"><a class="footnote-label" href="#footnoteref91_ioa2k3x">91.</a> Descriptions of this test can be found on Wikipedia&#8217;s <a href="https://en.wikipedia.org/wiki/Mirror_test">mirror test</a> article, in <a href="http://link.springer.com/article/10.1007/s10329-015-0488-9">Anderson &amp; Gallup Jr. (2015)</a>, and especially in <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199548019.001.0001/oxfordhb-9780199548019-e-4">Gallup Jr. et al. (2011)</a>. There are many interpretation subtleties here, but from the few sources I&#8217;ve read, Dr. Gallup Jr. seems to be reasoning about these subtleties in a roughly reasonable way. For an example claim of mirror self-recognition in a robot, see <a href="http://www.panstanford.com/books/9789814364492.html">Takeno (2012)</a>. For a more recent journalistic overview, see <a href="https://www.theatlantic.com/science/archive/2017/02/what-do-animals-see-in-the-mirror/516348/">Yong (2017)</a>.<br /><br />
    Note that body self-recognition and “conceptual” self-recognition might be quite different functions, and thus evidence for the presence of one might not be strong evidence for the presence of the other, as explained by <a href="https://global.oup.com/academic/product/social-9780199645046?lang=en&amp;cc=fr">Lieberman (2013)</a>, pp. 185-186:
<blockquote><p>For forty years we have taken mirror self-recognition as a decisive sign of self-awareness in others, but the truth is more complicated. In Cartesian terms, this test focuses on the recognition of our body as our body…</p>
<p>	…In an fMRI study, participants were shown adjectives, such as polite and talkative. For some of the trials, participants had to judge whether the adjective described George W. Bush, who was the U.S. president at the time. On other trials, participants had to judge whether the adjectives described themselves. The critical analysis examined whether there were any regions of the brain that were more active when people judged the applicability of an adjective to themselves as opposed to George Bush. There were only two regions of the brain whose activity followed this pattern.</p>
<p>	Just as in the mirror self-recognition studies, there was activity in the prefrontal cortex and parietal cortex. But unlike the mirror self-recognition studies, these activations were present in the medial prefrontal cortex (MPFC) and the precuneus — on the midline of the brain where the two hemispheres meet, rather than on the lateral surface of the brain near the skull… In other words, recognizing yourself in the mirror and thinking about yourself conceptually rely on very different neural circuits. Seeing yourself and knowing yourself are two different things…</p>
<p>	…this distinction clarifies what the mirror self-recognition test tells us about the animals that can pass it. Chimps, dolphins, and elephants all have some sense of their corporeal identity, that the body they see in the mirror is their body. However, the fMRI data suggests that passing this test does not imply that these animals engage in self-reflection the same way that we do, reflecting on whether we possess a particular personality trait or wondering what will become of us in ten years. It does not imply that these animals reflection the wisdom of their past decisions. And it certainly does not imply that these animals come to have a conceptual sense of self through introspective contemplation.</p></blockquote>
</li>
<li class="footnote" id="footnote92_b26i214"><a class="footnote-label" href="#footnoteref92_b26i214">92.</a> For the relation between sleep and phenomenal consciousness in animals, see e.g. the discussion in <a href="http://link.springer.com/article/10.1007/s10806-011-9364-9">Allen (2013)</a>, pp. 30-32. On the distribution of sleep across the animal kingdom, see <a href="http://www.sciencedirect.com/science/article/pii/S0166223608000623">Siegel (2008)</a>.
</li>
<li class="footnote" id="footnote93_1k420qs"><a class="footnote-label" href="#footnoteref93_1k420qs">93.</a> My primary source for this PCIF is <a href="http://cdp.sagepub.com/content/14/1/19.abstract">Smith &amp; Washburn (2005)</a>. I concluded that chimpanzees “probably?” exhibit uncertainty monitoring, because uncertainty monitoring has been observed in rhesus monkeys.
</li>
<li class="footnote" id="footnote94_p3actwm"><a class="footnote-label" href="#footnoteref94_p3actwm">94.</a> For example see <a href="http://science.sciencemag.org/content/355/6327/833">Loukola et al. (2017)</a>.
</li>
<li class="footnote" id="footnote95_zd29s2z"><a class="footnote-label" href="#footnoteref95_zd29s2z">95.</a> I have not investigated this PCIF, but some potentially relevant sources include <a href="https://global.oup.com/academic/product/the-evolutionary-emergence-of-language-9780199654840?cc=ca&amp;lang=en&amp;">Botha &amp; Everaert (2013)</a>; <a href="http://www.cambridge.org/us/academic/subjects/languages-linguistics/english-language-and-linguistics-general-interest/evolution-language?format=HB">Fitch (2010)</a>; <a href="http://yalebooks.com/book/9780300103397/doctor-dolittles-delusion">Anderson (2004)</a>.
</li>
<li class="footnote" id="footnote96_4tm6khe"><a class="footnote-label" href="#footnoteref96_4tm6khe">96.</a> Here, I have in mind the arguments of <a href="https://books.google.com/books?hl=en&amp;lr=lang_en&amp;id=zu9oAgAAQBAJ&amp;oi=fnd&amp;pg=PA160&amp;ots=oqP69pOQeq&amp;sig=J_FfOy3VzAsk_o-1-Wb0A-jpSGc#v=onepage&amp;q&amp;f=false">Bayne (2013)</a> concerning agency as a mark of phenomenal consciousness.
</li>
<li class="footnote" id="footnote97_c883i2t"><a class="footnote-label" href="#footnoteref97_c883i2t">97.</a> <a name="tooluse" id="tooluse"></a>There are many kinds of tool use, and it&#8217;s unclear which kinds are most indicative of phenomenal consciousness. In the foreword to <a href="https://jhupbooks.press.jhu.edu/content/animal-tool-behavior">Shumaker et al. (2011)</a>, Gordon M. Burghardt succinctly illustrates the diversity of animal tool use:<br /><blockquote><p>Ground squirrels kick sand into the faces of venomous snakes to deter attacks. Ant lions engage in a similar behavior in their sand pits to incapacitate prey. Degus (small rodents) use rakes to access food, an ability shared with many birds and non-human primates. Some mice set out markers to aid in finding their way home. Birds use small food items to bait fish, but crocodiles have turned the tables, using fish to attract birds, which they then attack. New Caledonian crows sometimes travel with a toolkit of proven implements for probing for food (including lizards in crevices). Crabs use all sorts of objects, animate and inanimate, to affix to themselves or to the shells they inhabit, for camouflage against predators. Apes are able to use tools of all kinds in both captivity and the wild. Through observation and practice they crack open nuts, apply herbal medications, open locks and doors, use sticks to stir liquids, saw wood, and even dig with a shovel. In fact, while tools are mostly used in foraging for food, they also are employed in many other contexts, such as to deter predators, facilitate courtship and copulation, mark territories, and intimidate competitors of their own species.</p></blockquote>
<p>In the book&#8217;s Introduction, the authors further illustrate the difficulty of deciding what should and shouldn&#8217;t count as “tool use” by listing 53 observed animal behaviors that different definitions classify differently. After surveying the strengths and weaknesses of several proposed definitions, they opt for the following definition of tool use:</p>
<blockquote><p>Our present definition of tool use is: The external employment of an unattached or manipulable attached environmental object to alter more efficiently the form, position, or condition of another object, another organism, or the user itself, when the user holds and directly manipulates the tool during or prior to use and is responsible for the proper and effective orientation of the tool.</p></blockquote>
<p>Perhaps more useful is table 1.1, in which the authors describe 26 “modes” of tool use and manufacture. Below are just a few example rows, quoted directly from table 1.1:</p>
<table><tr><th>Name of use mode</th>
<th>Function</th>
<th>Comments</th>
</tr><tr><td>Throw</td>
<td>Create or augment signal value of social display; amplify mechanical force; extend user&#8217;s reach</td>
<td>Propel an object through open space. Can be aimed or unaimed. The object is propelled by the user&#8217;s own energy.</td>
</tr><tr><td>Prop and Climb, Balance and Climb, Bridge, Reposition</td>
<td>Extend user&#8217;s reach by expanding accessible three-dimensional space; bodily comfort</td>
<td>Prop and Climb: Place and stabilize an elongate object vertically or diagonally against another object or surface, and then move up or climb up the object. Distal end of propped object touches the other object or surface. Stable.<br /><br />
Balance and Climb: Place an elongate object vertically and then move up or climb up the object. The distal end of the balanced object does not touch another object or surface. Unstable.<br /><br />
Bridge: Place an elongate object or organism over water or open space such that each end rests on a surface on opposite sides of the water or spatial gap. User locomotes on the subject. Stable.<br /><br />
Reposition: Relocate and climb on an object or organism. Includes rafting (placing a buoyant object on water to support user&#8217;s weight).</td>
</tr><tr><td>Symbolize</td>
<td>Abstract or represent reality</td>
<td>Carry, keep, or trade an object that represents another object, another organism, or a psychological state.</td>
</tr><tr><td>Detach</td>
<td>Structural modification of an object or an existing tool by the user or a conspecific so that the object/tool serves, or serves more effectively, as a tool</td>
<td>Remove the eventual tool from a fixed connection to the substrate or another object.</td>
</tr><tr><td>Add, Combine</td>
<td>As above [for ‘Detach’]</td>
<td>Join or connect two or more objects to make one tool that is held or directly manipulated in its entirety during its eventual use.</td>
</tr></table><p>Most of the rest of the book, then, catalogues published observations of these various modes of tool use, organized by taxa such as “insects,” “crustaceans,” “fish,” “birds,” “rodents,” “cetaceans,” “old world monkeys,” “gibbons,” “chimpanzees,” etc. In table 7.1, they organize observed cases of tool use by mode and taxon.</p>
<p>In the end, I decided not to choose one or more modes of tool use for inclusion in my table of PCIFs and taxa, but future creators of similar tables might want to. For example, perhaps “Symbolize” is a particularly consciousness-informative mode of animal tool use.<br /><br />
Another useful recent source on animal tool use is <a href="http://www.cambridge.org/us/academic/subjects/life-sciences/biological-anthropology-and-primatology/tool-use-animals-cognition-and-ecology?format=HB">Sanz et al. (2013)</a>.</p>
</li>
<li class="footnote" id="footnote98_hidxbd1"><a class="footnote-label" href="#footnoteref98_hidxbd1">98.</a> <a name="scrubjays" id="scrubjays"></a>What I have in mind is the kind of planning for the future exhibited by western scrub-jays in <a href="http://www.nature.com/nature/journal/v445/n7130/abs/nature05575.html">Raby et al. (2007)</a>:<br /><blockquote><p>Knowledge of and planning for the future is a complex skill that is considered by many to be uniquely human. We are not born with it; children develop a sense of the future at around the age of two and some planning ability by only the age of four to five. According to the Bischof-Köhler hypothesis, only humans can dissociate themselves from their current motivation and take action for future needs: other animals are incapable of anticipating future needs, and any future-oriented behaviours they exhibit are either fixed action patterns or cued by their current motivational state. The experiments described here test whether a member of the corvid family, the western scrub-jay (<em>Aphelocoma californica</em>), plans for the future. We show that the jays make provision for a future need, both by preferentially caching food in a place in which they have learned that they will be hungry the following morning and by differentially storing a particular food in a place in which that type of food will not be available the next morning. Previous studies have shown that, in accord with the Bischof-Koöhler hypothesis, rats and pigeons may solve tasks by encoding the future but only over very short time scales. Although some primates and corvids take actions now that are based on their future consequences, these have not been shown to be selected with reference to future motivational states, or without extensive reinforcement of the anticipatory act. The results described here suggest that the jays can spontaneously plan for tomorrow without reference to their current motivational state, thereby challenging the idea that this is a uniquely human ability.</p></blockquote>
</li>
<li class="footnote" id="footnote99_u1dk56i"><a class="footnote-label" href="#footnoteref99_u1dk56i">99.</a> I have not investigated this PCIF, but see section 9 of David DeGrazia&#8217;s “Self-awareness in animals,” which is chapter 11 in <a href="http://www.cambridge.org/us/academic/subjects/philosophy/philosophy-general-interest/philosophy-animal-minds?format=HB">Lurz (2009)</a>.
</li>
<li class="footnote" id="footnote100_mw9ggkb"><a class="footnote-label" href="#footnoteref100_mw9ggkb">100.</a> I have not investigated this PCIF, but see e.g. <a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118316757.ch19/summary">Kaminski (2016)</a>.
</li>
<li class="footnote" id="footnote101_rrftpdm"><a class="footnote-label" href="#footnoteref101_rrftpdm">101.</a> For discussions and debates about the relatively sophisticated behavior controlled by unconscious (or at least unconscious-to-us) processes in humans, see e.g. appendix 3 of <a href="http://academicworks.cuny.edu/gc_etds/1604/">Shevlin (2016)</a>; <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199600472.001.0001/oxfordhb-9780199600472-e-033">Prinz (2015)</a>; <a href="https://books.google.com/books?hl=en&amp;lr=lang_en&amp;id=QtvT-xbUKCoC&amp;oi=fnd&amp;pg=PA89&amp;ots=TEkW3QecH2&amp;sig=ofGB8-KZfgyljYmg1mV2pfUlRLE#v=onepage&amp;q&amp;f=false">Bargh &amp; Morsella (2010)</a>; <a href="http://www.penguinrandomhouse.com/books/298863/gut-feelings-by-gerd-gigerenzer/9780143113768">Gigerenzer (2007)</a>; ch. 15 of <a href="https://mitpress.mit.edu/books/cognitive-unconscious-and-human-rationality">Macchi et al. (2016)</a>; <a href="http://pps.sagepub.com/content/8/2/195.short">Hassin (2013)</a>; <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/unconscious-influences-on-decision-making-a-critical-review/86885344F7E8A44457C3FC63CFA3F3AF">Newell &amp; Shanks (2014)</a>; <a href="https://global.oup.com/academic/product/sight-unseen-9780199596966">Goodale &amp; Milner (2013)</a>; <a href="http://philpapers.org/rec/WEIOTU-3">Weiskrantz (2008)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/mila.12082/full">Shepherd (2015)</a>; <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195376746.001.0001/oxfordhb-9780195376746-e-12">Kihlstrom (2013)</a>; <a href="https://global.oup.com/academic/product/out-of-mind-9780198506300?q=de%20gelder%20out%20of%20mind&amp;lang=en&amp;cc=us">de Gelder et al. (2002)</a>. See also my section on <a href="http://www.openphilanthropy.org/some-initial-thoughts-moral-patients#Cortex">cortex-required views</a> and the appendices it links to.<br /><br />
	However, we must be careful not to exaggerate the powers of the unconscious human mind. For example, several results in this area have <a href="http://slatestarcodex.com/2016/08/25/devoodooifying-psychology/">fared poorly</a> in psychology&#8217;s “replication crisis.” (See also <a href="AppendixZ8">Appendix Z.8</a>.
</li>
<li class="footnote" id="footnote102_y677dat"><a class="footnote-label" href="#footnoteref102_y677dat">102.</a> For an example discussion involving learning by the rat spinal cord, see <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195304787.001.0001/oxfordhb-9780195304787-e-6">Allen et al. (2009)</a>. On the enteric nervous system, see <a href="https://www.newscientist.com/article/mg21628951.900-gut-instincts-the-secrets-of-your-second-brain/">Young (2012)</a>, <a href="http://www.morganclaypool.com/doi/abs/10.4199/C00039ED1V01Y201107ISP026">Wood (2011)</a>, and <a href="http://www.nature.com/nrgastro/journal/v13/n9/abs/nrgastro.2016.107.html">Rao &amp; Gershon (2016)</a>. On the autonomic nervous system more generally, see <a href="http://prism.ucalgary.ca/handle/1880/29130">Ryder (1996)</a>.
</li>
<li class="footnote" id="footnote103_edkz5na"><a class="footnote-label" href="#footnoteref103_edkz5na">103.</a> See e.g. my example extensions to <em>MESH: Hero</em> <a href="#Hero">here</a>, <a href="http://www.sciencedirect.com/science/article/pii/S0893608007001530">Herzog et al. (2007)</a>, and Table 1 of <a href="https://pdfs.semanticscholar.org/4bc7/0c160fa543f52d59e41aea5022172cb71073.pdf">Liu &amp; Schubert (2010)</a>.
</li>
<li class="footnote" id="footnote104_ziib6gi"><a class="footnote-label" href="#footnoteref104_ziib6gi">104.</a> On bacteria, see <a href="http://yalebooks.com/book/9780300167849/wetware">Bray (2011)</a> and <a href="http://journal.frontiersin.org/article/10.3389/fmicb.2015.00264/full">Lyon (2015)</a>. On plants, see <a href="http://link.springer.com/chapter/10.1057/9781137554895_2">Smith (2016)</a>.
</li>
<li class="footnote" id="footnote105_brks0qd"><a class="footnote-label" href="#footnoteref105_brks0qd">105.</a> See <a href="https://www.theatlantic.com/science/archive/2016/12/the-brainless-slime-that-can-learn-by-fusing/511295/">Yong (2016)</a>.
</li>
<li class="footnote" id="footnote106_ptpepma"><a class="footnote-label" href="#footnoteref106_ptpepma">106.</a> This quote is from <a href="https://global.oup.com/academic/product/cognition-evolution-and-behavior-9780195319842?cc=us&amp;lang=en&amp;">Shettleworth (2009)</a>, p. 5, which cites <a href="https://books.google.com/books?id=Tk3-EOp2KccC&amp;lpg=PA66&amp;ots=bG4DD2h8EV&amp;lr=lang_en&amp;pg=PA66#v=onepage&amp;q&amp;f=false">Dyer (1994)</a> as an example.
</li>
<li class="footnote" id="footnote107_smtyq6n"><a class="footnote-label" href="#footnoteref107_smtyq6n">107.</a> <a href="https://global.oup.com/academic/product/why-animals-matter-9780199747511?cc=us&amp;lang=en&amp;">Dawkins (2012)</a> gives the following example:<br /><blockquote><p>Security cameras are sensitive to movement and respond appropriately by switching on a light, sounding an alarm, or even ringing a police station, but most of us don&#8217;t worry too much about whether or not they are conscious, despite our tendency to describe them in anthropomorphic terms (‘Don&#8217;t do that or the camera will think you are an intruder’).</p>
<p>We know that what a surveillance camera does is very simple. It can detect movement and it can then respond in a totally automated way to raise the alarm and even to summon the police. We also know that if we looked out of the window and saw a strange man running across the lawn at night brandishing a gun, we would perform a similar task of alerting the police but we would do it in a completely different, conscious way. The end result is the same, but with a different way of getting there. One is the totally unconscious activation of a phone line, the other has the full panoply of conscious recognition of the presence of an intruder, followed by the experience of fear at what he might do, and then the conscious action of telephoning the police and explaining to them rationally what is happening.</p>
<p>This simple example shows why identifying where there is consciousness is so difficult. There is clearly a spectrum of mechanisms for producing a similar outcome that has security cameras at one end and ourselves peering into the night at the other. Where on this spectrum are we to put, say, slugs? Fish? Chimpanzees? Plants? The fact that so many of the attributes of consciousness, such as the ability to respond to stimuli and choose an appropriate action, can be mimicked by relatively simple machines shows that it is not necessary to feel or experience anything in order to have adaptive, appropriate behaviour. A few simple sensors, a bit of programming, and an electrically powered output of a sort we are all familiar with and you can do a lot of routine, everyday behaviour. Consciousness just isn&#8217;t necessary.</p></blockquote>
</li>
<li class="footnote" id="footnote108_kin0rxb"><a class="footnote-label" href="#footnoteref108_kin0rxb">108.</a> See e.g. ch. 17 of <a href="http://aima.cs.berkeley.edu/">Russell &amp; Norvig (2009)</a>.
</li>
<li class="footnote" id="footnote109_n4kplu0"><a class="footnote-label" href="#footnoteref109_n4kplu0">109.</a> See e.g. <a href="https://mitpress.mit.edu/books/deep-learning">Goodfellow et al. (2016)</a>.
</li>
<li class="footnote" id="footnote110_kbc2ws6"><a class="footnote-label" href="#footnoteref110_kbc2ws6">110.</a> Intuitively, we might rate PCIFs on a “strength of indication” scale from -1 to 1, such that:
<ul><li>A score of -1 means that if a system exhibits that PCIF, then the system is definitely <em>not</em> conscious.</li>
<li>A score of 0 means that if a system exhibits that PCIF, this doesn&#8217;t indicate the presence or absence of consciousness in that system at all.</li>
<li>A score of 1 means that if a system exhibits that PCIF, then the system is definitely conscious.</li>
</ul><p>Using this rating system, a true “sufficient” condition of consciousness could be scored as 1, or nearly that high. A property found to be totally irrelevant to whether or not a system is conscious, such as whether the most common English term for it includes the letter g, could be scored as 0. A true necessary condition of consciousness might be scored across a wide range of values, depending on the degree to which it is also a sufficient condition of consciousness. But the <em>inverse</em> of a true necessary condition of counsciousness would be scored as -1.</p>
<p>This scoring system would have to be extended to accomodate PCIFs that are scalar rather than binary, such as number of neurons. In such cases, the strength of indication would (perhaps) typically be a monotonic function, but probably not a linear function — i.e. more neurons is always more consciousness-indicating, all else equal, but strength of indication changes more between 100 neurons and 1 million and one hundred neurons than it does between 100 billion neurons and 100 billion and one million neurons, even though the difference between the two is one million neurons in both cases.</p>
</li>
<li class="footnote" id="footnote111_b76uipi"><a class="footnote-label" href="#footnoteref111_b76uipi">111.</a> See also <a href="/sites/default/files/David_Chalmers_05-20-16_%28public%29.pdf">notes from my conversation with David Chalmers</a>.
</li>
<li class="footnote" id="footnote112_zkwat0a"><a class="footnote-label" href="#footnoteref112_zkwat0a">112.</a> For example, language of a certain sort is sometimes argued to be a necessary condition for phenomenal consciousness. See, for example, the sources cited in (the endnotes for) this passage from ch. 6 of <a href="http://www.penguinrandomhouse.com/books/313935/anxious-by-joseph-ledoux/9780143109044/">LeDoux (2015)</a>:<br /><blockquote><p>As part of our daily lives we use language to label and describe our perceptions, memories, thoughts, beliefs, desires, and feelings. As we&#8217;ve seen, this capacity to talk about our inner states makes it relatively easy for us to study human consciousness scientifically. But the contribution of language goes far beyond simply providing a tool for assessing consciousness. Language, Daniel Dennett says, lays down tracks on which thoughts can travel. Many other philosophers of mind and scientists have argued for a strong relation between language and consciousness.</p></blockquote>
</li>
<li class="footnote" id="footnote113_cwcppm7"><a class="footnote-label" href="#footnoteref113_cwcppm7">113.</a> For example, <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=1007572&amp;fileId=s0140525x07000891">Merker (2007)</a>:<br /><blockquote><p>Few [cognitive scientists] or neuroscientists would today object to the assertion that “cortex is the organ of consciousness.”</p></blockquote>
<p>And <a href="http://onlinelibrary.wiley.com/doi/10.1111/papr.12207/full">Devor et al. (2014)</a>:</p>
<blockquote><p>…there has never been much doubt among neuroscientists or neurologists that the neural process that constitutes pain perception, as well as other forms of conscious experience, occurs in the cerebral cortex…</p>
<p>The dogma of a cortical seat of pain and consciousness is convenient. First, until recently, the presence of a ‘flat’ EEG (a marker of lost cortical function) was a key criterion for ‘brain death’ and a diagnostic basis for the ethical harvesting of vital organs for transplantation … conventional reasoning in the neurological community arrives at the conclusion that pain experience is absent, and indeed impossible, in the PVS [persistent vegetative state] patient [because] it is assumed that all conscious perception, including pain, resides in the cortex, and in the PVS patient cortical function is absent, [therefore] it follows that the PVS patient is incapable of feeling pain.</p></blockquote>
<p>Similarly, <a href="http://animalstudiesrepository.org/animsent/vol1/iss9/10/">Mallatt &amp; Feinberg (2016)</a>, who argue for an ancient origin of consciousness, say:</p>
<blockquote><p>Most investigators have for centuries located consciousness in the cerebral cortex. To this day, the dominant paradigm in consciousness studies is that primary consciousness of mapped mental images in mammals comes from the cerebral cortex or from interactions between the cortex and the thalamus, not from the superior colliculus/tectum as Merker claims. We agree with the dominant paradigm for mammals because so many of the neural correlates of mammalian exteroceptive consciousness are in this corticothalamic system (Koch, Massimini, Boly, &amp; Tononi, 2016). Medical neuroimaging and brain-lesion studies strongly support cortical consciousness when the results are interpreted in the most direct and straightforward way: damage to the cortex leads to loss of some sensory consciousness (Boly et al., 2013; Feinberg, 2009). Destruction of the visual, occipital, cortex causes blindness in primates. In his argument for tectal instead of cortical consciousness, Merker (2007) said that these loss phenomena are more complex than they appear, that the cortical damage actually inhibits the conscious role of the superior colliculus, etc. However, his interpretation is less parsimonious and therefore it requires extraordinary counterevidence to be believed. The indirect counterevidence that Merker provided — on the “Sprague effect” (p. 67) and on the cortex projecting to the superior colliculus (p. 76) — does not seem definitive enough to topple the dominant view of cortical consciousness in mammals.</p></blockquote>
<p><a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye (2016)</a>, who likewise argues against CRVs, notes that (pp. 79-80):</p>
<blockquote><p>The claim that in humans pain and other experiences require a neocortex is widely accepted. For example, the American Academy of Neurology asserts (1989):</p>
<p style="padding-left: 50pt; padding-right: 50pt;">Neurologically, being awake but unaware is the result of a functioning brainstem and the total loss of cerebral cortical functioning… Pain and suffering are attributes of consciousness requiring cerebral cortical functioning.</p>
<p>The Medical Task Force on Anencephaly (1990) says much the same thing in connection with congenital cases:</p>
<p style="padding-left: 50pt; padding-right: 50pt;">Infants with anencephaly, lacking functioning cerebral cortex are permanently unconscious… The suffering associated with noxious stimuli (pain) is a cerebral interpretation of the stimuli; therefore, infants with anencephaly presumably cannot suffer. (pp. 671-672)</p>
</blockquote>
</li>
<li class="footnote" id="footnote114_286c995"><a class="footnote-label" href="#footnoteref114_286c995">114.</a> For some interesting historical context, see <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.1993.tb17249.x/full">Thompson (1993)</a>. <a href="http://www.sciencedirect.com/science/article/pii/S1053810011000080">Ward (2011)</a> also covers some of the history very briefly:<br /><blockquote><p>The search for the neural correlates of consciousness (NCC) has been intense and productive in the past two decades since Crick and Koch (1990) focused attention on this project… Recent work has emphasized the importance of the thalamo-cortical system of the brain in generating conscious awareness. Within this system three possibilities for the critical brain activity most closely associated with consciousness have been proposed: it occurs primarily within the cortex (e.g., Crick &amp; Koch, 2003; Romijn, 2002), it occurs in the entire system of thalamo-cortical loops (e.g., Edelman &amp; Tononi, 2000; John, 2001, 2002; Llinás, Ribary, Contreras, &amp; Pedroarena, 1998), it occurs primarily within the thalamus (e.g., Penfield, 1975)… In this paper I discuss the third possibility… that the neural activity most closely associated with primary consciousness occurs primarily in the thalamus…</p>
<p>The proposal that the thalamus is a particularly important locus in the brain involved in generating consciousness also is not new. As Newman (1995) pointed out, views of the locus of the neural correlate of conscious awareness have oscillated around three foci for many years: the cerebral cortex, the reticular activating system, and the thalamus. Penfield (e.g., 1975, p. 19) was perhaps the most extreme proponent of the subcortical view, asserting that “The indispensable substratum of consciousness lies outside the cerebral cortex, probably in the diencephalon (the higher brainstem).” The thalamus is a major component of the diencephalon (it also includes the epithalamus and the hypothalamus). Penfield&#8217;s ideas were founded upon several major evidential bases: (1) the results of stimulating the brain, especially the temporal lobe, with low-intensity direct current electricity; (2) the results of surgery for epilepsy in which various chunks of brain, especially temporal lobe, were removed; and (3) the structural and functional anatomy of the brain that was known in the early 1970s. More recently, several authors have argued that the thalamic reticular nucleus (TRN) plays a role in consciousness by modulating the local 40-Hz oscillations observed in various parts of the brain via its inhibitory inputs to the dorsal thalamic nuclei (Min, 2010; Newman, 1995). Indeed, abolishing inhibitory interactions among the neurons of the TRN dramatically increases absenceepilepsy-like, low-frequency synchronous oscillations in the dorsal thalamic nuclei (Huntsman, Porcello, Homanics, DeLorey, &amp; Huguenard, 1999), indicating that such inhibition might play a major role in preventing the neural hyper-synchrony that characterizes epilepsy and its accompanying state of unconsciousness. Moreover, the TRN has been implicated in producing the unconscious state seen in absence epilepsy, presumably playing the role of strongly inhibiting thalamic neuron activity under the influence of cortical excitation (e.g., Steriade, 2005).</p>
<p>Another vociferous recent proponent of localizing the critical NCC for the state of consciousness in the thalamus has been Joseph Bogen, the surgeon who developed the split brain operation… Bogen argued that the intralaminar nuclei of the thalamus form the essential substrate of the state of phenomenal consciousness. His argument is complex, but critical to it is the fact that there are only two places in the CNS where very small bilateral lesions (those involving less than 1 g of neural tissue) abolish the state of consciousness: in the mesencephalic reticular formation and in the intralaminar nuclei of the thalamus. Moreover, the intralaminar nuclei are connected to much of the rest of the brain through diffuse reciprocal connections, making this a candidate for a central clearinghouse or modulator of cortical and subcortical activity.</p>
<p>Another proponent of a subcortical locus for a substrate of phenomenal awareness is Merker (2007). He updated Penfield and Jasper&#8217;s (1954) centrencephalic system proposal and reviewed extensive evidence that the top of the brainstem, and the superior colliculus in particular, forms a system that integrates motivation, the sensory world, and body capabilities to accomplish goal-directed action. His arguments rely on the extensive convergence of inputs from pretty much the entire brain into this region, including to and from parts of the thalamus. Indeed, the thalamus plays an important integrative role in his theory, although the theory does not specify it to be the substrate of consciousness… Among other important facts discussed by Merker (2007) is the remarkable observation that in over 750 operations to cure epilepsy, during which the patient was not anesthetized, Penfield and Jasper (1954) never once observed even an interruption in the continuity, let alone cessation, of a patient&#8217;s consciousness as they removed large chunks of cortex, sometimes even an entire hemisphere. Interestingly, in his later book Penfield (1975) identified the diencephalon with the “highest brain mechanism” that is directly responsible for consciousness. The part investigated thoroughly by Merker, on the other hand, was termed by Penfield “the brain&#8217;s computer” and was said to be responsible for sensory-motor integration, as updated and extended by Merker (2007). Both mechanisms acting together were thought to be necessary to explain human behavior because the diencephalon has privileged access to frontal and temporal areas of the cortex, whereas the older system just below this area at the roof of the brainstem has privileged access to sensory and motor mechanisms. It is thus likely that the entire upper brain stem plays a crucial role in human behavior…</p></blockquote>
</li>
<li class="footnote" id="footnote115_uqzwnug"><a class="footnote-label" href="#footnoteref115_uqzwnug">115.</a> Also as above, I wouldn&#8217;t be surprised if many of the studies cited in this section turn out to not “hold up” very well upon closer scrutiny; see <a href="#AppendixZ8">Appendix Z.8</a>.
</li>
<li class="footnote" id="footnote116_wbolmbm"><a class="footnote-label" href="#footnoteref116_wbolmbm">116.</a> One example is <a href="https://www.ncbi.nlm.nih.gov/pubmed/11662185">Veatch (1975)</a>, who identifies conscious experience with the neocortex but doesn&#8217;t cite any evidence in support of this connection. Similarly, <a href="http://link.springer.com/chapter/10.1007/978-94-009-2707-0_10">Bartlett &amp; Younger (1988)</a> and <a href="http://onlinelibrary.wiley.com/doi/10.1111/1467-8519.00059/abstract">Rich (1997)</a> repeatedly assert that consciousness requires cortical function, but they don&#8217;t argue for, or cite evidential support for, that assertion.<br /><br />
Another example is <a href="http://www.jstor.org/stable/2265108">Green &amp; Wikler (1980)</a>, though they do qualify their statement with “arguably”:
<blockquote><p>…it does not follow from our argument that all humans lacking the substrate of consciousness are dead. Anencephalic infants are lacking at birth the cortical material necessary for the development of cognitive functioning and, arguably, consciousness. Still, due to possession of a functioning brain stem, they may have spontaneous breathing and heartbeat, and a good suck.</p></blockquote>
<p>Even the more recent <a href="http://plato.stanford.edu/entries/death-definition/">DeGrazia (2016)</a> seems to assume a CRV, at least for humans (and again, without argument), e.g. in statements such as “…the cerebrum, the primary vehicle of conscious awareness…” and “in a permanent (irreversible) vegetative state (PVS), while the higher brain is extensively damaged, causing irretrievable loss of consciousness, the brainstem is largely intact…” and “Consider… anencephalic infants, who are born without cerebral hemispheres and never have the capacity for consciousness…” </p>
<p>However, one caveat about my discussion of CRVs is that different authors appeal to slightly different definitions of “consciousness,” and so it is not always the case that the authors I cite explicitly argued for or against the view that a cortex is required for “consciousness” as defined <a href="#Defined">above</a>. Still, these are arguments are sometimes used by others to make claims about the dependence or non-dependence of consciousness (as defined above) on a cortex, and certainly the arguments could easily be <em>adapted</em> to make such claims.</p>
<p>Note also that “many… court decisions have relied on the presumption that consciousness is permanently lost in the persistent vegetative state, and have assumed that physicians can reliably make this diagnosis [<a href="http://heinonline.org/HOL/Page?handle=hein.journals/amlmed13&amp;div=20&amp;g_sent=1&amp;collection=journals">Cranford &amp; Smith (1987)</a>; <a href="http://link.springer.com/chapter/10.1007/978-94-009-2707-0_6">Smith (1988)</a>]” (<a href="http://journals.lww.com/ccmjournal/Abstract/1992/12000/Rethinking_brain_death.18.aspx">Truog &amp; Fackler 1992</a>).</p>
<p>Another example is <a href="http://www.nature.com/nrn/journal/v17/n7/full/nrn.2016.44.html">Tononi et al. (2016)</a>, which seems to assume that at least <em>human</em> consciousness requires cortical function:</p>
<blockquote><p>Consciousness depends on the integrity of certain brain regions and the particular content of an experience depends on the activity of neurons in parts of the cerebral cortex. However, despite increasingly refined clinical and experimental studies, a proper understanding of the relationship between consciousness and the brain has yet to be established. For example, it is not known why the cortex supports consciousness when the cerebellum does not, despite having four times as many neurons, or why consciousness fades during deep sleep while the cerebral cortex remains active. There are also many other difficult questions about consciousness. Are patients with a functional island of cortex surrounded by widespread damage conscious, and if so, of what?…</p></blockquote>
</li>
<li class="footnote" id="footnote117_8an0xpp"><a class="footnote-label" href="#footnoteref117_8an0xpp">117.</a> <a href="http://www.tandfonline.com/doi/abs/10.1080/20026491051668">Rose (2002)</a>:<br /><blockquote><p>Extensive evidence demonstrates that our capacity for conscious awareness of our experiences and of our own existence depends on the functions of this expansive, specialized neocortex. This evidence has come from diverse sources such as clinical neuropsychology (Kolb and Whishaw, 1995), neurology (Young et al., 1998; Laureys et al., 1999, 2000a-c), neurosurgery (Kihlstrom et al., 1999), functional brain imaging (Dolan, 2000; Laureys et al., 1999, 2000a-c), electrophysiology (Libet, 1999) and cognitive neuroscience (Guzeldere et al., 2000; Merikle and Daneman, 2000; Preuss, 2000). A strong case has been made that it is mainly those cortical regions that have achieved such massive expansion in humans that are most centrally involved in the production of consciousness (Edelman and Tononi, 2000; Laureys et al., 1999, 2000a-c).</p>
<p>…The evidence that the neocortex is critical for conscious awareness applies to both types of consciousness [“primary” consciousness and “higher-order” consciousness]. Evidence showing that neocortex is the foundation for consciousness also has led to an equally important conclusion: that we are unaware of the perpetual neural activity that is confined to subcortical regions of the central nervous system, including cerebral regions beneath the neocortex as well as the brainstem and spinal cord (Dolan, 2000; Güzeldere et al., 2000; Jouvet, 1969; Kihlstrom et al., 1999; Treede et al., 1999).</p>
<p>…From the clinical perspective, primary consciousness is defined by: (1) sustained awareness of the environment in a way that is appropriate and meaningful, (2) ability to immediately follow commands to perform novel actions, and (3) exhibiting verbal or nonverbal communication indicating awareness of the ongoing interaction… Thus, reflexive or other stereotyped responses to sensory stimuli are excluded by this definition. Primary consciousness appears to depend greatly on the functional integrity of several cortical regions of the cerebral hemispheres especially the “association areas” of the frontal, temporal, and parietal lobes (Laureys et al., 1999, 2000a-c). Primary consciousness also requires the operation of subcortical support systems such as the brainstem reticular formation and the thalamus that enable a working condition of the cortex. However, in the absence of cortical operations, activity limited to these subcortical systems cannot generate consciousness (Kandel et al., 2000; Laureys et al., 1999, 2000a; Young et al., 1998). Wakefulness is not evidence of consciousness because it can exist in situations where consciousness is absent (Laureys et al., 2000a-c). Dysfunction of the more lateral or posterior cortical regions does not eliminate primary consciousness unless this dysfunction is very anatomically extensive (Young et al., 1998).</p>
<p>…Diverse, converging lines of evidence have shown that consciousness is a product of an activated state in a broad, distributed expanse of neocortex. Most critical are regions of “association” or homotypical cortex (Laureys et al., 1999, 2000a-c; Mountcastle, 1998), which are not specialized for sensory or motor function and which comprise the vast majority of human neocortex. In fact, activity confined to regions of sensory (heterotypical) cortex is inadequate for consciousness (Koch and Crick, 2000; Lamme and Roelfsema, 2000; Laureys et al., 2000a,b; Libet, 1997; Rees et al., 2000).</p></blockquote>
<p>About a decade later, <a href="http://onlinelibrary.wiley.com/doi/10.1111/faf.12010/full">Rose et al. (2014)</a> added:</p>
<blockquote><p>The neural basis of consciousness was reviewed and applied to the problem of fish pain by Rose (2002)… Subsequent research has further substantiated and refined the fundamental principles identified earlier, that, the existence of all the previously described forms of consciousness [primary consciousness and higher-order consciousness] depends on neocortex, particularly frontoparietal ‘association’ cortex in distinction from primary or secondary sensory or motor cortex (Laureys and Boly 2007; Amting et al. 2010; Vanhaudenhuyse et al. 2012). Primary consciousness also requires supporting operation of subcortical systems including (i) the brainstem reticular formation to enable a working condition of the cortex and (ii) interactions between the cortex and thalamus as well as cortex and basal ganglia structures (Edelman and Tononi 2000; Laureys et al. 1999, 2000a,b,c)… Human neocortex, the six-layered cortex that is unique to mammals, has specialized functional regions of sensory and motor processing, but activity confined to these regions is insufficient for consciousness (Koch and Crick 2000; Lamme and Roelfsma 2000; Laureys et al. 2000a,b; Rees et al. 2000). Although neocortex is usually identified as the critical substrate for consciousness, a critical role for some regions of mesocortex, particularly the cingulate gyrus, is well established. Mesocortical structures have fewer than six layers, but like neocortex, are unique to mammalian brains and highly interconnected with neocortex. The cingulate gyrus, in concert with neocortex, is particularly important for conscious awareness of the emotional aspect of pain (Vogt et al. 2003), other dimensions of emotional feelings (Amting et al. 2010) and self-awareness (Vanhaudenhuyse et al. 2012).</p></blockquote>
<p>Building on these earlier articles by Rose, <a href="http://link.springer.com/article/10.1007/s10539-014-9469-4">Key (2015)</a> argues:</p>
<blockquote><p>What is so unique about the cortex that enables inner mental states? First, the cortex is parcellated into discrete anatomically structures or cortical areas that process information related to specific functions. It is estimated that there are about 200 cortical areas in humans (Kaas 2012). For instance, the cortical visual system consists of over a dozen distinct regions with diverse subfunctions that are strongly interconnected by reciprocal axon pathways. One of the defining features of these subregions is that they become simultaneously active. Both recurrent activity and binding of neural activity across cortical regions are believed to be essential prerequisites for the subjective experience of vision (Sillito et al. 2006; Pollen 2011; Koivisto and Silvanto 2012). It has been shown that when neural processing of recurrent signalling from higher cortical regions entering the V1 visual cortex is perturbed by transcranial magnetic stimulation, the subjective awareness of a visual stimulus is disrupted (Koivisto et al. 2010, 2011; Jacobs et al. 2012; Railo and Koivisto 2012; Avanzini et al. 2013).</p>
<p>The subregionalisation of the neocortex also allows the formation of spatial maps of the sensory world, such as those associated with the representations of the surface of the body or the visual field. These topographical maps are important for the multiscale processing of sensory information (Kaas 1997; Thivierge and Marcus 2007). Variation in the size of the maps alters the sensitivity of responses to stimuli while spatial segregation of neurons responding to selective parts of a stimulus allows for finer perceptual discrimination. Painful and non-painful somatosensory stimuli are topographically mapped to overlying regions in the primary somatosensory cortex (SI) in humans (Mancini et al. 2012). These results are consistent with the known point-to-point topography from the body surface to SI (called somatotopy) that underlies spatial acuity. However, by using high resolution mapping in the squirrel monkey SI (sub-millimetre level) it was revealed that there were slight differences in the localisation of different somatosensory modalities (Chen et al. 2001). This slight physical separation of cortical neurons responding to different peripheral stimuli suggests that differences in the subjective quality of somatosensory sensations may arise as early as in SI. Somatotopic maps for painful stimuli are also present in the human SII and insular cortices (Baumgartner et al. 2010). Interestingly, different qualities of painful stimuli (such as heat and pinprick) are more distinctly mapped topographically to different regions of SII and the insular cortex than in SI. Similarly, painful and non-painful stimuli are mapped to separate regions in human SII (Torquati et al. 2005). This separation of cortical processing of heat and tactile stimuli within different cortical areas has also been observed in non-human primates (Chen et al. 2011). These multiple neural maps suggests that SII and the insular cortex play important roles in discriminating differences in the subjective quality of somatosensory stimuli, particularly painful from non-painful (Tommerdahl et al. 1996; Baumgartner et al. 2010; Chen et al. 2011; Mazzola et al. 2012). This idea is supported by evidence from direct electrical stimulation of discrete areas in the human insular cortex (Afif et al. 2010).</p>
<p>Second, the cortex is a laminated structure that enables the efficient processing and integration of different types of neural information by unique subpopulations of neurons (Schubert et al. 2007; Maier et al. 2010; Larkum 2013). Lamination appears to facilitate complex wiring patterns during development. If two populations of neurons were randomly distributed within a specific brain region and incoming axons were required to synapse with only one subpopulation, then those axons would need to rely on stochastic and hence error-prone searching to complete wiring. On the other hand, when similar neurons are partitioned together in a single lamina then a small set of molecular cues is able to guide axons with high precision to their appropriate post-synaptic target. Two principal afferent inputs (from the neocortex itself, and the thalamus) enter the neocortex and separately innervate distinct layers (Nieuwenhuys 1994). The main thalamic fibres terminate densely in layer IV (called the granular layer) while the neocortical fibres innervate different pyramidal neurons in layers I–III (supragranular layers) (Opris 2013). By selectively ablating Pax6, a developmentally significant patterning gene, in the cortex of mice it is possible to disrupt the laminar organisation of this structure (Tuoc et al. 2009). This altered cortical layering causes neurological deficits that are similar to those observed in humans with Pax6 haploinsufficiency (Tuoc et al. 2009) and provides strong experimental evidence of the importance of lamination to cortical function. A number of human brain disorders involve defects in cortical lamination that are detrimental to brain function (Guerrini et al. 2008; Guerrini and Parrini 2010; Bozzi et al. 2012).</p>
<p>Third, lamination facilitates the economical establishment of microcircuitry between neurons processing different properties of the stimulus. A vertical canonical microcircuit is established which leads to the emergence of functionally interconnected columns and minicolumns of neurons (Mountcastle 1997). For example, a hexagonal column in the primate somatosensory cortex is about 400 µm in width and contains populations of neurons that respond to the same stimulus (e.g. light touch or joint stimulation) arising from a specific topographical zone of the body. Columns can be associated with processing information related to a specific function (e.g. “visual tracking” and “arm reach” columns in the parietal cortex; Kass 2012). Each column itself consists of minicolumns (80–100 neurons) that are ~30–50 µm in diameter and interconnected by short-range horizontal processes (Buxhoeveden and Casanova 2002). While columns are most clearly distinguished in the sensory and motor cortices of primates, minicolumns appear to be ubiquitous in all animals with a neocortex (Kaas 2012). Minicolumns have a small receptive field within the larger receptive field of the column. The correlated activity in the fine-scale networks of minicolumns produces concentrated bursts of neural activity that may enable the cortex to transmit signals in the face of background noise (Ohiorhenuan et al. 2010). The function of the cortex seems to depend on the ability of canonical circuitry within the minicolumns to rapidly switch from feedforward to feedback processing between layers. During learned tasks in responses to cues in the awake monkey, information flows from layer 4 to layer 2/3 and then down to layer 5 in a feedforward loop in the temporal neocortex (Takeuchi et al. 2011; Bastos et al. 2012). This is followed shortly afterwards by a feedback loop from layer 5 to layer 2/3. Correlated firing of layer 2/3 and layer 5 neurons in minicolumns occurs during decision making in the monkey prefrontal cortex, an area responsible for executive control in primates (Opris et al. 2012). The accuracy of error-prone tasks was increased when layer 5 neurons were artificially stimulated by activity recorded during successful task execution. These results provide evidence for the role of the minicolumn as the fundamental processing unit of the neocortex associated with higher order behaviour (Bastos et al. 2012; Opris et al. 2012).</p>
<p>In summary, the unique morphology of the mammalian cortex facilitates multiscale processing of sensory information. Initially there is course scaling at the level of gross anatomical cortical regions specialising, for example, in processing of visual or somatosensory information. Some of these regions are then topographically mapped in order to preserve spatial relationships and facilitate selective processing of specific sensory features. Importantly, to preserve the holistic quality of a sensory stimulus, these subregions are strongly interconnected via axon pathways that create synchronized re-entrant loops of neural activity. Cortical regions are laminated which supports finer scale sensitivity in the processing of specific features. Finally, canonical microcircuits (minicolumns) bridge across layers to enhance signal contrast (Casanova 2010). Local connectivity between minicolumns enables the lowest level of stimulus binding that contributes to the holistic nature of the stimulus (Buxhoeveden and Casanova 2002).</p>
<p>I propose that only animals possessing the above neuroanatomical features (i.e. discrete cortical sensory regions, topographical maps, multiple cortical layers, columns/minicolumns and strong local and long-range interconnections), or their functionally analogous counterparts, have the necessary morphological prerequisites for experiencing subjective inner mental states such as pain.</p></blockquote>
<p>For much more on this, see <a href="http://animalstudiesrepository.org/animsent/vol1/iss3/1/">Key (2016)</a> and the many replies to it in the same issue of <em>Animal Sentience</em>. See also the brief article “An Argument in Defense of Fishing” by Michael LaChat on pp. 20-21 in <a href="http://afs.tandfonline.com/doi/abs/10.1577/1548-8446-21-7">volume 21, issue 7</a> (1996) of <em>Fisheries</em>.</p>
<p>An earlier defense of a CRV (at least, for the human case) was mounted by <a href="http://link.springer.com/chapter/10.1007/978-94-009-2707-0_3">Puccetti (1998)</a>: </p>
<blockquote><p>If… the neocortical surface is itself selectively destroyed… that is sufficient to obliterate all conscious functions.</p>
<p>The midbrain is of course just the top of the brain stem, where the superior and inferior colliculi trigger orientating reflexes related, respectively, to sources of visual and auditory stimuli: such reflexive responses do not require conscious mediation, as we all know from finding ourselves turned towards an abrupt movement in the peripheral visual field, or in the direction of a sudden sound, before such stimuli register in consciousness. If a brain structure does its job unconsciously, then there is no reason to think its integrity in a comatose patient is evidence of residual conscious functions. Similarly with the cerebellum, which pre orchestrates complex bodily movements, and under therapeutic electrode stimulation does not yield clear sensations [3]. The cerebellum probably also stores learned subroutines of behavior, like swimming or typing: precisely the kinds of things you do better when not concentrating on them.</p>
<p>…[Douglas N. Walton&#8217;s] statement [that “the pupillary reflex could, for all we know, indicate some presence of feeling or sensation even if the higher cognitive faculties are absent”]… reeks of superstition. As we all know, when the doctor flashes his penlight on the eye, we do not feel the pupil contract, then expand again when he turns the light off. If not, then why in the world does Walton suppose that a deeply comatose patient feels anything in the same testing situation? The whole point of evolving reflexes like this, especially in large brained animals that do little peripheral but lots of central information processing, is to shunt quick-response mechanisms away from the cerebrum so that the animal can make appropriate initial responses to stimuli before registering them consciously. If one could keep an excised human eye alive in vitro and provoke the pupillary reflex, the way slices of rat hippocampus have been stimulated to threshold for neuronal excitation, would Walton argue that the isolated <em>eye</em> might feel something as its pupil contracts?</p>
<p>…One thing I feel reasonably confident in stating is that sensations are not experienced without recruitment of populations of neurons in the grey matter on the cerebral cortical surface. And it is easy to see why this is so: the phylogenetic novelty of neocortex is due to brain expansion in primates beginning about 50 million years ago to accommodate increasing intelligence, for where else could new cell layers appear but on the outer surface of the brain [9]? That being the case, sensation migrated there as well, and although deeper structures certainly contribute complexly to the sentient input, this is not transduced as sensation until, at a minimum, some 104 neurons are provoked to discharge on the surface of at least one cerebral hemisphere at the same time [16]. It is also plain why the contribution of subcortical mechanisms to this input does not itself implicate conscious perception. If it did, we would have sensations in <em>seriatum</em>: a baseball leaving the pitcher&#8217;s hand would be seen as arriving by the hitter several times in succession as neural impulses course from retina to optic chiasm to geniculate body through the optic radiation to primary visual cortex in the occipital lobe. From an evolutionary viewpoint, that would be a recipe for disaster.</p>
<p>…What Walton is doing is confusing the normally necessary contribution of subcortical mechanisms to sensation with the sufficient condition of neocortical functions. In the case of the primary visual system in man this is indisputable: destruction of Brodmann&#8217;s area 17 alone, say by shrapnel wounds, brings permanent total blindness [7]; whereas a peripherally blind person with intact visual cortex can be induced to experience visual sensations by direct electrode stimulation of that grey matter [2].</p>
<p>…</p>
<p>[Another of Walton&#8217;s points] alludes to findings by Lober (reported in [12]), that some people recovered from infantile hydrocephaly, thus growing up with severely reduced cerebral hemispheres, can nevertheless function well: an example being that of a university student, IQ 126, who gained first class honors in mathematics. This Walton takes to be evidence that the neocortex is neither the sole seat of consciousness nor, perhaps, crucial to the return of conscious functions.</p>
<p>One wants to scream aloud a commonplace of clinical psychopathology: When neural plasticity enters the picture, all bets are off! The neural plasticity of the infant brain allows a lot less than the normal quantity of grey matter to take over a wide range of functions that are usually diffused in greater brain space. This is strikingly and uncontroversially demonstrated in complete hemispherectomy for infantile hemiplegia, where control of the whole body (except for distal finger movements in the arm contralateral to the missing half brain) is found in adulthood [1]. Furthermore, as Epstein has said (quoted in [12]), hydrocephalus is principally a disease of the white matter of the brain (the cerebral ventricles, swelled by overproduction of cerebrospinal fluid, disrupt the axons of association fibers around them). It is precisely the sparing of nerve cells in the grey matter, even in severe cases of hydrocephalus, that explains the retention of conscious functions and high-performance IQs.</p></blockquote>
<p><a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye (2016)</a> argues against CRVs, but he presents some of the case <em>for</em> CRVs this way (pp. 78-79):</p>
<blockquote><p>In humans, in standard cases, the sensory aspect of pain is generated by activity in the primary and secondary somatosensory cortices of the parietal lobe (SI and SII). The unpleasantness of pain — what we might call its “felt badness” — is closely tied to activity in the anterior cingulate cortex (ACC). Human functional imaging studies show that there is a significant correlation between the felt badness of pain and ACC activation… Furhter, when subjects feel pain from a stimulus that is held constant and a hypnotic suggestion is used to increase or decrease subjective unpleasantness, a correlation is found in regional cerebral blood flow in ACC but not in the primary somatosensory cortex… Also, patients with ACC lesions say that their pain is “less bothersome.”</p>
<p>If regions of SI and SII are damaged but not ACC, what results is an unpleasant sensation that is not pain. For example, a laser was used to deliver a thermal stimulus to a fifty-seven-year-old man with most of his SI damaged as a result of a stroke… When the stimulus significantly above normal threshold on his right hand was delivered to his left hand, the man reported an unpleasant sensation, but he denied that it was a case of pain. [Tye&#8217;s footnote here says: “Asked to classify his sensation from a list of terms that included ‘hot,’ ‘burning,’ and ‘pain,’ the patient picked none.”]</p>
<p>It appears, then, that the painfulness of pain in humans is based on activity in two different neural regions: the somatosensory cortex (comprised of SI and SII) and ACC.</p>
<p>Some animals, such as fish, lack a neocortex. So they lack these regions. This neurophysiological difference, it might be said, makes a crucial difference. A related thought is that the causal story for animals lacking a neocortex that lies behind this behavior… cannot be reconciled with the story for humans. So we aren&#8217;t entitled to infer a common cause [i.e. consciousness], even given common behavior. The neurophysiological difference between the nonhuman animals and humans defeats the explanation of behavior that [appeals] to pain.</p></blockquote>
<p><a href="http://www.sciencedirect.com/science/article/pii/S0166223603003345">Baars et al. (2003)</a> is focused on the human case, and reviews several lines of evidence suggesting that frontoparietal association cortex could be especially critical for consciousness. Their summary reads:</p>
<blockquote><p>…several lines of evidence suggest that [frontoparietal association areas] could have a special relationship with consciousness, even though they do not support the contents of sensory experience. (i) Conscious stimulation in the waking state leads to frontoparietal activation, but unconscious input does not; (ii) in unconscious states, sensory stimulation activates only sensory cortex, but not frontoparietal regions; (iii) the conscious resting state shows high frontoparietal metabolism compared with outward-directed cognitive tasks; and (iv) four causally very different unconscious states show marked metabolic decrements in the same areas.</p></blockquote>
</li>
<li class="footnote" id="footnote118_d4l61h9"><a class="footnote-label" href="#footnoteref118_d4l61h9">118.</a> See <a href="http://www.openphilanthropy.org/sites/default/files/James_Rose_11-18-16_%28public%29.pdf">notes from my conversation with James Rose</a>.
</li>
<li class="footnote" id="footnote119_zf3kxq7"><a class="footnote-label" href="#footnoteref119_zf3kxq7">119.</a> For example, <a href="http://www.sciencedirect.com/science/article/pii/S1364661316301206">Freud et al. (2016)</a>, despite being critics of the view, write:<br /><blockquote><p>The cortical visual system is almost universally thought to be segregated into two anatomically and functionally distinct pathways: a ventral occipitotemporal pathway that subserves object perception, and a dorsal occiptoparietal pathway that subserves object localization and visually guided action.</p></blockquote>
<p>Similarly, another paper critical of the view, <a href="http://booksandjournals.brillonline.com/content/journals/10.1163/187847510x503588">Cardoso-Leite &amp; Gorea (2010)</a>, describe Goodale &amp; Milner&#8217;s “two streams” account as “the most widespread account” of “how a physical stimulus can lead to a motor response, with or without an accompanying conscious experience.”</p>
</li>
<li class="footnote" id="footnote120_acdxds0"><a class="footnote-label" href="#footnoteref120_acdxds0">120.</a> This finding directly contradicts our intuitive “assumption of experience-based control” <a href="http://philreview.dukejournals.org/content/110/4/495.short">Clark (2001)</a>.
</li>
<li class="footnote" id="footnote121_qtlnu1m"><a class="footnote-label" href="#footnoteref121_qtlnu1m">121.</a> This image is <a href="https://commons.wikimedia.org/wiki/File:1424_Visual_Streams.jpg">File:1424 Visual Streams.jpg</a> from Wikimedia Commons, and is licensed under the Creative Commons Attribution 3.0 Unported license.
</li>
<li class="footnote" id="footnote122_if4m0fi"><a class="footnote-label" href="#footnoteref122_if4m0fi">122.</a> Some additional visual information travels not through the LGN but instead to the superior colliculus, then to the pulvinar, and then joins the dorsal stream of visual processing in the posterior parietal cortex. See <a href="https://global.oup.com/academic/product/basic-vision-9780199572021?cc=us&amp;lang=en&amp;">Snowden et al. (2012)</a>, ch. 11. In addition to these two pathways, there are several other retinal projections that are less well-studied (see <a href="https://global.oup.com/academic/product/the-visual-brain-in-action-9780198524724?cc=us&amp;lang=en&amp;">Milner &amp; Goodale 2006</a>, section 1.1).
</li>
<li class="footnote" id="footnote123_1mgps01"><a class="footnote-label" href="#footnoteref123_1mgps01">123.</a> <a href="https://global.oup.com/academic/product/the-visual-brain-in-action-9780198524724?cc=us&amp;lang=en&amp;">Milner &amp; Goodale (2006)</a>, p. 67:<br /><blockquote><p>…both the dorsal and ventral streams diverge anatomically from the primary visual cortex, but, as we noted in Chapter 2, the dorsal stream also has substantial inputs from several subcortical visual structures in addition to the input from V1… In contrast, the ventral stream appears to depend on V1 almost entirely for its visual inputs.</p></blockquote>
</li>
<li class="footnote" id="footnote124_gqxhs16"><a class="footnote-label" href="#footnoteref124_gqxhs16">124.</a> This quote from <a href="https://global.oup.com/academic/product/sight-unseen-9780199596966">Goodale &amp; Milner (2013)</a>, ch. 9.<br /><br /><a href="https://global.oup.com/academic/product/reference-and-consciousness-9780199243815?cc=us&amp;lang=en&amp;">Campbell (2002)</a>, pp. 55-56, offers a different analogy, that of a heat-seeking missile:
<blockquote><p>…conscious attention is what defines the target of processing for the visuomotor system, and thereby ensures that the object you intend to act on is the very same as the object with which the visuomotor system becomes engaged… This whole procedure may work even though your experience of the location of the object is not particularly accurate. If you see a penny in a mirror without realizing that you are seeing it in a mirror, you may use its apparent location in verifying that it is brown, and [in grasping] your hand correctly to pick it up, even though your experience of its location is not actually correct. There is an obvious analogy with the behaviour of a heat-seeking missile. Once the thing is launched, it sets the parameters for action on its target in its own way; but to have it reach the target you want, you have to have it roughly pointed in the right direction before it begins, so that it has actually locked on to the intended object.</p></blockquote>
</li>
<li class="footnote" id="footnote125_xj5d5gq"><a class="footnote-label" href="#footnoteref125_xj5d5gq">125.</a> Of course, vision neuroscience could still support some kind of CRV even if this “two streams” account turns out to be false. For example, even if the “two streams” account is wrong, it could still be the case that all subcortical visual processing is unconscious. E.g., the pupillary light reflex is controlled via subcortical visual processing, and we are not consciously aware of our own pupil dilation (<a href="http://neuroscience.uth.tmc.edu/s3/chapter07.html">Dragoi 2016</a>).<br /><br />
Also, there are several other lines of evidence (besides those reviewed in <a href="#AppendixC">Appendix C</a>) that could be used to argue for unconscious vision of various kinds, supported by different parts of the brain. For example see the literatures on backward masking, binocular rivalry, and blindsight, which I don&#8217;t review here. On backward masking, see <a href="https://global.oup.com/academic/product/visual-masking-9780198530671?cc=us&amp;lang=en&amp;">Breitmeyer &amp; Ogmen (2006)</a>. On binocular rivalry, see <a href="https://benjamins.com/#catalog/books/aicr.92/main">Miller (2015)</a>. On blindsight, see <a href="http://www.tandfonline.com/doi/abs/10.1080/02724980343000882">Cowey (2004)</a>.
</li>
<li class="footnote" id="footnote126_p75ycxb"><a class="footnote-label" href="#footnoteref126_p75ycxb">126.</a> <a href="https://global.oup.com/academic/product/sight-unseen-9780199596966">Goodale &amp; Milner (2013)</a>, ch. 7, summarize some of this evidence briefly:<br /><blockquote><p>The somatosensory (touch) system seems to have an organization remarkably similar to that of the visual system, with two distinct representations of the body existing in the brain, one for the guidance of action and the other for perception and memory. This model, proposed by [<a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/somatosensory-processes-subserving-perception-and-action/BCDB7C34D36AAB647D84CF5DC77EF9CE">Dijkerman &amp; De Haan (2007)</a>], is supported not only by neuroscience evidence, but by the fact that there are somatosensory illusions that fool our perception without fooling actions based on the same form of bodily sensation. Dijkerman and De Haan call the perceptual representation of the body (which is vulnerable to illusions) our “body image” and the metrical representation that guides action our “body schema.”</p>
<p>One illusion to which the body image is prone is the so-called “rubber hand illusion.” This illusion is induced by having a person rest one of their arms on a table, but hidden from sight, with an artificial arm made of rubber lying alongside it. The experimenter proceeds to stroke the person&#8217;s arm while simultaneously stroking the artificial arm in full view. The result is that the person gains a strong impression that the stroking sensations are located not where their real arm is really is, but as if shifted in space to where the artificial arm is lying. Yet when tested in a reaching task with the affected arm, they do not make erroneously long or short movements based on where they wrongly sense the starting point of their arm to be. Instead, their actions are guided on the basis of the true location of the arm, independently of their perceptions—presumably on the basis of the veridical body schema.</p>
<p>…There are even reports of brain-damaged individuals who have a somatosensory equivalent of blindsight — Yves Rossetti calls this phenomenon “numbsense.” Such patients are able to locate a touch on an arm which has completely lost the conscious sense of touch, by pointing with the other arm while blindfolded. Rossetti reports that his patient was amazed that she could do this. Even more dramatically…, Rossetti found that this numbsense ability evaporated to chance guessing when the patient was asked to delay two to three seconds before making her pointing response. It may be then that while the body schema may survive brain damage that disables the body image, it is constantly reinventing itself, with each reinvention having only a transient lifetime before being lost or replaced. Just like the dorsal visual stream.</p>
<p>…There is recent evidence to suggest that the auditory system too may be divided into perception and action pathways. Steve Lomber… has recently shown [<a href="http://www.nature.com/neuro/journal/v11/n5/abs/nn.2108.html">Lomber &amp; Malhotra (2008)</a>] that when one region of auditory cortex in the cat is temporarily inactivated by local cooling, the cat has no problem turning its head and body toward the sounds but cannot recognize differences in the patterns of those sounds, whereas when another quite separate area is cooled the cat can tell the sound patterns apart but can no longer turn towards them. These findings in the auditory system — and the work discussed earlier on the organization of the somatosensory system — suggest that a division of labor between perceiving objects and acting on them could be a ubiquitous feature of sensory systems in the mammalian cerebral cortex.</p></blockquote>
</li>
<li class="footnote" id="footnote127_b4j2gpg"><a class="footnote-label" href="#footnoteref127_b4j2gpg">127.</a> E.g. see <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195376746.001.0001/oxfordhb-9780195376746-e-12">Kihlstrom (2013)</a>.
</li>
<li class="footnote" id="footnote128_1o876te"><a class="footnote-label" href="#footnoteref128_1o876te">128.</a> <a href="http://www.sciencedirect.com/science/article/pii/S1364661305002998">Laureys (2005)</a>:<br /><blockquote><p>Voxel-based statistical analyses have sought to identify regions showing metabolic dysfunction in the vegetative state as compared with the conscious resting state in healthy controls. These studies have identified a metabolic dysfunction, not in one brain region but in a wide frontoparietal network encompassing the polymodal associative cortices…</p>
<p>…Awareness seems not to be exclusively related to activity in the frontoparietal network, but equally important is the relation of awareness to the functional connectivity within this network, and with the thalami. ‘Functional disconnections’ in long-range cortico–cortical (between latero-frontal and midline-posterior areas) and cortico–thalamo–cortical (between non-specific thalamic nuclei and lateral and medial frontal cortices) pathways have been identified in the vegetative state [6,9]. Moreover, recovery is accompanied by a functional restoration of the frontoparietal network [7] and some of its cortico–thalamo–cortical connections [9]. In addition to measuring resting brain function and connectivity, recent neuroimaging studies have identified brain areas that still show activation during external stimulation in vegetative patients.</p></blockquote>
</li>
<li class="footnote" id="footnote129_oj0bt5e"><a class="footnote-label" href="#footnoteref129_oj0bt5e">129.</a> See e.g. pp. 421-425 of <a href="https://books.google.com/books?id=PeacBAAAQBAJ&amp;lpg=PP1&amp;ots=B6vmpfLA42&amp;lr&amp;pg=PA407#v=onepage&amp;q&amp;f=false">Tononi et al. (2015a)</a>.
</li>
<li class="footnote" id="footnote130_naw48pr"><a class="footnote-label" href="#footnoteref130_naw48pr">130.</a> See e.g. pp. 425-426 of <a href="https://books.google.com/books?id=PeacBAAAQBAJ&amp;lpg=PP1&amp;ots=B6vmpfLA42&amp;lr&amp;pg=PA407#v=onepage&amp;q&amp;f=false">Tononi et al. (2015a)</a>.
</li>
<li class="footnote" id="footnote131_0dsb744"><a class="footnote-label" href="#footnoteref131_0dsb744">131.</a> One might wonder why Merker does not discuss cases of a related condition <a href="https://en.wikipedia.org/wiki/Hydrocephalus">hydrocephalus</a>, for example the famous patient described by John Lorber as “a young student… who has an IQ of 126 [and who] gained a first-class honors degree in mathematics” and yet who “has virtually no brain” (<a href="http://science.sciencemag.org/content/210/4475/1232">Lewin 1980</a>). See also <a href="https://www.thieme-connect.com/DOI/DOI?10.1055/s-2008-1044292">Jackson &amp; Lorber (1984)</a>’s report of one patient with a brain volume 56% of the normal volume, yet possessing “a first class degree in mathematics and… [an] IQ of 130” (this might be the same patient). (For another relatively dramatic case, see <a href="http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(07)61127-1/abstract">Feuillet et al. 2007</a>.)<br /><br />
Merker describes hydrocephalus as a “far more benign” condition than hydranencephaly, because “cortical tissue is is compressed by enlarging ventricles but is present in anatomically distorted form [<a href="http://journals.lww.com/neurosurgery/Abstract/1980/01000/Hydranencephaly_versus_Maximal_Hydrocephalus__An.4.aspx">Sutton et al. (1980)</a>].” Merker seems to be saying that hydranencephaly can result in far more dramatic loss of cortical tissue than hydrocephalus does, and from the literature I&#8217;ve skimmed, that seems to be correct.
</li>
<li class="footnote" id="footnote132_cja3z8z"><a class="footnote-label" href="#footnoteref132_cja3z8z">132.</a> <a href="http://cercor.oxfordjournals.org/content/23/4/833">Damasio et al. (2013)</a>; <a href="http://link.springer.com/article/10.1007/s00429-014-0986-3">Feinstein et al. (2016)</a>; <a href="http://www.jneurosci.org/content/29/9/2684.short">Starr et al. (2009)</a>; <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0038413">Philippi et al. (2012)</a>. If one considers some PVS patients to be “reporting” conscious experience as detected by neuroimaging alone (<a href="http://www.annualreviews.org/doi/abs/10.1146/annurev-psych-113011-143729">Owen 2013</a>; <a href="http://bjps.oxfordjournals.org/content/early/2015/04/14/bjps.axv012.short">Klein 2015</a>), these cases might serve as additional examples of this phenomenon.
</li>
<li class="footnote" id="footnote133_uq4024a"><a class="footnote-label" href="#footnoteref133_uq4024a">133.</a> For these and other details on Mike, see the following sources:
<ul><li>Teri Thomas’ <em>The Official Mike the Headless Chicken Book</em> (2000). The book was previously available online <a href="https://web.archive.org/web/20010401052046/http://www.miketheheadlesschickenbook.com/index.html">here</a>, and has an Amazon page (with no copies available when I checked) <a href="https://amazon.com/Official-Mike-Headless-Chicken-Book/dp/B004BQ179M/">here</a>. I was able to order the book, in September 2016, by calling the <a href="http://www.fruita.org/parksrec/page/fruita-community-center">Fruita Community Center</a> at 970-858-0360.</li>
<li>The story “<a href="https://books.google.com/books?id=pksEAAAAMBAJ&amp;lpg=PA53&amp;ots=shqCokQZQT&amp;pg=PA53#v=onepage&amp;q&amp;f=false">Headless Rooster: Beheaded chicken lives normally after freak decapitation by ax</a>” on pp. 53-54 of the Oct 22, 1945 issue of <em>Life</em> magazine.</li>
<li><a href="https://books.google.com/books?id=6OgdMLO0CdMC">Lambert &amp; Kinsley (2004)</a>, pp. 83-84.</li>
<li>The <a href="http://www.miketheheadlesschicken.org/mike/page/history">history</a> page on the website for the annual Mike the Headless Chicken Festival in Fruita, Colorado.</li>
<li><a href="http://blogs.scientificamerican.com/running-ponies/meet-miracle-mike-the-chicken-who-lived-for-18-months-without-his-head/">Crew (2014)</a>.</li>
<li>A 2015 <em>BBC Magazine</em> article, “<a href="http://www.bbc.com/news/magazine-34198390">The chicken that lived for 18 months without a head</a>.”</li>
<li>The 2001 PBS documentary <em><a href="http://www.shoppbs.org/product/index.jsp?productId=1428832&amp;cp=&amp;sr=1">The Natural History of the Chicken</a></em>, directed by Mark Lewis. The section on Mike, including interviews with some of the people who witnessed Mike post-decapitation, appears from 34:14-41:30.</li>
</ul><p>Note that while some popular sources report that Mike was killed specifically to serve as that night&#8217;s dinner, Lloyd Olsen&#8217;s great-grandson Troy Waters disputes this (unimportant) detail, claiming instead that Lloyd and his wife “were actually slaughtering, oh, 40 or 50 of them that day” (Thomas 2000, p. 1).</p>
<p>Here are a few additional details from Thomas (2000):</p>
<blockquote><p><strong>Q:</strong> Did Mike ever try to mate after he was beheaded?</p>
<p><strong>A:</strong> No. Mike was only about four and a half months old when he was beheaded, and that&#8217;s not old enough for roosters to mate. Usually they have to be about a year old for that. When Mike was beheaded, although his body continued to develop and gain weight, he didn&#8217;t mature in the traditional sense…</p>
<p><strong>Q:</strong> Did Mike suffer from pain or discomfort?</p>
<p><strong>A:</strong> Officers from several humane societies examined Mike on several occasions, and declared him to be free from suffering. According to one, account, he had lost the part of his brain that would have caused him to feel pain. That seems to have been verified by the fact that his owners had to tape up his feet to keep him from instinctively scratching his neck where his head would have been. The sharp spurs on his feet would have damaged his exposed neck if they hadn&#8217;t been taped up.</p>
<p>However… Troy Waters says that post-decapitation Mike was more docile than most chickens, and that most of the time he just lay in his straw-filled apple box. Lloyd and Hope had to prod him to get him to flap his wings and walk around for the tourists. He may have been depressed… or just in a fowl mood. [p. 21]</p>
<p>…</p>
<p>According to numerous accounts, Mike&#8217;s domeless existence was studied extensively by scientists and students at the University of Utah back when he was alive…</p>
<p>However, apparently that research is lost…, and no one at the University of Utah was able to find anything about it, even after extensive searches done of their archives at the request of the Fruita Chamber of Commerce last year, and PBS this year (2000). By the time I called, the librarians had heard all about the story of Mike, and they explained to me that the University of Utah had changed the structure of its life sciences department since the 1940s… Any records of Mike were simply lost, if they ever existed in the first place. [p. 60]</p></blockquote>
<p>I located only one other source on consciousness which mentions Mike the headless chicken: <a href="http://www.degruyter.com/view/j/revneuro.2009.20.3-4/revneuro.2009.20.3-4.151/revneuro.2009.20.3-4.151.xml">Leisman &amp; Koch (2009)</a>.</p>
</li>
<li class="footnote" id="footnote134_kxfyy2r"><a class="footnote-label" href="#footnoteref134_kxfyy2r">134.</a> <a href="http://animalstudiesrepository.org/animsent/vol1/iss9/10/">Mallatt &amp; Feinberg (2016)</a>:<br /><blockquote><p>Our own solution to the problem raised by the convincing evidence for cortical consciousness is that the consciousness shifted from the tectum to the enlarging cerebral cortex when mammals evolved from their reptile-like ancestors.</p></blockquote>
</li>
<li class="footnote" id="footnote135_ugatktu"><a class="footnote-label" href="#footnoteref135_ugatktu">135.</a> See especially <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=1007572&amp;fileId=s0140525x07000891">Merker (2007)</a>, including the response commentaries, and also <a href="http://onlinelibrary.wiley.com/doi/10.1111/papr.12207/full">Devor et al. (2014)</a> and <a href="http://animalstudiesrepository.org/animsent/vol1/iss3/23/">Merker (2016)</a>.
<p><a href="http://www.pnas.org/content/113/18/4900.short">Barron &amp; Klein (2016)</a> are especially succinct in their case against CRVs:</p>
<blockquote><p>There is now considerable evidence that, in humans, subjective experience can exist in the absence of self-reflexive consciousness, and that the two are supported by different neural structures. Midbrain structures, rather than cortex, seem to be especially important. [Merker (<a href="http://www.sciencedirect.com/science/article/pii/S1053810003000023">2005</a>, <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=1007572&amp;fileId=s0140525x07000891">2007</a>), <a href="http://www.sciencedirect.com/science/article/pii/S001002770000127X">Parvizi &amp; Damasio (2001)</a>, <a href="http://www.nature.com/nrn/journal/v14/n2/abs/nrn3403.html">Damasio &amp; Carvalho (2013)</a>, and <a href="http://www.pnas.org/content/110/Supplement_2/10357.short">Mashour &amp; Alkire (2013)</a>] have all argued that the integrated structures of the vertebrate midbrain are sufficient to support the capacity for subjective experience.</p>
<p>[<a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=1007572&amp;fileId=s0140525x07000891">Merker (2007)</a>] notes that subjective experience is remarkably sensitive to damage to midbrain structures. Conversely, there is evidence of preserved consciousness even in patients who lack a cortex [<a href="http://www.sciencedirect.com/science/article/pii/S1053810003000023">Merker (2005)</a>]. Further, although cortical damage can have profound effects on the contents of consciousness, damage to any portion of the cortex alone can spare the basic capacity for subjective experience [<a href="http://cercor.oxfordjournals.org/content/23/4/833">Damasio et al. (2013)</a>; <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0038413">Philippi et al. (2012)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0028393214000384">Herbet et al. (2014)</a>; <a href="http://www.sciencedirect.com/science/article/pii/0028393294900663">Kapur et al. (1994)</a>; <a href="http://science.sciencemag.org/content/269/5225/853">Friedman-Hill et al. (1995)</a>; <a href="https://scholar.google.com/scholar?hl=en&amp;q=%22Emotional+disturbances+associated+with+focal+lesions+of+the+limbic+frontal+lobe.%22&amp;btnG=&amp;lr=lang_en&amp;as_sdt=1%2C48">Damasio &amp; Hosen (1983)</a>]. Cortical damage alone can have profound effects on the contents of consciousness, but even massive cortical damage seems to spare subjective experience itself [<a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=1007572&amp;fileId=s0140525x07000891">Merker (2007)</a>; <a href="http://cercor.oxfordjournals.org/content/23/4/833">Damasio et al. (2013)</a>; <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0038413">Philippi et al. (2012)</a>]. Indeed, there is evidence of residual conscious awareness in patients with severe cortical damage who are otherwise unresponsive to the world, suggesting that preserved subcortical structures may continue to support subjective experience [<a href="http://www.tandfonline.com/doi/abs/10.1076/neur.8.4.394.16184">Owen et al. (2002)</a>; <a href="http://hdl.handle.net/1959.14/361254">Klein &amp; Hohwy (2015)</a>]. Although the mechanism of anesthetic action is still debated [<a href="http://online.liebertpub.com/doi/abs/10.1089/brain.2012.0107">Hudetz (2012)</a>], there is increasing evidence that the effect of anesthetics depends on the disconnection of cortical circuits from subcortical structures rather than on their direct cortical activity [<a href="http://science.sciencemag.org/content/322/5903/876">Alkire et al. (2008)</a>; <a href="http://www.jneurosci.org/content/33/9/4024.short">Gili et al. (2013)</a>]. Anesthetics [<a href="http://journals.lww.com/anesthesia-analgesia/Abstract/1996/08000/In_Vivo_Imaging_of_Human_Limbic_Responses_to.16.aspx">Gyulai et al. (1996)</a>] or electrical stimulation [<a href="http://www.sciencedirect.com/science/article/pii/S0028393214000384">Herbet et al. (2014)</a>], which affect cortical midline structures without affecting subcortical structures, do not abolish consciousness; they instead produce unresponsive but conscious dreamlike states. Conversely, emergence from anesthesia [<a href="http://www.pnas.org/content/110/Supplement_2/10357.short">Mashour &amp; Alkire (2013)</a>; <a href="http://www.jneurosci.org/content/32/14/4935.short">Långsjö et al. (2012)</a>] and coma/vegetative state [<a href="http://www.sciencedirect.com/science/article/pii/S0166223609001866">Schiff (2010)</a>] are predicted by the reengagement of subcortical structures and reintegration of those structures with cortical circuits. Other authors have noted the powerful subcortical effect of drugs, endogenous peptides, and direct stimulation on primitive motivational states [<a href="http://psycnet.apa.org/psycinfo/2008-07784-004">Panksepp (2008)</a>; <a href="http://www.pnas.org/content/110/Supplement_2/10357.short">Mashour &amp; Alkire (2013)</a>; <a href="https://global.oup.com/academic/product/the-primordial-emotions-9780199203147?cc=us&amp;lang=en&amp;">Denton (2006)</a>].</p>
<p>In sum, there is good evidence that subcortical structures underlie the basic capacity for subjective experience in humans. This is not to say that the cortex is unimportant for conscious experience, of course. Rather, the proposal is that subcortical structures support the basic capacity for experience, the detailed contents of which might be elaborated by or otherwise depend upon cortical structures [<a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00501/full">Merker (2013)</a>].</p></blockquote>
<p>For a reply to Barron &amp; Klein on these points, see <a href="http://animalstudiesrepository.org/animsent/vol1/iss9/6/">Allen-Hermanson (2016)</a>. For a counter-reply, and additional clarifications of Klein &amp; Barron&#8217;s case against CRVs, see <a href="http://animalstudiesrepository.org/animsent/vol1/iss9/21/">Klein &amp; Barron (2016)</a>.</p>
<p><a href="http://www.springer.com/us/book/9780387389752">Machado (2007)</a>, ch. 3, provides another fairly succinct case against CRVs, in the context of his discussion of the neurological grounds for pronouncing a medical patient “dead.” In the excerpt below, I have left out Machado&#8217;s many citations, to avoid cluttering the text:</p>
<blockquote><p>Any full account of death should include three distinct elements: the definition of death, the criterion (anatomical substratum) of brain death, and the tests to prove that the criterion has been satisfied. Undoubtedly, the term ‘criterion’ for referring to the anatomical substratum introduces confusion in this discussion, because protocols of tests (clinical and instrumental) for brain diagnosis are called ‘diagnostic criteria’ or ‘sets of diagnostic criteria’. Therefore, I will use the term ‘anatomical substratum’ instead of criterion.</p>
<p>During the last decades, three main brain-oriented formulations of death have been discussed: whole brain, brainstem death and higher brain standards… The whole brain criterion refers to the irreversible cessation of all intracranial structure functions. It has been accepted by society mainly for practical reasons…</p>
<p>The brainstem standard was adopted in several Commonwealth countries. Pallis emphasized that the capacity for consciousness and respiration are two hallmarks of life of the human being, and that brainstem death predicts an inescapable asystole. However, a physiopathological review of consciousness generation will provide a basis for not accepting Pallis’ definition of death…</p>
<p>The higher brain formulation springs largely from consideration of the persistent vegetative state (PVS), and has been mainly defended by philosophers. The higher brain theorists have defined human death as the ‘the loss of consciousness’ (definition), related to the irreversible destruction of the neocortex (anatomical substratum).</p>
<p>I will demonstrate in this chapter that consciousness does not bear a simple one-to-one relationship with higher or lower brain structures and that, consequently, the higher brain view is wrong, because the definition (consciousness) does not harmonize with the anatomical substratum (neocortex)…</p>
<p>…Two physiological components control conscious behavior: arousal and awareness… Arousal represents a group of behavioral changes that occurs when a person awakens from sleep or transits to a state of alertness… Awareness, also known as content of consciousness, represents the sum of cognitive and affective mental functions, and denotes the knowledge of one&#8217;s existence, and the recognition of the internal and external worlds…</p>
<p>In summary, a human being&#8217;s state of consciousness reflects both his or her level of arousal that depends on subcortical arousal-energizing systems and, the sum of the cognitive, affective, and other higher brain functions (content of consciousness or awareness), related to “complex physical and psychologic mechanisms by which limbic systems and the cerebrum enrich and individualize human consciousness.” Therefore, I will use the term arousal when referring to those subcortical arousal-energizing systems, and awareness, to denote the sum of those complex brain functions, related to limbic and cerebrum levels…</p>
<p>…Awareness is thought to be dependent upon the functional integrity of the cerebral cortex and its subcortical connections; each of its many parts are located, to some extent, in anatomically defined regions of the brain…</p>
<p>…Shewmon has discussed some examples of clear participation of subcortical structures in awareness. Experimental animals with complete decortication have been shown to be capable of complex interactions with the environment, which is evidence of some awareness. In lesions of the somatosensory cortex an evident loss of tactile, vibration and joint position sense is observed; nonetheless, conscious experience of pain and temperature is preserved, mediated by subcortical structures, probably the thalamus. This author also commented that two hydranencephalic patients (“prenatal destruction of the cerebral hemispheres with intact skull and scalp”) unquestionably manifested conscious behavior. These two cases are examples of the brainstem “plasticity” in newborns. Clinical and experimental evidence convincingly suggests that the brainstem of newborns is potentially capable of much more complex integrative functioning. This includes some functions commonly considered to be cortical, even in animals. Based on these subjects, the potential presence of some primitive form of awareness in anencephalics, and the possibility of subjective feeling of pain, has been suggested. Thus, according to Shewmon “the human brainstem and diencephalon, in the absence of cerebral cortex, can mediate consciousness and purposeful interaction with the environment.”</p>
<p>…PVS [persistent vegetative state] provides an anatomic-functional model in which arousal is preserved and awareness is apparently lacking. Therefore, it has been suggested that both components of consciousness [i.e., arousal and awareness] “are mediated by distinct anatomic, neurochemical and/or physiological systems.” Nonetheless, the potential plasticity of the brain has demonstrated that subcortical structures could mediate awareness, even with the complete absence of the cerebral cortex. Cases that have undergone hemispherectomy have shown clear signs of neuroplasticity. Austin and Grant reported 3 cases that had undergone total hemispherectomy (comprising cortex, white matter and basal ganglia), who continued speaking and were aware of their environment during the operation, done under local anesthesia…</p>
<p>Thus, awareness is not only related to the function of the neocortex (although it is of primary importance), but also to complex physical and psychological mechanisms, due to the interrelation of the ARAS [ascending reticular activating system], limbic system, and the cerebrum…</p>
<p>…we cannot simply differentiate and locate arousal as a function of the ARAS, and awareness as a function of the cerebral cortex. Substantial interconnections among the brainstem, subcortical structures and the neocortex, are essential for subserving and integrating both components of human consciousness.</p>
<p>The above considerations lead one to conclude that there is no single anatomical place of the brain “necessary and sufficient for consciousness.”</p>
<p>…</p>
<p>Can we deny the existence of internal awareness in PVS, because these patients apparently seem to be disconnected from the external world? The subjective dimension of awareness is philosophically impossible to test, but physiologically it seems conceivable that subjective awareness might continue. Karen Ann Quinlan&#8217;s brain showed severe damage of the thalamus, with the cerebral hemispheres relatively spared, and other authors have reported similar findings…</p>
<p>…Thus, in PVS cases it is impossible to deny a possible preservation of internal awareness. According to the neuropathological pattern, either subcortical structures could provide internal awareness, or some remaining activating pathways projecting to the cerebral cortex without relaying through the thalamus could stimulate the cerebral cortex. As consciousness is based on anatomy and physiology throughout the brain, it is impossible to classify a PVS case as dead. The brain is severely damaged, but not fully and irreversibly destroyed.</p></blockquote>
<p>In ch. 7, Machado adds:</p>
<blockquote><p>The perceptions of pain and suffering are conscious experiences; unconsciousness, by definition, precludes these experiences. The Multi-Society Task Force on PVS concluded that PVS patients are unconscious, and they “cannot experience pain and suffering.” However, it is important to argue that the pain response in newborns does not involve the cerebral cortex, which is one of the primary loci of damage in PVS. It logically follows that PVS patients have the same potential to experience pain and suffering as newborns. This argument also implies that brute animals cannot experience pain since they are not self-conscious. Howsepian considered this to be “at best counterintuitive and at worst patently false.”</p></blockquote>
</li>
<li class="footnote" id="footnote136_gmnjwnc"><a class="footnote-label" href="#footnoteref136_gmnjwnc">136.</a> I haven&#8217;t seen a poll on this question, this is just the sense I get from reading AI papers and talking to AI researchers, especially those in the important AI subfield of deep learning.
</li>
<li class="footnote" id="footnote137_kl263u6"><a class="footnote-label" href="#footnoteref137_kl263u6">137.</a> <a href="https://mitpress.mit.edu/books/consciousness-reconsidered">Flanagan (1992)</a>, p. 5. Note that Flanagan rejects consciousness inessentialism (p. 6).
</li>
<li class="footnote" id="footnote138_so64bbr"><a class="footnote-label" href="#footnoteref138_so64bbr">138.</a> I&#8217;m not the only one to prefer this term. E.g. <a href="https://www.ceeol.com/search/article-detail?id=92092">Rose &amp; Dietrich (2009)</a> write that “[‘conscious essentialism’] should probably be ‘consciousness inessentialism’ since it is a thesis about consciousness…”
</li>
<li class="footnote" id="footnote139_z3aj0a5"><a class="footnote-label" href="#footnoteref139_z3aj0a5">139.</a> This is not an argument against functionalism. In the example given here, the original version of my brain and the post-rewiring version of my brain are <em>different functions</em> (at the level that, by hypothesis, matters for consciousness), even though they result in the same input-output behavior at the level of my global behavior (modulo some extra energy expenditure by the version of my brain that has to undertake the additional computational work of encrypting and decrypting <em>I</em>, and performing computations on an encrypted form of <em>I</em> using fully homomorphic encryption).<br /><br />
For more on fully homomorphic encryption, see <a href="http://eprint.iacr.org/2015/1192.pdf">Armknecht et al. (2015)</a>.<br /><br />
In case you&#8217;re curious: yes, fully homomorphic encryption is possible within a machine learning context. See e.g. <a href="http://link.springer.com/article/10.1007/s00500-016-2296-6">Jiang et al. (2016)</a>.
</li>
<li class="footnote" id="footnote140_jtpq751"><a class="footnote-label" href="#footnoteref140_jtpq751">140.</a> As <a href="https://global.oup.com/academic/product/created-from-animals-9780192861290">Rachels (1990)</a>, p. 131, put it:<br /><blockquote><p>Descartes&#8217;s view [about the strong difference between human and animal minds] was extreme, even for his own time, and despite its wide influence most thinkers did not share it. Nevertheless, it was a possible view then, in a way that it is not possible now. The reason Descartes&#8217;s view of animals is not possible today - the reason his view seems so obviously wrong to us-is that between him and us came Darwin. Once we see the other animals as our kin, we have little choice but to see their condition as analogous to our own. Darwin stressed that, in an important sense, their nervous systems, their behaviours, their cries, aTe our nervous systems, our behaviours, and our cries, with only a little modification. They are our common property because we inherited them from the same ancestors. Not knowing this, Descartes was free to postulate far greater differences between humans and non-humans than is possible for us.</p></blockquote>
<p>I think <em>this</em> statement of the point is much too strong, though. Yes, we must take seriously the fact that our brains are on a continuum with those of other animals, but this fact alone cannot tell us which specific cognitive functions we share with specific other species, and which ones we do not. One could make the same argument as Rachels does for mirror self-recognition, and this argument would fail to correctly predict that chimpanzees exhibit mirror self-recognition and gorillas do not (<a href="http://link.springer.com/article/10.1007/s10329-015-0488-9">Anderson &amp; Gallup Jr. 2015</a>). Darwin alone cannot answer the specific questions of comparative psychology — that&#8217;s what the fields of comparative psychology and ethology are for.</p>
</li>
<li class="footnote" id="footnote141_si35ttz"><a class="footnote-label" href="#footnoteref141_si35ttz">141.</a> Compare to <a href="http://books.wwnorton.com/books/detail.aspx?ID=4294992671">Dennett (2017)</a>’s extended discussion of “competence without comprehension.” One could make a similar (but not identical) case for “competence without consciousness.”
</li>
<li class="footnote" id="footnote142_let6tpy"><a class="footnote-label" href="#footnoteref142_let6tpy">142.</a> See e.g. <a href="https://plato.stanford.edu/entries/panpsychism/">Seager &amp; Allen-Hermanson (2010)</a>; <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=FgPWBgAAQBAJ&amp;oi=fnd&amp;pg=PA246&amp;ots=u4Iqo7lh9x&amp;sig=IUZRJgSblQl3IfyabvW2CR8vlmk#v=onepage&amp;q&amp;f=false">Chalmers (2015)</a>; ch. 4 of <a href="http://www.polity.co.uk/book.asp?ref=9780745653440">Weisberg (2014)</a>.
</li>
<li class="footnote" id="footnote143_urgkkgu"><a class="footnote-label" href="#footnoteref143_urgkkgu">143.</a> For more on current debates about representation in the philosophy of mind, see e.g. <a href="https://books.google.com/books?id=DlU6CQAAQBAJ&amp;lpg=PP1&amp;pg=PT197#v=onepage&amp;q&amp;f=false">Rey (2015)</a>.
</li>
<li class="footnote" id="footnote144_jgpt26b"><a class="footnote-label" href="#footnoteref144_jgpt26b">144.</a> See <a href="https://mitpress.mit.edu/books/consciousness-color-and-content">Tye (2000)</a>, chs. 3-8, especially section 3.4. For some updates to Tye&#8217;s theory, see <a href="https://mitpress.mit.edu/books/consciousness-revisited">Tye (2009a)</a>. For an overview of FOR theories in general, see <a href="http://www.polity.co.uk/book.asp?ref=9780745653440">Weisberg (2014)</a>, ch. 7. For a brief introduction to representational theories of mind, see <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199262618.001.0001/oxfordhb-9780199262618-e-15">Tye (2009b)</a>.
</li>
<li class="footnote" id="footnote145_x12g9ta"><a class="footnote-label" href="#footnoteref145_x12g9ta">145.</a> <a href="https://mitpress.mit.edu/books/consciousness-color-and-content">Tye (2000)</a>, p. 62.
</li>
<li class="footnote" id="footnote146_rdz5ijz"><a class="footnote-label" href="#footnoteref146_rdz5ijz">146.</a> <a href="https://mitpress.mit.edu/books/consciousness-color-and-content">Tye (2000)</a>, pp. 62-63.
</li>
<li class="footnote" id="footnote147_31nnhuz"><a class="footnote-label" href="#footnoteref147_31nnhuz">147.</a> <a href="https://mitpress.mit.edu/books/consciousness-color-and-content">Tye (2000)</a>, ch. 8:<br /><blockquote><p>States with PANIC are nonconceptual states that track certain features, internal or external, under optimal conditions (and thereby represent those features). They are also states that stand ready and available to make a direct difference to beliefs and desires. It follows that creatures that are incapable of reasoning, of changing their behavior in light of assessments they make, based upon information provided to them by sensory stimulation of one sort or another, are not phenomenally conscious…</p>
<p>Consider, to begin with, the case of plants. There are many different sorts of plant behavior… [but] the behavior of plants is inflexible. It is genetically determined and, therefore, not modifiable by learning… [Plants] neither acquire beliefs and change them in light of things that happen to them nor do they have any desires… [Plants exhibit] no goal-directed behavior, no purpose, nothing that is the result of any learning, no desire <em>for</em> [e.g.] water. Plants, then, are not subject to any PANIC states… [and thus are] not phenomenally conscious.</p>
<p>…What about caterpillars? …Different kinds of caterpillars show different sorts of behavior upon hatching… Some, for example, eat the shells of the eggs from which they emerge; others crawl away from their cells immediately. But there is no clear reason to suppose that caterpillars are anything more than stimulus-response devices. They have a very limited range of behaviors available to them, each of which is automatically triggered at the appropriate time by the appropriate stimulus. Consider, for example, their sensitivity to light. Caterpillars have two eyes, one on each side of the head. Given equal light on both eyes, they move straight ahead. But given more light on one of the eyes, that side of the body toward the direction of most intense light, which is why caterpillars climb trees all the way to the top; the light there is strongest. Shift the light to the bottom of the tree, and the caterpillar will go down, not up, as it usually does, even if it means starving to death. Remove one of its eyes, and it will travel in a circle without ever changing its route.</p>
<p>Once one is made aware of these facts, there seems no more reason intuitively to attribute phenomenal consciousness to a caterpillar on the basis of how it moves than to an automatic door. The latter responds in a fixed, mechanical way to the presence of pressure on a plate in the floor or ground in front of it, just as the former responds mechanically to the presence of light. No learning, no variation in behavior with changed circumstances, no reasoned assessment occurs… Caterpillars, then, do not support states with PANIC any more than plants do.</p>
<p>…I come finally to the case of honey bees. There are many examples of sophisticated honey bee behavior. Bee colonies take on odors, primarily as a result of the food contained in the hives. These odors, which vary from hive to hive, are absorbed by the fur on the bees, and guards, placed at the entrance to the hive, learn to use it to check whether incoming bees are intruders or members of the colony. Scouts fly out from the hive each spring in search of a cavity suitable for a new hive. They use the sun as their main guide but they also rely upon landmarks. Upon returning, they dance to show bees in the hive what they have discovered. Their dance requires them to remember how the sun moves relative to the positions of the landmarks, enabling them to communicate the position of the cavity correctly. Recruit bees must learn what the dancers are telling them. This demands that they form some sort of cognitive map involving the landmarks. Scouts back from their trips attend to the dances of other scouts and then go out again to visit the different cavities. With their later return, they dance again. Eventually, the dances agree and the colony moves as one to the chosen spot…</p>
<p>Of course, some of this is preprogrammed. Bees choose neither to dance nor how to navigate; these activities are instinctive. But, equally clearly, in the above examples, the bees learn and use facts about their environments as they go along…</p>
<p>…[Another] example is provided by an experiment in which bees were shown some sugar solution on a plate near the hive. Then every five minutes or so, the plate was moved away so that the distance from the hive increased by one quarter. Initially, with the plate only four inches away, it was moved just one inch. But later when the food was four hundred feet away, the plate was removed another 100 feet. Amazingly, the bees caught on to this procedure and began to anticipate where the sugar would be next by flying there and waiting for the plate to arrive!</p>
<p>There seems to be ample evidence, then, that honey bees make decisions about how to behave in response to how things look, taste, and smell. They use the information their senses give them to identify things, to find their way around, to survive. They learn what to do in many cases as the situation demands. Their behavior is sometimes flexible and goal-driven. They are, therefore, the subjects of states with PANIC… [and thus] are phenomenally conscious…</p></blockquote>
</li>
<li class="footnote" id="footnote148_r3j301m"><a class="footnote-label" href="#footnoteref148_r3j301m">148.</a> Carruthers was, for decades, a leading defender of higher-order approaches to consciousness, but has recently (<a href="http://faculty.philosophy.umd.edu/pcarruthers/Carruthers%20recants.pdf">Carruthers 2017</a>) recanted, and now defends a first-order view.
</li>
<li class="footnote" id="footnote149_1unnkft"><a class="footnote-label" href="#footnoteref149_1unnkft">149.</a> Technically, if Tye&#8217;s “poised” criterion requires that conscious contents be poised to affect “belief” or “desire” (rather than <em>behavior</em> more generally), then dorsal stream processing might not satisfy Tye&#8217;s PANIC theory, but a very similar first-order theory would.
</li>
<li class="footnote" id="footnote150_jzt34jw"><a class="footnote-label" href="#footnoteref150_jzt34jw">150.</a> <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00002">Frankish (2016b)</a> lists some (strong) illusionist theories in footnote 2, reproduced below with links to the cited papers:<br /><blockquote><p>Defenders of illusionist positions (under various names) include Dennett (<a href="http://cogprints.org/254/">1988</a>; <a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">1991</a>; <a href="https://mitpress.mit.edu/books/sweet-dreams">2005</a>), <a href="http://link.springer.com/article/10.1007/s11098-005-1064-8">Hall (2007)</a>, <a href="http://press.princeton.edu/titles/9398.html">Humphrey (2011)</a>, <a href="https://global.oup.com/academic/product/consciousness-and-the-prospects-of-physicalism-9780199764037?lang=en&amp;cc=us">Pereboom (2011)</a>, Rey (<a href="http://link.springer.com/article/10.1007%2FBF00694849?LI=true">1992</a>; <a href="https://books.google.com/books?id=wEjqp9kTXnUC&amp;lpg=PA123&amp;ots=WxqdXB38SR&amp;lr&amp;pg=PA123#v=onepage&amp;q&amp;f=false">1995</a>; <a href="http://www.ingentaconnect.com//content/imp/jcs/2007/00000014/F0020009/art00006">2007</a>), and <a href="http://www.tandfonline.com/doi/abs/10.1080/09515089.2013.770940">Tartaglia (2013)</a>. As Tartaglia notes, Place and Smart also denied the existence of phenomenal properties, which Place described as ‘mythological’ (Place, <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.2044-8295.1956.tb00560.x/full">1956</a>, p. 49; Smart, <a href="http://www.jstor.org/stable/2182164">1959</a>, p. 151).</p></blockquote>
<p>Arguably, one might also cite <a href="http://link.springer.com/article/10.1007/s11023-016-9409-y">Chen et al. (2016)</a>, <a href="http://sro.sussex.ac.uk/64774/">Sloman &amp; Chrisley (2016)</a>, <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00010">Kammerer (2016)</a>, and perhaps some ancient Buddhist philosophers (<a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00006">Garfield 2016</a>).</p>
</li>
<li class="footnote" id="footnote151_csl6j1i"><a class="footnote-label" href="#footnoteref151_csl6j1i">151.</a> See also <a href="http://www.openphilanthropy.org/sites/default/files/Keith_Frankish_01-24-17_%28public%29.pdf">notes from my conversation with Keith Frankish</a>.
</li>
<li class="footnote" id="footnote152_ql90di2"><a class="footnote-label" href="#footnoteref152_ql90di2">152.</a> For example, <a href="http://faculty.philosophy.umd.edu/pcarruthers/Carruthers%20recants.pdf">Carruthers (2017)</a> is a <em>somewhat</em> illusionist account, and might (in the end) be relatively simple:<br /><blockquote><p>…what Carruthers &amp; Veillet (2011) proposed is that phenomenal consciousness can be operationalized as <em>whatever gives rise to the “hard problems” of consciousness</em>… That is, a given type of content can qualify as phenomenally conscious if and only if it seems ineffable, one can seemingly imagine zombie characters who lack it, one can imagine what-Mary-didn&#8217;t-know scenarios for it, and so on. For the very notion of phenomenal consciousness seems constitutively tied to these issues. If there is a kind of state or a kind of content for which none of these problems arise, then what would be the point of describing it as <em>phenomenally</em> conscious nonetheless? And conversely, if there is a novel type of content not previously considered in this context for which hard-problem thought-experiments can readily be generated, then that would surely be sufficient to qualify it as phenomenally conscious.</p>
<p>Once phenomenal consciousness is operationalized as whatever gives rise to hard-problem thought-experiments, however, it should be obvious that the initial challenge to first-order representationalism collapses. The reason why nonconceptual contents made available to central thought processes are phenomenally conscious, whereas those that are not so available are not, is simply that without thought one cannot have a thought-experiment. Only those nonconceptual contents available [via global broadcasting] to central thought are ones that will seem to slip through one&#8217;s fingers when one attempts to describe them (that is, be ineffable), only they can give rise to inversion and zombie thought-experiments, and so on. This is because those thought-experiments depend on a distinctively first-personal way of thinking of the experiences in question. This is possible if the experiences thought about are themselves available to the systems that generate and entertain such thoughts, but not otherwise. Experiences that are used for online guidance of action, for example, cannot give rise to zombie thought-experiments for the simple reason that they are not available for us to think about in a first-person way, as <em>this experience</em> or something of the sort. They can only be thought about third-personally, as <em>the experience that guides my hand when I grasp the cup</em>, or whatever.</p>
<p>There is simply no need, then, to propose that dual higher-order / first-order nonconceptual contents are necessary in order for globally broadcast experiences to acquire a subjective dimension and be <em>like</em> something to undergo. Once possession of such a dimension / possession of phenomenal consciousness is operationalized as whatever gives rise to hard-problem thought-experiments, then the mere fact of global broadcasting provides the required explanation. For it is nonconceptual content made available to central thought processes, and which is thus available to be thought about in a distinctively first-personal way, that grounds those thought-experiments.</p>
<p>If we suppose that this explanation on behalf of the first-order theorist is correct, however, then what should be said about phenomenally conscious experience in nonhuman animals? Presumably no animals have the conceptual resources to engage in hard-problem-type thought-experiments. (Indeed, the same may be true of many humans.) Does that mean that their experiences aren’t phenomenally conscious ones? Surely not. For giving rise to hard-problem thought-experiments is not supposed to be <em>constitutive</em> of phenomenal consciousness. Rather, it provides a theory-neutral way of delimiting the class of phenomenally conscious states in ourselves: roughly, phenomenally conscious states are the ones that are especially philosophically challenging or puzzling. Instead (according to first-order representationalism), what constitutes phenomenal consciousness is being a globally broadcast nonconceptual state. And there is plenty of reason to think that many species of animal (perhaps all vertebrates) have states of that general kind…</p>
<p>Seen from this perspective, indeed, there isn’t any deep issue about the phenomenally consciousness status of animal experience. Once we have established that an animal has a similar cognitive architecture to ourselves, with globally broadcast nonconceptual states that are made available to a range of different belief-forming, affect-generating, and executive decision-making systems, then there is simply no further question whether its experiences are <em>really like something</em> for the animal, or whether its experiences genuinely possess a subjective — <em>felt</em> — dimension. For there is no further property that needs to be added in order to render an experience phenomenally conscious. All that needs to be shown is that the animal possesses states of the same kind that we identify as phenomenally conscious (that is, which give rise to hard-problem thought-experiments) in ourselves.</p>
<p>Indeed, from this perspective it also emerges that there isn’t really a deep divide between creatures capable of phenomenal consciousness and ones that aren’t. For instance, we know that bees have structured belief-like states that guide them in the service of multiple goals, informed by perceptual input from a number of different sense-modalities… So they seem to possess simple minds… But suppose it turns out that bees nevertheless lack globally broadcast perceptual states. This might be because different types of perceptual content are made available only to specific decision-making systems, for example. Perhaps no perceptual states are broadcast to most such systems simultaneously. In which case they lack phenomenal consciousness, according to an account that identifies the latter with globally broadcast nonconceptual content. But so what? This doesn’t mean that bees are all “dark on the inside” or anything of the sort. Nor does it mean that there is any point in phylogeny when some special type of experience (one that is intrinsically <em>like</em> something to undergo) appears on the scene. Indeed, the question of when, precisely, phenomenal consciousness emerged in phylogeny makes no sense, from this perspective.</p>
<p>All that can be said is that there are a variety of kinds of nonconceptual perceptual state across creatures, some of which are available to inform more systems and some of which are available to inform fewer. These states thus differ in their functional roles, and some of these roles are more similar than others to the states in ourselves that give rise to hard-problem thought-experiments, that is all. Nothing special, or magical, or especially significant happened in evolution when global-broadcasting architectures first emerged on the scene. It was just more of the same, but somewhat differently organized.</p></blockquote>
</li>
<li class="footnote" id="footnote153_c9d3srf"><a class="footnote-label" href="#footnoteref153_c9d3srf">153.</a> By “lower-order,” I have in mind accounts of consciousness that are less complex than typical higher-order accounts, but which are more complex than typical first-order accounts.
</li>
<li class="footnote" id="footnote154_5xzemj7"><a class="footnote-label" href="#footnoteref154_5xzemj7">154.</a> One interpretation of “consciousness is a feature of fundamental physics” (i.e. panpsychism) seems clearly false, as there are no “spare degrees of freedom” in which an additional “consciousness” property could be “hiding” in the <a href="https://en.wikipedia.org/wiki/Standard_Model">Standard Model</a> (<a href="http://www.basicbooks.com/full-details?isbn=9780465018956">Wilczek 2008</a>, p. 164, calls it the “Core Theory”), and the weight of evidence supporting the Standard Model is enormous. Physicist Sean Carroll argues this point in chapter 42 of <a href="http://www.penguinrandomhouse.com/books/316646/the-big-picture-by-sean-carroll/9780525954828/">Carroll (2016)</a>:<br /><blockquote><p>Unlike brains, which are complicated and hard to explain, elementary particles such as photons are extraordinarily simple, and therefore relatively easy to study and understand. Physicists talk about different kinds of particles having different “degrees of freedom” — essentially, the number of different kinds of such particles that there are. An electron, for example, has two degrees of freedom. It has both electric charge and spin, but the electric charge can take on only one value (–1), while the spin comes in two possibilities: clockwise or counterclockwise. One times two is two, for two total degrees of freedom. An up quark, by contrast, has six degrees of freedom; like an electron, it has a fixed charge and two possible ways of spinning, but it also has three possible “colors,” and one times two times three is six. Photons have an electric charge fixed at zero, but they do have two possible spin states, so they have two degrees of freedom just like electrons do.</p>
<p>We could interpret the [proposal that consciousness is a fundamental property of the universe] in the most direct way possible, as introducing new degrees of freedom for each elementary particle. In addition to spinning clockwise or counterclockwise, a photon could be in one of (let&#8217;s say) two [states of consciousness]. Call them “happy” and “sad,” although the labels are more poetic than authentic.</p>
<p>This overly literal version of panpsychism cannot possibly be true. One of the most basic things we know about the Core Theory is exactly how many degrees of freedom each particle has. Recall the Feynman diagrams from [a previous chapter], describing particles scattering off of one another by exchanging other particles. Each diagram corresponds to a number that we can compute, the total contribution of that particular process to the end result, such as two electrons scattering off of each other by exchanging photons. Those numbers have been experimentally tested to exquisite precision, and the Core Theory has passed with flying colors.</p>
<p>A crucial ingredient in calculating these processes is the number of degrees of freedom associated with each particle. If photons had some hidden degrees of freedom that we didn&#8217;t know about, they would alter all of the predictions we make for any scattering experiment that involves such photons, and all of our predictions would be contradicted by the data. That doesn&#8217;t happen. So we can state unambiguously that photons do not come in “happy” and “sad” varieties, or any other manner of mental properties that act like physical degrees of freedom.</p>
<p>Advocates of panpsychism would probably not go as far as to imagine that mental properties play roles similar to true physical degrees of freedom, so that the preceding argument wouldn&#8217;t dissuade them. Otherwise these new properties would just be ordinary physical properties.</p>
<p>That leaves us in a position very similar to the zombie discussion: we posit new mental properties, and then insist that they have no observable physical effects [e.g. in particle scattering experiments]. What would the world be like if we replaced “protoconscious photons” with “zombie photons” lacking such mental properties? As far as the behavior of physical matter is concerned, including what you say when you talk or write or communicate nonverbally with your romantic partner, the zombie-photon world would be exactly the same as the world where photons have mental properties.</p>
<p>A good Bayesian can therefore conclude that the zombie-photon world is the one we actually live in. We simply don&#8217;t gain anything by attributing the features of consciousness to individual particles. Doing so is not a useful way of talking about the world; it buys us no new insight or predictive power. All it does is add a layer of metaphysical complication onto a description that is already perfectly successful.</p>
<p>Consciousness seems to be an intrinsically collective phenomenon, a way of talking about the behavior of complex systems with the capacity for representing themselves and the world within their inner states. Just because it is here full-blown in our contemporary universe doesn&#8217;t mean that there was always some trace of it from the very start. Some things just come into being as the universe evolves and entropy and complexity grow: galaxies, planets, organisms, consciousness.</p></blockquote>
<p>Perhaps because of this, panpsychists typically locate consciousness within physics in other ways, for example by positing that in addition to the usual relational-causal properties of the Standard Model (that best explain the results of our experiments), there is also an “intrinsic character” to physical things, and this intrinsic character is the ground of conscious experience (in either a “basic” or “emergent” way). I confess that to me, this sort of move by various panpsychists/non-reductivists about consciousness seems just as unmotivated as substance dualism does. (For an overview of some views of this kind, see ch. 4 of <a href="http://www.polity.co.uk/book.asp?ref=9780745653440">Weisberg 2014</a>.)</p>
</li>
<li class="footnote" id="footnote155_fssic5d"><a class="footnote-label" href="#footnoteref155_fssic5d">155.</a> On the neuroscience of human vision, see <a href="https://global.oup.com/academic/product/basic-vision-9780199572021?cc=us&amp;lang=en&amp;">Snowden et al. (2012)</a>, <a href="https://global.oup.com/academic/product/vision-and-the-visual-system-9780199936533?cc=us&amp;lang=en&amp;">Schiller &amp; Tehovnik (2015)</a>, and <a href="http://www.springer.com/la/book/9789811002113">Zhao (2016)</a>. Progress on machine vision is so rapid that anything I cite will be out-of-date within weeks or months, but see e.g. the readings for the University of Toronto&#8217;s “Deep Learning in Computer Vision” <a href="http://www.cs.utoronto.ca/~fidler/teaching/2015/CSC2523.html">Winter 2016 course</a>. For a brief overview of mirror self-recognition in animals, see <a href="http://link.springer.com/article/10.1007/s10329-015-0488-9">Anderson &amp; Gallup Jr. (2015)</a>. For an example claim of mirror self-recognition in a robot, see <a href="http://www.panstanford.com/books/9789814364492.html">Takeno (2012)</a>.
</li>
<li class="footnote" id="footnote156_lum47k4"><a class="footnote-label" href="#footnoteref156_lum47k4">156.</a> I should note that there has been considerable debate about the aptness of the analogy between the study of life and the study of consciousness. I won&#8217;t take up that debate here, but see e.g. <a href="https://global.oup.com/academic/product/the-conscious-mind-9780195117899?cc=us&amp;lang=en&amp;">Chalmers (1996)</a>, ch. 3:<br /><blockquote><p>It is interesting to see how a typical high-level property — such as life, say — evades the arguments put forward in the case of consciousness. First, it is straightforwardly inconceivable that there could be a physical replica of a living creature that was not itself alive. Perhaps a problem might arise due to context-dependent properties (would a replica that forms randomly in a swamp be alive, or be human?), but fixing environmental facts eliminates even that possibility. Second, there is no “inverted life” possibility analogous to the inverted spectrum. Third, when one knows all the physical facts about an organism (and possibly about its environment), one has enough material to know all the biological facts. Fourth, there is no epistemic asymmetry with life; facts about life in others are as accessible, in principle, as facts about life in ourselves. Fifth, the concept of life is plausibly analyzable in functional terms: to be alive is roughly to possess certain capacities to adapt, reproduce, and metabolize. As a general point, most high-level phenomena come down to matters of physical structure and function, and we have good reason to believe that structural and functional properties are logically supervenient on the physical.</p>
<p>…All this notwithstanding, a common reaction to the sort of argument I have given is to reply that a vitalist about life might have said the same things. For example, a vitalist might have claimed that it is logically possible that a physical replica of me might not be alive, in order to establish that life cannot be reductively explained. And a vitalist might have argued that life is a further fact, not explained by any account of the physical facts. But the vitalist would have been wrong. By analogy, might not the opponent of reductive explanation for consciousness also be wrong?</p>
<p>I think this reaction misplaces the source of vitalist objections. Vitalism was mostly driven by doubt about whether physical mechanisms could perform all the complex functions associated with life: adaptive behavior, reproduction, and the like. At the time, very little was known about the enormous sophistication of biochemical mechanisms, so this sort of doubt was quite natural. But implicit in these very doubts is the conceptual point that when it comes to explaining life, it is the performance of various functions that needs to be explained. Indeed, it is notable that as physical explanation of the relevant functions gradually appeared, vitalist doubts mostly melted away. With consciousness, by contrast, the problem persists even when the various functions are explained.</p>
<p>Presented with a full physical account showing how physical processes perform the relevant functions, a reasonable vitalist would concede that life has been explained. There is not even conceptual room for the performance of these functions without life. Perhaps some ultrastrong vitalist would deny even this, claiming that something is left out by a functional account of life—the vital spirit, perhaps. But the obvious rejoinder is that unlike experience, the vital spirit is not something we have independent reason to believe in. Insofar as there was ever any reason to believe in it, it was as an explanatory construct — “We must have such a thing in order to be able to do such amazing stuff.” But as an explanatory construct, the vital spirit can be eliminated when we find a better explanation of how the functions are performed. Conscious experience, by contrast, forces itself on one as an explanandum and cannot be eliminated so easily.</p>
<p>One reason a vitalist might think something is left out of a functional explanation of life is precisely that nothing in a physical account explains why there is something it is like to be alive. Perhaps some element of belief in a “vital spirit” was tied to the phenomena of one&#8217;s inner life. Many have perceived a link between the concepts of life and experience, and even today it seems reasonable to say that one of the things that needs to be explained about life is the fact that many living creatures are conscious. But the existence of this sort of vitalist doubt is of no comfort to the proponent of reductive explanation of consciousness, as it is a doubt that has never been overturned.</p></blockquote>
</li>
<li class="footnote" id="footnote157_0subqz5"><a class="footnote-label" href="#footnoteref157_0subqz5">157.</a> <a href="https://www.rep.routledge.com/articles/vitalism/v-1">Bechtel &amp; Richardson (1998)</a>:<br /><blockquote><p>The role of vitalism in physiology is exemplified in the work of the French anatomist Xavier Bichat (1771-1802). Bichat analysed living systems into parts, identifying twenty-one distinct kinds of tissue, and explaining the behaviour of organisms in terms of the properties of these tissues. He characterized the different tissues in terms of their ‘vital properties’, as forms of ‘sensibility’ and ‘contractility’. Bichat thought the sensibility and contractility of each tissue type constituted the limit to decomposing living matter into its parts. These vital properties preclude identifying life with any physical or chemical phenomenon because the behaviour of living tissues is irregular and contrary to forces exhibited by their inorganic constituents. Insofar as living matter maintains itself in the face of ordinary physical and chemical processes that would destroy it, Bichat thought it could not be explained in terms of those forces. He therefore allowed that there are additional fundamental forces in nature that are on a par with those Newton ascribed to all matter: ‘To create the universe God endowed matter with gravity, elasticity, affinity… and furthermore one portion received as its share sensibility and contractility’ (Bichat 1801, vol. 1: xxxvii). These are vital properties of living tissues.</p></blockquote>
</li>
<li class="footnote" id="footnote158_aga64z0"><a class="footnote-label" href="#footnoteref158_aga64z0">158.</a> For example here is <a href="https://www.rep.routledge.com/articles/vitalism/v-1">Bechtel &amp; Richardson (1998)</a> again:<br /><blockquote><p>…chemists in the early nineteenth century hoped to explain many of the reactions found in living organisms. Organic compounds are apparently formed only in living organisms, and thus appear to be products of vital activity. The physiological chemists of the early nineteenth century set out to show, contrary to initial appearances, that these products are the results of chemical processes. Jacob Berzelius (1779-1848) argued that chemistry could account for all of the reactions occurring within living organisms, and that organic and inorganic processes differ only in complexity. ‘There is,’ he said, ‘no special force exclusively the property of living matter which may be called a vital force’ (1836).</p></blockquote>
</li>
<li class="footnote" id="footnote159_f06lrnt"><a class="footnote-label" href="#footnoteref159_f06lrnt">159.</a> See e.g. <a href="https://books.google.com/books?id=NxfoCwAAQBAJ&amp;lpg=PA23&amp;ots=DP0cci93Xc&amp;lr&amp;pg=PA23#v=onepage&amp;q&amp;f=false">Wimsatt (1976)</a>.
</li>
<li class="footnote" id="footnote160_tiy75a1"><a class="footnote-label" href="#footnoteref160_tiy75a1">160.</a> E.g. they seem to be satisfiable by a very small neural network (<a href="http://www.sciencedirect.com/science/article/pii/S0893608007001530">Herzog et al. 2007</a>), or more generally a very short computer program.
</li>
<li class="footnote" id="footnote161_1ls15d3"><a class="footnote-label" href="#footnoteref161_1ls15d3">161.</a> One way this might happen is that we might conclude that consciousness is complex, but that “consciousness” properly refers to a highly disjunctive collection of a great many <em>different</em> complex processes.
</li>
<li class="footnote" id="footnote162_l3wm9wi"><a class="footnote-label" href="#footnoteref162_l3wm9wi">162.</a> For example <a href="https://global.oup.com/academic/product/cognition-evolution-and-behavior-9780195319842?cc=us&amp;lang=en&amp;">Shettleworth (2009)</a>, <a href="https://global.oup.com/academic/product/the-oxford-handbook-of-comparative-evolutionary-psychology-9780199738182?cc=us&amp;lang=en&amp;">Vonk &amp; Shackelford (2012)</a>, <a href="https://www.routledge.com/Animal-Learning-and-Cognition-3rd-Edition-An-Introduction-3rd-Edition/Pearce/p/book/9781841696560">Pearce (2008)</a>, <a href="https://he.palgrave.com/page/detail/animal-cognition-clive-d-l.wynne/?sf1=barcode&amp;st1=9781137367297">Wynne &amp; Udell (2013)</a>, <a href="http://www.cambridge.org/bo/academic/subjects/life-sciences/animal-behaviour/comparative-cognition?format=HB">Olmstead &amp; Kuhlmeier (2015)</a>, <a href="http://www.abc-clio.com/ABC-CLIOCorporate/product.aspx?pc=A4726C">Cheng (2016)</a>, or <a href="https://mitpress.mit.edu/books/animal-thinking">Menzel &amp; Fischer (2011)</a>.<br /><br />
See also task-specific and taxa-specific books on animal cognition, such as:
<ul><li><a href="https://jhupbooks.press.jhu.edu/content/animal-tool-behavior">Shumaker et al. (2011)</a> and <a href="http://www.cambridge.org/us/academic/subjects/life-sciences/biological-anthropology-and-primatology/tool-use-animals-cognition-and-ecology?format=HB">Sanz et al. (2013)</a> on tool use</li>
<li><a href="http://press.uchicago.edu/ucp/books/book/chicago/C/bo12789830.html">Whitehead &amp; Rendell (2014)</a> on culture among dolphins and whales</li>
<li><a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-144433221X.html">Brown et al. (2011)</a> and <a href="http://us.macmillan.com/whatafishknows/jonathanbalcombe">Balcombe (2016)</a> on fish cognition and behavior</li>
<li><a href="http://press.princeton.edu/titles/10808.html">Emery (2016)</a> and <a href="http://www.penguinrandomhouse.com/books/312321/the-genius-of-birds-by-jennifer-ackerman/9781594205217/">Ackerman (2016)</a> on birds</li>
</ul><p>(Some of these are popular books rather than academic books.)</p>
</li>
<li class="footnote" id="footnote163_3li9kek"><a class="footnote-label" href="#footnoteref163_3li9kek">163.</a> From James Rachels’ chapter “The Basic Argument for Vegetarianism” in <a href="https://books.google.com/books?id=TLbgAAAAMAAJ">Sapontzis (2004)</a>, p. 78.
</li>
<li class="footnote" id="footnote164_4r6m2sj"><a class="footnote-label" href="#footnoteref164_4r6m2sj">164.</a> See also this Quora thread: <a href="https://www.quora.com/Animals/What-is-the-most-intelligent-thing-a-non-human-animal-has-done">What is the most intelligent thing a non-human animal has done?</a>
</li>
<li class="footnote" id="footnote165_ut0l6rb"><a class="footnote-label" href="#footnoteref165_ut0l6rb">165.</a> See <a href="https://jhupbooks.press.jhu.edu/content/animal-tool-behavior">Shumaker et al. (2011)</a>, ch. 2, for an overview. Below are some examples. Note that the type of animal tool behavior is capitalized, e.g. Baiting and Inserting.<br /><blockquote><p>McMahan… described Baiting of prey by the neotropical assassin bug (<em>Salyavata variegata</em>), which uses previously captured termite carcasses to capture additional termites. By holding and shaking the carcass over the nest&#8217;s entrance hole, the assassin bug lures a termite into attempting to retrieve the carcass for its own consumption. Once a termite grasps the lure, the assassin bug slowly pulls the termite out of the nest entrance toward itself. Once the termite is within reach, the assassin bug quickly kills and eats its new victim. McMahan… reported that the assassin bug, if not disturbed, will repeat this process an average of seven or eight times. The author once saw an assassin bug capture thirty-one termites in this manner over the course of three hours.</p>
<p>		…</p>
<p>		Some female digger wasps Insert small twigs into nest burrows they have closed with soil and Probe with them… This behavior may settle and pack the soil and provide the female with sensory information regarding the adequacy of the closure.</p>
<p>		…</p>
<p>		Boxer crabs, also known as “pom-pom crabs”… Detach small anemones from the substrate and Brandish or Wave one in each cheliped [claw]… The crab moves with its chelipeds extended and waving. If the crab is mechanically disturbed, the chelipeds with the anemones are directed at the source of irritation. This behavior would presumably facilitate the discharge of stinging nematocysts by the anemones toward the threat. However, the crab&#8217;s use of anemones is not limited to protection or defense. If food is placed near the oral disc of the anemone, the crab immediately seizes the food with its anterior ambulatory appendages. Thus any food ensnared by the anemone in its own tentacles is apt to be appropriated by the crab. The crab also removes debris adhering to the body of the anemone and ingests the edible bits.</p>
<p>		…</p>
<p>		Finn, Tregenza, and Norman (2009) reported defensive tool use by the veined octopus (<em>Amphioctopus marginatus</em>). The octopuses frequently carried coconut shell halves and, when threatened, assembled them into a shelter by aligning the two halves of the coconut and hiding inside. These authors argued that the behavior is significant, as the octopuses carry the shells for future use as a shelter, despite the immediate energetic and locomotor costs. During travel, the octopus carries the shells under its body, in a form of locomotion termed “stilt walking,” which the researchers described as “ungainly and clearly less efficient than unencumbered locomotion”…</p></blockquote>
</li>
<li class="footnote" id="footnote166_1w5cnpo"><a class="footnote-label" href="#footnoteref166_1w5cnpo">166.</a> <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0004217">Finn et al. (2009)</a>.
</li>
<li class="footnote" id="footnote167_mcyfo7e"><a class="footnote-label" href="#footnoteref167_mcyfo7e">167.</a> <a href="http://www.nature.com/nature/journal/v445/n7126/abs/nature05511.html">Grosenick et al. (2007)</a>.
</li>
<li class="footnote" id="footnote168_1aossj7"><a class="footnote-label" href="#footnoteref168_1aossj7">168.</a> See e.g. <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0092895">Jelbert et al. (2014)</a> and the resources on <a href="http://users.ox.ac.uk/~kgroup/tools/tool_manufacture.shtml">this page</a> by the Behavioural Ecology Research Group at the University of Oxford.
</li>
<li class="footnote" id="footnote169_k4g7nfh"><a class="footnote-label" href="#footnoteref169_k4g7nfh">169.</a> See <a href="#scrubjays">previous footnote</a>.
</li>
<li class="footnote" id="footnote170_iyj0jdb"><a class="footnote-label" href="#footnoteref170_iyj0jdb">170.</a> The study of mirror self-recognition provides some examples (<a href="http://link.springer.com/article/10.1007/s10329-015-0488-9">Anderson &amp; Gallup Jr. 2015</a>):<br /><blockquote><p>…the results of any study must be independently replicated by other scientists in order for the findings to be considered reliable. The demonstration of mirror self-recognition in chimpanzees, orangutans and humans has been replicated many times by different investigators all over the world… In contrast, the track record for claims of self-recognition in other species has not been encouraging. Single published reports of mirror self-recognition in one elephant that failed on a re-test (Plotnik et al. 2006), one dolphin (Reiss and Marino 2001), and two magpies (Prior et al. 2008) have yet to be replicated. Indeed, recent evidence with other corvids suggests that apparent instances of mirror self-recognition by magpies may be an artifact of tactile cues (Soler et al. 2014). And in the case of cottontop tamarins (Hauser et al. 1995) an attempt to replicate the original positive results completely failed (Hauser et al. 2001).</p></blockquote>
<p>On scientific replication in general, see also <a href="AppendixZ8">Appendix Z.8</a>.</p>
</li>
<li class="footnote" id="footnote171_ytqbxfg"><a class="footnote-label" href="#footnoteref171_ytqbxfg">171.</a> <a href="https://books.google.com/books/about/Behavior_of_the_Lower_Organisms.html?id=oI4YAAAAYAAJ">Jennings (1906)</a>, pp. 335-337:<br /><blockquote><p>All that we have said thus far in the present chapter is independent of the question whether there exist in the lower organisms such subjective accompaniments of behavior as we find in ourselves, and which we call consciousness. We have asked merely whether there exist in the lower organisms objective phenomena of a character similar to what we find in the behavior of man. To this question we have been compelled to give an affirmative answer. So far as objective evidence goes, there is no difference in kind, but a complete continuity between the behavior of lower and of higher organisms.</p>
<p>Has this any bearing on the question of the existence of consciousness in lower animals? It is clear that objective evidence cannot give a demonstration either of the existence or of the non-existence of consciousness, for consciousness is precisely that which cannot be perceived objectively. No statement concerning consciousness in animals is open to verification or refutation by observation and experiment. There are no processes in the behavior of organisms that are not as readily conceivable without supposing them to be accompanied by consciousness as with it.</p>
<p>But the question is sometimes proposed: Is the behavior of lower organisms of the character which we should “naturally” expect and appreciate if they did have conscious states, of undifferentiated character, and acted under similar conscious states in a parallel way to man? Or is their behavior of such a character that it does not suggest to the observer the existence of consciousness?</p>
<p>If one thinks these questions through for such an organism as Paramecium, with all its limitations of sensitiveness and movement, it appears to the writer that an affirmative answer must be given to the first of the above questions, and a negative one to the second. Suppose that this animal were conscious to such an extent as its limitations seem to permit. Suppose that it could feel a certain degree of pain when injured; that it received certain sensations from alkali, others from acids, others from solid bodies, etc., — would it not be natural for it to act as it does? That is, can we not, through our consciousness, appreciate its drawing away from things that hurt it, its trial of the environment when the conditions are bad, its attempting to move forward in various directions, till it finds one where the conditions are not bad, and the like? To the writer it seems that we can; that Paramecium in this behavior makes such an impression that one involuntarily recognizes it as a little subject acting in ways analogous to our own. Still stronger, perhaps, is this impression when observing an Amoeba obtaining food… The writer is thoroughly convinced, after long study of the behavior of this organism, that if Amoeba were a large animal, so as to come within the everyday experience of human beings, its behavior would at once call forth the attribution to it of states of pleasure and pain, of hunger, desire, and the like, on precisely the same basis as we attribute these things to the dog. This natural recognition is exactly what Munsterberg (1900) has emphasized as the test of a subject. In conducting objective investigations we train ourselves to suppress this impression, but thorough investigation tends to restore it stronger than at first.</p>
<p>Of a character somewhat similar to that last mentioned is another test that has been proposed as a basis for deciding as to the consciousness of animals. This is the satisfactoriness or usefulness of the concept of consciousness in the given case. We do not usually attribute consciousness to a stone, because this would not assist us in understanding or controlling the behavior of the stone. Practically indeed it would lead us much astray in dealing with such an object. On the other hand, we usually do attribute consciousness to the dog, because this is useful; it enables us practically to appreciate, foresee, and control its actions much more readily than we could otherwise do so. If Amoeba were so large as to come within our everyday ken, I believe it beyond question that we should find similar attribution to it of certain states of consciousness a practical assistance in foreseeing and controlling its behavior. Amoeba is a beast of prey, and gives the impression of being controlled by the same elemental impulses as higher beasts of prey. If it were as large as a whale, it is quite conceivable that occasions might arise when the attribution to it of the elemental states of consciousness might save the unsophisticated human being from the destruction that would result from the lack of such attribution. In such a case, then, the attribution of consciousness would be satisfactory and useful. In a small way this is still true for the investigator who wishes to appreciate and predict the behavior of Amoeba under his microscope.</p>
<p>But such impressions and suggestions of course do not demonstrate the existence of consciousness in lower organisms. Anv belief on this matter can be held without conflict with the objective facts. All that experiment and observation can do is to show us whether the behavior of lower organisms is objectively similar to the behavior that in man is accompanied by consciousness. If this question is answered in the affirmative, as the facts seem to require, and if we further hold, as is commonly held, that man and the lower organisms are subdivisions of the same substance, then it may perhaps be said that objective investigation is as favorable to the view of the general distribution of consciousness throughout animals as it could well be. But the problem as to the actual existence of consciousness outside of the self is an indeterminate one; no increase of objective knowledge can ever solve it. Opinions on this subject must then be largely dominated by general philosophical considerations, drawn from other fields.</p></blockquote>
</li>
<li class="footnote" id="footnote172_cbk3b1l"><a class="footnote-label" href="#footnoteref172_cbk3b1l">172.</a> From a paper presented at a conference on Darwin and the Human Sciences, at the London School of Economics (1993). Quoted in <a href="https://mitpress.mit.edu/books/mindblindness">Baron-Cohen (1995)</a>, p. 4.
</li>
<li class="footnote" id="footnote173_uai65o2"><a class="footnote-label" href="#footnoteref173_uai65o2">173.</a> On anthropomorphism in general, see <a href="http://penguinrandomhouse.ca/books/307567/7-laws-magical-thinking#9780452298903">Hutson (2012)</a>, ch. 6; <a href="https://global.oup.com/academic/product/faces-in-the-clouds-9780195098914?cc=us&amp;lang=en&amp;">Guthrie (1993)</a>; <a href="https://books.google.com/books?id=AmgYIBQ-XKkC&amp;lpg=PP1&amp;pg=PA68#v=onepage&amp;q&amp;f=false">Horowitz (2010)</a>; <a href="http://psycnet.apa.org/?fa=main.doiLanding&amp;doi=10.1037/0033-295X.114.4.864">Epley et al. (2007)</a>; <a href="http://knopfdoubleday.com/book/209659/mindwise/9780307595911/">Epley (2011)</a>; <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195395761.001.0001/oxfordhb-9780195395761-e-018?&amp;mediaType=Article">Waytz et al. (2012)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0003347215003085">Urquiza-Haas &amp; Kotrschal (2015)</a>.<br /><br /><a href="https://global.oup.com/academic/product/cognition-evolution-and-behavior-9780195319842?cc=us&amp;lang=en&amp;">Shettleworth (2009)</a>, ch. 1, provides a summary of the debates over anthropomorphism in the study of animal cognition and behavior:
<blockquote><p>Crows [in Davis, California] crack walnuts by dropping them from heights of 5–10 meters or more onto sidewalks, roads, and parking lots. Occasionally they drop walnuts in front of approaching cars, as if using the cars to crush the nuts for them. Do crows intentionally use cars as nutcrackers? Some of the citizens of Davis, as well as some professional biologists (Maple 1974, in Cristol et al. 1997) were convinced that they do, at least until a team of young biologists at UC Davis put this anecdote to the test (Cristol et al. 1997). They reasoned that if crows were using cars as tools, the birds would be more likely to drop nuts onto the road when cars were coming than when the road was empty. Furthermore, if a crow was standing in the road with an uncracked walnut as a car approached, it should leave the nut in the road to be crushed rather than carry it away.</p>
<p>Cristol and his collaborators watched crows feeding on walnuts and recorded how likely the birds were to leave an uncracked walnut in the road when cars were approaching and when the road was empty. They found no support for the notion that crows were using automobiles as nutcrackers… </p>
<p>…The people in Davis and elsewhere (Nihei 1995; Caffrey 2001) who saw nutcracking as an expression of clever crows’ ability to reason and plan were engaging in an anthropomorphism that is common even among professional students of animal behavior (…Kennedy 1992; Wynne 2007a, 2007b). As we will see, such thinking can be a fertile source of ideas, but research often reveals that simple processes apparently quite unlike explicit reasoning are doing surprisingly complex jobs…</p>
<p>…</p>
<p>…some of Darwin&#8217;s [early] supporters… set out to collect anecdotes appearing to prove animals could think and solve problems the way people do. Their approach was not just anthropocentric but frankly <em>anthropomorphic</em>, explaining animals’ apparently clever problem solving in terms of human-like thinking and reasoning. But as we have seen in the case of the nutcracking crows, just because an animal&#8217;s behavior looks to the casual observer like what a person would do in a similar-appearing situation does not mean it can be explained in the same way. Such reasoning based on analogy between humans and other animals must be tested with experiments that take into account alternative hypotheses (Heyes 2008).</p>
<p>Fortunately for progress in understanding animal cognition, critics of extreme anthropomorphism were not slow to appear. E.L. Thorndike&#8217;s (1911/1970) pioneering experiments on how animals solve simple physical problems showed that gradual learning by trial and error was more common than human-like insight and planning (Galef 1998). C. Lloyd Morgan also observed animals in a systematic way but is now best known for stating a principle commonly taken as forbidding unsupported anthropomorphism. What Morgan (1894) called his Canon states, “In no case may we interpret an action as the outcome of the exercise of a higher psychical faculty, if it can be interpreted as the outcome of the exercise of one which stands lower in the psychological scale.” Morgan&#8217;s Canon is clearly not without problems (Sober 2005). What is the “psychological scale”? Don&#8217;t “higher” and “lower” assume the phylogenetic scale? In contemporary practice “lower” usually means associative learning, that is, classical and instrumental conditioning or untrained species-specific responses. “Higher” is reasoning, planning, insight, in short any cognitive process other than associative learning.</p>
<p>For an example of how Morgan&#8217;s Canon might be applied today, suppose… that crows had been found to drop nuts in front of cars more than on the empty road. An obvious “simple” explanation is that they had been reinforced more often when dropping a nut when a car was coming than when the road was empty and thereby had learned to discriminate these two situations. A “higher,” anthropomorphic, explanation might be that having seen fallen nuts crushed by cars the insightful crows reasoned that they could drop the nuts themselves. The contrast between these explanations suggests a straightforward test: observe naive crows to see if the discrimination between approaching cars and empty roads develops gradually (supporting the “simple” explanation) or appears suddenly, without any previous trial and error (supporting the “higher” explanation). Unfortunately, competing explanations do not always make such readily discriminable predictions about observable behavior. Even when they do, experiments designed to pit them against each other may not yield clear results. Then agnosticism may be the most defensible policy (Sober 2005).</p>
<p>In practice, the field of comparative cognition as it has developed in the past 30–40 years has a very strong bias in favor of “simple” mechanisms (Sober 2001; Wasserman and Zentall 2006a). The burden of proof is generally on anyone wishing to explain behavior in terms of processes other than associative learning and/or species-typical perceptual and response biases. To many, anthropomorphism is a dirty word in scientific study of animal cognition (Mitchell 2005; Wynne 2007a, 2007b). But dismissing anthropomorphism altogether is not necessarily the best way forward. “Anthropodenial” (de Waal 1999) may also be a sin. After all, if other species share common ancestors with us, then we share an a priori unspecifiable number of biological processes with any species one cares to name. Thus in some ways, as Morgan apparently thought (Sober 2005), the simplest account of any behavior is arguably the anthropomorphic one, that behavior analogous to ours is the product of a similar cognitive process. Note, however, that “simple” has shifted here from the cognitive process to the explanation (Karin-D&#8217;Arcy 2005), from “simpler for them” to “simpler for us” (Heyes 1998).</p>
<p>Where do these considerations leave Morgan&#8217;s Canon? A reasonable modern interpretation of the Canon (Sober 2005) is that a bias in favor of simple associative explanations is justified because basic conditioning mechanisms are widespread in the animal kingdom, having been found in every animal, from worms and fruitflies to primates, in which they have been sought (Papini 2008). Thus they may be evolutionarily very old, present in species ancestral to all present-day animals and reflecting adaptations to universal causal regularities in the world and/or fundamental properties of neural circuits. As species diverged, other mechanisms may have become available on some branches of the evolutionary tree, and it might be said to be the job of comparative psychologists to understand their distribution (Papini 2002).</p>
<p>But for such a project to make sense, it must be clear what is meant by associative explanations and what their limits are. Associative learning… is basically the learning that results from experiencing contingencies, or predictive relationships, between events. At the theoretical level, such experience in Pavlovian (stimulus-stimulus) or instrumental (response-stimulus) conditioning has traditionally been thought of as strengthening excitatory or inhibitory connections between event representations. Thus one might say that any cognitive performance that does not result from experience of contingencies between events and/or cannot be explained in terms of excitatory and/or inhibitory connections is nonassociative.</p>
<p>Path integration… is one example: an animal moving in a winding path from home implicitly integrates distance and direction information into a vector leading straight home. As another, on one view of conditioning… the flow of events in time is encoded as such and computed on to compare rates of food presentation during a signal and in its absence. Other nonassociative cognitive processes which might be (but rarely if ever have been) demonstrated in nonhumans include imitation, that is, storing a representation of an actor&#8217;s behavior and later reproducing the behavior; insight; and any kind of reasoning or higher-order representations or computations on event representations. As we will see throughout the book, discriminating nonassociative “higher” processes from associative ones is seldom straightforward, in part because the learning resulting from associative procedures may have subtle and interesting cognitive content. In any case, the goal of comparative research should be understanding the cognitive mechanisms underlying animal behavior in their full variety and complexity rather than partitioning them into rational or nonassociative vs. associative (Papineau and Heyes 2006).</p>
<p>In conclusion, neither blanket anthropomorphism nor complete anthropodenial is the answer (Mitchell 2005). Evolutionary continuity justifies anthropomorphism as a source of hypotheses. When it comes to comparing human cognition with that of other species, it is most likely that — just as with our genes and other physical characters — we will find some processes shared with many other species, some with only a few, and some that are uniquely human. One of the most exciting aspects of contemporary research on comparative cognition is the increasing detail and subtlety in our picture of how other species’ minds are both like and not like ours.</p></blockquote>
</li>
<li class="footnote" id="footnote174_hu0qiip"><a class="footnote-label" href="#footnoteref174_hu0qiip">174.</a> <a href="http://www.tandfonline.com/doi/abs/10.1080/00048409912349231">Carruthers (1999)</a> offers the following account, which I would guess is true of many incidents of attributing conscoiusness to animals:<br /><blockquote><p>[My view that most animals lack phenomenal consciousness] is highly controversial, of course… It also conflicts with a powerful common-sense intuition to the contrary. But I suggest that this intuition may well be illusory, and can easily be explained away. For notice that one important strategy we often adopt when attributing mental states to a subject, is to try imagining the world Jkom the subject&#8217;s point of view, to see how things then seem. But when we do that, what we inevitably get are imaginings of conscious perceptions and thoughts, and of experiences with phenomenal feels to them. So, of course we naturally assume that the experiences of a cat will be like something, once we have got to the point of accepting (correctly, in my view) that the cat does have experiences. But this may merely reflect the fact that imaginings of perceptual states are always imaginings of conscious perceptual states, that is all. It may go no deeper than the fact that we have no idea how to imagine a non-conscious perception.</p></blockquote>
</li>
<li class="footnote" id="footnote175_smrg89h"><a class="footnote-label" href="#footnoteref175_smrg89h">175.</a> The major possible exception, at least for reasonable-scale systems, is human-style verbal self-report of conscious experience.
</li>
<li class="footnote" id="footnote176_udoxf1p"><a class="footnote-label" href="#footnoteref176_udoxf1p">176.</a> Other “sophisticated” PCIFs from <a href="#PCIFsTable">my table</a> of PCIFs might include, for example, intentional deception, teaching others, some forms of tool behavior, spontaneously planning for future days without reference to current motivational state, taking into account another&#8217;s spatial perspective, play behaviors, and grief behaviors. Unfortunately, my ratings for “apparent cognitive-behavioral sophistication” below draw from much more information than I took the time to record in my table of PCIFs and taxa.
</li>
<li class="footnote" id="footnote177_hp33qax"><a class="footnote-label" href="#footnoteref177_hp33qax">177.</a> For arguments about why the absolute number of pallial neurons might be especially indicative of “higher” cognitive functions (and thus perhaps consciousness), see Herculano-Houzel (<a href="https://mitpress.mit.edu/books/human-advantage">2016</a>, <a href="http://www.sciencedirect.com/science/article/pii/S2352154616302637">2017</a>).
</li>
<li class="footnote" id="footnote178_koor0n9"><a class="footnote-label" href="#footnoteref178_koor0n9">178.</a> Technically, this four-factor approach isn&#8217;t entirely theory-agnostic, but it is <em>relatively</em> theory agnostic.
</li>
<li class="footnote" id="footnote179_ixo2079"><a class="footnote-label" href="#footnoteref179_ixo2079">179.</a> Here and in many other locations in this report, I should really say “brains or ganglia or perhaps entire nervous systems,” but instead I just say “brains” for brevity. For more on this, see <a href="https://global.oup.com/academic/product/the-peripheral-mind-9780199989607?cc=us&amp;lang=en&amp;">Aronyosi (2013)</a>.
</li>
<li class="footnote" id="footnote180_86gr0ue"><a class="footnote-label" href="#footnoteref180_86gr0ue">180.</a> See <a href="http://www.pnas.org/content/113/26/7255.abstract">Olkowicz et al. (2016)</a>.
</li>
<li class="footnote" id="footnote181_eltm3le"><a class="footnote-label" href="#footnoteref181_eltm3le">181.</a> As far as I know, the number of neurons in a chimpanzee brain has not been counted, but <a href="https://mitpress.mit.edu/books/human-advantage">Herculano-Houzel (2016)</a>, figure 4.15, estimates the number at 6 billion (using the neuronal scaling rule discovered to hold for other primates).
</li>
<li class="footnote" id="footnote182_im3s2gb"><a class="footnote-label" href="#footnoteref182_im3s2gb">182.</a> As far as I know, the number of neurons in a cow brain has not been counted, but <a href="https://mitpress.mit.edu/books/human-advantage">Herculano-Houzel (2016)</a>, p. 75 says that, given the neuronal scaling rules discovered to hold for primates vs. other mammals, “the chimpanzee can be expected to have at least twice as many neurons as a cow,” and in figure 4.15, she estimates the number of neurons in a chimpanzee brain as 6 billion.
</li>
<li class="footnote" id="footnote183_2uaaho2"><a class="footnote-label" href="#footnoteref183_2uaaho2">183.</a> This very rough guess was derived by comparing the average brain mass of chickens and rainbow trout, and by (unrealistically) assuming that chicken brains and rainbow trout briains follow the same neuronal scaling rule.
</li>
<li class="footnote" id="footnote184_glzdg6u"><a class="footnote-label" href="#footnoteref184_glzdg6u">184.</a> I couldn&#8217;t locate the number of neurons for a Gazami crab, so I simply guessed it might be similar to the number of neurons in a lobster. Wikipedia&#8217;s <a href="https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons">list of animals by number of neurons</a> says a lobster has 100,000 neurons. After I found this number, I saw that <a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye (2016)</a>, p. 156, also claims that “Crabs have… around a hundred thousand [neurons]”, though Tye doesn&#8217;t provide a source for this claim.
</li>
<li class="footnote" id="footnote185_gq1l4ky"><a class="footnote-label" href="#footnoteref185_gq1l4ky">185.</a> At the time of the 2016 match with Lee Sedol.
</li>
<li class="footnote" id="footnote186_uw349um"><a class="footnote-label" href="#footnoteref186_uw349um">186.</a> There is a vast literature on the limits of introspection. See e.g. <a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674013827">Wilson (2004)</a>; <a href="https://global.oup.com/academic/product/the-opacity-of-mind-9780199596195?cc=us&amp;lang=en&amp;">Carruthers (2011)</a>.
</li>
<li class="footnote" id="footnote187_12nrmor"><a class="footnote-label" href="#footnoteref187_12nrmor">187.</a> Page 44.
</li>
<li class="footnote" id="footnote188_t4j2e06"><a class="footnote-label" href="#footnoteref188_t4j2e06">188.</a> In Bayesian statistics, an “ignorance prior” is a probability distribution for which equal probability is assigned to all possibilities — i.e., when one is still “ignorant” of all (or nearly all) the relevant evidence, before the probability distribution is updated by the observation of some Bayesian evidence. See e.g. <a href="http://www.cambridge.org/us/academic/subjects/physics/theoretical-physics-and-mathematical-physics/probability-theory-logic-science?format=HB">Jaynes (2003)</a>, ch. 12.
</li>
<li class="footnote" id="footnote189_l5prlg4"><a class="footnote-label" href="#footnoteref189_l5prlg4">189.</a> Panpsychists, of course, will think that all my probabilities are too low. This difference of opinion likely traces back to the “metaphysical” debates about consciousness that I mostly skipped over in this report (but, see <a href="#Nature">here</a>).
</li>
<li class="footnote" id="footnote190_eqrzdks"><a class="footnote-label" href="#footnoteref190_eqrzdks">190.</a> I also have reservations about the lack of precision and comprehensiveness with which GNWT, as currently formulated, explains the explananda of consciousness (see <a href="#AppendixB">Appendix B</a>).
</li>
<li class="footnote" id="footnote191_c674xjk"><a class="footnote-label" href="#footnoteref191_c674xjk">191.</a> We&#8217;ve wrestled with related issues before, but do not feel satisfied. See <a href="http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/">Why we can’t take expected value estimates literally (even when they’re unbiased)</a>, <a href="http://www.givewell.org/modeling-extreme-model-uncertainty">Modeling Extreme Model Uncertainty</a>, <a href="http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/">Sequence thinking vs. cluster thinking</a>, and section 2 of <a href="http://www.openphilanthropy.org/blog/technical-and-philosophical-questions-might-affect-our-grantmaking">Technical and Philosophical Questions That Might Affect Our Grantmaking</a>. Various formal models have been proposed for acting under various kinds of “radical” uncertainty — e.g. Jaynes (<a href="http://www.amazon.com/gp/product/0521592712?pldnSite=1">2003</a>, ch. 18) and <a href="http://www.springer.com/gp/book/9783319423357">Jøsang (2016)</a> — but I haven’t studied them closely enough to endorse any of them in particular. See also the papers cited in <a href="http://link.springer.com/article/10.1007/s10670-014-9687-9">Romeijn &amp; Roy (2014)</a>.
</li>
<li class="footnote" id="footnote192_0z4srcj"><a class="footnote-label" href="#footnoteref192_0z4srcj">192.</a> See e.g. the literature on the “sophistication effect,” as described in Yudkowksy&#8217;s “<a href="http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/">Knowing About Biases Can Hurt People</a>” and papers such as <a href="https://www.princeton.edu/csdp/events/AchenBartels011107/AchenBartels011107.pdf">Achen &amp; Bartels (2006)</a>, and also the literature on differences between intelligence and rationality (e.g. <a href="https://mitpress.mit.edu/books/rationality-quotient">Stanovich 2016</a>).
</li>
<li class="footnote" id="footnote193_xbe07ur"><a class="footnote-label" href="#footnoteref193_xbe07ur">193.</a> This is sometimes called “the crowd within” effect. See <a href="http://pss.sagepub.com/content/19/7/645.short">Vul &amp; Pashler (2008)</a>; <a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00786/full">Steegen et al. (2014)</a>.
</li>
<li class="footnote" id="footnote194_i3lni4c"><a class="footnote-label" href="#footnoteref194_i3lni4c">194.</a> I elicited my probabilities via a Google Form (<a href="/sites/default/files/screenshot%20of%20animal%20consciousness%20elicitation.png">screenshot</a>; <a href="/sites/default/files/Luke%27s%20animal%20consciousness%20self-elicitations%20in%202016.xlsx">spreadsheet</a>). The form walked me through these steps:
<ol><li>First, it led me through two anti-anchoring exercises meant to minimize the effect of my earlier estimates on my current estimate (see below).</li>
<li>Second, the form asked me to give my probability of consciousness (of a sort that I would morally care about, given my current moral judgments) for each of the following animal taxa: chimpanzees, cows, chickens, rainbow trout, gazami crabs, and common fruit flies.</li>
</ol><p>My probabilities concerning the distribution question didn&#8217;t change very much, as the graph of my 6 self-elicitations shows:</p>
<p><img style="display: block; margin-left: auto; margin-right: auto;" src="/files/Research/Moral_Patienthood/Lukes_animal_consciousness_self-elicitations_in_2016_chart.png"   alt="Luke's animal consciousness self-elicitations in 2016 (chart).png" align="middle" /></p>
<p>The anti-anchoring exercise the form prompted me to engage in worked as follows:</p>
<ul><li>First, it prompted me to invent a written justification for a randomly-selected probability of consciousness in a randomly-chosen animal taxon. (Randomization was done using <a href="https://www.random.org/">random.org</a>.)</li>
<li>Second, the form prompted me to “take a deep breath,” and then name a musical artist I hadn&#8217;t heard for a while but would like to hear again soon.</li>
</ul></li>
<li class="footnote" id="footnote195_mdwo0rh"><a class="footnote-label" href="#footnoteref195_mdwo0rh">195.</a> Here&#8217;s an intuition pump: Did philosophical argumentation in the 17th and 18th centuries contribute much to the improvement of our understanding of “life,” or was such progress made almost exclusively by scientific means? See also <a href="http://www.sciencedirect.com/science/article/pii/S1053810083710020">Baars &amp; McGovern (1993)</a>.<br /><br />
	This said, it&#8217;s not the case that I see <em>no</em> role for philosophical argument. Rather, I think that <em>on the present margin</em>, “scientific” work is needed more urgently than “philosophical” work (though, the line between the two is fuzzy). I could see my intuition about this changing on a different margin. For example, it may be that substantial philosophical work will be invaluable once we have collected more scientific data than we have now.
</li>
<li class="footnote" id="footnote196_w3swqmf"><a class="footnote-label" href="#footnoteref196_w3swqmf">196.</a> For example, it could be informative for philosophers of consciousness to collaborate with neurologists in interviewing patients suffering from auto-activation deficit (see <a href="#FuzzinessAAD">here</a>) about the details of their conscious experiences at different stages in the progression of their symptoms, in a way that might test different hypotheses about consciousness. It could also be informative for philosophers to collaborate with scientists in the design of experiments aimed at empirically distinguishing the hypotheses put forward in <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/consciousness-accessibility-and-the-mesh-between-psychology-and-neuroscience/63BF0B7E5DB1654ED680E160D413F3D0">Block (2007b)</a> (and commentaries) to explain the results of the experiments described therein.
</li>
<li class="footnote" id="footnote197_0poxrtw"><a class="footnote-label" href="#footnoteref197_0poxrtw">197.</a> After writing this paragraph, I discovered that Jesse Prinz expressed a similar view in a 2014 interview with the Moscow Center for Consciousness Studies, starting around 54:27 in the <a href="https://www.youtube.com/watch?v=SpxgEQA7pu0&amp;t=54m27s">published recording</a>:<br /><blockquote><p>[In <em>The Conscious Brain</em>] I dedicate very little time to the debate between different metaphysical positions [on consciousness]. So rather than arguing for materialism, I presuppose it. Where most of what&#8217;s been written about consciousness by philosophers in recent decades has focused on the metaphysical debate, I spend just one chapter on it, and that&#8217;s not a very thorough discussion of the literature, and in that sense I ignored it, and intentionally. I felt that while there are very profound difficult problems… there are all these other problems that are extremely exciting and important, and we&#8217;ve dedicated so much time to these metaphysical problems that we&#8217;ve neglected the others… </p></blockquote>
</li>
<li class="footnote" id="footnote198_nfdx9oa"><a class="footnote-label" href="#footnoteref198_nfdx9oa">198.</a> In other words, consciousness has, for me, “clicked into place” in the way described by <a href="http://books.wwnorton.com/books/detail.aspx?ID=4294992671">Dennett (2017)</a>, ch. 1:<br /><blockquote><p>One cold, starry night over thirty years ago, I stood with some of my Tufts students looking up at the sky while my friend, the philosopher of science, Paul Churchland instructed us how to <em>see the plane of the ecliptic</em>, that is, to look at the other visible planets in the sky and picture them, and ourselves, as wheeling around the sun all on the same invisible plane. It helps to tip your head just so and remind yourself of where the sun must be, way back behind you. Suddenly, the orientation clicks into place and shazam, you <em>see</em> it! Of course we all knew for years that this was the situation of our planet in the solar system, but until Paul made us see it, it was a rather inert piece of knowledge.</p></blockquote>
</li>
<li class="footnote" id="footnote199_gidslz9"><a class="footnote-label" href="#footnoteref199_gidslz9">199.</a> This list does not include projects outside the scope of “What is the likely distribution of morally-relevant phenomenal consciousness?”, for example projects related to <em>other</em> criteria for moral patienthood, or projects related specifically to moral weight.
</li>
<li class="footnote" id="footnote200_lhf80zs"><a class="footnote-label" href="#footnoteref200_lhf80zs">200.</a> Yes, contra Dennett, I take my “first-person data” to be part of what needs to be explained. My reasons for favoring this view over Dennett&#8217;s “heterophenomenology” are basically the same as Chalmers’ in <a href="https://global.oup.com/academic/product/the-character-of-consciousness-9780195311105?cc=us&amp;lang=en&amp;">Chalmers (2010)</a>, pp. 52-58.
</li>
<li class="footnote" id="footnote201_qbypqdc"><a class="footnote-label" href="#footnoteref201_qbypqdc">201.</a> Variations on Searle&#8217;s Chinese Room, Block&#8217;s Chinese Nation, etc. as well as more recent hypothetical minds such as the computational “Mary” from <a href="https://dl.dropboxusercontent.com/u/8809125/DissolveTypeB.pdf">Yetter-Chappell (2015)</a>.
</li>
<li class="footnote" id="footnote202_9yzm540"><a class="footnote-label" href="#footnoteref202_9yzm540">202.</a> I&#8217;m not satisfied with any of the cases for insect consciousness or chimpanzee non-consciousness that I&#8217;ve seen so far, so I can&#8217;t just link to sources that I think make a good case for either, but I think I could piece together a good case for either from a variety of arguments and evidence I&#8217;ve come across via disparate sources.
</li>
<li class="footnote" id="footnote203_3i00qds"><a class="footnote-label" href="#footnoteref203_3i00qds">203.</a> <a name="self-report" id="self-report"></a>In some fields, self-report is considered so unreliable that it is avoided — but in consciousness studies, it&#8217;s the best we&#8217;ve got!<br /><br />
	I listed some sources relevant to the reliability of self-report measures in my earlier report on <a href="http://www.openphilanthropy.org/behavioral-treatments-insomnia">behavioral treatments for insomnia</a>:
<blockquote><p>Sources that provide theoretical considerations and non-systematic evidence in favor of substantial <em>a priori</em> concern about the accuracy of self-report measures include <a href="https://www.routledge.com/products/9780585247878">Stone et al. (1999)</a>; <a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470465468.html">Groves et al. (2009)</a>, especially section 7.3; <a href="http://link.springer.com/chapter/10.1007/978-1-4614-3876-2_6">Stalans (2012)</a>; <a href="https://books.google.com/books?id=l9mVYNUvbpgC&amp;lpg=PP1&amp;pg=PT51#v=onepage&amp;q&amp;f=false">Schwarz et al. (2008)</a>. Broad (and in some cases, systematic) empirical reviews (or unusually large-scale primary studies) comparing self-report measures to “gold standard” objective measures include <a href="http://www.journalslibrary.nihr.ac.uk/hta/volume-18/issue-51#abstract">Bryant et al. (2014)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1467-789X.2007.00347.x/full">Gorber et al. (2007)</a>; <a href="http://ijbnpa.biomedcentral.com/articles/10.1186/1479-5868-5-56">Prince et al. (2008)</a>; <a href="http://mcr.sagepub.com/content/63/2/217.short">Bhandari &amp; Wagner (2006)</a>; <a href="http://ntr.oxfordjournals.org/content/11/1/12.short">Gorber et al. (2009)</a>; <a href="http://www.tandfonline.com/doi/abs/10.1080/17477160802315010#.VpF9XvFHCnU">Adamo et al. (2009)</a>; <a href="http://link.springer.com/article/10.1186/1479-5868-9-148">Kowalski et al. (2012)</a>; <a href="http://rer.sagepub.com/content/75/1/63.short">Kuncel et al. (2005)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S1573441201050127">Bound et al. (2001)</a>; <a href="http://www.nber.org/papers/w15181">Meyer et al. (2009)</a>; <a href="http://erx.sagepub.com/content/39/2/179">Barnow &amp; Greenberg (2014)</a>. Finally, one cherry-picked primary study I found disheartening with regard to the accuracy of self-report was <a href="http://www.sciencedirect.com/science/article/pii/S0165176513003601">Suziedelyte &amp; Johar (2013)</a>. Please keep in mind that this is only a preliminary list of sources: I have not evaluated any of them closely, they may be unrepresentative of the literature on self-report as a whole, and I can imagine having a different impression of the typical accuracy of self-report measures if and when I complete [a separate investigation] on the accuracy of self-report measures… For those interested in the topic, I list some additional general sources I found useful, again without comment or argument at this time: <a href="https://global.oup.com/academic/product/the-science-of-real-time-data-capture-9780195178715">Stone et al. (2007)</a>; <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195342819.001.0001/oxfordhb-9780195342819-e-003">Smith (2011)</a>; <a href="https://books.google.com/books?hl=en&amp;lr=lang_en&amp;id=0B7iBwAAQBAJ&amp;oi=fnd&amp;pg=PT154&amp;ots=gzWF-0J7Qy&amp;sig=fq3NN_MDBm95cLvkNabvHfrkj_g#v=onepage&amp;q&amp;f=false">Fernandez-Ballesteros &amp; Botella (2007)</a>; <a href="http://link.springer.com/article/10.1023/A:1019637632584">Donaldson &amp; Grant-Vallone (2002)</a>; <a href="https://books.google.com/books?id=1kEv5qL22pQC&amp;lpg=PA387&amp;ots=vHmaUt0MHj&amp;dq=%22The%20measurement%20and%20interpretation%20of%20health%20in%20social%20surveys%22&amp;lr&amp;pg=PA387#v=onepage&amp;q&amp;f=false">Thomas &amp; Frankenberg (2002)</a>; <a href="http://psycnet.apa.org/psycinfo/2008-09949-013">Chan (2009)</a>; <a href="http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199231881.001.0001/acprof-9780199231881-chapter-6">Streiner (2008)</a>; <a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP003563.html">Fayers &amp;  Machin (2016)</a>, ch. 19.</p></blockquote>
</li>
<li class="footnote" id="footnote204_43qeaxc"><a class="footnote-label" href="#footnoteref204_43qeaxc">204.</a> See e.g. <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1747-9991.2009.00275.x/abstract">Klein (2010)</a>.
</li>
<li class="footnote" id="footnote205_0lwpnk7"><a class="footnote-label" href="#footnoteref205_0lwpnk7">205.</a> The quoted phrase is taken from <a href="https://books.google.com/books?id=0wx17lC075EC&amp;lpg=PP1&amp;pg=PA1277#v=onepage&amp;q&amp;f=false">Güzeldere et al. (2000)</a>:<br /><blockquote><p>Many [researchers] have labored to develop various constructive explanatory accounts of consciousness. This group can be characterized by a common ontological denominator, say, a commitment to a materialist/naturalist framework; but here, too, we find differences of opinion. Consciousness is explained in terms of causal/functional roles (Lewis, 1966; Lycan, 1987), representational properties (Dretske, 1995; Tye, 1995), emergent biological properties (Flanagan, 1992; Searle, 1992), higher-order mental states (Armstrong, 1980; Rosenthal, 1997), or computer-related metaphors (Dennett, 1991), or a combination of these.</p>
<p>	Within this latter group, we find a certain group of philosophers characterized by an emerging commitment to a particular methodological strategy, a commitment shared by some psychologists and neuroscientists interested in explaining the nature and function of our subjective experiences. Research into the nature and function of consciousness has made some recent advances, especially in the field of cognitive neuroscience, on the basis of a triangulation of data coming from the phenomenological reports of patients, psychological testing at the cognitive/behavioral level, and neurophysiological and neuroanatomical findings. Churchland (1986) calls this strategy for studying the mind the “co-evolutionary strategy”; and Shallice (1988), Dennett (1978, 1991), and Flanagan (1985, 1991, 1992) each promote co-evolutionary methodologies that attempt to bring into equilibrium the phenomenological, the psychological, and the neurobiological in understanding the mind.</p></blockquote>
</li>
<li class="footnote" id="footnote206_5o03dye"><a class="footnote-label" href="#footnoteref206_5o03dye">206.</a> See e.g. <a href="https://global.oup.com/academic/product/the-unity-of-consciousness-9780199215386">Bayne (2010)</a>; <a href="https://mitpress.mit.edu/books/sensory-integration-and-unity-consciousness">Bennett &amp; Hill (2014)</a>.
</li>
<li class="footnote" id="footnote207_8re11e1"><a class="footnote-label" href="#footnoteref207_8re11e1">207.</a> Here is Dennett&#8217;s account (<a href="https://www.pdcnet.org//pdc/bvdb.nsf/purchase?openform&amp;fp=philtopics&amp;id=philtopics_1994_0022_0001_0505_0567&amp;onlyautologin=true">Dennett 1994</a>) of a prediction he made, using his theory of consciousness, which had not been tested at the time:<br /><blockquote><p>On the last page of <em>Consciousness Explained</em>, I described an experiment with eye-trackers that had not been done and predicted the result. The experiment has since been done, by John Grimes at the Beckmann Institute in Champaign Urbana [<a href="http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780195084627.001.0001/acprof-9780195084627-chapter-4">Grimes 1996</a>], and the results were much more powerful than I had dared hope. I had inserted lots of safety nets (I was worried about luminance boundaries and the like — an entirely gratuitous worry as it turns out). Grimes showed subjects high-resolution color photographs on a computer screen and told the subjects to study them carefully, since they would be tested on the details. (The subjects were hence highly motivated, like Betsy, to notice, detect, discriminate, or judge whatever it was they were seeing.) They were also told that there might be a change in the pictures while they were studying them (for ten seconds each). If they ever saw (yes, “saw,” the ordinary word) a change, they were to press the button in front of them — even if they could not say (or judge, or discriminate) what the change was. So the subjects were even alerted to be on the lookout for sudden changes. Then when the experiment began, an eye-tracker monitored their eye movements and during a randomly chosen saccade changed some large and obvious feature in each picture. (Some people think I must be saying that this feature was changed, and then changed back, during the saccade. No. The change was accomplished during the saccade, and the picture remained changed thereafter.) Did the subjects press the button, indicating they had seen a change? Usually not; it depended on how large the change was. Grimes, like me, had expected the effect to be rather weak, so he began with minor, discreet changes in the background. Nobody ever pressed the button, so he began getting more and more outrageous. For instance, in a picture of two cowboys sitting on a bench, Grimes exchanged their heads during the saccade and still, most subjects didn&#8217;t press the button! In an aerial photograph of a bright blue crater lake, the lake suddenly turned jet black — and half the subjects were oblivious to the change, in spite of the fact that this is a portrait of the lake. (What about the half that did notice the change? They had apparently done what Betsy did when she saw the thimble in the epistemic sense: noted, judged, identified, the lake as blue.)</p>
<p>		What does this show? It shows that your brain doesn&#8217;t bother keeping a record of what was flitting across your retinas (or your visual cortex), even for the fraction of a second that elapses from one saccade to the next. So little record is kept that if a major change is made during a saccade — during the changing of the guards, you might say — the difference between the scene thereafter and the scene a fraction of a second earlier, though immense, is typically not just unidentifiable; it is undetectable. The earlier information is just about as evanescent as the image on the wall in the camera obscura. Only details that were epistemically seen trigger the alarm when they are subsequently changed. If we follow Dretske&#8217;s usage, however, we must nevertheless insist that, for whatever it is worth, the changes in the before and after scenes were not just visible to you; you saw them, though of course you yourself are utterly clueless about what the changes were, or even that there were changes.</p></blockquote>
</li>
<li class="footnote" id="footnote208_hhjaa2f"><a class="footnote-label" href="#footnoteref208_hhjaa2f">208.</a> See <a href="http://www.pnas.org/content/113/26/7255.abstract">Olkowicz et al. (2016)</a>.
</li>
<li class="footnote" id="footnote209_aoha24p"><a class="footnote-label" href="#footnoteref209_aoha24p">209.</a> For more detail, see <a href="#tooluse">this footnote</a>.
</li>
<li class="footnote" id="footnote210_eem9sl2"><a class="footnote-label" href="#footnoteref210_eem9sl2">210.</a> I haven&#8217;t seen many surveys of experts on consciousness, but I&#8217;ll list some related sources. <a href="https://books.google.com/books?id=o9ZRc6-FDg8C&amp;lpg=PP1&amp;pg=PA117#v=onepage&amp;q&amp;f=false">McDermott (2007)</a> reports the results of an informal survey of Fellows of the American Association for Artificial Intelligence from 2003. <a href="http://www.journals.uchicago.edu/doi/abs/10.1086/233288">Miller (2000)</a> didn&#8217;t conduct a survey, but claimed that “Almost every member of the American Philosophical Association would agree that all mammals are conscious, and that all conscious experience is of <em>some</em> moral significance.”<br /><br />
	Some authors point to the <a href="https://web.archive.org/web/20131109230457/http://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf">Cambridge Declaration on Consciousness</a> (2012) as evidence that there is now a scientific consensus that:
<blockquote><p>The neural substrates of emotions do not appear to be confined to cortical structures… Systems associated with affect are concentrated in subcortical regions where neural homologies [between humans and animals] abound… The absence of a neocortex does not appear to preclude an organism from experiencing affective states. Convergent evidence indicates that non-human animals have the neuroanatomical, neurochemical, and neurophysiological substrates of conscious states along with the capacity to exhibit intentional behaviors. Consequently, the weight of evidence indicates that humans are not unique in possessing the neurological substrates that generate consciousness. Nonhuman animals, including all mammals and birds, and many other creatures, including octopuses, also possess these neurological substrates.</p></blockquote>
<p>	However:</p>
<ol><li>The document reads more like a political document than a scientific document. (See e.g. <a href="http://www.consciousentities.com/2012/10/the-cambridge-declaration/">this commentary</a>.)</li>
<li>As far as I can tell, the declaration was signed by a small number of people, perhaps about 15 people, and thus hardly demonstrates a “scientific consensus.”</li>
<li>Several of the signers of the declaration have since written scientific papers that seem to treat cortex-required views as a live possibility, e.g. <a href="http://www.nature.com/nrn/journal/v17/n5/full/nrn.2016.22.html">Koch et al. (2016)</a> and <a href="https://www.elsevier.com/books/the-neurology-of-consciousness/laureys/978-0-12-801175-1">Laureys et al. (2015)</a>, p. 427.</li>
</ol><p>	For a much lengthier critique of the Cambridge Declaration on Consciousness, which I only partially agree with and which is posted to a creationism blog, see <a href="http://www.uncommondescent.com/intelligent-design/craig-and-his-critics-why-the-cambridge-declaration-on-consciousness-is-more-propaganda-than-science/">here</a>.</p>
</li>
<li class="footnote" id="footnote211_w07tqak"><a class="footnote-label" href="#footnoteref211_w07tqak">211.</a> When put into computational terms, the “triviality objection” is explained by <a href="https://mitpress.mit.edu/books/rediscovery-mind">Searle (1992)</a>, pp. 208-209, in this way:<br /><blockquote><p>…the wall behind my back is right now implementing the Wordstar program, because there is some pattern of molecule movements that is isomorphic with the formal structure of Wordstar. But if the wall is implementing Wordstar, if it is a big enough wall it is implementing any program, including any program implemented in the brain.</p></blockquote>
<p>Note that <em>if</em> this is a problem, it is seems to be a problem for functionalist accounts of anything, not just consciousness.<br /><br />
I will also quote the description of the problem from <a href="https://mitpress.mit.edu/books/good-and-real">Drescher (2006)</a>, ch. 2:</p>
<blockquote><p>…any message can be construed as a substitution-cipher encoding of any other message (of the same length), simply by contriving the appropriate key. The contrivance is easy: just align the corresponding letters of the two messages and for each pair of aligned letters, choose as the corresponding key-number however many alphabet steps are needed to get from the “unencrypted” letter to the “encrypted” letter…</p>
<p>Returning to the subject at hand, the point… is that construing physical events (such as brain activity) as representations (of external things such as flowers, and of other brain events themselves) leaves room for… mischievously creative interpretations…</p>
<p>…For example, we could pick up a random rock and construe it as playing a game of chess. We&#8217;d already need to have a detailed description of the series of internal states that a real chess-playing computer goes through in the course of a particular game. Then, we&#8217;d just point to as many atoms in the rock as we need and we&#8217;d stipulate, for each atom at each moment, that its state at that moment represents a particular constituent state of the chess-playing computer. That is, we&#8217;d build a translation table with entries like the following:</p>
<p>If the rock&#8217;s atoms numbered 458,620,198,259,728 through 458,620,198,570,954 at time <em>t</em> are in such-and-such state (the state they were in fact in at <em>t</em>), that represents the chess-playing computer&#8217;s transistor number 11,252,664,293 being in thus-and-such state at <em>t</em> (the state it was in fact in at <em>t</em>).</p>
<p>Of course, there&#8217;d be no uniformity to our interpretation scheme — the same state, exhibited by different atoms, would have an entirely different “meaning” in each case. Even the same state of the same atom would “mean” entirely different things at different times… And of course, we have no hope of writing down every entry of the mapping table — it&#8217;s just too huge. Nonetheless, we can speak of the interpretation scheme that the hypothetical table implements…</p>
<p>Here, though, is a problem regarding [consciousness]. The problem is that we could likewise contrive a joke interpretation scheme according to which a rock is conscious. As with the joke chess-machine interpretation, we could (in principle) devise this scheme by taking a conscious entity — say, me — and recording all the states in its brain over a period of several minutes. We then map some portion of the rock onto some part of the brain. And we contrive a mapping function that, at each next moment, just asserts by fiat that the state of a given portion of the rock (the specific placement of individual atoms there, say) represents the next recorded state of the corresponding brain portion&#8217;s state. Under this joke interpretation scheme, the rock undergoes the same sequence of conscious (and unconscious) thoughts over then next few minutes as I did during the few recorded minutes. Or, we could in principle create a different mapping table that attributes to the rock a series of thoughts and feelings all its own.</p>
<p>Of course, [Dennett&#8217;s] intentional-stance test easily disqualifies such joke interpretations from being taken seriously (just as with the joke chess-machine interpretation). Still, there is a reason this joke interpretation poses a problem. The intentional stance only tells us what representation (if any) an <em>external observer</em> has reason to ascribe to an object of interest. And as noted above, a certain practicality follows from an intentional-stance-supported interpretation: it lets us predict that the correspondence in question would or will continue to exist under a reasonable range of circumstances. In contrast, it is of no more practical value to contrivedly ascribe a consciousness-implementing set of representations to a rock than it is to contrivedly ascribe a chess-implementing set of representations. In that sense, a rock is clearly no more engaged in conscious experience than it is engaged in chess playing.</p>
<p>But if we are asking whether an object — be it a person or a computer or a rock — is conscious, we are asking (at least in large measure) about the object&#8217;s <em>own</em> point of view, about how (or whether at all) the object feels to <em>itself</em>, regardless of any external observer&#8217;s perspective or the practical merits thereof. If… consciousness is just a particular kind of representational process, then why isn&#8217;t it the case that at least as far as the rock itself is concerned, the rock possesses the same stream of consciousness as I do, by virtue of the (albeit impractical) joke interpretation?</p></blockquote>
<p>On this topic, see also <a href="https://mitpress.mit.edu/books/representation-and-reality">Putnam (1988)</a>, especially the appendix; <a href="http://link.springer.com/article/10.1007/BF00974167">Chrisley (1994)</a>; <a href="http://link.springer.com/article/10.1007/s11098-008-9231-3">Godfrey-Smith (2009)</a>; <a href="http://link.springer.com/article/10.1007/s11023-012-9280-4">Shagrir (2012)</a>; section 6 of <a href="https://books.google.com/books?id=2OPxCwAAQBAJ&amp;lpg=PA261&amp;ots=5thXpvIs7G&amp;lr=lang_en&amp;pg=PA261#v=onepage&amp;q&amp;f=false">Aaronson (2013)</a>; <a href="http://j-cs.org/issues/__vol012i4/1.html">Chalmers (2011)</a> and the replies cited in <a href="http://j-cs.org/issues/__vol013i3/1.html">Chalmers (2012)</a>.</p>
</li>
<li class="footnote" id="footnote212_hebomkp"><a class="footnote-label" href="#footnoteref212_hebomkp">212.</a> <a href="https://books.google.com/books?id=2OPxCwAAQBAJ&amp;lpg=PA261&amp;ots=5thXpvIs7G&amp;lr=lang_en&amp;pg=PA261#v=onepage&amp;q&amp;f=false">Aaronson (2013)</a>:<br /><blockquote><p>…consider a waterfall (though any other physical system with a large enough state space would do as well)… say, Niagara Falls. Being governed by laws of physics, the waterfall implements some mapping <em>f</em> from a set of possible initial states to a set of possible final states. If we accept that the laws of physics are <em>reversible</em>, then <em>f</em> must also be injective. Now suppose we restrict attention to some finite subset <em>S</em> of possible initial states, with |<em>S</em>| = <em>n</em>. Then <em>f</em> is just a one-to-one mapping from <em>S</em> to some output set <em>T</em> = <em>f</em>(<em>S</em>) with |<em>T</em>| = <em>n</em>. The “crucial observation” is now this: given <em>any</em> permutation σ from the set of integers {1, … , <em>n</em>} to itself, there is some way to label the elements of <em>S</em> and <em>T</em> by integers in {1, … , <em>n</em>}, such that we can interpret <em>f</em> as implementing σ. For example, if we let <em>S</em> = {<em>s</em><sub>1</sub>, … , <em>s</em><sub><em>i</em></sub>} and <em>f</em> (<em>s</em><sub><em>i</em></sub>) = <em>t</em><sub><em>i</em></sub>, then it suffices to label the initial state <em>s</em><sub><em>i</em></sub> by <em>i</em> and the final state <em>t</em><sub><em>i</em></sub> by σ(<em>i</em>). But the permutation σ could have any “semantics” we like: it might represent a program for playing chess, or factoring integers, or simulating a different waterfall. Therefore “mere computation” cannot give rise to semantic meaning.</p>
<p>…To my mind… perhaps the easiest way to demolish the waterfall argument is through computational complexity considerations.</p>
<p>Indeed, suppose we actually wanted to use a waterfall to help us calculate chess moves. How would we do that? In complexity terms, what we want is a <em>reduction</em> from the chess problem to the waterfall-simulation problem. That is, we want an efficient algorithm that somehow <em>encodes</em> a chess position <em>P</em> into an initial state <em>s</em><sub><em>p</em></sub> ∈ <em>S</em> of the waterfall, in such a way that a good move from <em>P</em> can be read out efficiently from the waterfall&#8217;s corresponding final state, <em>f</em>(<em>s</em><sub><em>p</em></sub> ∈ <em>T</em>. But <em>what would such an algorithm look like?</em> We cannot say for sure — certainly not without detailed knowledge about <em>f</em> (i.e., the physics of waterfalls), as well as the means by which the <em>S</em> and <em>T</em> elements are encoded as binary strings. But for any reasonable choice, it seems overwhelmingly likely that any reduction algorithm would just <em>solve the chess problem itself</em>, without using the waterfall in an essential way at all! A bit more precisely, I conjecture that, given any chess-playing algorithm <em>A</em> that accesses a “waterfall oracle” <em>W</em>, there is an equally good chess-playing algorithm <em>A′</em>, with similar time and space requirements, that does <em>not</em> access <em>W</em>. If this conjecture holds, then it gives us a perfectly observer-independent way to formalize our intuition that the “semantics” of waterfalls have nothing to do with chess.</p>
<p>…</p>
<p>In my view, there is an important lesson here for debates about computationalism. Suppose we want to claim, for example, that a computation that plays chess is “equivalent” to some other computation that simulates a waterfall. Then our claim is only non-vacuous if it&#8217;s possible to <em>exhibit the equivalence</em> (i.e., give the reductions) within a model of computation that isn&#8217;t <em>itself</em> powerful enough to solve the chess or waterfall problems.</p></blockquote>
</li>
<li class="footnote" id="footnote213_ncunq4o"><a class="footnote-label" href="#footnoteref213_ncunq4o">213.</a> See also the final section of the <a href="/sites/default/files/David_Chalmers_05-20-16_%28public%29.pdf">notes from my conversation with David Chalmers</a>.
</li>
<li class="footnote" id="footnote214_hx5l2gk"><a class="footnote-label" href="#footnoteref214_hx5l2gk">214.</a> For example <a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tye (2016)</a>, <a href="http://us.macmillan.com/otherminds/petergodfreysmith">Godfrey-Smith (2016a)</a>, and <a href="http://books.wwnorton.com/books/detail.aspx?ID=4294992671">Dennett (2017)</a>. (For a condensed account of some of the key theoretical ideas in Godfrey-Smith 2016a, see <a href="https://www.cambridge.org/core/books/how-biology-shapes-philosophy/animal-evolution-and-the-origins-of-experience/BD327840021D6F8C430A91DDAC67E4E9">Godfrey-Smith 2016b</a>.)
</li>
<li class="footnote" id="footnote215_jsb7r8q"><a class="footnote-label" href="#footnoteref215_jsb7r8q">215.</a> <a href="http://www.jstor.org/stable/40971115">Dennett (1995)</a>, p. 700. The full quote is:<br /><blockquote><p>Lockwood says “probably” all birds are conscious, but maybe some of them — or even all of them — are rather like sleepwalkers! Or what about the idea that there could be unconscious pains (and that animal pain, though real, and — yes — morally important, was unconscious pain)? Maybe there is a certain amount of generous-minded delusion (which I once called the Beatrix Potter syndrome) in our bland mutual assurance that as Lockwood puts it, “<em>Pace</em> Descartes, consciousness, thus construed, isn&#8217;t remotely, on this planet, the monopoly of human beings.”</p>
<p>How, though, could we ever explore these “maybes”? We could do so in a constructive, anchored way by first devising a theory that concentrated exclusively on human consciousness — the one variety about which we will brook no “maybes” or “probablys” — and then look and see which features of that account apply to which animals, and why. There is plenty of work to do…</p></blockquote>
</li>
<li class="footnote" id="footnote216_082gc3w"><a class="footnote-label" href="#footnoteref216_082gc3w">216.</a> This general approach sometimes goes by names such as “ideal advisor theory” or, arguably, “reflective equilibrium.” Diverse sources explicating various extrapolation procedures (or fragments of extrapolation procedures) include: <a href="http://www.journals.uchicago.edu/doi/10.1086/293702">Rosati (1995)</a>; <a href="http://plato.stanford.edu/entries/reflective-equilibrium/">Daniels (2016)</a>; <a href="http://www.tandfonline.com/doi/full/10.1080/00048402.2013.833643">Campbell (2013)</a>; chapter 9 of <a href="https://www.polity.co.uk/book.asp?ref=9780745646596">Miller (2013)</a>; <a href="https://intelligence.org/files/IdealAdvisorTheories.pdf">Muehlhauser &amp; Williamson (2013)</a>; <a href="http://jme.bmj.com/content/40/5/303.short">Trout (2014)</a>; Yudkowsky&#8217;s “<a href="https://arbital.com/p/normative_extrapolated_volition/">Extrapolated volition (normative moral theory)</a>” (2016); <a href="http://onlinelibrary.wiley.com/doi/10.1111/phpr.12199/abstract">Baker (2016)</a>; <a href="http://press.uchicago.edu/ucp/books/book/chicago/R/bo3627517.html">Stanovich (2004)</a>, pp. 224-275; <a href="http://www.tandfonline.com/doi/abs/10.1080/13546783.2012.713178">Stanovich (2013)</a>.<br /><br />
On the prospects for values convergence, see e.g. <a href="http://onlinelibrary.wiley.com/doi/10.1111/1467-8284.00160/full">Sobel (1999)</a>; Döring and Andersen&#8217;s 2009 unpublished manuscript “Rationality, Convergence and Objectivity”; <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.0003-2638.1996.00155.x/abstract">Swanton (1996)</a>; the sources listed in footnote 19 of <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.2041-6962.2012.00136.x/full">Egan (2012)</a>; section 5.1 of <a href="http://link.springer.com/article/10.1023%2FA%3A1010744722791?LI=true">Sobel (2001)</a>; <a href="http://psycnet.apa.org/journals/gpr/9/3/203/">Dahlsgaard et al. (2005)</a>; <a href="http://www.penguin.com/book/the-better-angels-of-our-nature-by-steven-pinker/9780143122012">Pinker (2011)</a>; <a href="http://link.springer.com/chapter/10.1007/978-3-319-05308-0_3">Bicchieri &amp; Mercier (2014)</a>; <a href="http://us.macmillan.com/books/9780805096910">Shermer (2015)</a>; <a href="http://link.springer.com/article/10.1007/s11098-015-0588-9">Huemer (2016)</a>; <a href="https://global.oup.com/academic/product/from-valuing-to-value-9780198712640?cc=us&amp;lang=en&amp;">Sobel (2017)</a>.
</li>
<li class="footnote" id="footnote217_pdb4gwh"><a class="footnote-label" href="#footnoteref217_pdb4gwh">217.</a> I am even more skeptical, of course, that visiting aliens or future artificial intelligence systems capable of comprehending this report would, upon completing such an extrapolation procedure, converge on the same values.
</li>
<li class="footnote" id="footnote218_ypw7cqh"><a class="footnote-label" href="#footnoteref218_ypw7cqh">218.</a> For more on forecasting accuracy, see <a href="/blog/efforts-improve-accuracy-our-judgments-and-forecasts">this blog post</a>. My use of research on the psychological predictors of forecasting accuracy for the purposes of doing moral philosophy is one example of my support for the use of “ameliorative psychology” in philosophical practice — see e.g. Bishop &amp; Trout (<a href="https://global.oup.com/academic/product/epistemology-and-the-psychology-of-human-judgment-9780195162295?lang=en&amp;cc=us">2004</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1747-9991.2008.00161.x/full">2008</a>).
</li>
<li class="footnote" id="footnote219_5ysegta"><a class="footnote-label" href="#footnoteref219_5ysegta">219.</a> Specifically, the scenario I try to imagine (and make conditional forecasts about) looks something like this:
<ol><li>In the distant future, I am non-destructively “uploaded.” In other words, my brain and some supporting cells are scanned (non-destructively) at a fine enough spatial and chemical resolution that, when this scan is combined with accurate models of how different cell types carry out their information-processing functions, one can create an executable computer model of my brain that matches my biological brain&#8217;s input-output behavior almost exactly. This whole brain emulation (“em”) is then connected to a virtual world: computed inputs are fed to the em&#8217;s (now virtual) signal transduction neurons for sight, sound, etc., and computed outputs from the em&#8217;s virtual arm movements, speech, etc. are received by the virtual world, which computes appropriate changes to the virtual world in response. (I don&#8217;t think anything remotely like this will ever happen, but as far as I know it is a <em>physically possible</em> world that can be described in some detail; for one attempt, see <a href="https://global.oup.com/academic/product/the-age-of-em-9780198754626?cc=us&amp;lang=en&amp;">Hanson 2016</a>.) Given functionalism, this “em” has the same memories, personality, and conscious experience that I have, though it experiences quite a shock when it awakens to a virtual world that might look and feel somewhat different from the “real” world.</li>
<li>This initial em is copied thousands of times. Some of the copies interact inside the same virtual world, other copies are placed inside isolated virtual worlds.</li>
<li>Then, these ems spend a very long time (a) collecting and generating arguments and evidence about morality and related topics, (b) undergoing various experiences, in varying orders, and reflecting on those experiences, (c) dialoguing with ems sourced from other biological humans who have different values than I do, and perhaps with sophisticated chat-bots meant to simulate the plausible reasoning of other types of people (from the past, or from other worlds) who were not available to be uploaded, and so on. They are able to do these things for a very long time because they and their virtual worlds are run at speeds thousands of times faster than my biological brain runs, allowing subjective eons to pass in mere months of “objective” time.</li>
<li>Finally, at some time, the ems dialogue with each other about which values seem “best,” they engage in moral trade (<a href="http://www.journals.uchicago.edu/doi/abs/10.1086/682187">Ord 2015</a>), and they try to explain to me what values they think I should have and why. In the end, I am not forced to accept any of the values they then hold (collectively or individually), but I am able to come to much better-informed moral judgments than I could have without their input.</li>
</ol><p>For more context on this sort of values extrapolation procedure, see <a href="https://intelligence.org/files/IdealAdvisorTheories.pdf">Muehlhauser &amp; Williamson (2013)</a>.</p>
</li>
<li class="footnote" id="footnote220_zqqyoi6"><a class="footnote-label" href="#footnoteref220_zqqyoi6">220.</a> For more on forecasting “best practices,” see <a href="/blog/efforts-improve-accuracy-our-judgments-and-forecasts">this blog post</a>.
</li>
<li class="footnote" id="footnote221_raq9er2"><a class="footnote-label" href="#footnoteref221_raq9er2">221.</a> Following <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=116609&amp;fileId=S0265052502192077">Hanson (2002)</a> and ch. 2 of <a href="https://rucore.libraries.rutgers.edu/rutgers-lib/40469/">Beckstead (2013)</a>, I consider my moral intuitions in the context of Bayesian curve-fitting. To explain, I&#8217;ll quote <a href="https://rucore.libraries.rutgers.edu/rutgers-lib/40469/">Beckstead (2013)</a> at some length:<br /><blockquote><p>Curve fitting is a problem frequently discussed in the philosophy of science. In the standard presentation, a scientist is given some data points, usually with an independent variable and a dependent variable, and is asked to predict the values of the dependent variable given other values of the independent variable. Typically, the data points are <em>observations</em>, such as “measured height” on a scale or “reported income” on a survey, rather than true values, such as height or income. Thus, in making predictions about additional data points, the scientist has to account for the possibility of error in the observations. By an error process I mean anything that makes the observed values of the data points differ from their true values. Error processes could arise from a faulty scale, failures of memory on the part of survey participants, bias on the part of the experimenter, or any number of other sources. While some treatments of this problem focus on predicting observations (such as measured height), I&#8217;m going to focus on predicting the true values (such as true height).</p>
<p>…For any consistent data set, it is possible to construct a curve that fits the data exactly… If the scientist chooses one of these polynomial curves for predictive purposes, the result will usually be <em>overfitting</em>, and the scientist will make worse predictions than he would have if he had chosen a curve that did not fit the data as well, but had other virtues, such as a straight line. On the other hand, always going with the simplest curve and giving no weight to the data leads to <em>underfitting</em>…</p>
<p>I intend to carry over our thinking about curve fitting in science to reflective equilibrium in moral philosophy, so I should note immediately that curve fitting is not limited to the case of two variables. When we must understand relationships between multiple variables, we can turn to multiple-dimensional spaces and fit planes (or hyperplanes) to our data points. Different axes might correspond to different considerations which seem relevant (such as total well-being, equality, number of people, fairness, etc.), and another axis could correspond to the value of the alternative, which we can assume is a function of the relevant considerations. Direct Bayesian updating on such data points would be impractical, but the philosophical issues will not be affected by these difficulties.</p>
<p>…On a Bayesian approach to this problem, the scientist would consider a number of different hypotheses about the relationship between the two variables, including both hypotheses about the phenomena (the relationship between X and Y) and hypotheses about the error process (the relationship between observed values of Y and true values of Y) that produces the observations…</p>
<p>…Lessons from the Bayesian approach to curve fitting apply to moral philosophy. Our moral intuitions are the data, and there are error processes that make our moral intuitions deviate from the truth. The complete moral theories under consideration are the hypotheses about the phenomena. (Here, I use “theory” broadly to include any complete set of possibilities about the moral truth. My use of the word “theory” does not assume that the truth about morality is simple, systematic, and neat rather than complex, circumstantial, and messy.) If we expect the error processes to be widespread and significant, we must rely on our priors more. If we expect the error processes to be, in addition, biased and correlated, then we will have to rely significantly on our priors even when we have a lot of intuitive data.</p></blockquote>
<p>Beckstead then summarizes the framework with the following table (p. 32):</p>
<table><tr><th></th>
<th>Science</th>
<th>Moral Philosophy</th>
</tr><tr><td>Hypotheses about phenomena</td>
<td>Different trajectories of a ball that has been dropped</td>
<td>Moral theories (specific versions of utilitarianism, Kantianism, contractulaism, pluralistic deontology, etc.)</td>
</tr><tr><td>Hypotheses about error processes</td>
<td>Our position measurements are accurate on average, and are within 1 inch 95% of the time (with normally distributed error)</td>
<td>Different hypotheses about the causes of error in historical cases; cognitive and moral biases; different hypotheses about the biases that cause inconsistent judgments in important philosophical cases</td>
</tr><tr><td>Observations</td>
<td>Recorded position of a ball at different times recorded with a certain clock</td>
<td>Intuitions about particular cases or general principles, and any other relevant observations</td>
</tr><tr><td>Background theory</td>
<td>The ball never bounces higher than the height it started at. The ball always moves along a continuous trajectory.</td>
<td>Meta-ethical or normative background theory (or theories)</td>
</tr></table></li>
<li class="footnote" id="footnote222_le00uu6"><a class="footnote-label" href="#footnoteref222_le00uu6">222.</a> For more on this, see <a href="/sites/default/files/Carl_Shulman_08-19-16_%28public%29.pdf">my conversation with Carl Shulman</a>, <a href="http://www.journals.uchicago.edu/doi/abs/10.1086/683441">O&#8217;Neill (2015)</a>, the literature on the evolution of moral values (e.g. <a href="http://www.brill.com/products/book/evolved-morality-biology-and-philosophy-human-conscience">de Waal et al. 2014</a>; <a href="https://mitpress.mit.edu/books/moral-psychology">Sinnott-Armstrong &amp; Miller 2007</a>; <a href="https://mitpress.mit.edu/books/evolution-morality">Joyce 2005</a>), the literature on moral psychology (e.g. <a href="https://global.oup.com/academic/product/the-moral-psychology-handbook-9780199582143?cc=us&amp;lang=en&amp;">Doris 2010</a>; <a href="https://global.oup.com/academic/product/moral-brains-9780199357666?cc=us&amp;lang=en&amp;">Liao 2016</a>; <a href="http://link.springer.com/book/10.1007/978-3-319-01369-5">Christen et al. 2014</a>; <a href="https://www.ncbi.nlm.nih.gov/pubmed/16209802">Sunstein 2005</a>), the literature on how moral values vary between cultures and eras (e.g. see <a href="https://global.oup.com/academic/product/the-geography-of-morals-9780190212155?cc=us&amp;lang=en&amp;">Flanagan 2016</a>; <a href="http://press.princeton.edu/titles/10419.html">Morris 2015</a>; <a href="http://knopfdoubleday.com/book/56526/the-moral-consequences-of-economic-growth/">Friedman 2005</a>; <a href="https://global.oup.com/academic/product/the-emotional-construction-of-morals-9780199283019?cc=us&amp;lang=en&amp;">Prinz 2007</a>, pp. 187-195), and the literature on moral thought experiments (e.g. <a href="https://www.routledge.com/What-If-Collected-Thought-Experiments-in-Philosophy/Tittle/p/book/9780321202789">Tittle 2004</a>, ch. 7). See also <a href="http://aristotelian.oxfordjournals.org/content/116/2/127">Wilson (2016)</a>’s comments on internal and external validity in ethical thought experiments, and <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2017/00000024/F0020001/art00002">Bakker (2017) on “alien philosophy.”<br /><br />
I do not read much fiction, but I suspect that some types of fiction — e.g. historical fiction, fantasy, and science fiction — can help readers to temporarily transport themselves into fully-realized alternate realities, in which readers can test how their moral intuitions differ when they are temporarily “lost” in an alternate world.
</a></li>
<li class="footnote" id="footnote223_xcdq1ww"><a class="footnote-label" href="#footnoteref223_xcdq1ww">223.</a> There are many sources which discuss how people&#8217;s values seem to change along with (and perhaps in response to) components of my proposed extrapolation procedure, such as learning more facts, reasoning through more moral arguments, and dialoguing with others who have different values. See e.g. <a href="https://www.cambridge.org/core/journals/perspectives-on-politics/article/changing-mass-priorities-the-link-between-modernization-and-democracy/816079F2778E68E46984469D634741D0">Inglehart &amp; Welzel (2010)</a>, <a href="http://www.penguin.com/book/the-better-angels-of-our-nature-by-steven-pinker/9780143122012">Pinker (2011)</a>, <a href="http://us.macmillan.com/books/9780805096910">Shermer (2015)</a>, and <a href="http://www.journals.uchicago.edu/doi/abs/10.1086/686003">Buchanan &amp; Powell (2016)</a>.
</li>
<li class="footnote" id="footnote224_8gwwydj"><a class="footnote-label" href="#footnoteref224_8gwwydj">224.</a> For example, as I&#8217;ve learned more, considered more moral arguments, and dialogued more with people who don&#8217;t share my values, my moral values have become more “secular-rational” and “self-expressive” (<a href="https://www.cambridge.org/core/journals/perspectives-on-politics/article/changing-mass-priorities-the-link-between-modernization-and-democracy/816079F2778E68E46984469D634741D0">Inglehart &amp; Welzel 2010</a>), more geographically global, more extensive (e.g. throughout more of the animal kingdom), less <a href="https://en.wikipedia.org/wiki/Person-affecting_view">person-affecting</a>, and subject to greater moral uncertainty (<a href="http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.627884">MacAskill 2014</a>).
</li>
<li class="footnote" id="footnote225_dlfumws"><a class="footnote-label" href="#footnoteref225_dlfumws">225.</a> Or, as <a href="https://www.pdcnet.org/pdc/bvdb.nsf/purchase?openform&amp;fp=jphil&amp;id=jphil_2008_0105_0008_0389_0415">Allen-Hermanson (2008)</a> might put it: what if fishes are “natural zombies,” or “naturally blindsighted” about <em>all</em> their sensory and internal states?
</li>
<li class="footnote" id="footnote226_rek4jj3"><a class="footnote-label" href="#footnoteref226_rek4jj3">226.</a> See <a href="https://mitpress.mit.edu/books/feeling-pain-and-being-pain">Grahek (2007)</a>.
</li>
<li class="footnote" id="footnote227_56yq829"><a class="footnote-label" href="#footnoteref227_56yq829">227.</a> It would be interesting to test my hypothesis on several subjects with AAD, for example the 13 patients of <a href="http://brain.oxfordjournals.org/content/136/10/3076.short">Leu-Semenescu et al. (2013)</a>, if they are still alive.
</li>
<li class="footnote" id="footnote228_yntutgh"><a class="footnote-label" href="#footnoteref228_yntutgh">228.</a> In other words, neuroscientists don&#8217;t yet know much about what David Marr called the “algorithmic level” (<a href="https://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis">Wikipedia</a>).<br /><br />
Here is the explanation of Marr&#8217;s levels of analysis from <a href="http://www.cambridge.org/us/academic/textbooks/cognitivescience">Bermudez (2014)</a>, p. 47:
<blockquote><p>Marr distinguishes three different levels for analyzing cognitive systems. The highest is the <em>computational level</em>. Here cognitive scientists analyze in very general terms the particular type of task that the system performs…</p>
<p>The guiding assumption here is that cognition is ultimately to be understood in terms of information processing, so that the job of individual cognitive systems is to transform one kind of information (say, the information coming into a cognitive system through its sensory systems) into another type of information (say, information about what type of objects there might be in the organism&#8217;s immediate environment). A computational analysis identifies the information with which the cognitive system has to begin (the <em>input</em> to that system) and the information with which it needs to end up (the <em>output</em> from that system).</p>
<p>The next level down is what Marr calls the <em>algorithmic level</em>. The algorithmic level tells us how the cognitive system actually solves the specific information- processing task identified at the computational level. It tells us how the input information is transformed into the output information. It does this by giving algorithms that effect that transformation. An algorithmic level explanation takes the form of specifying detailed sets of information-processing instructions that will explain how, for example, information from the sensory systems about the distribution of light in the visual field is transformed into a representation of the three-dimensional environment around the perceiver.</p>
<p>In contrast, the principal task at the <em>implementational level</em> is to find a physical realization for the algorithm – that is to say, to identify physical structures that will realize the representational states over which the algorithm is defined and to find mechanisms at the neural level that can properly be described as computing the algorithm in question.</p></blockquote>
<p>For a nice illustration of some reasons why it&#8217;s so difficult for neuroscientists to study brain function at the algorithmic level given current tools, see <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268">Jonas &amp; Kording (2017)</a>.</p>
</li>
<li class="footnote" id="footnote229_hjrkux7"><a class="footnote-label" href="#footnoteref229_hjrkux7">229.</a> To run my Python code without needing to install any software, you can use an online Python sandbox such as <a href="https://www.tutorialspoint.com/execute_python3_online.php">this one</a> from Tutorials Point. Or, you can install a full-featured Python <a href="https://en.wikipedia.org/wiki/Integrated_development_environment">IDE</a> such as <a href="https://www.jetbrains.com/pycharm/download/">PyCharm</a> (the Community edition is free).
</li>
<li class="footnote" id="footnote230_9jppn2i"><a class="footnote-label" href="#footnoteref230_9jppn2i">230.</a> Python implementations vary; see <a href="http://programmers.stackexchange.com/questions/24558/is-python-interpreted-or-compiled">here</a>.
</li>
<li class="footnote" id="footnote231_xbbr7z9"><a class="footnote-label" href="#footnoteref231_xbbr7z9">231.</a> In general, I think lots of philosophical discussion and argument should be conducted using short and long snippets of source code, to improve the clarity and concreteness of those discusions. Steven Phillips calls this approach to philosophy “<a href="http://www.executablephilosophy.org/">executable philosophy</a>” (see also <a href="https://tryingtobeawesome.com/files/exphil-intro-draft03-v2-final.html">this incomplete draft</a> of his thoughts on the subject). See also Yudkowsky&#8217;s “<a href="https://arbital.com/p/executable_philosophy/">Executable Philosophy</a>,” which includes a similar recommendation about philosophical practice on a list which includes several other recommendations. (As far as I know, Phillips and Yudkowsky use this term independently of each other.)<br /><br />
One example of this approach being used in philosophy of consciousness is Brian Tomasik&#8217;s “<a href="http://reducing-suffering.org/simple-program-illustrate-hard-problem-consciousness/">A Simple Program to Illustrate the Hard Problem of Consciousness</a>.” To make his “simple program” more comprehensible to people who are not Python programmers, I added extensive comments to his code (and bumped the syntax to Python Version 3): see <a href="/sites/default/files/Tomasik%20hard%20problem.py.txt">here</a>.<br /><br />
For related but not identical ideas about philosophical methodology, see discussions on computational explanations and computational models in philosophy, e.g. <a href="http://onlinelibrary.wiley.com/doi/10.1002/9780470757017.ch26/summary?primaryCompoundsResultsPerPage=250">Grim (2004)</a>; <a href="http://link.springer.com/article/10.1007/s11229-016-1101-5">Rusanen &amp; Lappi (2016)</a>.
</li>
<li class="footnote" id="footnote232_5q8s5re"><a class="footnote-label" href="#footnoteref232_5q8s5re">232.</a> This “short program argument” is a generalization of <a href="http://www.sciencedirect.com/science/article/pii/S0893608007001530">Herzog et al. (2007)</a>’s “small network argument.” It is also similar to some remarks in <a href="http://link.springer.com/chapter/10.1007/978-1-4615-9317-1_1">Rey (1983)</a>:<br /><blockquote><p>…it seems to me to be entirely feasible… to render an existing computing machine intentional by providing it with a program that would include the following:</p>
<p>1. The alphabet, formation, and transformation rules for quantified modal logic (the system&#8217;s “language of thought”).</p>
<p>2. The axioms for your favorite inductive logic and/or abductive system of hypotheses, with a “reasonable” function for selecting among them on the basis of given input.</p>
<p>3. The axioms of your favorite decision theory, and some set of basic preferences.</p>
<p>4. Mechanical inputs, via sensory transducers, for Clauses 2 and 3.</p>
<p>5. Mechanical connections that permit the machine to realize its outputs (e.g., its “most preferred” basic act descriptions).</p>
<p>Any computer that functioned according to such a program would, I submit, realize significant Rational Regularities, complete with intensionality. Notice, for example, that it would be entirely appropriate — and probably unavoidable — for us to explain and predict its behavior and internal states on the basis of those regularities. It would be entirely reasonable, that is to say, for us to adopt toward it what Dennett (1971) has called the “intentional stance.”</p>
<p>…However clever a machine programmed with Clauses 1-5 might become, counting thereby as a thinking thing, surely it would not also count thereby as conscious. The program is just far too trivial. Moreover, we are already familiar with systems satisfying at least Clauses 1-5 that we also emphatically deny are conscious: there are all those unconscious neurotic systems postulated in so many of us by Freud, and all those surprisingly intelligent, but still unconscious, subsystems for perception and language postulated in us by contemporary cognitive psychology. (Some evidence of the cognitive richness of unconscious processing is provided by the interesting review of such material in Nisbett &amp; Wilson, 1977, but especially by such psycholinguistic experiments as that by Lackner &amp; Garrett, 1973, in which subliminal linguistic material provided to one ear biased subjects in their understanding of ambiguous sentences provided to the other ear.) In all of these cases we are, I submit, quite reasonably led to ascribe beliefs, preferences, and sometimes highly elaborate thought processes to a system on the basis of the Rational Regularities, despite the fact that the systems involved are often not the least bit “conscious” of any such mental activity at all. It is impossible to imagine these psychological theories getting anywhere without the ascription of unconscious content — and it is equally difficult to imagine any animals getting anywhere without the exploitation of it. Whatever consciousness will turn out to be, it will pretty certainly need to be distinguished from the thought processes we ascribe on the basis of the rational regularities.</p>
<p>How easily this point can be forgotten, neglected, or missed altogether is evidenced by the sorts of proposals about the nature of consciousness one finds in some of the recent psychobiological literature. The following seem to be representative:</p>
<p style="padding-left: 50pt; padding-right: 50pt;">Consciousness is usually defined by the ability: (1) to appreciate sensory information; (2) to react critically to it with thoughts or movements; (3) to permit the accumulation of memory traces. (Moruzzi, 1966)</p>
<p style="padding-left: 50pt; padding-right: 50pt;">Perceptions, memories, anticipatory organization, a combination of these factors into learning — all imply rudimentary consciousness. (Knapp, 1976)</p>
<p style="padding-left: 50pt; padding-right: 50pt;">Modern views… regard human conscious activity as consisting of a number of major components. These include the reception and processing (recoding) of information, with the selection of its most important elements and retention of the experience thus gained in the memory; enunciation of the task or formulation of an intention, with the preservation of the corresponding modes of activity, the creation of a pattern or model of the required action, and production of the appropriate program (plan) to control the selection of necessary actions; and finally the comparison of the results of the action with the original intention … with correction of the mistakes made. (Luria, 1978)</p>
<p style="padding-left: 50pt; padding-right: 50pt;">Consciousness is a process in which information about multiple individual modalities of sensation and perception is combined into a unified, multidimensional representation of the state of the system and its environment and is integrated with information about memories and the needs of the organism, generating emotional reactions and programs of behavior to adjust the organism to its environment. (John, 1976)</p>
<p>What I find astonishing about such proposals is that they are all more-or-less satisfiable by almost any information-processing system, for precisely what modern computational machinery is designed to do is to receive, process, unify, and retain information; create (or “call”) patterns, models, and subroutines to control its activity; and, by all means, to compare the results of its action with its original intention in order to adjust its behavior to its environment. This latter process is exactly what the “feedback” that Wiener (1954) built into his homing rocket was for! Certainly, most of the descriptions in these proposals are satisfied by any recent game-playing program (see, e.g., Berliner, 1980). And if it&#8217;s genuine “modalities,” “thoughts,” “intentions,” “perceptions,” or “representations” that are wanted, then, as I&#8217;ve argued, supplementing the program with Clauses 1-5 will suffice, but without rendering anything a whit more conscious.</p></blockquote>
<p><a href="https://mitpress.mit.edu/books/unity-self">White (1991)</a>, ch. 6, summed up Rey&#8217;s point like so:</p>
<blockquote><p>…a survey of recent characterizations of consciousness by philosophers and psychologists reveals that most or all characterizations would be satisfied by information-processing devices that either exist now or would be trivial extensions of devices that exist.</p></blockquote>
<p>See also Rey (<a href="https://books.google.com/books?id=wEjqp9kTXnUC&amp;lpg=PA123&amp;ots=WxqdXB38SR&amp;lr&amp;pg=PA123#v=onepage&amp;q&amp;f=false">1995</a>; <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00016">2016</a>).</p>
</li>
<li class="footnote" id="footnote233_ozlx949"><a class="footnote-label" href="#footnoteref233_ozlx949">233.</a> Even if they are not functionalists, they could still clarify their views by saying “<em>If</em> I was a functionalist, <em>then</em> such-and-such computer program exhibits the kind of functional behavior and cognitive processing that I think would be sufficient for moral patienthood.”
</li>
<li class="footnote" id="footnote234_tq5r1k9"><a class="footnote-label" href="#footnoteref234_tq5r1k9">234.</a> For more details on the algorithm whose behavior is shown in the video, see the section on VI.A of <a href="http://ieeexplore.ieee.org/document/5586133/?arnumber=5586133&amp;tag=1">Togelius et al. (2010)</a>.
</li>
<li class="footnote" id="footnote235_ebg2chi"><a class="footnote-label" href="#footnoteref235_ebg2chi">235.</a> However, if you <em>do</em> think this algorithm is a moral patient — because it seems to have goals and aversions, is capable of planning (its path through the level), and so on — and you are some kind of utilitarian, then this may have some surprising implications. For example, suppose you think that this Mario-controlling algorithm is a moral patient, but only has a tiny fraction of the “moral weight” that a human has, such that when Mario reaches the goal at the end of the level, that has about 1/1000th as much positive moral value as when you have a single spoonful of ice cream. In that case, it might still be the case that the most morally valuable thing you could do per dollar — given your moral intuitions — is to run this Mario-controlling algorithm trillions of times using rented cloud computation from e.g. <a href="https://aws.amazon.com">Amazon Web Services</a>.<br /><br />
Similarly, if you (unlike me) have the intuition that today&#8217;s reinforcement learning algorithms are moral patients, there are practical code modifications that could be made today to reduce the risk that these (very common) algorithms are instantiating negative phenomenal experiences: see <a href="https://arxiv.org/abs/1410.8233">Tomasik (2014)</a>, p. 17.
</li>
<li class="footnote" id="footnote236_08h5ofm"><a class="footnote-label" href="#footnoteref236_08h5ofm">236.</a> Search for “SUPERHRO” on <a href="http://www.kaser.com/mesh.html">this page</a>.
</li>
<li class="footnote" id="footnote237_jyqym4k"><a class="footnote-label" href="#footnoteref237_jyqym4k">237.</a> I might be mis-remembering the details of these algorithms, but these details don&#8217;t matter much to my illustration.
</li>
<li class="footnote" id="footnote238_lxi7ddo"><a class="footnote-label" href="#footnoteref238_lxi7ddo">238.</a> The closest analogue of this exercise I&#8217;ve seen elsewhere is <a href="http://link.springer.com/chapter/10.1007/978-1-4615-9317-1_1">Rey (1983)</a>, though I discovered that article <em>after</em> writing a first draft of this section.
</li>
<li class="footnote" id="footnote239_4om8psz"><a class="footnote-label" href="#footnoteref239_4om8psz">239.</a>  We also have to assume the game is deterministic, i.e. that it doesn&#8217;t allow for random number generation. Off the top of my head, I can&#8217;t recall whether this is true for <em>MESH: Hero</em>.
</li>
<li class="footnote" id="footnote240_s29rzgm"><a class="footnote-label" href="#footnoteref240_s29rzgm">240.</a> Implementers of BDI systems can make a wide variety of choices about how to implement “beliefs,” “desires,” and “intentions,” how these relate to one another, and how they determine an agent&#8217;s actions. For example, <a href="http://www.scs-europe.net/services/ecms2006/ecms2006%20pdf/25-game.pdf">Davies et al. (2006)</a>, <a href="http://ieeexplore.ieee.org/document/6632604/?arnumber=6632604">Palazzo et al. (2013)</a>, and <a href="https://arizona.pure.elsevier.com/en/publications/hierarchical-en-route-planning-under-the-extended-belief-desire-i">Kim et al. (2014)</a> make different choices about these things. Still, if an agent roughly fits the BDI architecture, it&#8217;s more likely that Dennett&#8217;s “intentional stance” could be used to interpret or predict its actions.<br /><br />
As an example, Daniel Dewey (a Program Officer for the Open Philanthropy Project) describes the <a href="http://www.scs-europe.net/services/ecms2006/ecms2006%20pdf/25-game.pdf">Davies et al. (2006)</a> BDI system with the following table:
<table><tr><th>Sources</th>
<th>Beliefs</th>
<th>Desires</th>
<th>Intentions</th>
</tr><tr><td><a href="http://www.scs-europe.net/services/ecms2006/ecms2006%20pdf/25-game.pdf">Davies et al. (2006)</a>; <a href="http://link.springer.com/chapter/10.1007/0-387-26350-0_6">Pokahr et al. (2005)</a></td>
<td>Set of facts stored in an object-oriented style.</td>
<td>“Goals” = “concrete, momentary desires of an agent.” May be used to make plans or not depending on the agent&#8217;s believed context. Goals may be to execute certain actions, to reach certain states of the world, to reach certain internal states (e.g. the agent learns particular things), etc. May include “subgoals” created by plans to achieve other goals.</td>
<td>A library of plan templates, added to a list of active plans when certain goals are active and certain beliefs are held. Plans are procedures that include templated external actions (e.g. moving the agent) and internal actions (manipulating beliefs, creating subgoals).</td>
</tr></table><p>On BDI architectures in general, see e.g. <a href="https://en.wikipedia.org/wiki/Belief%E2%80%93desire%E2%80%93intention_software_model">Wikipedia</a> and <a href="https://mitpress.mit.edu/books/reasoning-about-rational-agents">Wooldridge (2000)</a>.</p>
</li>
<li class="footnote" id="footnote241_dmzmagb"><a class="footnote-label" href="#footnoteref241_dmzmagb">241.</a> <a href="http://www.tandfonline.com/doi/abs/10.1080/00048409912349231">Carruthers (1999)</a>:<br /><blockquote><p>The conclusion C1 [that “The mental states of non-human animals lack phenomenal feels”]… generates, quite naturally, a further question…</p>
<p style="padding-left: 50pt; padding-right: 50pt;">Question 1: Given C1, ought we to conclude that sympathy (and other moral attitudes) towards the sufferings and disappointments of non-human animals is inappropriate?</p>
<p>In my [<a href="http://www.cambridge.org/us/academic/subjects/philosophy/political-philosophy/animals-issue-moral-theory-practice">Carruthers (1992)</a>], chapter 8, I argued tentatively for a positive answer to this question. But I am now not so sure. Indeed, the main burden of this paper is to demonstrate that there is a powerful case for answering Q1 in the negative…</p>
<p>…</p>
<p>I propose… to defend the following claim:</p>
<p style="padding-left: 50pt; padding-right: 50pt;">A6: Only subjective frustrations or thwartings of desire count as psychological harms, and are appropriate objects of sympathetic concern.</p>
<p>However, the sense of ‘subjective’ in A6 need not be… that of possessing phenomenological properties. Rather, the sense can be that of being believed in by the subject, on this account, a desire counts as being subjectively frustrated, in the relevant sense, if the subject believes that it has been frustrated, or believes that the desired state of affairs has not (and/or will not) come about. Then there would be nothing to stop a phenomenology-less frustration of desire from counting as subjective, and from constituting an appropriate object of moral concern. So we have a question:</p>
<p style="padding-left: 50pt; padding-right: 50pt;"> Q2: Which is the appropriate notion of subjective to render A6 true? — (a) possessing phenomenology? or (b) being believed in by the subject?</p>
<p>If the answer to Q2 is (a), then animal frustrations and pains, in lacking phenomenology by C1, will not be appropriate objects of sympathy or concern. This would then require us to answer Q1 in the affirmative, and animals would, necessarily, be beyond the moral pale. However, if the answer to Q2 is (b), then there will be nothing in C1 and A6 together to rule out the appropriateness of moral concern for animals; and we shall then have answered Q1 in the negative.</p>
<p>It is important to see that desire-frustration can be characterised in a purely first-order way, without introducing into the account any higher-order belief concerning the existence of that desire… So, suppose that an animal has a strong desire to eat, and that this desire is now activated; suppose, too, that the animal is aware that it is not now eating; then that seems sufficient for its desire to be subjectively frustrated, despite the fact that the animal may be incapable of higher-order belief.</p>
<p>…</p>
<p>So putting A6 and Q2 together, in effect, we have the question:</p>
<p style="padding-left: 50pt; padding-right: 50pt;">Q3: What is bad or harmful, from the point of view of a sympathetic observer, about the frustration or thwarting of desire? — (a) the phenomenology associated with desire frustration? or (b) the fact of learning that the object of desire has not been achieved?</p>
<p>…</p>
<p>If my assumptions… are granted, then the main point is (at least tentatively) established: the most basic form of psychological harm, from the perspective of a sympathetic observer, consists in the known or believed frustration of first-order desires (which need not require that agents have knowledge that they have those desires — just knowledge of what states of affairs have come about). That is to say, the answer to Q3 is (b). So the proper object of sympathy, when we sympathise with what has happened to an agent, is the known (or believed) frustration of first-order desire. And it follows, then (given A1 and A2), that the non-conscious desires of non-human animals are at least possible, or appropriate, objects of moral sympathy and concern. (Whether they should then be objects of such concern is a further distinctively moral question, to be answered by considerations pertaining to ethical theory rather than to philosophical psychology.) And it emerges that the complete absence of phenomenology from the lives of most non-human animals, derived in C1, is of little or no direct relevance to ethics.</p></blockquote>
<p><a href="http://link.springer.com/article/10.1007/s11098-004-3635-5">Carruthers (2004)</a> develops this line of thinking further.<br /><br />
Several others have advocated views which might be interpreted as according moral patienthood to animals on account of their having preferences that can be satisfied or frustrated, regardless of whether they are also conscious. See e.g. <a href="https://global.oup.com/academic/product/why-animals-matter-9780199747511?cc=us&amp;lang=en&amp;">Dawkins (2012)</a>, chs. 7-9, and the endorsement of that account by <a href="http://animalstudiesrepository.org/animsent/vol1/iss3/25/">Rose (2016)</a>.</p>
</li>
<li class="footnote" id="footnote242_n0m1xww"><a class="footnote-label" href="#footnoteref242_n0m1xww">242.</a> For example <a href="http://plato.stanford.edu/entries/grounds-moral-status/">Jaworska &amp; Tannenbaum (2013)</a> write:<br /><blockquote><p>Historically, the most famous [account of moral status grounded in intellectual capacities] was given by Kant, according to whom autonomy, the capacity to set ends via practical reasoning, must be respected… and grounds the dignity of all rational beings… Beings without reason may be treated as a mere means…</p></blockquote>
<p>Similarly, <a href="http://plato.stanford.edu/entries/respect/">Dillon (2014)</a> writes:</p>
<blockquote><p>The most influential position on [the topic of respect for persons] is found in the moral philosophy of Immanuel Kant… Indeed, most contemporary discussions of respect for persons explicitly claim to rely on, develop, or challenge some aspect of Kant&#8217;s ethics. Central to Kant&#8217;s ethical theory is the claim that all persons are owed respect just because they are persons, that is, free rational beings. To be a person is to have a status and worth that is unlike that of any other kind of being: it is to be an end in itself with dignity. And the only response that is appropriate to such a being is respect. Respect (that is, moral recognition respect) is the acknowledgment in attitude and conduct of the dignity of persons as ends in themselves. Respect for such beings is not only appropriate but also morally and unconditionally required: the status and worth of persons is such that they must always be respected…</p></blockquote>
</li>
<li class="footnote" id="footnote243_1zasdnq"><a class="footnote-label" href="#footnoteref243_1zasdnq">243.</a> This isn&#8217;t <em>really</em> a rebuttal against Braithwaite&#8217;s argument for fish consciousness, because it is easy find details of her account that are technically not satisfied by the program I sketched here, even for the specific features I&#8217;ve listed above; rather, I sketched the program above and pointed to Braithwaite&#8217;s account merely to illustrate the more general point I make in the next paragraph.<br /><br />
In any case, here is Braithwaite&#8217;s summary of her case for fish consciousness, from <a href="https://books.google.com/books?id=aMvonPqzu_cC&amp;printsec=frontcover">Braithwaite (2010)</a>, ch. 4:
<blockquote><p>So pulling the different threads together, fish really do appear to possess key traits associated with consciousness. Their ability to form and use mental representations indicates fish have some degree of access consciousness. They can consider a current mental state and associate it with a memory. Having an area of the brain specifically associated with processing emotion and evidence that they alter their view of an aversive situation depending on context suggests that fish have some form of phenomenal consciousness: they are sentient. This leaves monitoring and self consciousness, which I argue is in part what the eel and the grouper are doing: considering their actions and pondering the consequences. The grouper is clearly deciding it has no chance to get the prey itself and so swims off to get the eel. The eel is deciding that an easy meal is on offer. On balance then, fish have a capacity for some forms of consciousness, and so I conclude that they therefore have the mental capacity to feel pain.</p></blockquote>
<p>Braithwaite doesn&#8217;t mention nociceptors or the transmission of nociceptive signalling for central processing in this quote, but it&#8217;s clear from earlier sections of the book that these two features of fish neurobiology are critical to her confidence in conscious fish pain.</p>
<p>This version of <em>MESH: Hero</em> might also satisfy the criteria for having “interests” of the sort that <a href="http://www.cambridge.org/us/academic/subjects/philosophy/ethics/morally-deep-world-essay-moral-significance-and-environmental-ethics?format=PB">Johnson (1993)</a> argues are sufficient for moral status. Note that unlike some authors defending the moral status of plants and ecosystems, Johnson is explicit that his account might accord moral status to certain kinds of machines (pp. 145-146).</p>
</li>
<li class="footnote" id="footnote244_t8gdzew"><a class="footnote-label" href="#footnoteref244_t8gdzew">244.</a> A much more satisfying, but also more costly to write, version of this exercise would involve doing the following:
<ol><li>Collect several dozen functionalist theories of consciousness and moral patienthood.</li>
<li>Summarize their key features.</li>
<li>Think of a basic program design that would allow you to chart an efficient course through as many of these theories of consciousness and moral patienthood as possible, merely by adding 1-5 new “features” for each updated version of the program.</li>
<li>Write the code for each version of that program.</li>
<li>Briefly describe each version of the program in order, and after each program version description, quote the theory or theories of consciousness or moral patienthood that now seem to be satisfied by the program.</li>
</ol></li>
<li class="footnote" id="footnote245_nl9ryy5"><a class="footnote-label" href="#footnoteref245_nl9ryy5">245.</a> I don&#8217;t worry about just <em>any</em> “large deep reinforcement learning agent” or “complicated candidate solution,” of course. E.g. I might start to worry if I can&#8217;t trace what the system is doing and it exhibits some highly sophisticated behavior that matches human conscious behavior in certain ways. On the other hand, I might not worry if a strong argument can be made that a given (large and complicated) system is essentially doing a very high-dimensional variant on linear regression, and is not engaging in e.g. dynamic control of memory and attention subsystems, as is the case for some deep learning agent architectures (see e.g. <a href="https://arxiv.org/abs/1606.03813">Marblestone et al. 2016</a>).
</li>
<li class="footnote" id="footnote246_z57c5i3"><a class="footnote-label" href="#footnoteref246_z57c5i3">246.</a> I owe this phrase to Yudkowsky, “<a href="http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/">How an Algorithm Feels from Inside</a>.”
</li>
<li class="footnote" id="footnote247_q5ohtp1"><a class="footnote-label" href="#footnoteref247_q5ohtp1">247.</a> <a href="https://global.oup.com/academic/product/consciousness-and-the-social-brain-9780199928644?cc=us&amp;lang=en&amp;">Graziano (2013)</a>, ch. 1.
</li>
<li class="footnote" id="footnote248_y4hh2ok"><a class="footnote-label" href="#footnoteref248_y4hh2ok">248.</a> Or, to be more accurate: How do such-and-such brain processes <em>instantiate</em> consciousness?
</li>
<li class="footnote" id="footnote249_ps7566l"><a class="footnote-label" href="#footnoteref249_ps7566l">249.</a> One way to think about this is from the perspective of “inference to the best explanation” or “explanationism,” according to which theories are judged by how well they perform on a list of common-sense “explanatory virtues.” Years ago, I <a href="http://commonsenseatheism.com/?p=11046">collected</a> the following list of commonly-endorsed explanatory virtues from philosophical defenders of inference to the best explanation:
<ol><li>Testability: better explanations render specific predictions that can be falsified or corroborated.</li>
<li>Scope (aka “comprehensiveness” or “consilience”): better explanations explain more types of phenomena.</li>
<li>Precision: better explanations explain phenomena with greater precision.</li>
<li>Simplicity: better explanations make use of fewer claims, especially fewer as yet unsupported claims (“lack of ad-hoc-ness”).</li>
<li>Mechanism: better explanations provide more information about underlying mechanisms.</li>
<li>Unification: better explanations unify apparently disparate phenomena (also sometimes called “consilience”).</li>
<li>Predictive novelty: better explanations don&#8217;t just “retrodict” what we already know, but predict things we observe only after they are predicted.</li>
<li>Analogy (aka “fit with background knowledge”): better explanations generally fit with what we already know with some certainty.</li>
<li>Past explanatory success: better explanations fit within a tradition or trend with past explanatory success (e.g. astronomy, not astrology).</li>
</ol><p>On this framework, a more precise way to state my core complaint about current theories of consciousness is that they are lacking in <em>precision</em> and <em>scope</em>.</p>
<p>(Of course, they may be lacking in other explanatory virtues, too.)</p>
</li>
<li class="footnote" id="footnote250_ssginck"><a class="footnote-label" href="#footnoteref250_ssginck">250.</a> One can compare my exercise to section 4 (“Some case studies”) from <a href="http://www.ingentaconnect.com/content/imp/jcs/1995/00000002/00000003/653">Chalmers (1995)</a>:<br /><blockquote><p>In the last few years, a number of works have addressed the problems of consciousness within the framework of cognitive science and neuroscience. This might suggest that the foregoing analysis is faulty, but in fact a close examination of the relevant work only lends the analysis further support. When we investigate just which aspects of consciousness these studies are aimed at and which aspects they end up explaining, we find that the ultimate target of explanation is always one of the easy problems. I illustrate this with two representative examples.</p></blockquote>
<p>After explaining Crick &amp; Koch&#8217;s temporal binding theory, Chalmers says:</p>
<blockquote><p>The details of how this binding might be achieved are still poorly understood, but suppose that they can be worked out. What might the resulting theory explain? Clearly it might explain the binding of information, and perhaps it might yield a more general account of the integration of information in the brain. Crick and Koch also suggest that these oscillations activate the mechanisms of working memory, so that there may be an account of this and perhaps other forms of memory in the distance. The theory might eventually lead to a general account of how perceived information is bound and stored in memory for use by later processing.</p>
<p>Such a theory would be valuable, but it would tell us nothing about why the relevant contents are experienced. Crick and Koch suggest that these oscillations are the neural correlates of experience. This claim is arguable— does not binding also take place in the processing of unconscious information?— but even if it is accepted, the explanatory question remains: why do the oscillations give rise to experience? The only basis for an explanatory connection is the role they play in binding and storage, but the question of why binding and storage should themselves be accompanied by experience is never addressed. If we do not know why binding and storage should give rise to experience, telling a story about the oscillations cannot help us. Conversely, if we knew why binding and storage gave rise to experience, the neurophysiological details would be just the icing on the cake. Crick and Koch&#8217;s theory gains its purchase by assuming a connection between binding and experience and so can do nothing to explain that link.</p></blockquote>
<p>Chalmers then elaborates a similar complaint about some other theories.<br /><br />
One difference between Chalmers’ complaint and mine is that, as an illusionist about consciousness who thus “replaces the hard problem with the illusion problem” (<a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00002">Frankish 2016b</a>), one way to view my complaint is that it is the complaint that the theories of consciousness surveyed here fail to explain the illusions of conscious experience.<br /><br />
But really, my complaint is more general than this, and not dependent on illusionism in particular. If later I decide that (say) I am a Prinz-style realist about consciousness (<a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00015">Prinz 2016</a>), the my core complaint will remain: simply, these theories do not explain enough consciousness explananda, with enough precision, to be satisfying.</p>
</li>
<li class="footnote" id="footnote251_hkdzfs7"><a class="footnote-label" href="#footnoteref251_hkdzfs7">251.</a> See Crick &amp; Koch (<a href="http://authors.library.caltech.edu/40352/">1990</a>, <a href="http://psycnet.apa.org/psycinfo/1998-01071-001">1998</a>).<br /><br />
Note that they later abandoned their early theory of consciousness. <a href="http://www.macmillanlearning.com/Catalog/product/questforconsciousnessaneurobiologicalapproach-firstedition-koch">Koch (2004)</a>, p. 46, writes:
<blockquote><p>Today, Francis [Crick] and I no longer think that synchronized firing is a sufficient condition for the [neural correlates of consciousness]. A functional role more in line with the data is that synchronization assists a nascent coalition  in its competetition with other nascent coalitions. As explained in Chapter 9, this occurs when you attend to an object or event. A neuronal substrate of this bias could be synchronized firing in certain frequency bands… Once a coalition has established itself as a winner and you are conscious of the associated attributes, the coalition may be bale to maintain itself without the assistance of synchrony, at least for a time. Thus, one might expect synchronized oscillations to occur in the early stages of perception, but not necessarily in later ones.</p></blockquote>
</li>
<li class="footnote" id="footnote252_z9xm36t"><a class="footnote-label" href="#footnoteref252_z9xm36t">252.</a> See <a href="http://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-5-42">Tononi (2004)</a>; <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003588">Oizumi et al. (2014)</a>; <a href="http://www.scholarpedia.org/article/Integrated_information_theory">Tononi (2015)</a>; <a href="http://www.nature.com/nrn/journal/v17/n7/full/nrn.2016.44.html">Tononi et al. (2015b)</a>. I have not read these sources in full.
</li>
<li class="footnote" id="footnote253_sj60434"><a class="footnote-label" href="#footnoteref253_sj60434">253.</a> I am hardly an expert on IIT, so my criticisms could be misguided, but even if they are, I hope they will help to illustrate how I think about theories of consciousness.
</li>
<li class="footnote" id="footnote254_yxehgkp"><a class="footnote-label" href="#footnoteref254_yxehgkp">254.</a> For example, IIT predicts enormous quantities of consciousness in the “trivially simple network” of <a href="http://www.pnas.org/content/103/28/10799.short">Seth et al. (2006)</a> and the expander graph of <a href="http://www.scottaaronson.com/blog/?p=1799">Aaronson (2014a)</a>.<br /><br /><a href="http://www.scottaaronson.com/tononi.docx">Tononi (2014)</a> replied to the latter example by confirming that Aaronson&#8217;s expander graph <em>would</em> be enormously conscious according to IIT, but then saying that we shouldn&#8217;t trust our intuitions that the expander graph <em>isn&#8217;t</em> enormously conscious:
<blockquote><p>[Aaronson&#8217;s] main point that certain systems that are simple — in the sense that they are easy to describe — could have large values of PHI, [is correct]… Resorting to expander graphs is actually overkill. This is because systems that are even simpler to describe than expander graphs, for example a 2D lattice of identical logic gates (a “grid”) could also achieve very large values of PHI. So things are “even worse” for IIT… [Aaronson also] argues that some systems with high PHI may not only have a structure that is simple to describe, but they may perform computations that are also just as simple to describe, such as parity checks. In fact, the situation for IIT is actually “worse”, since it allows for a large 2D grid to be conscious even if it were doing nothing, with all gates switched off at a fixed point. Thus, if IIT can be invalidated by an expander graph doing not much at all, it can be invalidated all the more by a mere grid doing absolutely nothing…</p>
<p>…However, [Aaronson&#8217;s] “commonsense” intuition that such simple systems cannot possibly be conscious is wrong and should be revised.</p>
<p>…it can be dangerous to rely too much on one&#8217;s pre-theoretical intuitions, however strong they may seem. Examples in science are numerous, starting with the strong intuitions people once had that the earth must be still and the sun must revolve around it, or that the earth cannot be round because otherwise we would fall off. Concerning consciousness, the reliability of pre-theoretical intuitions is even worse, because different people often hold radically different ones…</p></blockquote>
<p><a href="http://www.scottaaronson.com/blog/?p=1823">Aaronson (2014b)</a>’s responses to Tononi are roughly the same ones I would give (though, see also the additional exchanges between David Chalmers and Scott Aaronson on that post), so I won&#8217;t repeat them here. </p>
</li>
<li class="footnote" id="footnote255_tn23z6q"><a class="footnote-label" href="#footnoteref255_tn23z6q">255.</a> <a href="https://global.oup.com/academic/product/consciousness-and-the-social-brain-9780199928644?cc=us&amp;lang=en&amp;">Graziano (2013)</a>, ch. 11.<br /><br />
In <a href="http://www.theatlantic.com/science/archive/2016/03/phlegm-theories-of-consciousness/472812/">Graziano (2016)</a>, he makes his case against IIT this way:
<blockquote><p>[One] popular explanation of consciousness is the integrated information theory. Actually, there are several different theories that fit into this same general category. They share the underlying idea that consciousness is caused by linking together large amounts of information. It&#8217;s one thing to process a few disconnected scraps of information. But when information is connected into vast brain-spanning webs, then, according to the proposal, subjective consciousness emerges.</p>
<p>I can&#8217;t deny that information is integrated in the brain on a massive scale. Vast networks of information play a role in many brain functions. If you could de-integrate the information in the brain, a lot of basic functions would fail, probably including consciousness. And yet, as a specific explanation of consciousness, this one is definitely a phlegm theory.</p>
<p>Again, it flatters intuition. Most people have an intuition about consciousness as an integrated whole. Your various impressions and thoughts are somehow rolled together into a single inner you. That&#8217;s the impression we get, anyway.</p>
<p>You see this same trope in science fiction: If you bundle enough information into a computer, creating a big enough connected mass of data, it&#8217;ll wake up and start to act conscious, like Skynet. This appeal to our latent biases has given the integrated information theory tremendous currency. It&#8217;s compelling to many respected figures in the field of neuroscience, and is one of the most popular current theories.</p>
<p>And yet it doesn&#8217;t actually explain anything. What exactly is the mechanism that leads from integrated information in the brain to a person who ups and claims, “Hey, I have a conscious experience of all that integrated information!” There isn&#8217;t one.</p>
<p>If you point a wavelength detector at the sky, it will compute that the sky is blue. If you build a machine that integrates the blueness of the sky with a lot of other information – the fact that the blue stuff is a sky, that it&#8217;s above the earth, that it extends so far here and so far there – if the machine integrates a massive amount of information about that sky – what makes the machine claim that it has a subjective awareness of blue? Why doesn&#8217;t it just have a bunch of integrated information, without the subjective awareness? The integration theory doesn&#8217;t even try to explain. It flatters our intuitions while explaining nothing.</p>
<p>Some scholars retreat to the position that consciousness must be a primary property of information that cannot be explained. If information is present, so is a primordial, conscious experience of it. The more information that is integrated together, the richer the conscious experience. This type of thinking leads straight to a mystical theory called panpsychism, the claim that everything in the universe is conscious, each in its own way, since everything contains at least some information. Rocks, trees, rivers, stars. This theory is the ultimate in phlegm theories. It has enormous intuitive appeal to people who are prone to project consciousness onto the objects around them, but it explains absolutely nothing. One must simply accept consciousness as an elemental property and abandon all hope of understanding it.</p></blockquote>
</li>
<li class="footnote" id="footnote256_5nmsma6"><a class="footnote-label" href="#footnoteref256_5nmsma6">256.</a> E.g. see the section on “multiple complexes” in <a href="http://www.nature.com/nrn/journal/v17/n7/full/nrn.2016.44.html">Tononi et al. (2016)</a>.
</li>
<li class="footnote" id="footnote257_f6pzely"><a class="footnote-label" href="#footnoteref257_f6pzely">257.</a> Proponents of IIT do say <em>some</em> things about IIT and reportability (e.g. see <a href="http://www.nature.com/nrn/journal/v17/n7/full/nrn.2016.44.html">Tononi et al. 2016</a>), but if they&#8217;ve said anything about IIT specifically <em>predicts</em> the specific features of conscious self-report we observe, then I have been unable to understand what that account is, in what I&#8217;ve read about IIT so far.
</li>
<li class="footnote" id="footnote258_9eqw3w5"><a class="footnote-label" href="#footnoteref258_9eqw3w5">258.</a> For an introduction to LIDA, see <a href="http://www.sciencedirect.com/science/article/pii/S2212683X16300196">Franklin et al. (2016)</a>.
</li>
<li class="footnote" id="footnote259_98jrkaj"><a class="footnote-label" href="#footnoteref259_98jrkaj">259.</a> Note that the Müller-Lyer illusion image included below does not appear in the quoted passage of Weisberg (2014); I added it for convenience.
</li>
<li class="footnote" id="footnote260_0725jem"><a class="footnote-label" href="#footnoteref260_0725jem">260.</a> <a href="https://books.google.com/books?id=7w6IYeJRqyoC&amp;printsec=frontcover&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjLkMuXsYvQAhWHllQKHXPEAlkQ6AEIHTAA#v=onepage&amp;q&amp;f=falseÎ">Baars (1988)</a>; <a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00200/full">Baars et al. (2013)</a>; <a href="https://global.oup.com/academic/product/embodiment-and-the-inner-life-9780199226559?cc=us&amp;lang=en&amp;">Shanahan (2010)</a>, ch. 4; <a href="http://www.penguin.com/book/consciousness-and-the-brain-by-stanislas-dehaene/9780143126263">Dehaene (2014)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0959438813002298">Dehaene et al. (2014)</a>; <a href="http://academicworks.cuny.edu/gc_etds/1604/">Shevlin (2016)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S2212683X12000060">Franklin et al. 2012</a>. I have not read these sources in full.
</li>
<li class="footnote" id="footnote261_xei3azq"><a class="footnote-label" href="#footnoteref261_xei3azq">261.</a> As with IIT, I am hardly an expert on GWT, so my criticisms could be misguided, but even if they are, I hope they will help to illustrate how I think about theories of consciousness.
</li>
<li class="footnote" id="footnote262_faedewz"><a class="footnote-label" href="#footnoteref262_faedewz">262.</a> Quoted text is from <a href="http://www.penguin.com/book/consciousness-and-the-brain-by-stanislas-dehaene/9780143126263">Dehaene (2014)</a>, ch. 5:<br /><blockquote><p>When we say that we are aware of a certain piece of information, what we mean is just this: the information has entered into a specific storage area that makes it available to the rest of the brain. Among the millions of mental representations that constantly criss-cross our brains in an unconscious manner, one is selected because of its relevance to our present goals. Consciousness makes it globally available to all our high-level decision systems. We possess a mental router, an evolved architecture for extracting relevant information and dispatching it. The psychologist Bernard Baars calls it a “global workspace”: an internal system, detached from the outside world, that allows us to freely entertain our private mental images and to spread them across the mind&#8217;s vast array of specialized processors (figure 24).</p></blockquote>
<p>Figure 24, borrowed from <a href="http://www.pnas.org/content/95/24/14529.short">Dehaene et al. (1998)</a>, is:</p>
<p><img style="display: block; margin-left: auto; margin-right: auto;" src="/files/Research/Moral_Patienthood/GNWT.png"   alt="GNWT.png" align="middle" /></p>
<p>Dehaene continues:</p>
<blockquote><p>According to this theory, consciousness is just brain-wide information sharing. Whatever we become conscious of, we can hold it in our mind long after the corresponding stimulation has disappeared from the outside world. That&#8217;s because our brain has brought it into the workspace, which maintains it independently of the time and place at which we first perceived it. As a result, we may use it in whatever way we please. In particular, we can dispatch it to our language processors and name it; this is why the capacity to report is a key feature of a conscious state. But we can also store it in long-term memory or use it for our future plans, whatever they are…</p>
<p>…</p>
<p>Like the psychologist Bernard Baars, I believe that consciousness reduces to what the workspace does: it makes relevant information globally accessible and flexibly broadcasts it to a variety of brain systems…</p>
<p>Flexible information sharing requires a specific neuronal architecture to link the many distant and specialized regions of the cortex into a coherent role. Can we identify such a structure inside our brains? …Unlike the dense mosaic of cells that make up our skin, the brain comprises enormously elongated cells: neurons. With their long axon, neurons possess the property, unique among cells, of measuring up to meters in size. A single neuron in the motor cortex may send its axon to extraordinarily distant regions of the spinal cord, in order to command specific muscles. Most interestingly, …long-distance projection cells are quite dense in the cortex… From their locations in the cortex, nerve cells shaped like pyramids often send their axons all the way to the back of the brain or to the other hemisphere…</p>
<p>Importantly, not all brain areas are equally well connected. Sensory regions, such as the primary visual area V1, tend to be choosy and to establish only a small set of connections, primarily with their neighbors. Early visual regions are arranged in a coarse hierarchy: area V1 speaks primarily to V2, which in turns speaks to V3 and V4, and so on. As a result, early visual operations are functionally encapsulated: visual neurons initially receive only a small fraction of the retinal input and process it in relative isolation, without any “awareness” of the overall picture.</p>
<p>In the higher association areas of the cortex, however, connectivity loses its local nearest-neighbor or point-to-point character, thus breaking the modularity of cognitive operations. Neurons with long-distance axons are most abundant in the prefrontal cortex… This region connects to many other sites in the inferior parietal lobe, the middle and anterior temporal lobe, and the anterior and posterior cingulate areas that lie on the brain&#8217;s midline. These regions have been identified as major hubs — the brain&#8217;s main interconnection centers. All are heavily connected by reciprocal projections: if area A projects to area B, then almost invariably B also sends a projection back to A… Furthermore, long-distance connections tend to form triangles: if area A projects jointly to areas B and C, then they, in turn, are very likely to be interconnected.</p>
<p>These cortical regions are strongly connected to additional players, such as the central lateral and intralaminar nuclei of the thalamus (involved in attention, vigilance, and synchronization), the basal ganglia (crucial for decision making and action), and the hippocampus (essential for memorizing the episodes of our lives and for recalling them). Pathways linking the cortex with the thalamus are especially important. The thalamus is a collection of nuclei, each of which enters into a tight loop with at least one region of the cortex and often many of them at once. Virtually all regions of the cortex that are directly interconnected also share information via a parallel information route through a deep thalamic relay. Inputs from the thalamus to the cortex also play a fundamental role in exciting the cortex and maintaining it in an “up” state of sustained activity…</p>
<p>The workspace thus rests on a dense network of interconnected brain regions — a decentralized organization without a single physical meeting site. At the top of the cortical hierarchy, an elitist board of executives, distributed in distant territories, stays in sync by exchanging a plethora of messages… We are now in a position to understand why these associative areas systematically ignite whenever a piece of information enters our awareness: those regions possess precisely the long-distance connectivity needed to broadcast messages across the long distances of the brain.</p></blockquote>
<p>Later in the same chapter, Dehaene describes how his theory says a visual percept would be come conscious:</p>
<blockquote><p>Suppose we could track all the connections that are activated as we consciously recognize a face… What kind of network would we see? Initially, very short connections, located inside our retinas, clean up the incoming image. The compressed image is then sent, via the massive cable of the optic nerve, to the visual thalamus, then on to the primary visual area in the occipital lobe. Via local U-shaped fibers, it gets progressively transmitted to several clusters of neurons in the right fusiform gyrus, where researchers have discovered… patches of neurons tuned to faces. All this activity remains unconscious. What happens next? Where do the fibers go? The Swiss anatomist Stéphanie Clarke found the surprising answer [<a href="http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-0193(1997)5:5%3C347::AID-HBM3%3E3.0.CO;2-3/full">Di Virgilio &amp; Clarke (1997)</a>]: all of a sudden, long-distance axons allow the visual information to be dispatched to virtually any corner of the brain. From the right inferior temporal lobe, massive and direct connections project, in a single synaptic step, to distant areas of the associative cortex, including those in the opposite hemisphere. The projections concentrate in the inferior frontal cortex (Broca&#8217;s area) and in the temporal association cortex (Wernicke&#8217;s area). Both regions are key nodes of the human language network — and at this stage, therefore, words begin to be attached to the incoming visual information.</p>
<p>Because these regions themselves participate in a broader network of workspace areas, the information can now be further disseminated to the entire inner circle of higher-level executive systems; it can circulate in a reverberating assembly of active neurons. According to my theory, access to this dense network is all that is needed for the incoming information to become conscious.</p></blockquote>
<p>With this (and more) in place, Dehaene finally describes what his theory says a particular <em>conscious state</em> is:</p>
<blockquote><p>[My theory] proposes that a conscious state is encoded by the stable activation, for a few tenths of a second, of a subset of active workspace neurons. These neurons are distributed in many brain areas, and they all code for different facets of the same mental representation. Becoming aware of the Mona Lisa involves the joint activation of millions of neurons that care about objects, fragments of meaning, and memories.</p>
<p>During conscious access, thanks to the workspace neurons’ long axons, all these neurons exchange reciprocal messages, in a massively parallel attempt to achieve a coherent and synchronous interpretation. Conscious perception is complete when they converge. The cell assembly that encodes this conscious content is spread throughout the brain: fragments of relevant information, each distilled by a distinct brain region, cohere because all the neurons are kept in sync, in a top-down manner, by neurons with long-distance axons.</p>
<p>Neuronal synchrony may be a key ingredient. There is growing evidence that distant neurons form giant assemblies by synchronizing their spikes with ongoing background electrical oscillations. If this picture is correct, the brain web that encodes each of our thoughts resembles a swarm of fireflies that harmonize their discharges according to the overall rhythm of the group&#8217;s pattern. In the absence of consciousness, moderate-size cell assemblies may still synchronize locally — for instance, when we unconsciously encode a word&#8217;s meaning inside the language networks of our left temporal lobe. However, because the prefrontal cortex does not gain access to the corresponding message, it cannot be broadly shared and therefore remains unconscious.</p>
<p>Let us conjure one more mental image of this neuronal code for consciousness. Picture the sixteen billion cortical neurons in your cortex. Each of them cares about a small range of stimuli. Their sheer diversity is flabbergasting: in the visual cortex alone, one finds neurons that care about faces, hands, objects, perspective, shape, lines, curves, colors, 3-D depth… Each cell conveys only a few bits of information about the perceived scene. Collectively, though, they are capable of representing an immense repertoire of thoughts. The global workspace model claims that, at any given moment, out of this enormous potential set, a single object of thought gets selected and becomes the focus of our consciousness. At this moment, all the relevant neurons activate in partial synchrony, under the aegis of a subset of prefrontal cortex neurons.</p>
<p>It is crucial to understand that, in this sort of coding scheme, the silent neurons, which do not fire, also encode information. Their muteness implicitly signals to others that their preferred feature is not present or is irrelevant to the current mental scene. A conscious content is defined just as much by its silent neurons as by its active ones.</p></blockquote>
<p>(According to <a href="http://www.pnas.org/site/aboutpnas/rightperm.xhtml">this page</a>, the figure above from <a href="http://www.pnas.org/content/95/24/14529.short">Dehaene et al. (1998)</a> was copyrighted in 1998 by the National Academy of Sciences and does not require permission for noncommercial use.)</p>
</li>
<li class="footnote" id="footnote263_7hs6e4m"><a class="footnote-label" href="#footnoteref263_7hs6e4m">263.</a> However, I do not agree with those who argue that higher-order theories strongly imply that consciousness is rare. I suspect that even if consciousness is a fairly complicated, self-representational, higher-order phenomenon, it <em>might</em> still be implemented by small insect ganglia. The question is: <em>is it?</em>
</li>
<li class="footnote" id="footnote264_p5xp1nj"><a class="footnote-label" href="#footnoteref264_p5xp1nj">264.</a> See <a href="https://global.oup.com/academic/product/consciousness-and-the-social-brain-9780199928644?cc=us&amp;lang=en&amp;">Graziano (2013)</a>.<br /><br />
Note that Graziano&#8217;s theory is vulnerable to objections over “vague psychological language” like those I raise <a href="#ThirdPerson">elsewhere</a>. For example, Brian Tomasik suggests (<a href="http://reducing-suffering.org/why-your-laptop-may-be-marginally-sentient/#Attention_schema_theory_and_Windows_Task_Manager">here</a>) that much of Graziano&#8217;s broad theory seems to be satisfied by the Windows Task Manager, or a slightly modified version of it. I doubt that Graziano thinks the Windows Task Manager is conscious, but if not, the analogy to Windows Task Manager may allow Graziano to state his theory more precisely, in a way that excludes the Windows Task Manager from consciousness.
</li>
<li class="footnote" id="footnote265_arxxx7p"><a class="footnote-label" href="#footnoteref265_arxxx7p">265.</a> See <a href="https://mitpress.mit.edu/books/good-and-real">Drescher (2006)</a>, ch. 2, and <a href="/sites/default/files/Gary_Drescher_07-18-16_%28public%29.pdf">notes from my conversation with Gary Drescher</a>.<br /><br />
Many readers might wonder what a “gensym” is. It is a function for creating symbols in the <a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)">Lisp</a> programming language, though the term “gensym” is also sometimes used to refer to the generated symbol itself. I have not used Lisp before, but Daniel Dewey, a Program Officer for the Open Philanthropy Project, offers this brief explanation:
<blockquote><p>Symbols are a Lisp data type, like numbers or strings. For Drescher&#8217;s purposes, the important feature of symbols is that the only acceptable operation on a symbol is checking whether it&#8217;s identical with, or different from, another symbol. Instances of other data types, like numbers or strings, can be checked for equality (1 != 2, or “Gary Drescher” = “Gary Drescher”), but can also be operated on in other ways, exposing additional information they contain; for example, numbers can be added or multiplied, and strings can be broken up into their component characters. Symbols don&#8217;t contain any other information, and can&#8217;t be added, multiplied, or split apart; they can only be compared. A program can tell that the symbols ‘italic’ and ‘italic’ are identical, and that the symbols ‘italic’ and ‘bold’ are different, but it can&#8217;t get any other information about ‘italic’ or ‘bold’. “Gensym” is a function that generates a new symbol that&#8217;s guaranteed not to be identical with any symbol that&#8217;s been generated so far. (Because some versions of Lisp are designed to be useful instead of philosophically pure, some versions of Lisp don&#8217;t totally follow these properties, and add more information to symbols that programs can access, but that&#8217;s not relevant to Drescher&#8217;s analogy.)</p></blockquote>
<p>Note that the core idea of Drescher&#8217;s “qualia as gensyms” account was described at least as early as <a href="http://consc.net/papers/c-and-c.html">Chalmers (1990)</a>:</p>
<blockquote><p>Very briefly, here is what I believe to be the correct account of why we think we are conscious, and why it seems like a mystery. The basic notion is that of pattern processing. This is one of the things that the brain does best. It can take raw physical data, usually from the environment but even from the brain itself, and extract patterns from these. In particular, it can discriminate on the basis of patterns. The original patterns are in the environment, but they are transformed on their path through neural circuits, until they are represented as quite different patterns in the cerebral cortex. This process can also be represented as information flow (not surprisingly), from the environment into the brain. The key point is that once the information flow has reached the central processing portions for the brain, further brain function is not sensitive to the original raw data, but only to the pattern (to the information!) which is embodied in the neural structure.</p>
<p>Consider color perception, for instance. Originally, a spectral envelope of light-wavelengths impinges upon our eyes. Immediately, some distinctions are collapsed, and some pattern is processed. Three different kinds of cones abstract out information about how much light is present in various overlapping wavelength-ranges. This information travels down the optic nerve (as a physical pattern, of course), where it gets further transformed by neural processing into an abstraction about how much intensity is present on what we call the red-green, yellow-blue, and achromatic scales. What happens after this is poorly-understood, but there is no doubt that by the time the central processing region is reached, the pattern is very much transformed, and the information that remains is only an abstraction of certain aspects of the original data.</p>
<p>Anyway, here is why color perception seems strange. In terms of further processing, we are sensitive not to the original data, not even directly to the physical structure of the neural system, but only to the patterns which the system embodies, to the information it contains. It is a matter of access. When our linguistic system (to be homuncular about things) wants to make verbal reports, it cannot get access to the original data; it does not even have direct access to neural structure. It is sensitive only to pattern. Thus, we know that we can make distinctions between certain wavelength distributions, but we do not know how we do it. We&#8217;ve lost access to the original wavelengths - we certainly cannot say “yes, that patch is saturated with 500-600 nm reflections”. And we do not have access to our neural structure, so we cannot say “yes, that&#8217;s a 50 Hz spiking frequency”. It is a distinction that we are able to make, but only on the basis of pattern. We can merely say “Yes, that looks different from that.” When asked “How are they different?”, all we can say is “Well, that one&#8217;s red, and that one&#8217;s green”. We have access to nothing more - we can simply make raw distinctions based on pattern - and it seems very strange.</p>
<p>So this is why conscious experience seems strange. We are able to make distinctions, but we have direct access neither to the sources of those distinctions, or to how we make the distinctions. The distinctions are based purely on the information that is processed. Incidentally, it seems that the more abstract the information-processing - that is, the more that distinctions are collapsed, and information recoded - the stranger the conscious experience seems. Shape- perception, for instance, strikes us as relatively non-strange; the visual system is extremely good at preserving shape information through its neural pathways. Color and taste are strange indeed, and the processing of both seems to involve a considerable amount of recoding.</p>
<p>The story for “internal perception” is exactly the same. When we reflect on our thoughts, information makes its way from one part of the brain to another, and perhaps eventually to our speech center. It is to only certain abstract features of brain structure that the process is sensitive. (One might imagine that if somehow reflection could be sensitive to every last detail of brain structure, it would seem very different.) Again, we can perceive only via pattern, via information. The brute, seemingly non-concrete distinctions thus entailed are extremely difficult for us to understand, and to articulate. That is why consciousness seems strange, and that is why the debate over the Mind-Body Problem has raged for thousands of years.</p></blockquote>
<p>A related idea, cast in terms of neural networks, can be found in <a href="http://link.springer.com/chapter/10.2991%2F978-94-91216-62-6_15">Loosemore (2012)</a>, which <a href="http://reducing-suffering.org/hard-problem-consciousness/">Tomasik (2014)</a> summarizes like this:</p>
<blockquote><p>Loosemore presents what I consider a biologically plausible sketch of connectionist concept networks in which a concept&#8217;s meaning is assessed based on related concepts. For instance, “chair” activates “legs”, “back”, “seat”, “sitting”, “furniture”, etc. (p. 294). As we imagine lower-level concepts, the associations that get activated become more primitive. At the most primitive level, we could ask for the meaning of something like “red”. Since our “red” concept node connects directly to sensory inputs, we can&#8217;t decompose “red” into further understandable concepts. Instead, we “bottom out” and declare “red” to be basic and ineffable. But our concept-analysis machinery still claims that “red” is <em>something</em> — namely, some additional property of experience. This leads us to believe in qualia as “extra” properties that aren&#8217;t reducible.</p></blockquote>
</li>
<li class="footnote" id="footnote266_xu73wb6"><a class="footnote-label" href="#footnoteref266_xu73wb6">266.</a> I quote the relevant passage below, though it is likely hard to follow without first reading the rest of <a href="http://www.ingentaconnect.com/content/imp/jcs/2003/00000010/F0020004/1350">Sloman &amp; Chrisley (2003)</a>:<br /><blockquote><p>…we explain qualia by providing an explanation of <em>the phenomena that generate philosophical thinking of the sort found in discussions of qualia</em>. Note that we are not talking merely about explaining behaviour, for we have repeatedly discussed explanations of how internal, possibly externally undetectable, states and processes can occur in certain virtual machine architectures.</p>
<p>The concept of ‘qualia’ arose out of philosophical discussions of our ability to attend to aspects of internal information processing (internal self-awareness). That possibility is inherent in any system that has the H-CogAff architecture (see section V.5), though different varieties of the phenomenon will be present in different architectures, depending on the forms of representation and modes of monitoring available to meta-management. Some forms will provide the ability to attend not only to what is perceived in the environment, but to also features of the mode of perception that are closely related to properties of intermediate sensory data-structures…</p>
<p>Consider perceiving a table. Most adults (though not young children) can attend not only to the table and its fixed 3-D shape, but also to the 2-D appearance of the table in which angles and relative lengths of lines change as you change your viewpoint (or the table is rotated…). The appearance can also change as you squint, tap your eyeball, put on coloured spectacles, etc. This is exactly the sort of thing that led philosophers (and others) to think about qualia (previously called ‘sense data’) as something internal, non-physical, knowable only from inside, etc. If meta-management processes have access to intermediate perceptual states, then this can produce self-monitoring of sensory contents, leading robot philosophers with this architecture to discover ‘the problem(s) of qualia’. And the same would go for anything which has that architecture: six robots with the H-CogAff architecture discussing various aspects of their experience of the same table seen from different viewpoints could get bogged down discussing consciousness, just like six blind philosophers.</p>
<p style="padding-left: 50pt; padding-right: 50pt;"><strong>What qualia are</strong>: qualia are what humans or future human-like robots refer to when referring to the objects of internal self observation.</p>
<p>…there is another way in which an information processing system can refer to its own states, which explain some aspects of notions of qualia.</p>
<p>Concept formation is a huge topic, but, for now, consider the likelihood that in many organisms there are processes of concept formation which emerge from interactions between a self-organising classification system and the information fed into it. A well known example of a mechanism that can achieve this is a Kohonen net (Kohonen, 1989). We describe as ‘architecture-driven’ sets of concepts created within an architecture as part of the individual history of the organism or machine. If individual A1 develops its own concepts used to describe internal states of another agent A2 on the basis of assumptions about the information processing architecture of A2, then the concepts are architecture-driven in relation to A1, and architecture-based in relation to A2, or any other system with the assumed architecture. It is possible for A1 to use architecture-driven architecture-based concepts to refer to itself. Architecture-driven concepts can refer to many different sorts of things, e.g. colours and shapes of objects in the environment, tastes, tactile qualities of objects, etc., if the concepts are developed by an organism on the basis of perceptual experience of those objects.</p>
<p>…Now suppose that an agent A with a meta-management system… uses a self-organising process to develop concepts for categorising its own internal virtual machine states as sensed by internal monitors. These will be architecture-driven concepts, but need not be architecture-based if the classification mechanism does not use an implicit or explicit theory of the architecture of the system it is monitoring, but merely develops a way of organising its ‘sensory’ input data. If such a concept C is applied by A to one of its internal states, then the only way C can have meaning for A is in relation to the set of concepts of which it is a member, which in turn derives only from the history of the self-organising process in A…</p>
<p>This means that if two agents A and B have each developed concepts in this way, then if A uses its concept Ca, to think the thought ‘I am having experience Ca’, and B uses its concept Cb, to think the thought ‘I am having experience Cb’, the two thoughts are intrinsically private and incommunicable, even if A and B actually have exactly the same architecture and have had identical histories leading to the formation of structurally identical sets of concepts. A can wonder: ‘Does B have an experience described by a concept related to B as my concept Ca is related to me?&#8221; But A cannot wonder ‘Does B have experiences of type Ca’, for it makes no sense for the concept Ca to be applied outside the context for which it was developed, namely one in which A&#8217;s internal sensors classify internal states. They cannot classify states of B.</p>
<p>When different agents use architecture-driven concepts, produced by self organising classifiers, to classify internal states of a virtual machine, and are not even partly explicitly defined in relation to some underlying causes (e.g. external objects or a presumed architecture producing the sensed states), then there is nothing to give those concepts any user-independent content, in the way that our colour words have user-independent content because they refer to properties of physical objects in a common environment. Thus self-referential architecture-driven concepts used by different individuals are strictly non-comparable: not only can you not know whether your concepts are the same as mine, the question is incoherent. If we use the word ‘qualia’ to refer to the virtual machine states or entities to which these concepts are applied, then asking whether the qualia in two experiencers are the same would then would be analogous to asking whether two spatial locations in different frames of reference are the same, when the frames are moving relative to each other. But it is hard to convince some people that this makes no sense, because the question is grammatically well-formed. Sometimes real nonsense is not obvious nonsense.</p>
<p>We have now indicated how the process of coming to think about and ask questions about qualia is explained by the nature of the architecture of the thinker. The process arises when architecture-driven concepts produced by a self-monitoring sub-system refer internally. In talking about hese concepts we are using architecture-based concepts.</p></blockquote>
<p>See also <a href="http://sro.sussex.ac.uk/64774/">Sloman &amp; Chrisley (2016)</a>.</p>
</li>
<li class="footnote" id="footnote267_2yux6es"><a class="footnote-label" href="#footnoteref267_2yux6es">267.</a> As <a href="https://plato.stanford.edu/entries/consciousness-higher/">Carruthers (2016)</a> puts it, conscious states will seem “ineffable” or “indescribable” because those states<br /><blockquote><p>…have fine-grained contents that can slip through the mesh of any conceptual net. We can always distinguish many more shades of red than we have concepts for, or could describe in language (other than indexically — e.g., ‘<em>That</em> shade’).</p></blockquote>
</li>
<li class="footnote" id="footnote268_a97p3s7"><a class="footnote-label" href="#footnoteref268_a97p3s7">268.</a> <a href="http://analysis.oxfordjournals.org/content/29/2/48.extract">Armstrong (1968)</a>:<br /><blockquote><p>To produce [the “headless woman”] illusion, a woman is placed on a suitably illuminated stage with a dark background and a black cloth is placed over her head. It looks to the spectators as if she has no head. The spectators cannot see the woman&#8217;s head. But they gain the impression that they can see that the woman has not got a head. (<em>Cf</em>. ‘I looked inside, and saw that he was not there.’) Unsophisticated spectators might conclude that the woman did not in fact have a head.</p>
<p>What the example shows is that, in certain cases, it is very natural for human beings to pass from something that is true: ‘I do not perceive that X is Y’, to something that may be false: ‘I perceive that X is not Y’. We have here one of those unselfconscious and immediate movements of the mind of which Hume spoke, and which he thought to be so important in our mental life.</p>
<p>It can now be suggested by the Materialist that we tend to pass from something that is true:</p>
<p style="padding-left: 50pt; padding-right: 50pt;">I am not introspectively aware that mental images are brain-processes</p>
<p>to something that is false:</p>
<p style="padding-left: 50pt; padding-right: 50pt;">I am introspectively aware that mental images are not brain-processes.</p>
<p>…Does ordinary experience, then, involve the illusion of the truth of anti-materialism? The Materialist can now admit that it does involve such an illusion, but urge that the illusion is no more than the illusion involved in the “headless woman”: the taking of an absence of awareness of X to be an awareness of the absence of X.</p></blockquote>
<p>See also <a href="http://www.tandfonline.com/doi/abs/10.1080/00048400600758912">Smart (2006)</a>, a kind of follow-up to Armstrong&#8217;s article.</p>
</li>
<li class="footnote" id="footnote269_s7fso4g"><a class="footnote-label" href="#footnoteref269_s7fso4g">269.</a> Kammerer&#8217;s explanation of his “theoretical introspection hypothesis” (TIH) is several pages long (and very much worth reading), but <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00020">Frankish (2016c)</a> provides a simplified but helpful summary:<br /><blockquote><p>Introspection is informed by an innate and modular theory of mind and epistemology, which states that (a) we acquire perceptual information via mental states — experiences — whose properties determine how the world appears to us, and (b) experiences can be fallacious, a fallacious experience of A being one in which we are mentally affected in the same way as when we have a veridical experience of A, except that A is not present.</p></blockquote>
<p>Here, I&#8217;ll interject to add that Kammerer does not require that our innate “theories” of mind and epistemology (which inform our introspection) represent or “state” (a) and (b). For example, one could give a dispositionalist account of these innate theories of mind and epistemology which nevertheless roughly capture statements (a) and (b). If anything like Kammerer&#8217;s account is true, I would (personally) expect it to be a dispositionalist account. Anyway, back to Frankish&#8217;s summary:</p>
<blockquote><p>Given this theory, Kammerer notes, it is incoherent to suppose that we could have a fallacious experience [i.e. an illusory experience] of an experience, E. For that would involve being mentally affected in the same way as when we have a veridical experience of E, without E being present. But when we are having a veridical experience of E, we are having E (otherwise the experience wouldn&#8217;t be veridical). So, if we are mentally affected in the same way as when we are having a veridical experience of E, then we are having E. So E is both present and not present, which is contradictory…</p>
<p>Kammerer proposes that this explains the peculiar hardness of the illusion problem. The illusionist thesis cannot be coherently articulated using our everyday concept of illusion, which is rooted in our naïve concept of fallacious experience. Moreover, if the naïve theory Kammerer sketches does inform our introspective activity, then we shall not be able to form any imaginative conception of what it would be like for illusionism to be true. Hence the common claim that, where consciousness is concerned, appearance is reality. As Kammerer stresses, this does not mean that illusionism actually is incoherent. It simply means that in order to state it we must employ a technical concept of illusion — as, say, a cognitively impenetrable, non-veridical mental representation that is systematically generated in certain circumstances.</p></blockquote>
<p>Frankish notes that one might develop a similar illusionist account of our sense that introspective acquaintance is “direct”:</p>
<blockquote><p>Of course, even if Kammerer is right about the source of our intuitive resistance to illusionism, this would not show that illusionism is true, though it would help to dispel one common objection to it. Realists will say that phenomenality is not an illusion even in a technical sense: our relation to our phenomenal properties is one of direct acquaintance, which does not depend on potentially fallible representational processes. Perhaps Kammerer could employ the strategy again here, arguing that our concept of introspective acquaintance is also a theoretical one.</p></blockquote>
<p>Kammerer&#8217;s account depends on a “theory theory” of introspection, according to which introspection interprets its (mental) objects through a theory or theories, e.g. a theory of mind. Kammerer&#8217;s example of such a “theory theory” of introspection is that of <a href="https://books.google.com/books?id=8AHDK62jPSgC&amp;lpg=PR5&amp;pg=PA157#v=onepage&amp;q&amp;f=false">Nichols &amp; Stich (2003)</a>, illustrated with “boxological” diagrams like this:</p>
<p><img src="/files/Research/Moral_Patienthood/NicholsStichBoxological.png" /></p>
<p>Personally, I expect introspection will turn out to be a big mess of competing processes, <em>a la</em> <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=rQlpAgAAQBAJ&amp;oi=fnd&amp;pg=PA29#v=onepage&amp;q&amp;f=false">Schwitzgebel (2012)</a>:</p>
<blockquote><p>My thesis is: introspection is not a single process but a plurality of processes. It&#8217;s a plurality both <em>within</em> and <em>between</em> cases: most individual introspective judgments arise from a plurality of processes (that&#8217;s the within-case claim), and the collection of processes issuing in introspective judgments differs from case to case (that&#8217;s the between-case claim). Introspection is not the operation of a single cognitive mechanism or small collection of mechanisms. Introspective judgments arise from a shifting confluence of many processes, recruited opportunistically.</p>
<p>The following analogy might be helpful. Suppose you&#8217;re at a psychology conference or a high school science fair and you&#8217;re trying to quickly take in a poster. You are not equipped with a dedicated faculty of poster-taking-in. Rather, you opportunistically deploy a variety of processes with the aim of getting the gist of the poster: you look at the poster-or perhaps only listen to a recital of portions of it, if you&#8217;re in the mood or visually impaired-you attend to what the poster&#8217;s author is saying about it; you follow out implications, charitably rejecting some interpretations of the poster&#8217;s content as too obviously foolish; you think about what it makes sense to claim given the social and scientific context and other work by the author or the author&#8217;s advisor, if you know any; you pose questions and assess the author&#8217;s responses both for overt content and for emotional flavor. Although the cognitive systems involved range widely and are not dedicated just to taking in posters, not just any activity counts as taking in a poster-one&#8217;s judgments about the poster must aim to reflect a certain kind of sensitivity to its contents. Likewise for introspection, I will suggest: the cognitive activities range widely and vary between cases-that is the main claim I will defend-and yet, as I will suggest near the end of this essay, it wouldn&#8217;t be natural to call a judgment introspective if it weren&#8217;t formed with the aim or intention of reflecting a certain kind of sensitivity to the target mental state.</p></blockquote>
<p>Schwitzgebel illustrates his own theory of introspection like this:</p>
<p><img src="/files/Research/Moral_Patienthood/SchwitzgebelIntrospection.png" /></p>
<p>If one has a Schwitzgebel-like model of how introspection works, this poses a challenge to a Kammerer-like explanation of why it seems to us that consciousness, uniquely, cannot be an illusion. However, it could still be the case that something like Kammerer&#8217;s account is an important <em>piece</em> of the “big mess” of introspection, and thereby goes a long way toward explaining what it aims to explain concerning phenomenal consciousness.</p>
</li>
<li class="footnote" id="footnote270_bzxzksq"><a class="footnote-label" href="#footnoteref270_bzxzksq">270.</a> By “toy program,” I have in mind something perhaps 5x-20x as large and complicated as Brian Tomasik&#8217;s “<a href="http://reducing-suffering.org/simple-program-illustrate-hard-problem-consciousness/">Simple Program to Illustrate the Hard Problem of Consciousness</a>.”
</li>
<li class="footnote" id="footnote271_7umxj8k"><a class="footnote-label" href="#footnoteref271_7umxj8k">271.</a> See <a href="https://arxiv.org/abs/1610.08602">Kotseruba et al. (2016)</a> for an overview of cognitive architectures.
</li>
<li class="footnote" id="footnote272_ec4fazw"><a class="footnote-label" href="#footnoteref272_ec4fazw">272.</a> If these first three steps of the project were described in a book, the structure of the exposition might be similar to that of <a href="https://books.google.com/books?id=7w6IYeJRqyoC&amp;printsec=frontcover&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjLkMuXsYvQAhWHllQKHXPEAlkQ6AEIHTAA#v=onepage&amp;q&amp;f=falseÎ">Baars (1988)</a>, chs. 2-9 — but much longer, and with links to open-source code for every version of the program/model.
</li>
<li class="footnote" id="footnote273_4irlfy6"><a class="footnote-label" href="#footnoteref273_4irlfy6">273.</a> Another caveat about this project concerns the <em>moral</em> implications of trying to build potentially-conscious machines (<a href="http://www.basicbooks.com/full-details?isbn=9780465020690">Metzinger 2010</a>, pp. 194-196):<br /><blockquote><p>Imagine you are a member of an ethics committee considering scientific grant applications. One says:</p>
<p style="padding-left: 50pt; padding-right: 50pt;">We want to use gene technology to breed mentally retarded human infants. For urgent scientific reasons, we need to generate human babies possessing certain cognitive, emotional, and perceptual deficits. This is an important and innovative research strategy, and it requires the controlled and reproducible investigation of the retarded babies’ psychological development after birth. This is not only important for understanding how our own minds work but also has great potential for healing psychiatric diseases. Therefore, we urgently need comprehensive funding.</p>
<p>No doubt you will decide immediately that this idea is not only absurd and tasteless but also dangerous. One imagines that a proposal of this kind would not pass any ethics committee in the democratic world. The point of this thought experiment, however, is to make you aware that the unborn [conscious machines] of the future would have no champions on today&#8217;s ethics committees. The first machines satisfying a minimally sufficient set of conditions for conscious experience and selfhood would find themselves in a situation similar to that of the genetically engineered retarded human infants. Like them, these machines would have all kinds of functional and representational deficits — various disabilities resulting from errors in human engineering. It is safe to assume that their perceptual systems — their artificial eyes, ears, and so on—would not work well in the early stages. They would likely be half-deaf, half-blind, and have all kinds of difficulties in perceiving the world and themselves in it — and if they were true [conscious machines], they would, <em>ex hypothesi</em>, also be able to suffer.</p>
<p>If they had a stable bodily self-model, they would be able to feel sensory pain as their own pain. If their postbiotic self-model was directly anchored in the low-level, self-regulatory mechanisms of their hardware — just as our own emotional self-model is anchored in the upper brainstem and the hypothalamus — they would be consciously feeling selves. They would experience a loss of homeostatic control as painful, because they had an inbuilt concern about their own existence. They would have interests of their own, and they would subjectively experience this fact. They might suffer emotionally in qualitative ways completely alien to us or in degrees of intensity that we, their creators, could not even imagine. In fact, the first generations of such machines would very likely have many negative emotions, reflecting their failures in successful self-regulation because of various hardware deficits and higher-level disturbances. These negative emotions would be conscious and intensely felt, but in many cases we might not be able to understand or even recognize them.</p>
<p>Take the thought experiment a step further. Imagine these postbiotic [conscious machines] as possessing a cognitive self-model — as being intelligent thinkers of thoughts. They could then not only conceptually grasp the bizarreness of their existence as mere objects of scientific interest but also could intellectually suffer from knowing that, as such, they lacked the innate “dignity” that seemed so important to their creators. They might well be able to consciously represent the fact of being only second-class sentient citizens, alienated postbiotic selves being used as interchangeable experimental tools. How would it feel to “come to” as an advanced artificial subject, only to discover that even though you possessed a robust sense of selfhood and experienced yourself as a genuine subject, you were only a commodity?</p>
<p>The story of the first artificial [conscious machines], those postbiotic phenomenal selves with no civil rights and no lobby in any ethics committee, nicely illustrates how the capacity for suffering emerges along with the phenomenal [self]… It also presents a principled argument against the creation of artificial consciousness as a goal of academic research…</p></blockquote>
<p>This kind of worry might constitute another reason to leave some parts of the cognitive architecture as “black boxes,” and never actually <em>run</em> the cognitive architecture.</p>
</li>
<li class="footnote" id="footnote274_tqcwgzk"><a class="footnote-label" href="#footnoteref274_tqcwgzk">274.</a> But see also <a href="https://global.oup.com/academic/product/basic-vision-9780199572021?cc=us&amp;lang=en&amp;">Snowden et al. (2012), ch. 11; </a><a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199686858.001.0001/oxfordhb-9780199686858-e-027?mediaType=Article">Goodale &amp; Ganel (2016)</a>; <a href="http://www.nature.com/nrn/journal/v12/n4/abs/nrn3008.html">Kravitz et al. (2011)</a>. For wide-ranging discussions of this topic, see the essays in <a href="https://global.oup.com/academic/product/perception-action-and-consciousness-9780199551118?cc=us&amp;lang=en&amp;">Gangopadhyay et al. (2010)</a>.
</li>
<li class="footnote" id="footnote275_qi2m5ea"><a class="footnote-label" href="#footnoteref275_qi2m5ea">275.</a> G&amp;M-13, ch. 4. For more on the evolution of visual systems, see <a href="https://global.oup.com/academic/product/the-visual-brain-in-action-9780198524724?cc=us&amp;lang=en&amp;">Milner &amp; Goodale (2006)</a>, ch. 1.
</li>
<li class="footnote" id="footnote276_dckieen"><a class="footnote-label" href="#footnoteref276_dckieen">276.</a> This quote from <a href="https://global.oup.com/academic/product/the-visual-brain-in-action-9780198524724?cc=us&amp;lang=en&amp;">Milner &amp; Goodale (2006)</a>, p. 65.<br /><br />
Another piece of evidence sometimes cited (e.g. by <a href="https://www.ted.com/talks/daniel_wolpert_the_real_reason_for_brains">Wolpert 2011</a>) in favor of the view that brains are primarily for controlling behavior is the fact that a tunicate (“sea squirt”), upon swimming to and attaching itself to a suitable rock (and thus no longer need to plan and control its movement), digests its own brain. (<a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">Dennett 1991</a>, p. 177, humorously remarks: “When it finds its spot and takes root, it doesn&#8217;t need its brain anymore, so it eats it! It&#8217;s rather like getting tenure.”)<br /><br />
Wolpert and Dennett don&#8217;t cite any sources, but see e.g. <a href="http://www.nrcresearchpress.com/doi/abs/10.1139/z04-177">Mackie &amp; Burighel (2005)</a>, p. 169; <a href="http://az.oxfordjournals.org/content/22/4/817.abstract">Cloney (1982)</a>. 
</li>
<li class="footnote" id="footnote277_f509crk"><a class="footnote-label" href="#footnoteref277_f509crk">277.</a> Not her real name. The next several paragraphs draw from, and quote from, G&amp;M-13, ch. 1
</li>
<li class="footnote" id="footnote278_hff3xin"><a class="footnote-label" href="#footnoteref278_hff3xin">278.</a> G&amp;M-13, Figure 1.3.
</li>
<li class="footnote" id="footnote279_48ykgpy"><a class="footnote-label" href="#footnoteref279_48ykgpy">279.</a> Technically, G&amp;M-13 doesn&#8217;t say whether it was G&amp;M, or some other experimenters, who showed Dee the flashlight, and I haven&#8217;t bothered to find out for sure. For simplicity, I&#8217;ve simply guessed that it was G&amp;M who did this.
</li>
<li class="footnote" id="footnote280_auydx0l"><a class="footnote-label" href="#footnoteref280_auydx0l">280.</a> G&amp;M-13, Figure 1.4.
</li>
<li class="footnote" id="footnote281_2e18916"><a class="footnote-label" href="#footnoteref281_2e18916">281.</a> G&amp;M-13, Figure 1.9.
</li>
<li class="footnote" id="footnote282_ahginma"><a class="footnote-label" href="#footnoteref282_ahginma">282.</a> G&amp;M-13, Figure 1.5.
</li>
<li class="footnote" id="footnote283_aun5sce"><a class="footnote-label" href="#footnoteref283_aun5sce">283.</a> For the next several passages, I am now following the discussion in, and quoting from, G&amp;M-13, ch. 2.
</li>
<li class="footnote" id="footnote284_zs31dq4"><a class="footnote-label" href="#footnoteref284_zs31dq4">284.</a> This wasn&#8217;t a deficit in Dee&#8217;s ability to rotate the card. G&amp;M-13 report:<br /><blockquote><p>We were able to rule out that possibility by asking her to <em>imagine</em> a slot at different orientations. Once she had done this, she had no difficulty rotating the card to show us the orientation she&#8217;d been asked to imagine. It was only when she had to look at a real slot and match its orientation that her deficit appeared.</p></blockquote>
</li>
<li class="footnote" id="footnote285_1qfru7p"><a class="footnote-label" href="#footnoteref285_1qfru7p">285.</a> G&amp;M-13, Figure 2.2.
</li>
<li class="footnote" id="footnote286_swlt5r3"><a class="footnote-label" href="#footnoteref286_swlt5r3">286.</a> G&amp;M-13, Figure 2.3.
</li>
<li class="footnote" id="footnote287_h53ipgd"><a class="footnote-label" href="#footnoteref287_h53ipgd">287.</a> Of course for a square Efron block, width-wise and length-wise are the same.
</li>
<li class="footnote" id="footnote288_9ihy2an"><a class="footnote-label" href="#footnoteref288_9ihy2an">288.</a> <a href="http://www.tandfonline.com/doi/abs/10.1080/13554790008402753">Heider (2000)</a>. For a detailed account of another patient, “John,” with an overlapping but non-identical set of symptoms, see <a href="https://www.routledge.com/A-Case-Study-in-Visual-Agnosia-Revisited-To-see-but-not-to-see/Humphreys-Riddoch/p/book/9781848720732">Humphreys &amp; Riddoch (2013)</a>.
</li>
<li class="footnote" id="footnote289_6rgw9me"><a class="footnote-label" href="#footnoteref289_6rgw9me">289.</a> G&amp;M-13, ch. 3.
</li>
<li class="footnote" id="footnote290_sitc56w"><a class="footnote-label" href="#footnoteref290_sitc56w">290.</a> This section draws from, and quotes from, G&amp;M-13, ch. 4.
</li>
<li class="footnote" id="footnote291_zmn7hgf"><a class="footnote-label" href="#footnoteref291_zmn7hgf">291.</a> This section draws from G&amp;M-13, chs. 7-8.
</li>
<li class="footnote" id="footnote292_xmiuuqj"><a class="footnote-label" href="#footnoteref292_xmiuuqj">292.</a> The “virtual workbench” image is used with permission from MIT Press. The full citation is:
<p>Y. Hu and M. A. Goodale, “Grasping after a Delay Shifts Size-Scaling from Absolute to Relative Metrics,” Journal of Cognitive Neuroscience, 12:5 (September, 2000), pp. 8556-868. © 2000 by the Massachusetts Institute of Technology, published by the MIT Press. <a href="http://www.mitpressjournals.org/doi/abs/10.1162/089892900562462">http://www.mitpressjournals.org/doi/abs/10.1162/089892900562462</a></p>
</li>
<li class="footnote" id="footnote293_mrxi055"><a class="footnote-label" href="#footnoteref293_mrxi055">293.</a> The image is G&amp;M-13, Figure 8.1.
</li>
<li class="footnote" id="footnote294_8g9qprs"><a class="footnote-label" href="#footnoteref294_8g9qprs">294.</a> See G&amp;M-13, ch. 8; <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199686858.001.0001/oxfordhb-9780199686858-e-027?mediaType=Article">Goodale &amp; Ganel (2016)</a>.
</li>
<li class="footnote" id="footnote295_kme01s6"><a class="footnote-label" href="#footnoteref295_kme01s6">295.</a> Here again, I&#8217;m quoting from G&amp;M-13, ch. 4.
</li>
<li class="footnote" id="footnote296_8zbtbjf"><a class="footnote-label" href="#footnoteref296_8zbtbjf">296.</a> For a review, see <a href="https://global.oup.com/academic/product/the-visual-brain-in-action-9780198524724?cc=us&amp;lang=en&amp;">Milner &amp; Goodale (2006)</a>, pp. 42-66, plus a few updates in ch. 8.<br /><br />
There is some neurophysiological evidence against G&amp;M&#8217;s proposed functional dissociation, though (<a href="http://booksandjournals.brillonline.com/content/journals/10.1163/187847510x503588">Cardoso-Leite &amp; Gorea 2010</a>):
<blockquote><p>…accumulating neurophysiological evidence was also pointing to many instances where neurons and cortical sites in the ventral and dorsal streams behave contrary to predictions of the dissociation theory. For example, both neurophysiological and neuroimaging studies show evident dorsal stream responsiveness to stimulus features supposed to be processed in the ventral stream such as shape (e.g., Konen and Kastner, 2008; Lehky and Sereno, 2007) and color (e.g., Claeys et al., 2004; Toth and Assad, 2002). Equivalently some prototypical dorsal processing features such as motion are equally well processed in the ventral stream (e.g., Gur and Snodderly, 2007). Also, while the temporal processing characteristics of the two streams have also been cited in favor of their functional dissociation (with magnocellular neurons in dorsal areas responding earlier to visual stimulation than the parvocellular neurons in the ventral stream; e.g., Nowak and Bullier, 1997; Rossetti et al., 2003), the significance of such latency differences has been obscured by numerous reports that visual information processing is not strictly feedforward (as supposed in the classic view) so that frontal areas may respond to visual stimuli at about the same time as V1 (Lamme and Roelfsema, 2000; Schmolesky et al., 1998; Zanon et al., 2009). Hence, efferent signals from the frontal cortex may modulate processing in both the dorsal and ventral extrastriate areas (Moore and Armstrong, 2003; Moore and Fallah, 2001, 2004).</p></blockquote>
</li>
<li class="footnote" id="footnote297_x1pahci"><a class="footnote-label" href="#footnoteref297_x1pahci">297.</a> Single-cell recordings currently require cutting a hole in the skull, and are thus only considered for humans in cases when a hole in the skull must be made for clinical reasons.
</li>
<li class="footnote" id="footnote298_h7dedbw"><a class="footnote-label" href="#footnoteref298_h7dedbw">298.</a> For reviews, see Quian Quiroga (<a href="http://www.nature.com/nrn/journal/v13/n8/abs/nrn3251.html">2012</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1111/joa.12228/full">2014</a>).
</li>
<li class="footnote" id="footnote299_sse47j3"><a class="footnote-label" href="#footnoteref299_sse47j3">299.</a> See e.g. <a href="https://global.oup.com/academic/product/the-visual-brain-in-action-9780198524724?cc=us&amp;lang=en&amp;">Milner &amp; Goodale (2006)</a>, ch. 8.
</li>
<li class="footnote" id="footnote300_x4ytrze"><a class="footnote-label" href="#footnoteref300_x4ytrze">300.</a> For example partial critiques of the two streams theory, see <a href="http://onlinelibrary.wiley.com/doi/10.1111/cogs.12226/full">Briscoe &amp; Schwenkler (2015)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S1364661316301206">Freud et al. (2016)</a>; <a href="http://booksandjournals.brillonline.com/content/journals/10.1163/187847510x503588">Cardoso-Leite &amp; Gorea (2010)</a>; <a href="http://www.tandfonline.com/doi/abs/10.1080/17588920903388950">Schenk &amp; McIntosh (2009)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0028393208004259">Clark (2009)</a>; <a href="http://link.springer.com/referenceworkentry/10.1007/978-1-4614-6675-8_316">Gorea (2015)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/mila.12082/full">Shepherd (2015)</a>, sec. 4.1.
</li>
<li class="footnote" id="footnote301_t0kh11h"><a class="footnote-label" href="#footnoteref301_t0kh11h">301.</a> <a href="http://onlinelibrary.wiley.com/doi/10.1111/cogs.12226/full">Briscoe &amp; Schwenkler (2015)</a>; <a href="http://booksandjournals.brillonline.com/content/journals/10.1163/187847510x503588">Cardoso-Leite &amp; Gorea (2010)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0028393208004259">Clark (2009)</a>.
</li>
<li class="footnote" id="footnote302_83qfztk"><a class="footnote-label" href="#footnoteref302_83qfztk">302.</a> <a href="http://www.sciencedirect.com/science/article/pii/S1364661316301206">Freud et al. (2016)</a>; <a href="http://booksandjournals.brillonline.com/content/journals/10.1163/187847510x503588">Cardoso-Leite &amp; Gorea (2010)</a>.
</li>
<li class="footnote" id="footnote303_3u7kebd"><a class="footnote-label" href="#footnoteref303_3u7kebd">303.</a> E.g. see <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1747-9991.2009.00275.x/abstract">Klein (2010)</a>.
</li>
<li class="footnote" id="footnote304_bk7yeti"><a class="footnote-label" href="#footnoteref304_bk7yeti">304.</a> E.g. see <a href="http://www.pnas.org/content/113/28/7900.full">Eklund et al. (2016)</a>.
</li>
<li class="footnote" id="footnote305_wjmgiyo"><a class="footnote-label" href="#footnoteref305_wjmgiyo">305.</a> <a href="http://www.sciencedirect.com/science/article/pii/S0304394001025848">Sneddon (2002)</a>, <a href="http://rspb.royalsocietypublishing.org/content/270/1520/1115">Sneddon et al. (2003)</a>, and <a href="http://www.sciencedirect.com/science/article/pii/S000689930701582X">Ashley et al. (2007)</a> for rainbow trout, and <a href="http://www.sciencedirect.com/science/article/pii/S0168159111003285">Gentle (2011)</a> and <a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118999196.ch37/summary">Egger et al. (2014)</a> for chickens. On chicken behavior and cognition more generally, see <a href="http://link.springer.com/article/10.1007/s10071-016-1064-4">Marino (2017)</a> and <a href="http://www.cabi.org/bookshop/book/9781780642499">Nicol (2015)</a>.
</li>
<li class="footnote" id="footnote306_9nw7gdb"><a class="footnote-label" href="#footnoteref306_9nw7gdb">306.</a> For convenience, I quote below some sections of the paper that describe key neuroanatomical differences in phylogeny, and explain (in brackets) a few especially important but perhaps unfamiliar terms:<br /><blockquote><p>One of the most important terms in understanding the evolution of the brain is ‘homology’. Two structures are homologous if they can be traced to a common ancestor… When we seek to understand the evolution of the brain, we identify homologies and draw conclusions about what has changed or remained the same, based on those homologies. For example, the cerebellum of all fishes and tetrapods [amphibians, reptiles, birds, and mammals] is considered to be homologous, because there are similarities of origin, structure and function. In contrast, the cerebral cortex exists in its present form only in mammals…</p>
<p>	Among the many phyla of invertebrates, two major groups can be distinguished: those with radial symmetry and those with bilateral symmetry. These groups differ fundamentally in the structure of their nervous systems. Radially symmetrical organisms evolved earlier and their nervous systems are net-like, with rings of neurons and typically no concentration of nerve cells in one place. In contrast, bilaterally symmetrical organisms tend to have concentrations of neurons in the head region that are often called ganglia but resemble the brains of vertebrates…</p>
<p>	Bilaterally symmetrical invertebrates are very numerous, and their nervous systems vary from simple to complex…</p>
<p>	More complex nervous systems are found in the [flatworms and segmented worms], which have a pair of nerves running the length of their bodies connected at each segment in a ladder-like arrangement and paired head ganglia at the anterior end. Among the most complex brains in invertebrates are the insects and crustaceans… and the octopi… Their nervous systems consist of paired ganglia at each segment and, in the head, fused paired ganglia forming the brain. The brain consists of multiple lobes, as many as 40 in octopi…</p>
<p>	The earliest vertebrates still extant today are the jawless fishes, cyclostomes and hagfish. The brains of these animals possess all of the characters of vertebrates, including five divisions of the brain: medulla oblongata and pons (together called the hindbrain), midbrain, diencephalon and telencephalon. (Lampreys lack a cerebellum, and its presence is debated in hagfish, but it is present in all other groups.)…</p>
<p>	In all vertebrates, the medulla oblongata is the caudalmost [toward the tail] region of the hindbrain…</p>
<p>	Major sensory and motor tracts that connect the brain and spinal cord run through the medulla. These tracts are more extensive in mammals than in nonmammalian vertebrates: nonmammals do not have corticospinal tracts (because they have no neocortex) although some descending tracts run from the telencephalon to at least the hindbrain in birds…</p>
<p>	Whether the cerebellum exists in hagfish is debated, and it is absent in lampreys…, but appears as a well-developed structure in chondrichthyes, the cartilaginous fishes, and in all gnathostomes (jawed vertebrates). It has been proposed that the cerebellum evolved by duplication of cerebellum-like structures in the dorsolateral wall of the hindbrain in cartilaginous fishes, which receive input from the lateral line system and the electrosensory system… </p>
<p>	A cerebellum with similar cells and circuits and layers is present in other vertebrates, but has expanded independently in the taxa with elaborate cerebella. Sharks, fishes and birds, as well as mammals, possess an elaborate cerebellum with multiple lobes. In each group and also in the simpler cerebellums of cyclostomes, amphibians and nonavian reptiles, a cerebellar cortex is found, with the same three layers found in mammals…</p>
<p>	The roof of the midbrain forms sensory centres in all vertebrates. In mammals, these are termed the superior (vision) and inferior (auditory) colliculi. The homologous structures in nonmammals are the optic tectum and the torus semicircularis, respectively. The optic tectum or superior colliculus is laminated, and it receives retinal input to its superficial layers and auditory, somatosensory, and where present, electrosensory input to its deep layers. In animals that rely heavily on vision, such as birds, however, the optic tectumis larger and more elaborate, suggesting that its most important functions are with vision…</p>
<p>	The forebrain (diencephalon and cerebral hemispheres) is the most diverse portion of the vertebrate brain. Certain general principles apply to its organisation, but wide variation in structure is seen. In all vertebrates, the diencephalon consists of four divisions from dorsal to ventral: the epithalamus, dorsal thalamus, ventral thalamus and hypothalamus. The dorsal thalamus and, in some organisms, the ventral thalamus and/or hypothalamus relay sensory information to the telencephalon. The epithalamus and the hypothalamus function in the regulation of visceral functions, including reproduction, circadian rhythms, and sleep and waking…</p>
<p>	The cerebral hemispheres, or telencephalon, in all vertebrates can be divided into dorsal [toward the animal&#8217;s back] and ventral [toward the animal&#8217;s front] parts, called pallial and subpallial. In mammals, pallial regions can be subdivided into the olfactory bulb and associated cortex (lateral pallium), neocortex (dorsal pallium), hippocampus (medial pallium), and claustrum and pallial amygdala (or ventral pallium). Subpallial regions become the basal ganglia and some limbic regions. In most nonmammals, the medial pallium is recognised as the equivalent of the hippocampus. The lateral pallium is also recognised as the olfactory cortex. But the identity of the dorsal pallium and the equivalent structures is controversial.</p>
<p>	…</p>
<p>	The basal ganglia can be recognised in all vertebrates. In lampreys, equivalences to all major components of the basal ganglia can be identified: the striatum, the globus pallidus, the subthalamic nucleus and the substantia nigra pars compacta, although identification of the globus pallidus is not yet conclusive… In cartilaginous fishes, a structure termed the area periventricularis ventralis is thought to be equivalent to the dorsal striatum, and a nucleus superficialis basalis may correspond to the ventral striatum. In actinopterygian fishes, it is also believed that there are structures corresponding to the dorsal and ventral striatum. In amphibians, a dorsal and ventral striatum can be recognised, and a nucleus that is believed to be pallidal, called the entopeduncular nucleus. In nonavian reptiles and birds, the dorsal striatum is called the lateral striatum, and the ventral striatum is the medial striatum. A major difference in basal ganglia among vertebrate groups is that a major output in mammals goes to the thalamus, which projects back to the cortex, whereas in nonmammals, and especially in anamniotes, the basal ganglia project downstream and modulate the downstream motor pathways. Mammals have descending pathways as well, but the loops involving the cortex predominate…</p>
<p>	The cerebral hemispheres of mammals appear to be very different from those of nonmammalian vertebrates because all mammals have cerebral cortex, a layered structure on the surface of the brain. Nonmammalian amniotes have some cortical structures, equivalent to the olfactory cortex (lateral pallium) and hippocampus (medial pallium) of mammals, but these are made up of only three layers. The structure that is different in mammals is the neocortex (dorsal pallium), which is extensive and is made up of six interconected layers of cells…</p>
<p>	There is no general agreement about the homologies of the cell populations of the pallium in mammals and nonmammals. Two major positions are represented at the present time, one called the neocortex hypothesis…, the other called the claustroamygdaloid hypothesis … The neocortex hypothesis is that the cell populations of the pallium in nonmammals, which are arranged in nuclei, are homologous with populations of cells in the neocortex of mammals, which are arranged in layers. The claustroamygdaloid hypothesis is that the cell populations of the pallium in nonmammals are homologous with cell populations in the amygdala and claustrum of mammals…</p></blockquote>
</li>
<li class="footnote" id="footnote307_6xlyih9"><a class="footnote-label" href="#footnoteref307_6xlyih9">307.</a> My sources for the information in this table are <a href="http://onlinelibrary.wiley.com/doi/10.1002/9780470015902.a0000088.pub3/full">Powers (2014)</a> and T.M. Preuss’ chapter “Primate Brain Evolution” in <a href="http://store.elsevier.com/Evolutionary-Neuroscience/isbn-9780123751683/">Kaas (2009)</a>. (Presumably the relevant chapters in <a href="http://www.sciencedirect.com/science/referenceworks/9780128040966">Kaas 2016</a> are more up-to-date, but I haven&#8217;t read them.)
</li>
<li class="footnote" id="footnote308_nicpt0d"><a class="footnote-label" href="#footnoteref308_nicpt0d">308.</a> See M.C. Corballis’ chapter “The Evolution of Hemispheric Specializations of the Human Brain” in <a href="http://store.elsevier.com/Evolutionary-Neuroscience/isbn-9780123751683/">Kaas (2009)</a>.
</li>
<li class="footnote" id="footnote309_zsll3zr"><a class="footnote-label" href="#footnoteref309_zsll3zr">309.</a> T.M. Preuss, in his chapter “Primate Brain Evolution” (<a href="http://store.elsevier.com/Evolutionary-Neuroscience/isbn-9780123751683/">Kaas 2009</a>, ch. 35), reviews the debate:<br /><blockquote><p>Among mammals, only primates have a region of cortex with a well-developed granular layer on the dorsolateral surface of the frontal lobe (Brodmann, 1909). The region is present in all primates that have been examined… Owing in part to the influence of Brodmann, the granular dorsolateral prefrontal cortex initially came to be regarded as a hallmark of the primate brain. The fact that some neurologists in the early part of the twentieth century regarded this region as the seat of higher-order cognitive functions reinforced this view. Modern experimental studies in nonhuman primates… reveal it to have strong connections with the higher-order parietal and temporal areas discussed above, and functional studies in humans and nonhuman primates indicate that different parts of the granular frontal cortex are involved in attention, working memory, and planning…</p>
<p>	The idea that dorsolateral prefrontal cortex is special to primates has, nevertheless, been challenged (see the reviews of Preuss, 1995a, 2006). With the introduction of the first generation of techniques for studying cortical connectivity (lesion-degeneration techniques), it became clear that the cortical regions differed in their patterns of connectivity as well as their histology. Early research on the forebrain connections of the cortex focused on connections with the thalamus because cortical lesions produce degeneration in thalamic nuclei that project to them; most other connections could not be reliably resolved until improved methods became available in the 1970s. Rose and Woolsey (1949) championed the idea that regions of cortex could be defined by the thalamic nuclei that projected to them. As the dorsolateral prefrontal cortex, the largest prefrontal region in primates, receives its major thalamic inputs from the mediodorsal thalamic (MD) nucleus, prefrontal cortex came to be defined as MD-projection cortex (Rose and Woolsey, 1948). As it happens, all mammals that have been examined have a MD nucleus and a cortical territory to which it projects, so by this reasoning, all mammals possess a homologue of dorsolateral prefrontal cortex, even though the MD-projection cortex of nonprimates lacks the well-developed granular layer that marks this region in primates (Rose and Woolsey, 1948; Akert, 1964). It was also reported that dopamine-containing nuclei of the brainstem project very strongly to MD-projection cortex in both primates and nonprimates, and this has also been used to identify homologues in different mammals (Divac et al., 1978). Attempts have also been made to refine this analysis by identifying homologues of specific subdivisions of primate dorsolateral prefrontal cortex in nonprimates (Akert, 1964). A region of special interest has been the cortex that lines the principal sulcus of macaques (principalis cortex), because lesions of this region impair performance on spatial working memory tasks, a set of cognitive tasks that have been adapted for use in a wide range of mammals. Using the criteria ofMDprojections, dopamine projections, and involvement in spatial working memory tasks, homologues of macaque principalis cortex have been proposed in nonprimate species, and most importantly in rats, which are the most widely used model animals in mammalian neuroscience. In rats, the principalis homologue has usually been localized to the medial surface of the frontal lobe, and some workers have identified it specifically with area 32 (the prelimbic area)…</p>
<p>	This might seem a satisfactory account of prefrontal homologies, but there are difficulties with both the evidence and the reasoning (Preuss, 1995a). For one thing, in primates, MD projects not only to the granular, dorsolateral prefrontal frontal cortex, but also to agranular regions, including orbital cortex, the classical anterior cingulate areas (areas 24 and 32 of Brodmann), and even to insular and premotor cortex. For another, while dorsolateral prefrontal cortex receives dopaminergic inputs, the strongest dopamine projections in primates are actually to the motor region and the orbital and medial cortex. Finally, in primates, lesions of the medial frontal cortex, involving the cingulate region and sparing the dorsolateral region, produce impairments on spatial working memory tasks. Thus, none of the features that have been used to identify homologues of granular prefrontal cortex in nonprimates are actually diagnostic of granular prefrontal cortex in primates. In fact, the medial frontal cortex of rodents very closely resembles the agranular parts of the medial frontal cortex of primates on a variety of structural and functional grounds – both are limbic regions, after all. It is true that the medial frontal cortex of rodents resembles primate granular frontal cortex in certain respects, but these are also the ways that the medial frontal cortex of primates resembles the dorsolateral prefrontal cortex of primates; the similarities are not diagnostic. Moreover, primate granular frontal cortex has additional features of areal organization and connectivity that do not match any known region of frontal cortex in any nonprimate mammal (Preuss, 1995a).</p>
<p>	On present evidence, then, there are good grounds for concluding that dorsolateral prefrontal cortex is in fact one of the distinctive features of the primate brain.</p></blockquote>
</li>
<li class="footnote" id="footnote310_am702s6"><a class="footnote-label" href="#footnoteref310_am702s6">310.</a> See e.g. <a href="http://www.pnas.org/content/109/42/16974.short">Dugas-Ford et al. (2012)</a> on possible neocortex homologs in avian brains. I say “not really,” because even if Dugas-Ford &amp; colleagues are right, it is still not the case that birds have the typical mammalian 6-layer neocortex.
</li>
<li class="footnote" id="footnote311_8w74jhr"><a class="footnote-label" href="#footnoteref311_8w74jhr">311.</a> The distinction between a telencephalon formed by eversion and a telencephalon formed by evagination is explained succinctly by <a href="http://onlinelibrary.wiley.com/doi/10.1002/9780470015902.a0000088.pub3/full">Powers (2014)</a>:<br /><blockquote><p>In all vertebrates except actinopterygian fishes [i.e. ray-finned fishes, e.g. trout and nearly all other well-known fishes], the telencephalon developed by evagination, that is, it grew outward, away from the lateral ventricle in all directions. In actinopterygian fishes, however, the telencephalon developed by a different method, called eversion. The roof of the lateral ventricle thinned and stretched to form a membrane over the ventricle on the surface of the hemisphere, and the telencephalon forms a solid mass (with no ventricle). Because of this unique pattern of development, establishment of homologies between the actinopterygian and tetrapod brain is difficult…</p></blockquote>
</li>
<li class="footnote" id="footnote312_spp7zsl"><a class="footnote-label" href="#footnoteref312_spp7zsl">312.</a> Though, see <a href="http://www.sciencedirect.com/science/article/pii/S1053810011001176">Frankish (2012a)</a>, where Frankish argues (in different terms) that Carruthers’ account of consciousness may be an example of strong illusionism being “mis-sold” as weak illusionism.<br /><br />
Theories which use a “phenomenal concepts strategy” might in some cases qualify as examples of weak illusionism. For example, <a href="https://mitpress.mit.edu/books/consciousness-color-and-content">Tye (2000)</a>, p. 23:
<blockquote><p>I accept that experiences are fully, robustly physical but I maintain that there is no explanatory gap posed by their phenomenology. The gap, I claim, is unreal; it is a cognitive illusion to which we only too easily fall prey… There aren&#8217;t two sorts of natural phenomena — the irreducibly subjective and the objective. The so-called “explanatory gap” derives largely from a failure to recognize the special features of phenomenal concepts. These concepts, I maintain, have a character that not only explains why we have the intuition that something important is left out by the physical (and/or functional) story but also explains why this intuition is not to be trusted.</p></blockquote>
<p>Sensorimotor theory might also qualify as a weak (or perhaps even strong) illusionist theory. <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/div-classtitlea-sensorimotor-account-of-vision-and-visual-consciousnessdiv/BA1638CB7389102A12B336CE687EC270">O&#8217;Regan &amp; Noe (2001)</a>:</p>
<blockquote><p>In our view, the qualia debate rests on what Ryle (1949/1990) called a category mistake. Qualia are meant to be properties of experiential states or events. But experiences, we have argued, are not states. They are ways of acting. They are things we do. There is no introspectibly available property determining the character of one&#8217;s experiential states, for there are no such states. Hence, there are, in this sense at least, no (visual) qualia. Qualia are an illusion, and the explanatory gap is no real gap at all.</p>
<p>It is important to stress that in saying this we are not denying that experience has a qualitative character. We have already said a good deal about the qualitative character of experience and how it is constituted by the character of the sensorimotor contingencies at play when we perceive… Our claim, rather, is that it is confused to think of the qualitative character of experience in terms of the occurrence of something (whether in the mind or brain). Experience is something we do and its qualitative features are aspects of this activity.</p>
<p>…Many philosophers, vision scientists, and lay people will say that seeing always involves the occurrence of raw feels or qualia. If this view is mistaken, as we believe, then how can we explain its apparent plausibility to so many? In order to make our case convincing, we must address this question.</p>
<p>In our view, there are two main sources of the illusion. The first pertains to the unity and complexity of experience. We tend to overlook the complexity and heterogeneity of experience, and this makes it seem as if in experience there are unified sensation-like occurrences. The second source of illusion has to do with the felt presence of perceptible qualities. Because, when we see, we have continuous access to features of a scene, it is as if we continuously represent those features in consciousness…</p></blockquote>
</li>
<li class="footnote" id="footnote313_jrp9b3j"><a class="footnote-label" href="#footnoteref313_jrp9b3j">313.</a> Some of my reasons for wanting to avoid saying things like “phenomenal consciousness is an illusion” or “phenomenal properties are illusory” were expressed by <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00008">Graziano (2016)</a>:<br /><blockquote><p>The attention schema theory [Graziano&#8217;s theory] has much in common with illusionism. It clearly belongs to the same category of theory, and is especially close to the approach of Dennett (1991). But I confess that I baulk at the term ‘illusionism’ because I think it miscommunicates. To call consciousness an illusion risks confusion and unwarranted backlash. To me, consciousness is not an illusion but a useful caricature of something real and mechanistic…</p>
<p>In my own discussions with colleagues, I invariably encounter the confusion and backlash. To most people, an illusion is something that does not exist. Calling consciousness an illusion suggests a theory in which there is nothing present that corresponds to consciousness. However, in the attention schema theory, and in the illusionism described by Frankish, something specific is present. In the attention schema theory, the real item that exists inside us is covert attention — the deep processing of selected information. Attention truly does exist. Our internal model of it lacks details and therefore provides us with a blurred, seemingly magicalist account of it.</p>
<p>Second, in normal English, to experience an illusion is to be fooled. To call consciousness an illusion suggests to most people that the brain has made an error. In the attention schema theory, and also in the illusionism approach described by Frankish, the relevant systems in the brain are not in error. They are well adapted. Internal models always, and strategically, leave out the unnecessary detail.</p>
<p>Third, most people understand illusions to be the result of a subjective experience. The claim that consciousness is an illusion therefore sounds inherently circular. Who is experiencing the illusion? It is difficult to explain to people that the experiencer is not itself conscious, and that what is important is the presence of the information and its impact on the system. The term illusion instantly aligns people&#8217;s thoughts in the wrong direction.</p>
<p>All of the common objections I encounter have answers. They are based on a misunderstanding of illusionism. But the misunderstanding is my point. Why use a misleading word that requires one to backtrack and explain? For these reasons, in my own writing I have avoided calling consciousness an illusion except in specific circumstances, such as the consciousness we attribute to a ventriloquist puppet, in which the term seems to apply more exactly.</p>
<p>Perhaps I am too much of a visual physiologist at heart. To me, an illusion is a mistake in a sensory internal model. It introduces a consequential discrepancy between the internal model and the real world. That discrepancy can cause errors in behaviour. In contrast, an internal model, at all times, with or without an illusion, is an efficient, useful compression of data. It is never literally accurate. Even when it is operating correctly and guiding behaviour usefully, it is a caricature of reality. I am comfortable calling consciousness a caricature, but not an illusion. It is a cartoonish model of something real.</p></blockquote>
<p>See also <a href="http://sro.sussex.ac.uk/64774/">Sloman &amp; Chrisley (2016)</a>’s comments on potential terminological distinctions between illusionism, eliminativism, “revisionism,” and “hallucinationism.”</p>
</li>
<li class="footnote" id="footnote314_5r266y5"><a class="footnote-label" href="#footnoteref314_5r266y5">314.</a> In <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00002">Frankish (2016b)</a>, Frankish writes:<br /><blockquote><p>Is the illusionist claiming that we are mistaken in thinking we have conscious experiences? It depends on what we mean by ‘conscious experiences’. If we mean experiences with phenomenal properties, then illusionists do indeed deny that such things exist. But if we mean experiences of the kind that philosophers <em>characterize</em> as having phenomenal properties, then illusionists do not deny their existence. They simply offer a different account of their nature, characterizing them as having merely quasi-phenomenal properties. Similarly, illusionists deny the existence of phenomenal consciousness properly so-called, but do not deny the existence of a form of consciousness (perhaps distinct from other kinds, such as access consciousness) which consists in the possession of states with quasi-phenomenal properties and is commonly mischaracterized as phenomenal. Henceforth, I shall use ‘consciousness’ and ‘conscious experience’ without qualification in an inclusive sense to refer to states that might turn out to be either genuinely phenomenal or only quasi-phenomenal. In this sense realists and illusionists agree that consciousness exists.</p>
<p>Do illusionists then recommend eliminating talk of phenomenal properties and phenomenal consciousness? Not necessarily. We might reconceptualize phenomenal properties as quasi-phenomenal ones. Recall Pereboom&#8217;s analogy with secondary qualities. The discovery that colours are mind-dependent did not lead scientists to deny that objects are coloured. Rather, they reconceptualized colours as the properties that cause our colour sensations. Similarly, we might respond to the discovery that experiences lack phenomenal properties by reconceptualizing phenomenal properties as the properties that cause our representations of phenomenal feels — that is, quasi-phenomenal properties…</p>
<p>In everyday life… we would surely continue to talk of the feel or quality of experience in the traditional, substantive sense. As subjects of experience, our interest is in how things seem to us introspectively — the illusion itself, not the mechanisms that cause it. Such talk may fail to pick out real properties, but it is not empty or pointless.</p></blockquote>
<p>When I defined consciousness by example <a href="#Defined">above</a>, the paper I built on was <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00018">Schwitzgebel (2016)</a>, which is a response to <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00002">Frankish (2016b)</a>. In his response to Schwitzgebel&#8217;s proposed definition, Frankish replied (<a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00020">Frankish 2016c</a>):</p>
<blockquote><p>[Schwitzgebel offers] a definition by example, describing a range of uncontentious positive and negative cases and identifying phenomenal consciousness as ‘the most folk-psychologically obvious thing or feature that the positive examples possess and that the negative examples lack’… </p>
<p>I think Schwitzgebel succeeds in identifying an important folkpsychological kind — indeed the very one that should be our focus in theorizing about consciousness… </p>
<p>…He has defined a neutral explanandum for theories of consciousness, which both realists and illusionists can adopt. (I have referred to this as consciousness in an inclusive sense. We might call it simply <em>consciousness</em>, or, if we need to distinguish it from other forms, <em>putative phenomenal consciousness</em>.)</p></blockquote>
<p>So, my guess is that Frankish would not mind my way of talking, so long as we have some way of distinguishing what the phenomenal realist means by “qualia” and “phenomenal properties” and so on from what the illusionist means by such terms.</p>
</li>
<li class="footnote" id="footnote315_whzt5un"><a class="footnote-label" href="#footnoteref315_whzt5un">315.</a> Indeed, when reading the journal issue linked above, I often found myself wondering what the authors meant by terms like “what it&#8217;s like” and “phenomenal properties” and “quasi-phenomenal properties,” and sometimes I couldn&#8217;t tell which author I would agree with more if I was able to understand better what each of them was trying to say. Moreover, it seems likely to me that even assuming something like “strong illusionism” about consciousness is correct, our attempts to describe it using the terms and concepts and metaphors we&#8217;ve come up with so far will look quite naive and confused 50 years from now. In my view, detailed empirical work and computational modeling could be the most useful inputs, in the long run, to the clarification of illusionist and other hypotheses about consciousness (see my comments starting <a href="#ThirdPerson">here</a>). Nevertheless, I will attempt to clarify (what I see as) the illusionist view, using the concepts and metaphors available to me now.
</li>
<li class="footnote" id="footnote316_d46xilt"><a class="footnote-label" href="#footnoteref316_d46xilt">316.</a> This is Figure 0.6 in <a href="https://global.oup.com/academic/product/basic-vision-9780199572021?cc=us&amp;lang=en&amp;">Snowden et al. (2012)</a> — i.e. <em>Basic Vision: An Introduction to Visual Perception, Revised Edition</em> by Robert Snowden et al (2012): Figure 0.6 (p.8) — reprinted here by permission of <a href="http://global.oup.com/">Oxford University Press</a>. The image was adapted from the figure on p. 46 of <a href="https://books.google.com/books?id=EX1tQgAACAAJ&amp;hl=en">Shepard (1990)</a>, which in turn was an elaboration of a figure on p. 298 of Shepard&#8217;s chapter “Psychophysical complementarity” in <a href="https://books.google.com/books?id=zFZ9AAAAMAAJ">Kubovy &amp; Pomerantz (1981)</a>.
</li>
<li class="footnote" id="footnote317_856hmlf"><a class="footnote-label" href="#footnoteref317_856hmlf">317.</a> In this case, we do know some things about why the visual illusion is produced, but reading the relevant studies isn&#8217;t necessary for knowing that our perception has been tricked.
</li>
<li class="footnote" id="footnote318_oeoiwgd"><a class="footnote-label" href="#footnoteref318_oeoiwgd">318.</a> <a href="http://www.sciencedirect.com/science/article/pii/S0039625707000501">Grzybowski &amp; Aydin (2007)</a>.
</li>
<li class="footnote" id="footnote319_q58dbxx"><a class="footnote-label" href="#footnoteref319_q58dbxx">319.</a> See e.g. <a href="http://jov.arvojournals.org/article.aspx?articleid=2193447">Hansen et al. (2009)</a>.
</li>
<li class="footnote" id="footnote320_7ia0b6x"><a class="footnote-label" href="#footnoteref320_7ia0b6x">320.</a> <a href="https://books.google.com/books?id=1FnPAAAAMAAJ">Baird (1905)</a>, ch. 1.
</li>
<li class="footnote" id="footnote321_kggibcp"><a class="footnote-label" href="#footnoteref321_kggibcp">321.</a> Quote from <a href="https://books.google.com/books?id=xze89PCLaWMC&amp;lpg=PP1&amp;pg=PA53#v=onepage&amp;q&amp;f=false">Heilman (1991)</a>.
</li>
<li class="footnote" id="footnote322_onu4s0c"><a class="footnote-label" href="#footnoteref322_onu4s0c">322.</a> <a href="http://www.sciencedirect.com/science/article/pii/027826269190083K">Hartmann et al. (1991)</a>.
</li>
<li class="footnote" id="footnote323_egfwadz"><a class="footnote-label" href="#footnoteref323_egfwadz">323.</a> <a href="https://books.google.com/books?id=HSoVAgAAQBAJ&amp;lpg=PA147&amp;ots=30NI58YixA&amp;lr&amp;pg=PA147#v=onepage&amp;q&amp;f=false">Young &amp; Leafhead (1996)</a>.
</li>
<li class="footnote" id="footnote324_c1m99m9"><a class="footnote-label" href="#footnoteref324_c1m99m9">324.</a> Some of “illusions” of this sort might be more accurately called “delusions,” but I won&#8217;t bother to make this distinction here. See e.g. <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00004">Blackmore (2016)</a>.
</li>
<li class="footnote" id="footnote325_aiukp2t"><a class="footnote-label" href="#footnoteref325_aiukp2t">325.</a> See e.g. <a href="https://global.oup.com/academic/product/the-conscious-mind-9780195117899?cc=us&amp;lang=en&amp;">Chalmers (1996)</a>, pp. 193-195.
</li>
<li class="footnote" id="footnote326_q407huz"><a class="footnote-label" href="#footnoteref326_q407huz">326.</a> <a href="https://books.google.com/books?id=ZqsytrHg8LkC">Searle (1997)</a>, p. 112. Italics modified.
</li>
<li class="footnote" id="footnote327_1u3628r"><a class="footnote-label" href="#footnoteref327_1u3628r">327.</a> For additional clarifications on this point, see <a href="http://schwitzsplinters.blogspot.com/2007/07/making-sense-of-dennetts-views-on.html">Schwitzgebel (2007a)</a>, the two papers it links to (<a href="http://link.springer.com/article/10.1007/s11097-006-9034-y">Schwitzgebel 2007b</a>; <a href="http://link.springer.com/article/10.1007%2Fs11097-006-9044-9?LI=true">Dennett 2007</a>), and <a href="http://philreview.dukejournals.org/content/117/2/245.short">Schwitzgebel (2008)</a>, especially this passage from section x:<br /><blockquote><p>I sometimes hear the following objection: When we make claims about our phenomenology, we&#8217;re making claims about how things <em>appear</em> to us, not about how anything actually <em>is</em>. The claims, thus divorced from reality, can&#8217;t be false; and if they&#8217;re true, they&#8217;re true in a peculiar way that shields them from error. In looking at an illusion, for example, I may well be wrong if I say the top line is longer; but if I say it <em>appears</em> or <em>seems</em> to me that the top line is longer, I can&#8217;t in the same way be wrong. The sincerity of the latter claim seemingly guarantees its truth. It&#8217;s tempting, perhaps, to say this: If something <em>appears</em> to appear a certain way, necessarily it appears that way. Therefore, we can&#8217;t misjudge appearances, which is to say, phenomenology.</p>
<p>This reasoning rests on an equivocation between what we might call an <em>epistemic</em> and a <em>phenomenal</em> sense of “appears” (or, alternatively, “seems”). Sometimes, we use the phrase “it appears to me that such-andsuch” simply to express a judgment — a hedged judgment, of a sort — with no phenomenological implications whatsoever. If I say, “It appears to me that the Democrats are headed for defeat,” ordinarily I&#8217;m merely expressing my opinion about the Democrats’ prospects. I&#8217;m not attributing to myself any particular phenomenology. I&#8217;m not claiming to have an image, say, of defeated Democrats, or to hear the word “defeat” ringing in my head. In contrast, if I&#8217;m looking at an illusion in a vision science textbook, and I say that the top line “appears” longer, I&#8217;m not expressing any sort of judgment about the line. I know perfectly well it&#8217;s not longer. I&#8217;m making instead, it seems, a claim about my phenomenology, about my visual experience.</p>
<p>Epistemic uses of “appears” <em>might</em> under certain circumstances be infallible in the sense of the previous section. Maybe, if we assume that they&#8217;re sincere and normally caused, their truth conditions will be a subset of their existence conditions — though a story needs to be told here. But phenomenal uses of “appears” are by no means similarly infallible. This is evident from the case of weak, nonobvious, or merely purported illusions. Confronted with a perfect cross and told there may be a “horizontal-vertical illusion” in the lengths of the lines, one can feel uncertainty, change one&#8217;s mind, and make what at least plausibly seem to be errors about whether one line “looks” or “appears” or “seems” in one&#8217;s visual phenomenology to be longer than another. You might, for example, fail to notice — or worry that you may be failing to notice — a real illusion in your experience of the relative lengths of the lines; or you might (perhaps under the influence of a theory) erroneously report a minor illusion that actually isn&#8217;t part of your visual experience at all. Why not?</p></blockquote>
<p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/consciousness-accessibility-and-the-mesh-between-psychology-and-neuroscience/63BF0B7E5DB1654ED680E160D413F3D0">Block (2007b)</a> argues against the idea that we can be wrong about our own subjective experiences, in his discussion of what he calls “hyper-illusions.”</p>
</li>
<li class="footnote" id="footnote328_2bg53s4"><a class="footnote-label" href="#footnoteref328_2bg53s4">328.</a> Quoted from <a href="https://mitpress.mit.edu/books/perplexities-consciousness">Schwitzgebel (2011)</a>, ch. 3.
</li>
<li class="footnote" id="footnote329_1722emc"><a class="footnote-label" href="#footnoteref329_1722emc">329.</a> Dennett fabricated #4.<br /><br />
Dennett lists sources for these phenomena in a footnote, which I&#8217;ve reformatted below to match the citation style of this report and to include links:
<blockquote><p>For the red and green patch, see <a href="http://science.sciencemag.org/content/221/4615/1078">Crane &amp; Piantanida (1983)</a> and <a href="https://www.hackettpublishing.com/color-for-philosophers">Hardin (1988)</a>; for the disappearing color boundary… see <a href="http://store.elsevier.com/product.jsp?isbn=9780323138147">Spillmann and Werner (1990)</a>; for the auditory barber pole, see <a href="http://scitation.aip.org/content/asa/journal/jasa/36/12/10.1121/1.1919362">Shepard (1964)</a>; for the Pinocchio effect, see <a href="http://brain.oxfordjournals.org/content/111/2/281.short">Lackner (1988)</a>…</p></blockquote>
</li>
<li class="footnote" id="footnote330_tdwb8w8"><a class="footnote-label" href="#footnoteref330_tdwb8w8">330.</a> For extensive debate on these issues, see also <a href="https://mitpress.mit.edu/books/describing-inner-experience">Hurlburt &amp; Schwitzgebel (2007)</a> and <a href="http://www.ingentaconnect.com/content/imp/jcs/2011/00000018/00000001">volume 18, issue 1</a> of the <em>Journal of Consciousness Studies</em>.
</li>
<li class="footnote" id="footnote331_fgx72p3"><a class="footnote-label" href="#footnoteref331_fgx72p3">331.</a> For discussion of additional visual illusions, see e.g. <a href="https://global.oup.com/academic/product/the-oxford-compendium-of-visual-illusions-9780199794607?cc=us&amp;lang=en&amp;">Shapiro &amp; Todorovic (2017)</a> and Michael Bach&#8217;s website on <a href="http://michaelbach.de/ot/">optical illusions and visual phenomena</a>. On auditory illusions, see the website for <a href="http://philomel.com/index.php">Diana Deutsch&#8217;s Audio Illusions</a>.
</li>
<li class="footnote" id="footnote332_rnjjoqe"><a class="footnote-label" href="#footnoteref332_rnjjoqe">332.</a> Perhaps there are as many answers to this question as there are pairs of realists and illusionists.
</li>
<li class="footnote" id="footnote333_ga887yz"><a class="footnote-label" href="#footnoteref333_ga887yz">333.</a> See also <a href="http://www.sciencedirect.com/science/article/pii/S1053810011001772">Frankish (2012b)</a>.
</li>
<li class="footnote" id="footnote334_ba2wemy"><a class="footnote-label" href="#footnoteref334_ba2wemy">334.</a> Among the properties of “classic” qualia, the “intrinsic character” property is typically thought to be the most directly incompatible with physicalism. It might also be the most difficult to explain. Because of this, I&#8217;ll elaborate here what is usually meant by the “intrinsic character” of (classic) qualia, even though I do not take the time to elaborate the meaning of other properties of classic qualia.<br /><br /><a href="http://www.jstor.org/stable/2214186">Harman (1990)</a> puts it this way:
<blockquote><p>…when you attend to a pain in your leg or to your experience of the redness of an apple, you are aware of an intrinsic quality of your experience, where an intrinsic quality is a quality something has in itself, apart from its relations to other things. This quality of experience cannot be captured in a functional definition, since such a definition is concerned entirely with relations, relations between mental states and perceptual input, relations among mental states, and relations between mental states and behavioral output.</p></blockquote>
<p>Or, here is <a href="http://www.polity.co.uk/book.asp?ref=9780745653440">Weisberg (2014)</a>, pp. 54-56:</p>
<blockquote><p>Theories in physics deal in what we can call “relational” information only. Relational information tells us how things are lawfully connected, how changing one thing changes another according to the physical laws posited by the theory. And then, in turn, we can define physical things like electrons, protons, or quarks in terms of how they fit into this framework of causal connections… on this view to be an electron is just to be the kind of thing that fits into this pattern of causal relations – that plays this “causal role.” We learn with extreme precision from physical theory just what this special electron “role” is. But we don’t learn, so it seems, just what the thing playing the role is, on its own. It’s like learning all we can about what it is to be a goalie: a goalie is the person who defends the goal and can use his or her hands to stop the ball, who can’t touch the ball when it’s passed back by his or her own team, who usually kicks the goal kicks, and so on, without learning who is playing goalie for our team. We know all about the goalie role without learning that its Hope Solo or Tim Howard. We learn the role without learning about the role-filler, beyond that he or she plays that role. But there may be other things true of the role-filler beyond what he or she does in the role.</p>
<p>…</p>
<p>Another way to put this is that all physics tells us about is what physical stuff is <em>disposed to do</em>: what physical law dictates will occur, given certain antecedent conditions. In philosophers’ terms, physics only tells us about the <em>dispositional properties</em> of physical stuff, not the <em>categorical base</em> of these dispositions. A paradigm dispositional property is <em>fragility</em>. Something is fragile if it is disposed to break in certain situations. When we learn that a window is fragile, we learn that it will likely break if hit even softly. But there is a further question we can ask about the window or any fragile object. What is it about the makeup of the window that gives it this dispositional property? The answer is that the window has a certain molecular structure with bonds that are relatively weak in key areas. This molecular structure is what we call the <em>categorical base</em> for the window’s fragility. Returning to physics, at its most basic level, all we get is the dispositional properties, not the categorical base. We learn that electrons (or quarks or “hadrons”) are things disposed to do such-and-such, to be repelled by certain charges, to move in certain ways. We don’t learn what’s “underneath” making this happen.</p></blockquote>
<p>For more on the idea of intrinsicality, see e.g. Marshall (<a href="http://onlinelibrary.wiley.com/doi/10.1111/phpr.12156/full">2014</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1111/nous.12087/full">2016</a>).</p>
<p>For an accessible explanation of what it means for classic qualia to be “subjective” and “ineffable,” see e.g. ch. 1 of <a href="https://books.google.com/books?id=dRpzhcWWbxIC">Frankish (2005)</a>.</p>
</li>
<li class="footnote" id="footnote335_c9wieie"><a class="footnote-label" href="#footnoteref335_c9wieie">335.</a> Though as with all the intuitions reported here, carrying out the “<a href="#ExtremeEffort">extreme effort</a>” version of my process for making moral judgments could result in a different judgment.
</li>
<li class="footnote" id="footnote336_7y8ocgy"><a class="footnote-label" href="#footnoteref336_7y8ocgy">336.</a>  And, to be clear: that person would also have greatly impoverished behavior, and wouldn&#8217;t talk the way we do about qualia.
</li>
<li class="footnote" id="footnote337_qx3yxzw"><a class="footnote-label" href="#footnoteref337_qx3yxzw">337.</a> I borrow the framing of this section from Eliezer Yudkowsky&#8217;s “<a href="https://arbital.com/p/rescue_utility/">Rescuing the utility function</a>” (2016). An alternate framing might be to say that we don&#8217;t know how to translate our value for something in our “manifest image” of the world into an appropriate value for something in the “scientific image” of the world (see <a href="http://books.wwnorton.com/books/detail.aspx?ID=4294992671">Dennett 2017</a>, ch. 4; <a href="https://books.google.com/books?id=m1pFy1ba7rkC&amp;hl=en&amp;sa=X">Sellars 1962</a>).<br /><br />
For an example toy formalization of the “cross-ontology value translation” problem discussed in this section, see <a href="http://arxiv.org/abs/1105.3821">de Blanc (2011)</a>. See also section 5 of <a href="https://intelligence.org/files/RealisticWorldModels.pdf">Soares (2015)</a>, section 3.2 of <a href="https://intelligence.org/files/ValueLearningProblem.pdf">Soares (2016)</a>, <a href="http://dspace.ut.ee/bitstream/handle/10062/54240/Rao_Parnpuu_MA_2016.pdf">Pärnpuu (2016)</a>, and Yudkowsky&#8217;s “<a href="https://arbital.com/p/ontology_identification/">Ontology Identification</a>” (2016).<br /><br /><a name="concepts" id="concepts"></a>For additional accounts of how scientific concepts evolve along with scientific progress, see e.g. Thagard (<a href="http://press.princeton.edu/titles/5050.html">1992</a>, <a href="http://press.princeton.edu/titles/6669.html">1999</a>, <a href="https://books.google.com/books?hl=en&amp;lr=lang_en&amp;id=sdyOAgAAQBAJ&amp;oi=fnd&amp;pg=PA374&amp;ots=qz_7Rmg2ol&amp;sig=61UvZZNQKfECu3BlGkVk8tInJzg#v=onepage&amp;q&amp;f=false">2008</a>), <a href="https://uwspace.uwaterloo.ca/handle/10012/5570">Grisdale (2010)</a>, <a href="http://www.nature.com/nrn/journal/v6/n11/abs/nrn1789.html">Laureys (2005)</a>, <a href="http://www.springer.com/us/book/9789048123612">Chalmers (2009)</a>, and Chang (<a href="https://global.oup.com/academic/product/inventing-temperature-9780195171273?lang=en&amp;cc=us">2004</a>, <a href="http://www.springer.com/us/book/9789400739314">2012</a>).<br /><br /><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/consciousness-accessibility-and-the-mesh-between-psychology-and-neuroscience/63BF0B7E5DB1654ED680E160D413F3D0">Block (2007b)</a> cites some further examples:
<blockquote><p>…as theory, driven by experiment, advances, important distinctions come to light among what appeared at first to be unified phenomena (see Block &amp; Dworkin 1974, on temperature; Churchland 1986; 1994; 2002, on life and fire).</p></blockquote>
</li>
<li class="footnote" id="footnote338_tnkwobh"><a class="footnote-label" href="#footnoteref338_tnkwobh">338.</a> In reality, things are bit more complex than water = H<sub>2</sub>O, both scientifically and historically (<a href="http://www.springer.com/us/book/9789400739314">Chang 2012</a>), but for the purposes of this illustration please assume the simple view that water = H<sub>2</sub>O.
</li>
<li class="footnote" id="footnote339_to6xmq6"><a class="footnote-label" href="#footnoteref339_to6xmq6">339.</a> I&#8217;m not aware of any real-world water-lovers, but there do seem to be some real-world life-lovers. For example Albert Schweitzer (see <a href="https://global.oup.com/academic/product/moral-status-9780198236689?lang=de&amp;cc=lt">Warren 1997</a>, ch. 2) and <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8519.2008.00688.x/full">Mautner (2009)</a>.
</li>
<li class="footnote" id="footnote340_r1ano84"><a class="footnote-label" href="#footnoteref340_r1ano84">340.</a> <a href="http://www.sciencedirect.com/science/article/pii/S1096495901003001">Clegg (2001)</a>; <a href="http://msb.embopress.org/content/3/1/137.short">Kitano (2007)</a>.
</li>
<li class="footnote" id="footnote341_qeq93jw"><a class="footnote-label" href="#footnoteref341_qeq93jw">341.</a> Of course if you count the reproduction of single-celled organisms by cell division as defining a “life span,” then many single-celled organisms live for mere hours.
</li>
<li class="footnote" id="footnote342_p0h0nsw"><a class="footnote-label" href="#footnoteref342_p0h0nsw">342.</a> <a href="http://www.springer.com/us/book/9783540768166">Schulze-Makuch &amp; Irwin (2008)</a>, pp. 10-12:<br /><blockquote><p>By the traditional definition viruses are not considered living entities because they cannot reproduce and grow by themselves and do not metabolize. Nevertheless, they possess a genetic code that enables them to reproduce and direct a limited amount of metabolism inside another living cell. They thus fulfill the traditional criteria only part of the time and under special circumstances… Since viruses presumably evolved from bacteria that clearly are alive, do they represent a case in which a living entity has been transformed to a non-living state by natural selection? Or, alternatively, if viruses were indeed the precursors of the three domains of life (Archaea, Bacteria, and Eukarya) as recently suggested…, where would we draw the line between life and non-life? If we accept the proposition that viruses are not alive, how would we consider parasitic organisms or bacterial spores? Parasites cannot grow by themselves either and spores remain in dormant stages with no dynamic biological attributes until they become active under favorable environmental conditions. Thus, if we consider parasites or bacterial spores to be alive, the logical consequence would be to consider viruses alive as well.</p></blockquote>
<p>Consider also the multi-part virus reported in <a href="http://www.cell.com/cell-host-microbe/fulltext/S1931-3128(16)30310-9">Ladner et al. (2016)</a>.</p>
</li>
<li class="footnote" id="footnote343_3t4m9z0"><a class="footnote-label" href="#footnoteref343_3t4m9z0">343.</a> <a href="http://www.springer.com/us/book/9783540768166">Schulze-Makuch &amp; Irwin (2008)</a>, pp. 8-9:<br /><blockquote><p>…just as cells grow in favorable environments with nutrients available, inorganic crystals can grow so long as ion sources and favorable surroundings are provided. Furthermore, just as the development of living organisms follows a regulated trajectory, so does the process of local surface reversibility regulate the course of silicate or metal oxide crystals that grow in aqueous solutions (Cairns-Smith 1982).</p>
<p>…The visible consequence of reproduction in living organisms is the multiplication of individuals into offspring of like form and function. Mineral crystals do not reproduce in a biological sense, but when they reach a certain size they break apart along their cleavage planes. This is clearly a form of multiplication. The consequence of biological reproduction is also the transmission of information. Biological information is stored in the one-dimensional form of a linear code (DNA, RNA), that, at the functional level, is translated into the 3-dimensional structure of proteins. Prior to multiplication, the one-dimensional genetic code is copied, and complete sets of the code are transmitted to each of the two daughter cells that originate from binary fission. An analogous process occurs in minerals, where information may be stored in the two-dimensional lattice of a crystal plane. If a mineral has a strong preference for cleaving across the direction of growth and in the plane in which the information is held (Cairns-Smith 1982), the information can be reproduced.</p></blockquote>
<p>Note that I have not checked Schulze-Makuch &amp; Irwin&#8217;s account for accuracy, as the exact details don&#8217;t matter much to my story.</p>
</li>
<li class="footnote" id="footnote344_lg6486h"><a class="footnote-label" href="#footnoteref344_lg6486h">344.</a> <a href="http://link.springer.com/article/10.1007/s10670-007-9061-2">Antony (2008)</a> argues in favor of a sharp dividing line (of a certain sort) between the conscious and the non-conscious, but he thinks most theorists disagree with him:<br /><blockquote><p>Not all theorists agree [with me] that there can be no borderline conscious states or creatures. In fact most think there can be, and are… In fact, most philosophers working on consciousness think borderline conscious states and creatures <em>must</em> be possible. That is because most philosophers working on consciousness reduce consciousness to complex physical (e.g., neurophysiological) or functional properties, and concepts for such properties are vague.</p></blockquote>
<p><a href="https://books.google.com/books?id=qzt1QgAACAAJ">Papineau (1993)</a> presents this “fuzzy” view of consciousness this way (pp. 123-126):</p>
<blockquote><p>…any physicalist account of consciousness is likely to make consciousness a <em>vague</em> property. In the next section I shall argue that questions of consciousness may not only be vague, but quite <em>arbitrary</em>, in application to beings unlike ourselves…</p>
<p>The point about vagueness is suggested by the analogy with life. If life is simply a matter of a certain kind of physical complexity — the kind of complexity that fosters survival and reproduction, as I put it above — then it would seem to follow that there is no sharp line between life and non-life. For there is nothing in the idea of such physical complexity to give us a definite cut-off point beyond which you have enough complexity to qualify as alive. Rather as with baldness, or being a pile of sand, we should expect there to be some clear cases of life, and some clear cases of non-life, but a grey area in between where there there is no fact of the matter. And of course this is just what we do find. While there is no doubt that trees are alive and stones are not, there are borderline cases in between, like viruses, or certain kinds of simpler self-replicating molecules, where our physicalist account of life simply leaves it indeterminate whether these are living beings or not.</p>
<p>But now, if consciousness is like life, we should expect a similar point to apply to consciousness. For any physicalist account of consciousness is likely to make consciousness depend similarly on the possession of some kind of structural complexity — the kind of complexity which qualifies you as having self-representing states, say, or short-term memories. Yet any kind of such complexity is likely to come in degrees, with no clear cut-off point beyond which you definitely qualify as conscious, and before which you don&#8217;t. So we should expect there to be borderline cases — such as the states of certain kinds of insects, say, or fishes, or cybernetic devices — where our physicalist account simply leaves it indeterminate whether these are conscious states or not.</p>
<p>…I think that the physicalist approach to consciousness is correct. So I reject the intuition that there is a sharp line between conscious and non-conscious states…</p>
<p>It would be a mistake to conclude from this, however, that consciousness is unimportant or unreal. Any number of genuine and important properties are vague. Consider the difference between being elastic or inelastic, or between being young or old, or, for that matter, between being alive and not being alive. All these distinctions will admit indeterminate borderline cases. But all of them involve perfectly serious properties, properties which enter into significant generalizations, are explanatorily important, and so on.</p></blockquote>
<p><a href="http://www.jstor.org/stable/40971115">Dennett (1995)</a> concurs:</p>
<blockquote><p>The very idea of there being a dividing line between those creatures “it is like something to be” and those that are mere “automata” begins to look like an artifact of our traditional presumptions… Consciousness, I claim, even in the case we understand best — our own — is not an all-or-nothing, on-or-off phenomenon. If this is right, then consciousness is not the sort of phenomenon it is assumed to be by most of the participants in the debates over animal consciousness. Wondering whether it is “probable” that all mammals have it thus begins to look like wondering whether or not any birds are <em>wise</em> or reptiles have <em>gumption</em>: a case of overworking a term from folk psychology that has losts its utility along with its hard edges.</p></blockquote>
<p>Also see <a href="https://mitpress.mit.edu/books/consciousness-color-and-content">Tye (2000)</a>, pp. 180-181:</p>
<blockquote><p>Some philosophers will no doubt respond that the boundary between the creatures that are phenomenally conscious and those that are zombies cannot be blurry. Conscious experience or feeling is either present or it isn&#8217;t… [But] it seems to me that we can make sense of the idea of a borderline experience. Suppose you are participating in a psychological experiment and you are listening to quieter and quieter sounds through headphones. As the process continues, a point may come at which you are unsure whether you hear anything at all. Now it could be that there is a still a fact of the matter here (as on the dimming light model); but, equally, it could be that whether you still hear anything is objectively indeterminate. So, it could be that there is no fact of the matter about whether there is anything it is like for you to be in the state you are in at that time. In short, it could be that you are undergoing a borderline experience.</p></blockquote>
<p>See also <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1475-4975.1988.tb00171.x/abstract">Unger (1988)</a>, ch. 7 of <a href="https://global.oup.com/academic/product/thinking-about-consciousness-9780199243822?cc=us&amp;lang=en&amp;">Papineau (2002)</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1111/1533-6077.00012/abstract">Papineau (2003)</a>, the sources cited in footnote 6 of <a href="http://link.springer.com/article/10.1007/s10670-007-9061-2">Antony (2008)</a>, <a href="http://link.springer.com/article/10.1007%2Fs11098-016-0790-4">Simon (2016)</a>, and <a href="https://www.academia.edu/29703924/A_Multi-Factor_Account_of_Degrees_of_Awareness">Fakezas &amp; Overgaard (2016)</a>.</p>
</li>
<li class="footnote" id="footnote345_m4a59r3"><a class="footnote-label" href="#footnoteref345_m4a59r3">345.</a> In this report, I rely most on my expectation that consciousness is fuzzy <em>between</em> individuals, in the sense the cognitive architectures of a set of individuals (say, across different species) might differ in a variety of ways, such that it is unclear which individuals we should consider to be “conscious” or “not conscious” (when they are reasonably developed, awake, etc.).<br /><br />
But I also suspect consciousness will turn out to be “fuzzy” <em>within</em> individuals, such that that if we considered a large set of time slices of an individual&#8217;s cognitive architecture, we would again struggle (in some cases) to decide which states were “conscious” or “not conscious,” even assuming we had a completed theory of consciousness and perfect information about each time slice. This view is similar to Dennett&#8217;s view that it is often indeterminate whether a given cognitive process was “conscious” or not (<a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">Dennett 1991</a>, chs 5-6; <a href="https://mitpress.mit.edu/books/sweet-dreams">Dennett 2005</a>, ch. 4), and seems to imply a rejection of what Dennett calls the “Cartesian theater.”<br /><br />
Despite this, I do not avoid talking about cognitive processes being “conscious” or “not conscious,” or “entering consciousness” or “exiting consciousness,” just as I do not avoid talking about “which beings are conscious” or “unconscious animals.” I continue to use the common and arguably naive phrasings simply because it is very difficult to communicate about consciousness without them. Perhaps with practice, or when we understand consciousness better than we do now, I will find it easier to write about consciousness in a more precise way that explicitly acknowledges the fuzziness of the concept.
</li>
<li class="footnote" id="footnote346_13of3xf"><a class="footnote-label" href="#footnoteref346_13of3xf">346.</a> The other two major options I considered were:
<ul><li>Rather than investigating the likely distribution of “consciousness,” I could have investigated the likely distribution of better-characterized phenomena instead, such as neural nociception, goal-directed behavior, bodily self-awareness, long-term memory, or some <em>collection</em> of such attributes. But, since I don&#8217;t whether any collection of these things is sufficient for phenomenal consciousness of any sort, I might not have learned much, from such an investigation, about which beings are conscious, or about which beings I would judge to be moral patients due to their consciousness.</li>
<li>I could have characterized in some detail the varieties of “consciousness” I do and don&#8217;t intuitively morally value, and then investigated the likely distribution of <em>those</em> roughly-sketched varieties of consciousness. But at that level of detail, my moral intuitions might have been fairly uncommon, and thus my findings might not have been of much value to others, including key decision-makers at the Open Philanthropy Project. Moreover, since the thing I (most confidently) intuitively value is still “subjective experience” of a certain sort, and since I don&#8217;t know which physical processes are sufficient for subjective experience of that sort, it would have remained difficult to know what I was looking for in the animal kingdom and other taxa, even after using my moral intuitions to identify the types of “consciousness” I&#8217;d be looking for. Also, extracting evidence about the distribution of those types of consciousness from the relevant scientific literature would have required extra work, since there is no literature explicitly investigating “evidence of types of consciousness Luke Muehlhauser intuitively morally values,” but there <em>are</em> substantial literatures explicitly investigating the likely distribution of phenomenal consciousness (defined roughly as <a href="#Defined">above</a>).</li>
</ul></li>
<li class="footnote" id="footnote347_ff29zuk"><a class="footnote-label" href="#footnoteref347_ff29zuk">347.</a> I suspect this is true for other writers on consciousness as well, and in many cases I think I would understand their reasoning better if I knew more about their moral intuitions, even if their reasoning doesn&#8217;t <em>explicitly</em> appeal to their moral intuitions.
</li>
<li class="footnote" id="footnote348_ykk0qf3"><a class="footnote-label" href="#footnoteref348_ykk0qf3">348.</a> See also <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1475-4975.1988.tb00171.x/abstract">Unger (1988)</a>.
</li>
<li class="footnote" id="footnote349_oba26bu"><a class="footnote-label" href="#footnoteref349_oba26bu">349.</a> Arguably, Tye&#8217;s <a href="#FirstOrder">PANIC theory</a> doesn&#8217;t imply that dorsal stream processing should be conscious, for dorsal stream processing arguably impacts behavior without impacting “beliefs” or “desires,” but one can easily imagine a very similar first-order theory which <em>does</em> imply that dorsal stream processing should be conscious.
</li>
<li class="footnote" id="footnote350_4snsl3z"><a class="footnote-label" href="#footnoteref350_4snsl3z">350.</a> Two example responses that are unpersuasive to me are summarized by <a href="http://plato.stanford.edu/entries/consciousness-higher/">Carruthers (2016)</a>:<br /><blockquote><p>What options does a first-order theorist have to resist this conclusion? One is to deny that the data are as problematic as they appear (as does Dretske 1995). It can be said that the unconscious states in question lack the kind of fineness of grain and richness of content necessary to count as genuinely perceptual states. On this view, the contrast discussed above isn&#8217;t really a difference between conscious and unconscious perceptions, but rather between conscious perceptions, on the one hand, and unconscious belief-like states, on the other. Another option is to accept the distinction between conscious and unconscious perceptions, and then to explain that distinction in first-order terms. It might be said, for example, that conscious perceptions are those that are available to belief and thought, whereas unconscious ones are those that are available to guide movement (Kirk 1994).</p></blockquote>
<p>Another reply I found unpersuasive, this time responding specifically to the proposed counterexample of blindsight, is given by <a href="https://mitpress.mit.edu/books/consciousness-color-and-content">Tye (2000)</a>, p. 63:</p>
<blockquote><p>If their reports are to be taken at face value, blindsight subjects… have no phenomenal consciousness in the blind region. What is missing, on the PANIC theory, is the presence of appropriately poised, nonconceptual, representational states. There are nonconceptual states, no doubt representationally impoverished, that make a cognitive difference in blindsight subjects. For some information from the blind field does reach the cognitive centers and controls their guessing behavior. But there is no complete, unified representation of the visual field, the content of which is poised to make a direct difference in beliefs. Blindsight subjects do not believe their guesses. The cognitive processes at play in these subjects are not belief-forming at all.</p></blockquote>
</li>
<li class="footnote" id="footnote351_do6itxh"><a class="footnote-label" href="#footnoteref351_do6itxh">351.</a> One other reply that may have some merit, but which I don&#8217;t discuss here, is given by <a href="http://faculty.philosophy.umd.edu/pcarruthers/Carruthers%20recants.pdf">Carruthers (2017)</a>:<br /><blockquote><p>So the questions posed to first-order theories were these: Why should nonconceptual contents have <em>feel</em> or be <em>like</em> something to undergo when available to central thought processes, while lacking such properties otherwise? How can these differences in functional role confer on one set of contents a distinctive subjective dimension that the other set lacks? In short: how does <em>role</em> (specifically, global broadcasting) create phenomenal character?</p>
<p>The seeming-unanswerability of these questions motivated Carruthers to propose his dual-content theory, according to which one effect of global broadcasting is to make first-order nonconceptual contents available to a higher-order mentalizing or “mindreading” faculty capable of entertaining higher-order thoughts about those experiences. This, when combined with the truth of some or other kind of consumer semantics, was said to add a dimension of higher-order nonconceptual content to the first-order experiences in question. Every globally broadcast experience is then <em>both</em> a nonconceptual representation of the world or body (<em>red</em>, say) and a nonconceptual representation of that experience of the world or body (<em>seeming red</em>, or <em>experiencing red</em>, say). Globally broadcast experiences are thus not just world-presenting but also self-presenting. They thereby acquire a subjective dimension and become <em>like</em> something to undergo. Moreover, <em>only</em> globally broadcast experiences have this sort of dual content, on the assumption that only such experiences are available to the mindreading faculty. Hence the conscious / unconscious distinction can genuinely be explained, it was claimed.</p>
<p>…a first-order theorist needs to say <em>something</em> about why nonconceptual contents should be phenomenally conscious if globally broadcast, but not otherwise. It is obviously true (almost by definition) that global broadcasting renders nonconceptual content <em>access</em>-conscious. But what is it about global broadcasting that renders nonconceptual content <em>phenomenally</em> conscious? Even if one seeks to deny that these concepts pick out distinct properties, something needs to be said to explain why <em>what-it-is-likeness</em> and other properties distinctive of phenomenal consciousness should only co-occur with global broadcasting.</p>
<p>The way forward for first-order theorists, I suggest, is to co-opt the operationalization of phenomenal consciousness first proposed by Carruthers &amp; Veillet (2011)… [namely] that phenomenal consciousness can be operationalized as <em>whatever gives rise to the “hard problems” of consciousness</em>… That is, a given type of content can qualify as phenomenally conscious if and only if it seems ineffable, one can seemingly imagine zombie characters who lack it, one can imagine what-Mary-didn&#8217;t-know scenarios for it, and so on. For the very notion of phenomenal consciousness seems constitutively tied to these issues. If there is a kind of state or a kind of content for which none of these problems arise, then what would be the point of describing it as <em>phenomenally</em> conscious nonetheless? And conversely, if there is a novel type of content not previously considered in this context for which hard-problem thought-experiments can readily be generated, then that would surely be sufficient to qualify it as phenomenally conscious.</p>
<p>Once phenomenal consciousness is operationalized as whatever gives rise to hard-problem thought-experiments, however, it should be obvious that the initial challenge to first-order representationalism collapses. The reason why nonconceptual contents made available to central thought processes are phenomenally conscious, whereas those that are not so available are not, is simply that without thought one cannot have a thought-experiment. Only those nonconceptual contents available [via global broadcasting] to central thought are ones that will seem to slip through one&#8217;s fingers when one attempts to describe them (that is, be ineffable), only they can give rise to inversion and zombie thought-experiments, and so on. This is because those thought-experiments depend on a distinctively first-personal way of thinking of the experiences in question. This is possible if the experiences thought about are themselves available to the systems that generate and entertain such thoughts, but not otherwise. Experiences that are used for online guidance of action, for example, cannot give rise to zombie thought-experiments for the simple reason that they are not available for us to think about in a first-person way, as <em>this experience</em> or something of the sort. They can only be thought about third-personally, as <em>the experience that guides my hand when I grasp the cup</em>, or whatever.</p>
<p>There is simply no need, then, to propose that dual higher-order / first-order nonconceptual contents are necessary in order for globally broadcast experiences to acquire a subjective dimension and be <em>like</em> something to undergo.</p></blockquote>
<p>One reason I don&#8217;t (yet) find this line of reasoning compelling is that I&#8217;m not sure it makes sense to operationalize phenomenal consciousness as “whatever gives rise to the ‘hard problems’ of consciousness.”</p>
</li>
<li class="footnote" id="footnote352_jir8s47"><a class="footnote-label" href="#footnoteref352_jir8s47">352.</a> Of course, I don&#8217;t think phenomenology can be dissociated from <em>function</em>, as <a href="http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(11)00125-2">Cohen &amp; Dennett (2011)</a> worry about. I&#8217;m a functionalist, after all. But I don&#8217;t see why qualia couldn&#8217;t be dissociated from the representations, verbal reports, etc. that I call “me,” and that “I” have introspective access to.
</li>
<li class="footnote" id="footnote353_a9zoa1g"><a class="footnote-label" href="#footnoteref353_a9zoa1g">353.</a> In addition to the sources discussed below, see also <a href="https://mitpress.mit.edu/books/unity-self">White (1991)</a>, ch. 6; <a href="https://global.oup.com/academic/product/thinking-about-consciousness-9780199243822?cc=us&amp;lang=en&amp;">Papineau (2002)</a>, section 7.14; <a href="http://www.brain-mind-institute.org/ICBM-2012/proceedings-html/full%20paper/paper%2016.pdf">Munevar (2012)</a>; <a href="https://scholar.google.com/scholar?cluster=5116687434480526515&amp;hl=en&amp;as_sdt=1,48">Schechter (2014)</a>.
</li>
<li class="footnote" id="footnote354_623ykfh"><a class="footnote-label" href="#footnoteref354_623ykfh">354.</a> I should also mention that Block&#8217;s arguments might even undermine the case for being as complex as <em>first-order theorists</em> suggest it is.
</li>
<li class="footnote" id="footnote355_w871m39"><a class="footnote-label" href="#footnoteref355_w871m39">355.</a> This quote from <a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>.
</li>
<li class="footnote" id="footnote356_8ofgxye"><a class="footnote-label" href="#footnoteref356_8ofgxye">356.</a> <a href="http://www.ingentaconnect.com/contentone/imp/jcs/2016/00000023/F0020011/art00012">Marsinek &amp; Gazzaniga (2016)</a>.
</li>
<li class="footnote" id="footnote357_gragp1w"><a class="footnote-label" href="#footnoteref357_gragp1w">357.</a> See <a href="http://brainsciencepodcast.com/bsp/2015/bsp117-gazzaniga">episode 117</a> of the <em>Brain Science Podcast with Ginger Campbell</em>.
</li>
<li class="footnote" id="footnote358_pp4khu0"><a class="footnote-label" href="#footnoteref358_pp4khu0">358.</a> See also <a href="http://www.openphilanthropy.org/sites/default/files/Derek_Shiller_01-24-17_%28public%29.pdf">notes from my conversation with Derek Shiller</a>.
</li>
<li class="footnote" id="footnote359_r1geyc5"><a class="footnote-label" href="#footnoteref359_r1geyc5">359.</a> See also <a href="/sites/default/files/Aaron_Sloman_07-03-16_%28public%29.pdf">notes from my conversation with Aaron Sloman</a>. For a contrary follow-up on the experiments by Pitts et al., see <a href="http://www.sciencedirect.com/science/article/pii/S030645221500370X">Rutiku et al. (2015)</a>.
</li>
<li class="footnote" id="footnote360_a648j0e"><a class="footnote-label" href="#footnoteref360_a648j0e">360.</a> For a shorter explanation of Dennett (1991)’s positive theory of consciousness, see ch. 3 of <a href="http://www.bloomsbury.com/us/daniel-dennett-9781847060082/">Thompson (2009)</a>.
</li>
<li class="footnote" id="footnote361_8yidfik"><a class="footnote-label" href="#footnoteref361_8yidfik">361.</a> Here is what <a href="https://global.oup.com/academic/product/the-conscious-brain-9780195314595?cc=us&amp;lang=en&amp;">Prinz (2012)</a>, pp. 342-343, says about the distribution question:<br /><blockquote><p>…there are ways in which the AIR theory can be applied to answer perennial questions about which creatures are conscious. Are human infants conscious? Are nonhuman animals conscious? It is difficult to answer these questions on the basis of behavior alone. Attention is difficult to distinguish from orienting, and working memory is difficult to distinguish from what ethologists call reference memory, a longer-term storage of objects and their locations (Green and Stanton, 1989). To search for consciousness in infants and animals, it would help to look for the neural signatures of consciousness. In infant brains, connections between areas are underdeveloped (Homae et al., 2010), and this may limit the capacity for allocating attention. The mammalian brain is similar across species, and we do know that gamma activity can be found in rodents (e.g., Vianney-Rodrigues, Iancu, and Welsh, 2011). But there are also differences across mammalian brains. At a cellular level, human visual streams are even subtly different from those of great apes and monkeys (Preuss and Coleman, 2002). We don’t yet know whether these differences have functional implications or implications for consciousness, but given the large numbers of similarities, it is likely that primates and perhaps all mammals are conscious. Many of the experiments cited in developing the AIR theory were performed on mammals, and the extraordinary lessons we have learned from that research may ultimately bear on the ethics of its continuation. What about birds? Their brains are built out of structures that are related to the mammalian subcortex, but these structures have evolved to function like the mammalian neocortex, resulting in working-memory and attention capacities that look surprisingly similar to our own (Güntürkün, 2005; Kirsch et al., 2008; Milmine, Rose, and Colombo, 2008). Cephalopods engage in strategic hunting behavior, and that may require attending to and briefly storing information about the spatial locations of their prey. The brain mechanisms of octopus working memory have been explored (Shomrat et al., 2008). There has also been work on the neural mechanisms of short-term memory in crabs (Tomsic, Berón de Astrada, and Sztarker, 2003). Pushing things even farther, there are studies of attention in fruit flies, and their capacity to attend is linked to genes that allow for short-term memory (van Swinderen, 2007). Obviously, there is also great variation between us and these other creatures. Given the doubts raised about multiple realizability in chapter 9, it wouldn’t be surprising to discover that only mammals have what it takes to be conscious on the AIR theory. But there is also astonishing continuity across animal phyla, so it is important to investigate the extent of similarity with respect to the mechanisms of consciousness. Then we can decide whether we can eat octopuses with impunity.</p></blockquote>
</li>
<li class="footnote" id="footnote362_u08bp1m"><a class="footnote-label" href="#footnoteref362_u08bp1m">362.</a> Chapter 5 is “Hemispheric interactions and specializations: insights from the split brain” (pp. 103-120) by Margaret Funnell, Paul Corballis, and Michael Gazzaniga.
</li>
<li class="footnote" id="footnote363_xklfbsj"><a class="footnote-label" href="#footnoteref363_xklfbsj">363.</a> On the potential implications of such “mindmelding” for arguments about the privacy of consciousness, see <a href="https://global.oup.com/academic/product/mindmelding-9780199231904?cc=us&amp;lang=en&amp;">Hirstein (2012)</a>.
</li>
<li class="footnote" id="footnote364_zwd1q8x"><a class="footnote-label" href="#footnoteref364_zwd1q8x">364.</a> Locked-in syndrome is included in this table because even though people cannot give verbal report while they are locked in, there are cases in which people have provided verbal report of their experiences during locked-in syndrome after recovering out of a locked-in state.
</li>
<li class="footnote" id="footnote365_173yta2"><a class="footnote-label" href="#footnoteref365_173yta2">365.</a> If terminal lucidity is genuine phenomenon, it might not be particularly interesting as a variation in phenomenal experience, but it might provide a (fleeting) opportunity to ask the patient what their subjective experience of less-lucid states of consciousness were like for them. Of course, less-fleeting opportunities are also available in cases of normal recovery from disorders of consciousness and other varieties of conscious experience, and these cases are far more numerous.
</li>
<li class="footnote" id="footnote366_zjcki0m"><a class="footnote-label" href="#footnoteref366_zjcki0m">366.</a> I asked Merker via email, in August 2016, whether he thought there are likely to be any cases of hydranencephalics who are able to describe their subjective experiences. He said that such cases “are not likely to exist as far as I am able to tell. These children – of any age – are utterly without anything that may be called human language. When a minority of [caregivers] answer [a survey question about word use] in the affirmative, they are referring to things like one or a few sounds that the child uses less than randomly, say ‘ba’ when the mother is present. These may be associatively learned vocalizations, reinforced by delighted caregiver reactions, but do not amount to anything even close to propositional language. The cognitive state of these children is one of profound dementia…”
</li>
<li class="footnote" id="footnote367_yzzkabp"><a class="footnote-label" href="#footnoteref367_yzzkabp">367.</a> Quoted from <a href="https://www.quartoknows.com/books/9781937994433/The-Complete-Tales-Poems-of-Edgar-Allan-Poe.html">Poe (2014)</a>, pp. 369-370. I borrow the Poe example from Eliezer Yudkowsky.
</li>
<li class="footnote" id="footnote368_ct13f5r"><a class="footnote-label" href="#footnoteref368_ct13f5r">368.</a> My own replies to these arguments are similar to those made by e.g. <a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">Dennett (1991)</a>; <a href="http://link.springer.com/article/10.1007/s11245-014-9266-3">Carruthers &amp; Schier (2014)</a>.
</li>
<li class="footnote" id="footnote369_w2fmx8a"><a class="footnote-label" href="#footnoteref369_w2fmx8a">369.</a> <a href="http://www.penguinrandomhouse.com/books/313935/anxious-by-joseph-ledoux/9780143109044/">LeDoux (2015)</a>, ch. 2, provides one example:<br /><blockquote><p>In June 2014, a psychology website&#8217;s headline read: “Fear Center in Brain Larger Among Anxious Kids.” The story that followed described a study that measured the level of anxiety in a large group of children based on a questionnaire answered by their parents. The brains of these children were then imaged and the findings related to the parents’ assessments. The results showed that the larger the amygdala of the child, the higher the level of anxiety rated by the parents. Let&#8217;s consider what this actually means. In this study the parents did what animal researchers often do: They based a conclusion about anxiety, an inner feeling, on observations of behavior — their child seemed nervous, edgy, or had trouble concentrating or sleeping. Thus, although the size of the amygdala might well correlate with certain behaviors, whether it was related to feelings of anxiety was not tested. The website&#8217;s headline was inaccurate in three respects: (1) What was being measured was behavioral activity, not the feeling of anxiety; (2) the kids were not anxious in the clinical sense, in spite of some being described as “anxious” in the story; and (3) the amygdala is not the fear center (and certainly not the anxiety center) if by fear or anxiety we mean a conscious feeling.</p></blockquote>
</li>
<li class="footnote" id="footnote370_h838g5h"><a class="footnote-label" href="#footnoteref370_h838g5h">370.</a> My usage is inspired by the proposal in <a href="http://www.penguinrandomhouse.com/books/313935/anxious-by-joseph-ledoux/9780143109044/">LeDoux (2015)</a>, ch. 2:<br /><blockquote><p>When scientifically discussing fear and anxiety, we should let the words “fear” and “anxiety” have their everyday meaning — namely, as descriptions of conscious experiences that people have when threatened by present or anticipated events. The scientific meaning will obviously go deeper and be more complex than the lay meaning, but both will refer to the same fundamental concept. In addition, we should avoid using these words that refer to conscious feeling when discussing systems that nonconsciously detect threats and control defense responses to them.</p>
<p>Thus, rather than saying that fear stimuli activate a fear system to produce fear responses, we should state that <em>threat stimuli</em> elicit <em>defense responses</em> via activation of a <em>defensive system</em>. Because “threat” and “defense” are not terms derived specifically from human subjective experiences, using them would go a long way toward making it easier to distinguish brain mechanisms underlying the conscious feeling of being afraid or anxious from mechanisms that detect and respond to actual or perceived danger. Similarly, what we now call fear conditioning can simply be called what it is: threat conditioning. So, in place of “fear CSs” and “fear CRs,” we can refer instead to “threat CSs” and “defensive CRs”…</p></blockquote>
</li>
<li class="footnote" id="footnote371_p4tdsjo"><a class="footnote-label" href="#footnoteref371_p4tdsjo">371.</a> For evidence and argument on the topic of unconscious emotions, see <a href="http://www.penguinrandomhouse.com/books/313935/anxious-by-joseph-ledoux/9780143109044/">LeDoux (2015)</a>; <a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP002470.html">Keltner et al. (2013)</a>, ch. 7; <a href="http://www.jneurosci.org/content/30/30/10039.short">Amting et al. (2010)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0065345414000023">Dawkins (2015)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/faf.12010/full">Rose et al. (2014)</a>; <a href="http://www.guilford.com/books/Emotion-and-Consciousness/Barrett-Niedenthal-Winkielman/9781593854584">Barrett et al. (2005)</a>; <a href="https://books.google.com/books?id=bVhMJvRsEvEC&amp;lpg=PP1&amp;pg=PA210#v=onepage&amp;q&amp;f=false">Hofree &amp; Winkielman (2012)</a>; <a href="http://www.guilford.com/books/The-Rise-Consciousness-Development-Emotional-Life/Michael-Lewis/9781462512522">Lewis (2013)</a>; <a href="https://global.oup.com/academic/product/emotion-and-decision-making-explained-9780199659890?cc=us&amp;lang=en&amp;">Rolls (2013)</a>, ch. 10; <a href="http://www.tandfonline.com/doi/abs/10.1080/02699930302289">Berridge &amp; Winkielman (2003)</a>; <a href="http://cdp.sagepub.com/content/13/3/120.short">Winkielman &amp; Berridge (2004)</a>; <a href="https://books.google.com/books?id=cbKhDAAAQBAJ&amp;lpg=PP1&amp;ots=5CTpqw1oud&amp;lr&amp;pg=PA133#v=onepage&amp;q&amp;f=false">Berridge &amp; Kringelbach (2016)</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0278262606001266">Sato &amp; Aoki (2006)</a>; <a href="http://www.nature.com/nrn/journal/v11/n10/abs/nrn2889.html">Tamietto &amp; de Gelder (2010)</a>; <a href="http://analysis.oxfordjournals.org/content/67/4/292.short">Hatzimoysis (2007)</a>; <a href="http://psycnet.apa.org/psycinfo/2015-55789-001/">Anselme &amp; Robinson (2016)</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/jzo.12434/full">Dawkins (2017)</a>.
</li>
<li class="footnote" id="footnote372_tk6w6un"><a class="footnote-label" href="#footnoteref372_tk6w6un">372.</a> [TODO: add link to conversation notes when they are published]
</li>
<li class="footnote" id="footnote373_bo7xgba"><a class="footnote-label" href="#footnoteref373_bo7xgba">373.</a> For a “network theory of well-being” that integrates both subjective and objective factors, see <a href="https://global.oup.com/academic/product/the-good-life-9780199923113?cc=us&amp;lang=en&amp;">Bishop (2015)</a>. <a href="http://www.tandfonline.com/doi/full/10.1080/00048402.2015.1014926">Lin (2015)</a> proposes a <em>subjective</em> list theory of well-being. See <a href="http://link.springer.com/chapter/10.1007/978-3-319-21365-1_24">Daswani &amp; Leike (2015)</a> for a suggested formal definition of well-being for reinforcement learning agents, and see <a href="http://link.springer.com/article/10.1007/s11229-015-0883-1">Oesterheld (2016)</a> for an example preliminary formalization of preference satisfaction.
</li>
<li class="footnote" id="footnote374_59ab739"><a class="footnote-label" href="#footnoteref374_59ab739">374.</a> See e.g. the discussion of intensity and other factors in <a href="https://www.academia.edu/29703924/A_Multi-Factor_Account_of_Degrees_of_Awareness">Fakezas &amp; Overgaard (2016)</a>.
</li>
<li class="footnote" id="footnote375_dwgsp2x"><a class="footnote-label" href="#footnoteref375_dwgsp2x">375.</a> See e.g. <a href="http://reducing-suffering.org/small-animals-clock-speed/">Tomasik (2016a)</a>, <a href="https://books.google.com/books?id=BKn6AwAAQBAJ&amp;lpg=PA149&amp;ots=CSG3nd9RfI&amp;lr&amp;pg=PA149#v=onepage&amp;q&amp;f=false">Lee (2014)</a>, <a href="http://onlinelibrary.wiley.com/doi/10.1111/phc3.12107/full">Phillips (2014)</a>, and the following points from <a href="https://global.oup.com/academic/product/the-age-of-em-9780198754626?cc=us&amp;lang=en&amp;">Hanson (2016)</a>, ch. 6, about processing speed and body size:<br /><blockquote><p>The natural oscillation periods of most consciously controllable human body parts are greater than a tenth of a second. Because of this, the human brain has been designed with a matching reaction time of roughly a tenth of a second. As it costs more to have faster reaction times, there is little point in paying to react much faster than body parts can change position.</p>
<p>…the first resonant period of a bending cantilever, that is, a stick fixed at one end, is proportional to its length, at least if the stick&#8217;s thickness scales with its length. For example, sticks twice as long take twice as much time to complete each oscillation. Body size and reaction time are predictably related for animals today… [<a href="http://www.sciencedirect.com/science/article/pii/S0003347213003060">Healy et al. (2013)</a>]</p></blockquote>
</li>
<li class="footnote" id="footnote376_9absl8e"><a class="footnote-label" href="#footnoteref376_9absl8e">376.</a> Here is Bayne on subject unity:<br /><blockquote><p>My conscious states possess a certain kind of unity insofar as they are all mine; likewise, your conscious states possess that same kind of unity insofar as they are all yours. We can describe conscious states that are had by or belong to the same subject of experience as subject unified. Within subject unity we need to distinguish the unity provided by the subject of experience across time (diachronic unity) from that provided by the subject at a time (synchronic unity).</p></blockquote>
<p>On representational unity, Bayne says:</p>
<blockquote><p>Let us say that conscious states are representationally unified to the degree that their contents are integrated with each other. Representational unity comes in a variety of forms. A particularly important form of representational unity concerns the integration of the contents of consciousness around perceptual objects—what we might call ‘object unity’. Perceptual features are not normally represented by isolated states of consciousness but are bound together in the form of integrated perceptual objects. This process is known as feature-binding. Feature-binding occurs not only within modalities but also between them, for we enjoy multimodal representations of perceptual objects.</p></blockquote>
<p>And on phenomenal unity, he says:</p>
<blockquote><p>Subject unity and representational unity capture important aspects of the unity of consciousness, but they don&#8217;t get to the heart of the matter. Consider again what it&#8217;s like to hear a rumba playing on the stereo whilst seeing a bartender mix a mojito. These two experiences might be subject unified insofar as they are both yours. They might also be representationally unified, for one might hear the rumba as coming from behind the bartender. But over and above these unities is a deeper and more primitive unity: the fact that these two experiences possess a conjoint experiential character. There is something it is like to hear the rumba, there is something it is like to see the bartender work, and there is something it is like to hear the rumba while seeing the bartender work. Any description of one&#8217;s overall state of consciousness that omitted the fact that these experiences are had together as components, parts, or elements of a single conscious state would be incomplete. Let us call this kind of unity — sometimes dubbed ‘co-consciousness’ — phenomenal unity.</p>
<p>Phenomenal unity is often in the background in discussions of the ‘stream’ or ‘field’ of consciousness. The stream metaphor is perhaps most naturally associated with the flow of consciousness — its unity through time — whereas the field metaphor more accurately captures the structure of consciousness at a time. We can say that what it is for a pair of experiences to occur within a single phenomenal field just is for them to enjoy a conjoint phenomenality — for there to be something it is like for the subject in question not only to have both experiences but to have them together. By contrast, simultaneous experiences that occur within distinct phenomenal fields do not share a conjoint phenomenal character.</p></blockquote>
</li>
<li class="footnote" id="footnote377_grsi8o6"><a class="footnote-label" href="#footnoteref377_grsi8o6">377.</a> In a 2004 interview conducted by Keith Frankish (<a href="http://www.open.edu/openlearn/history-the-arts/culture/philosophy/thought-and-experience?track=14">audio</a>; <a href="http://media-podcast.open.ac.uk/feeds/aa308-thought-experience/transcript/aa308thought13.pdf">transcript</a>), Dennett said:<br /><blockquote><p>Animals are of course awake, they can feel pain, and they can experience pleasure, but they can&#8217;t, I think, …dwell on things the way we can. They can&#8217;t shift their attention the way we can. They can&#8217;t reflect on things the way we can. That sort of recursive, reflective mulling over and letting one thing remind you of something else and so forth, and being able to control that to some degree — that&#8217;s what animals (I think) can&#8217;t do: not chimpanzees, not dolphins, not dogs… their consciousness is so disunified, so fragmented, so impoverished compared to ours, that to call them conscious is almost certainly to misimagine their circumstances. </p>
<p>…I like to ask people, “What is it like to be a brace of oxen, a pair of oxen yoked together?” And they say “Well, it&#8217;s not like anything, of course. I mean it&#8217;s like something to be one ox, and it&#8217;s like something to be the other ox — the left and the one on the right — but it&#8217;s not like anything to be a brace of oxen, because they aren&#8217;t unified in the right way.” Well, but you&#8217;d be amazed [at] the extent of which many animals are like a brace of oxen, [at] how much disunity is possible in a mammalian nervous system. It&#8217;s this further unification which is the fruits of the Joycean machine [Dennett&#8217;s term for the “virtual machine” that he thinks enables human consciousness; see <a href="https://www.google.com/search?tbs=bks:1&amp;q=isbn:9780713990379">Dennett (1991)</a>] that gets installed on us.</p>
<p>What is it like to be an ant colony? Well, it&#8217;s not like anything to be an ant colony, even if it&#8217;s like something to be an individual ant, so people think. Well, stop and think. A brain is composed of billions of neurons, each one of those is a lot stupider than an ant. They happen to be enclosed in a skull and their inter-communications are rich but of the same sort that is possible between one ant and another. Now if we opened up somebody&#8217;s head and we found inside, not neurons but millions of little ants, maybe we would say, “Oh gosh, maybe it&#8217;s not like anything to be this person.” Well, an ant colony can exhibit a lot of the same unified behaviour, a lot of the same protracted projects… that an organism inside a skin can exhibit. </p>
<p>Now if you think it&#8217;s pretty obvious that an ant colony is not something that is itself conscious… then you should be at least willing to entertain the hypothesis that a bird is just as unconscious as an ant colony is. Now I&#8217;m deliberately setting the bar high, forcing the burden of proof onto those who say, it&#8217;s just obvious that, say, other mammals (at least) are conscious the way we are. I say, “No, it&#8217;s not obvious, prove it.” And the more we learn about specific organisms — that&#8217;s why you have to do the science — the more we find out that a lot of things that are obvious to philosophers in the armchair are just false. </p>
<p>What is it like to be a rabbit? Well you may think that it&#8217;s obvious that rabbits have an inner life that&#8217;s something like ours. Well it turns out that if you put a patch over a rabbit&#8217;s left eye and train it in a particular circumstance to be (say) afraid of something, and then you move the patch to the right eye, so that the very same signal, the very same circumstance that it has been trained to be afraid of, now is coming in the other eye, you have a naive rabbit, because in the rabbit brain the connections that are standard in our brains just aren&#8217;t there, there isn&#8217;t that unification. What is it like to be <em>which</em> rabbit? The rabbit on the left, or the rabbit on the right? The disunity in a rabbit&#8217;s brain is stunning when you think about it, and you just haven&#8217;t tested many species to see just how disunified they can be. The answer is they can be quite disunified.</p></blockquote>
<p>See also his earlier <a href="http://www.jstor.org/stable/40971115">Dennett (1995)</a>.<br /><br />
I asked Dennett which rabbit study he was referring to in the quote above, and he pointed me to a research abstract that I have not read: Ian Steele-Russell&#8217;s “The absence of interhemispheric communication in the rabbit” in the Society for Neuroscience&#8217;s <em>Abstracts</em>, <a href="http://www.worldcat.org/title/abstracts-24th-annual-meeting-miami-beach-florida-november-13-18-1994/oclc/35534066&amp;referer=brief_results">Volume 20</a> (1994): 414.11. (This abstract is in Part 2 of Volume 20; Part 1 runs through 383.20).<br /><br />
This type of study is known as an “interocular transfer” study. For reviews of such studies — and other studies of interhemispheric transfer — in a variety of species, see e.g. <a href="https://books.google.com/books?id=bbZqAAAAMAAJ&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjB9va9l-DQAhVN5GMKHSmbCrgQ6AEIHDAA">Steele-Russell et al. (1979)</a>.<br /><br />
Here is but one bizarre-seeming result from this literature, reported in <a href="http://psycnet.apa.org/psycinfo/1994-97122-019">Remy &amp; Watanabe (1993)</a>:</p>
<blockquote><p>…an ubiquitous and particularly striking result in many [interocular transfer] studies [in birds] is the “mirror image reversal” effect (MIR). Thus, when pigeons were trained monocularly on a mirror-image discrimination (e.g., 45° vs. 135° oblique lines) and then exposed to the stimuli with the untrained eye, they preferred the previously negative… stimulus [<a href="http://science.sciencemag.org/content/148/3667/252">Mello (1965)</a>].</p></blockquote>
</li>
<li class="footnote" id="footnote378_dp78o80"><a class="footnote-label" href="#footnoteref378_dp78o80">378.</a> On “researcher degrees of freedom,” see <a href="http://journals.sagepub.com/doi/abs/10.1177/0956797611417632">Simmons et al. (2011)</a>, <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf">Gelman &amp; Loken 2013</a>, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2694998">Simonsohn et al. (2015)</a>, <a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01832/full">Wicherts et al. (2016)</a>, and FiveThirtyEight&#8217;s <a href="https://projects.fivethirtyeight.com/p-hacking/">p-hacking tool</a>.
</li>
<li class="footnote" id="footnote379_lz9ole9"><a class="footnote-label" href="#footnoteref379_lz9ole9">379.</a> E.g. <a href="http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124">Ioannidis 2005b</a>; <a href="http://journals.sagepub.com/doi/abs/10.1177/0956797611417632">Simmons et al. 2011</a>; <a href="https://elifesciences.org/content/5/e21451">Nissen et al. 2016</a>; <a href="http://www.sciencedirect.com/science/article/pii/S0164121215000679">Jørgensen et al. 2016</a>; <a href="http://rsos.royalsocietypublishing.org/content/3/9/160384">Smaldino &amp; McElreath 2016</a>.
</li>
<li class="footnote" id="footnote380_36u26xj"><a class="footnote-label" href="#footnoteref380_36u26xj">380.</a> My claim about “most of the situations for which it is used” is just a guess, based on my own experience reading or skimming hundreds of meta-analyses which use the DL algorithm, across many different fields in the life and social sciences.
</li>
<li class="footnote" id="footnote381_htnob7x"><a class="footnote-label" href="#footnoteref381_htnob7x">381.</a> See <a href="http://www.openphilanthropy.org/sites/default/files/Joel_Hektner_12-17-2015_%28public%29.pdf">notes from my conversation with Joel Hektner</a>.
</li>
<li class="footnote" id="footnote382_0lsc2oa"><a class="footnote-label" href="#footnoteref382_0lsc2oa">382.</a> This is likely a problem for many meta-analyses, not just primary studies (<a href="http://www.sciencedirect.com/science/article/pii/S0895435608000930">Bender et al. 2008</a>; <a href="http://www.bmj.com/content/343/bmj.d4829">Tendall et al. 2011</a>).
</li>
<li class="footnote" id="footnote383_gl0rb98"><a class="footnote-label" href="#footnoteref383_gl0rb98">383.</a> For a readable guide to many common statistical errors, see <a href="https://www.nostarch.com/statsdonewrong">Reinhart et al. (2015)</a>.
</li>
<li class="footnote" id="footnote384_nc7f9q7"><a class="footnote-label" href="#footnoteref384_nc7f9q7">384.</a> But also see <a href="http://journals.sagepub.com/doi/abs/10.1177/1948550615612150">Fiedler &amp; Schwarz (2015)</a>.
</li>
<li class="footnote" id="footnote385_4dfbxtp"><a class="footnote-label" href="#footnoteref385_4dfbxtp">385.</a> See e.g. <a href="http://www.collabra.org/articles/10.1525/collabra.13/">Vanpaemel et al. (2015)</a> for psychology, and <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2669564">Chang &amp; Li (2015)</a> for economics.
</li>
<li class="footnote" id="footnote386_zqcz3w2"><a class="footnote-label" href="#footnoteref386_zqcz3w2">386.</a> On why <a href="http://onlinelibrary.wiley.com/doi/10.1002/14651858.MR000012.pub3/abstract">Odgaard-Jensen et al. (2011)</a> and <a href="http://onlinelibrary.wiley.com/doi/10.1002/14651858.MR000034.pub2/full">Anglemyer et al. (2014)</a> may have gotten somewhat different results, see <a href="http://link.springer.com/referenceworkentry/10.1007/978-94-017-8706-2_45-1">Howick &amp; Mebius (2015)</a>.
</li>
</ul>        </div>
        </div>
</div>
     <div class="comment-links"></div>
  </div> <!-- /content -->
  
      <div class="links">
          </div> <!-- /links -->
  
  </article> <!-- /article #node -->  </div>
</div>
  </div>
              </div>
      </div>

      
      
    </section>

  </div>

      <footer id="footer">
      <div class="container">
            <div class="region region-footer">
    <div id="block-block-1" class="block block-block contextual-links-region">

    
  <div class="content">
    <ul class="menu"><li><a href="/get-involved/contact-us">contact us</a></li>
<li><a href="/get-involved/jobs">jobs</a></li>
<li><a href="/about/press-kit">press kit</a></li>
</ul><form action="http://openphilanthropy.us12.list-manage.com/subscribe/post?u=5f851555ed522f52a8cc7157f&amp;id=204e32798b" class="subscribe" method="post" target="_blank"><input id="txt-email" name="EMAIL" placeholder="Enter email to subscribe to updates" type="text" /><input title="Submit" type="submit" value="Submit" /></form>
<ul class="social"><li><a class="facebook" href="https://www.facebook.com/openphilanthropy" target="_blank">facebook</a></li>
<li><a class="twitter" href="https://twitter.com/open_phil" target="_blank">twitter</a></li>
<li><a class="rss" href="/blog/rss" target="_blank">rss</a></li>
</ul><div class="copyright">© Open Philanthropy Project. This work is licensed under a Creative Commons Attribution-Noncommercial-ShareAlike 3.0 United States License. <a href="/privacy-policy">Privacy policy</a></div>
  </div>
</div>
  </div>
      </div>
    </footer>
  
</div>
    <div class="region region-page-bottom">
    <div id="admin-menu" class="admin-menu-site-openphilanthropy-org"><div id="admin-menu-wrapper">
<ul id="admin-menu-icon" class="dropdown"><li class="admin-menu-icon admin-menu-toolbar-category expandable"><a href="/"><span>Home</span></a>
<ul class="dropdown"><li><a href="/admin/index">Index</a></li></ul></li></ul>
<ul id="admin-menu-menu" class="dropdown"><li class="admin-menu-toolbar-category expandable"><a href="/admin/content">Content</a>
<ul class="dropdown"><li class="expandable"><a href="/node/add">Add content</a>
<ul class="dropdown"><li><a href="/node/add/blog">Blog</a></li><li><a href="/node/add/focus-area">Focus Area</a></li><li><a href="/node/add/grant">Grant</a></li><li><a href="/node/add/home">Home</a></li><li><a href="/node/add/page">Page</a></li><li><a href="/node/add/webform">Webform</a></li></ul></li><li class="expandable"><a href="/admin/content/comment">Comments</a>
<ul class="dropdown"><li><a href="/admin/content/comment/approval">Unapproved comments (0)</a></li></ul></li></ul></li><li class="admin-menu-toolbar-category expandable"><a href="/admin/structure">Structure</a>
<ul class="dropdown"><li class="expandable"><a href="/admin/structure/menu">Menus</a>
<ul class="dropdown"><li><a href="/admin/structure/menu/add">Add menu</a></li><li class="expandable"><a href="/admin/structure/menu/manage/devel">Development</a>
<ul class="dropdown"><li><a href="/admin/structure/menu/manage/devel/add">Add link</a></li><li><a href="/admin/structure/menu/manage/devel/delete">Delete menu</a></li><li><a href="/admin/structure/menu/manage/devel/edit">Edit menu</a></li></ul></li><li class="expandable"><a href="/admin/structure/menu/manage/main-menu">Main menu</a>
<ul class="dropdown"><li><a href="/admin/structure/menu/manage/main-menu/add">Add link</a></li><li><a href="/admin/structure/menu/manage/main-menu/delete">Delete menu</a></li><li><a href="/admin/structure/menu/manage/main-menu/edit">Edit menu</a></li></ul></li><li class="expandable"><a href="/admin/structure/menu/manage/management">Management</a>
<ul class="dropdown"><li><a href="/admin/structure/menu/manage/management/add">Add link</a></li><li><a href="/admin/structure/menu/manage/management/delete">Delete menu</a></li><li><a href="/admin/structure/menu/manage/management/edit">Edit menu</a></li></ul></li><li class="expandable"><a href="/admin/structure/menu/manage/navigation">Navigation</a>
<ul class="dropdown"><li><a href="/admin/structure/menu/manage/navigation/add">Add link</a></li><li><a href="/admin/structure/menu/manage/navigation/delete">Delete menu</a></li><li><a href="/admin/structure/menu/manage/navigation/edit">Edit menu</a></li></ul></li><li class="expandable"><a href="/admin/structure/menu/manage/user-menu">User menu</a>
<ul class="dropdown"><li><a href="/admin/structure/menu/manage/user-menu/add">Add link</a></li><li><a href="/admin/structure/menu/manage/user-menu/delete">Delete menu</a></li><li><a href="/admin/structure/menu/manage/user-menu/edit">Edit menu</a></li></ul></li><li><a href="/admin/structure/menu/settings">Settings</a></li></ul></li><li class="expandable"><a href="/admin/structure/views">Views</a>
<ul class="dropdown"><li><a href="/admin/structure/views/add">Add new view</a></li><li><a href="/admin/structure/views/view/all_urls">All URLs (Content)</a></li><li><a href="/admin/structure/views/view/author_posts">Author Posts (Content)</a></li><li><a href="/admin/structure/views/view/blog">Blog archive (Content)</a></li><li><a href="/admin/structure/views/view/blog_archive_monthly">Blog Archive Monthly (Content)</a></li><li><a href="/admin/structure/views/view/blog_monthly_archive">Blog Monthly Archive Menu (Content)</a></li><li><a href="/admin/structure/views/view/conversations">Conversations (Content)</a></li><li><a href="/admin/structure/views/view/grants_db">Grants Database (Content)</a></li><li><a href="/admin/structure/views/view/grants_database">Grants Recent (giving page) (Content)</a></li><li><a href="/admin/structure/views/view/in_the_news">In the News (Content)</a></li><li><a href="/admin/structure/views/view/wp_blog_latest_posts">Recent blog posts (Content)</a></li><li><a href="/admin/structure/views/view/related_content_manual">Related Content (manual listing) (Content)</a></li><li><a href="/admin/structure/views/view/relevant_blog_posts">Relevant Blog Posts (Content)</a></li><li class="expandable"><a href="/admin/structure/views/settings">Settings</a>
<ul class="dropdown"><li><a href="/admin/structure/views/settings/advanced">Advanced</a></li></ul></li><li><a href="/admin/structure/views/view/timeline">Timeline (Content)</a></li></ul></li></ul></li><li class="admin-menu-toolbar-category expandable"><a href="/admin/config">Configuration</a>
<ul class="dropdown"><li class="expandable"><a href="/admin/config/user-interface">User interface</a>
<ul class="dropdown"><li class="expandable"><a href="/admin/config/user-interface/print">Printer, email and PDF versions</a>
<ul class="dropdown"><li class="expandable"><a href="/admin/config/user-interface/print/email">email</a>
<ul class="dropdown"><li><a href="/admin/config/user-interface/print/email/strings">Text strings</a></li></ul></li><li class="expandable"><a href="/admin/config/user-interface/print/common">Settings</a>
<ul class="dropdown"><li><a href="/admin/config/user-interface/print/common/strings">Text strings</a></li></ul></li></ul></li></ul></li></ul></li><li class="admin-menu-toolbar-category"><a href="/sitemap">Sitemap</a></li><li class="admin-menu-toolbar-category expandable"><a href="/node/2">Style Guide</a>
<ul class="dropdown"><li><a href="/node/115">Page Layouts</a></li></ul></li><li class="admin-menu-toolbar-category"><a href="/admin/help">Help</a></li></ul>
<ul id="admin-menu-account" class="dropdown"><li class="admin-menu-action"><a href="/user/logout">Log out</a></li><li class="admin-menu-action admin-menu-account"><a href="/about/team/luke-muehlhauser">Hello <strong>Luke Muehlhauser</strong></a></li></ul>
<ul id="admin-menu-users" class="dropdown"><li class="admin-menu-action admin-menu-users"><a href="/user">1 / 0</a></li></ul>
<ul id="admin-menu-search" class="dropdown"><li placeholder="Search" class="admin-menu-search"><div class="form-item form-type-textfield">
 <input placeholder="Search" class="admin-menu-search form-text" type="text" size="60" maxlength="128" />
</div>
</li></ul></div></div>  </div>
</body>
</html>
