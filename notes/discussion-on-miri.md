---
layout: post
date:   ""
title: "Conversation notes about MIRI"
---

Here is a discussion about MIRI I had with Daniel Filan and another person whose name I'm not including; let's call him Peter. Daniel is a PhD student under Stuart Russell at UC Berkeley. Peter has a PhD related to AGI.

OpenPhil recently published a critical review of MIRI’s research work, and didn’t give them as much money as they would have if the review had been more positive. I think that OpenPhil’s review is too critical, and I want to donate to MIRI because this is one of the few times I think I’m going to see a funding opportunity which OpenPhil won’t fill.

I want to talk about the following points.

1. The main points in OpenPhil’s criticism of MIRI’s research work are
  (i) MIRI has made relatively limited progress on the Agent Foundations research agenda so far, and (ii) this research agenda has little potential to decrease potential risks from advanced AI in comparison with other research directions that we would consider supporting. To what extent do I disagree with OpenPhil on this?
2. Does MIRI have good things it can do with money at the moment?

## Has MIRI made relatively limited progress on the Agent Foundations research agenda so far?


Peter disagrees with OpenPhil; he says they’ve made good progress on three of five of the areas that MIRI put in their agenda. Logic inductors and the grain of truth thing weren’t part of the evaluation, because the logical inductors thing wasn’t written up yet and the Grain of Truth paper was not main-authored by MIRI (MIRI came up with the core idea and Jan Leike worked it out, without MIRI it wouldn’t have happened).


Daniel thinks that if MIRI had sent out the logical inductors thing, the research review would have been more positive. (Peter says this is probably true, but it’s hard to say how much.) Daniel’s impression hanging around MIRI is that MIRI is more productive because of the logical inductors result. (Peter says this sounds believable.)


## Does the Agent Foundations research agenda have little potential to decrease these risks, compared to other research directions?

Re (ii), Peter says: It’s looking more and more plausible that the current wave of progress on deep learning is going to lead directly to AGI. This makes research directions more directly related to ML seem more relatively promising. (It’s worth noting that even compared to when MIRI started doing research as their main focus (early 2014), machine learning now looks more likely to lead to AGI.)

The MIRI Agent Foundations agenda is more useful if AI comes later rather than sooner.

They say they’ll devote half their staff time to the ML stuff next year.

Peter: MIRI is comparatively poorly equipped to do ML research because they don’t have experts, can’t get experts, don’t have infrastructure. So it makes sense if most of the ML safety research happens at industry institutions which are better equipped. But ML safety research is such a good idea that it’s worth doing a little from MIRI anyway.


Daniel: Concrete problems in AI safety is quite different from MIRI’s ML agenda. CPAS is more crisply defined and smaller. Jessica’s agenda is more about the “turning fuzzy ideas into math” thing, meaning that it ends up less concrete than Dario’s.


- Peter: this is partially because the CPAS problems are designed to make sense for household robots. Dario thinks 30-50% of the problem with AI safety is CPAS-style stuff. MIRI doesn’t think that.
- Peter says that another striking difference is that Dario’s is better done. He thinks that this is plausibly because MIRI is kind of bad at writing papers. (Daniel doesn’t think as much that MIRI papers are bad.)
  - Buck thinks that maybe part of the reason that Dario’s agenda has more citations in it than Jessica’s is that Dario’s is easier to produce citations for.


Is OpenPhil wrong to disvalue the Agent Foundations agenda? Peter says probably. He says “My impression was that OpenPhil didn't see how it connects to anything. I buy the connection, even though it's quite a long-term strategy. Someone should pursue this strategy (diversification is good)”


Daniel says Scott Garrabant thinks that MIRI might just finish Agent Foundations with 2 years (he thinks they might also not finish).


Peter says the limiting factor is moving insights from Agent Foundations into ML algorithms. He says that many people about DeepMind etc are actually pretty interested in MIRI’s work and would definitely try to


The fact that Daniel Dewey thought about this agenda for 100h and decided it was not useful is an update for Daniel Filan.


## Should we donate?


Peter says that he thinks MIRI is particularly valuable because no-one else is going to do agent foundations.

## How likely is it that OpenPhil increases funding when they reconsider the grant in a year or whatever?

Peter thinks that it’s somewhat likely. Daniel thinks it’s very likely.

Peter’s guess is that OpenPhil will maintain their funding. He says he thinks they’d increase their funding if MIRI had 2 more results as good as the logical inductors one.

Daniel says it’s more likely that OpenPhil will increase their opinion of the importance of Agent Foundations than decrease it. (Cause they don’t have much distance down.)

OpenPhil didn’t take the ML agenda into account at all. So if OpenPhil judges MIRI to have done nonzero ML safety work, that would increase how much OpenPhil wants to give.
